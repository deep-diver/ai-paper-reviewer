[{"figure_path": "https://arxiv.org/html/2412.14711/x1.png", "caption": "Figure 1: Compute flows of vanilla MoE with TopK routing and ReMoE with ReLU routing. Positive values are shown in orange, and negative values in blue, with deeper colors representing larger absolute values. Zeros, indicating sparsity and computation savings, are shown in white. The red dash arrows in TopK routing indicate discontinuous operations. Compared with TopK routing MoE, ReMoE uses ReLU to make the compute flow fully differentiable.", "description": "This figure illustrates the computational flow of both traditional Mixture-of-Experts (MoE) models with TopK routing and the proposed ReMoE model with ReLU routing.  The color intensity represents the magnitude of the values, with orange for positive values and blue for negative values.  White indicates a zero value, highlighting the sparsity achieved through the selective activation of experts in both architectures. The discontinuous nature of the TopK routing, indicated by dashed red arrows, is a key difference compared to the smooth, continuous computation flow enabled by the ReLU routing in ReMoE, making the training process more efficient.", "section": "Our Method: ReMoE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x2.png", "caption": "Figure 2: Comparison between TopK and ReLU.", "description": "The figure compares the TopK and ReLU activation functions.  The TopK function, commonly used in Mixture-of-Experts (MoE) models, introduces a discontinuity at the k-th largest value by setting all smaller values to zero. This discontinuity creates challenges during model training because it makes the training process non-differentiable at the point of the discontinuity. In contrast, the ReLU function is continuous, and thus differentiable. The figure visually illustrates how the TopK function introduces this abrupt jump (discontinuity), while ReLU remains a continuous smooth function.", "section": "3 OUR METHOD: REMOE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x3.png", "caption": "Figure 3: The sparsity of ReMoE with E=8,k=1formulae-sequence\ud835\udc388\ud835\udc581E=8,k=1italic_E = 8 , italic_k = 1 is effectively maintained around the desired target. Sparsity values for all steps are plotted without averaging or sampling. The mean and standard deviation are calculated excluding the first 100 warm-up steps.", "description": "This figure shows the sparsity of the ReLU Mixture-of-Experts (MoE) model, ReMoE, during training.  The model uses 8 experts (E=8) and targets 1 active expert (k=1).  The plot displays the sparsity at each training step.  No averaging or sampling of the data has been done; all individual data points are plotted.  The reported mean and standard deviation for sparsity only take into account the training data after the first 100 steps (a warm-up period), as the model's behavior during this initial phase is different from its stabilized later performance.", "section": "Controlling Sparsity via Adaptive L1 Regularization"}, {"figure_path": "https://arxiv.org/html/2412.14711/x4.png", "caption": "(a) Sparsity Sisubscript\ud835\udc46\ud835\udc56S_{i}italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT", "description": "This figure displays the sparsity (\ud835\udc46\ud835\udc56) over training steps. Sparsity is the proportion of inactive router outputs. The adaptive L1 regularization dynamically adjusts sparsity to maintain the desired level.  The figure helps to visualize how the adaptive sparsity control method works and how it achieves the desired sparsity level of (1\u2212\ud835\udc58/\ud835\udc38) during training, where k is the number of active experts and E is the total number of experts.", "section": "Controlling Sparsity via Adaptive L1 Regularization"}, {"figure_path": "https://arxiv.org/html/2412.14711/x5.png", "caption": "(b) Coefficient term \u03bbisubscript\ud835\udf06\ud835\udc56\\lambda_{i}italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and regularization term \u2112r\u2062e\u2062gsubscript\u2112\ud835\udc5f\ud835\udc52\ud835\udc54\\mathcal{L}_{reg}caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT", "description": "The figure shows the trends of the coefficient \u03bb\ud835\udc56 and the regularization term \u2112\ud835\udc5f\ud835\udc52\ud835\udc54 during the training process.  The coefficient \u03bb\ud835\udc56 is adaptively adjusted based on the current average sparsity, increasing when sparsity is below the target and decreasing when sparsity is above the target.  The regularization term \u2112\ud835\udc5f\ud835\udc52\ud835\udc54, which uses the L1 norm, encourages sparsity in the ReLU output, driving the outputs towards zero and thus promoting sparsity in expert activation.", "section": "3.3 CONTROLLING SPARSITY VIA ADAPTIVE L\u2081 REGULARIZATION"}, {"figure_path": "https://arxiv.org/html/2412.14711/x6.png", "caption": "(c) Language model loss \u2112l\u2062msubscript\u2112\ud835\udc59\ud835\udc5a\\mathcal{L}_{lm}caligraphic_L start_POSTSUBSCRIPT italic_l italic_m end_POSTSUBSCRIPT and overall regularization \u03bbi\u2062\u2112r\u2062e\u2062gsubscript\ud835\udf06\ud835\udc56subscript\u2112\ud835\udc5f\ud835\udc52\ud835\udc54\\lambda_{i}\\mathcal{L}_{reg}italic_\u03bb start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT", "description": "The figure shows the training curves of the language model loss (\u2112l\u2062msubscript\u2112\ud835\udc59\ud835\udc5a) and the overall regularization term (\u03bbi\u2062\u2112r\u2062e\u2062gsubscript\ud835\udf06\ud835\udc56subscript\u2112\ud835\udc5f\ud835\udc52\ud835\udc54).  The language model loss represents the model's performance on the training data, decreasing as the model learns.  The overall regularization term is added to the loss to control the sparsity of the model, enforcing a desired level of sparsity.  The regularization loss increases during the training process in the first two stages and plateaus in the final stage, indicating the effectiveness of the regularization technique in achieving the desired sparsity.", "section": "3.3 CONTROLLING SPARSITY VIA ADAPTIVE L\u2081 REGULARIZATION"}, {"figure_path": "https://arxiv.org/html/2412.14711/x7.png", "caption": "Figure 4: Natural Three Stage Training in ReMoE.", "description": "This figure illustrates the three natural stages of training in the ReMoE model.  Stage I is a dense stage where sparsity is low and the language model loss (Llm) decreases rapidly.  Stage II is a sparsifying stage where the regularization term (Lreg) becomes significant, driving down the sparsity level towards the target.  Finally, Stage III is a sparse stage where sparsity stabilizes around the target value and Llm continues to decrease while the regularization term remains relatively stable. The plot shows the sparsity (Si), the regularization coefficient (\u03bbi), the language model loss (Llm), and the overall regularization term (\u03bbiLreg) over training steps.", "section": "3 OUR METHOD: REMOE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x8.png", "caption": "Figure 5: Training curves of different routing methods.", "description": "This figure compares the training curves of various Mixture-of-Experts (MoE) routing methods.  The x-axis represents the number of tokens processed during training (in billions), and the y-axis represents the validation loss.  Different lines represent different routing methods, allowing for a visual comparison of their convergence rates and final performance.  The figure provides insights into the relative training efficiency and stability of each routing approach.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.14711/x9.png", "caption": "(a) Scaling in N\ud835\udc41Nitalic_N", "description": "This figure shows how the validation loss of different models changes with the number of active parameters (N).  Three different model sizes are evaluated: 182M, 469M, and 978M parameters. For each model size, the validation loss is plotted for both the ReMoE model and a standard MoE model.  The plot demonstrates the performance of ReMoE in comparison to a standard MoE model across different scales of active model parameters.", "section": "4.3 SCALABILITY OF REMOE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x10.png", "caption": "(b) Scaling in E\ud835\udc38Eitalic_E", "description": "This figure demonstrates the scalability of ReMoE and MoE models as the number of experts (E) increases. The x-axis represents the number of experts, while the y-axis shows the validation loss achieved after training on 30 billion tokens.  The results across various expert counts highlight ReMoE's consistent superior performance compared to the standard MoE, indicating that ReMoE leverages the increased expert capacity more effectively.", "section": "4.3 SCALABILITY OF REMOE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x11.png", "caption": "(c) Scaling in G\ud835\udc3aGitalic_G", "description": "This figure demonstrates the scalability of ReMoE (and MoE for comparison) with respect to granularity (G). Granularity refers to the level of detail or fineness in the model's expert networks.  A higher granularity means that each expert is divided into more specialized sub-experts, allowing for finer-grained control of the model's capacity.  The Y-axis shows the validation loss achieved after training on 30 billion tokens for various granularities.  The graph reveals how the model's performance changes as the granularity increases, highlighting the impact of this hyperparameter on model efficiency and generalization.", "section": "4.3 SCALABILITY OF REMOE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x12.png", "caption": "Figure 6: Scalability of ReMoE with respect to the number of active parameters (N\ud835\udc41Nitalic_N), expert count (E\ud835\udc38Eitalic_E), and granularity (G\ud835\udc3aGitalic_G). Default config is N=182\u2062M,E=8,G=1,k=1formulae-sequence\ud835\udc41182Mformulae-sequence\ud835\udc388formulae-sequence\ud835\udc3a1\ud835\udc581N=182\\text{M},E=8,G=1,k=1italic_N = 182 M , italic_E = 8 , italic_G = 1 , italic_k = 1. The Y-axis represents the validation loss of each model after training on 30B tokens. ReMoE consistently outperforms MoE across all configurations.", "description": "This figure displays the scalability of the ReMoE model (and its comparison with the MoE model) across three key aspects: the number of active parameters (N), the number of experts (E), and the granularity (G).  Each subplot shows the validation loss achieved after training on 30 billion tokens. The default setting is N=182M, E=8, G=1, and k=1.  The results clearly demonstrate that ReMoE consistently outperforms MoE across various configurations and scales.", "section": "4.3 SCALABILITY OF REMOE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x13.png", "caption": "Figure 7: Correlation between expert allocation and token frequency in ReMoE. X-axis is sorted by average active expert count and token frequency is in log-scale.", "description": "This figure illustrates the relationship between the frequency of tokens in the input text and the number of experts assigned to those tokens by the ReMoE model. The x-axis is sorted based on the average number of active experts for each token, and the y-axis (token frequency) uses a logarithmic scale to better visualize the wide range of token frequencies.  The graph shows that less frequent tokens tend to have more experts assigned to them, implying that the model devotes more computational resources to processing less common words, possibly because they are more ambiguous or carry more crucial contextual information. Conversely, highly frequent tokens (like common function words) usually have fewer experts allocated to them, suggesting a more efficient processing strategy for these well-understood words. This demonstrates ReMoE's ability to dynamically allocate computational resources based on token significance, optimizing performance.", "section": "5.1 Dynamic Expert Allocation in ReMoE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x14.png", "caption": "(a) Training curves of MoE and ReMoE with and without load balancing", "description": "This figure compares the training curves of Mixture-of-Experts (MoE) models with and without load balancing, and also includes ReMoE models with and without load balancing.  The x-axis represents the number of tokens processed during training, and the y-axis represents the training loss.  The curves show how the training loss changes over time for each model.  It demonstrates the impact of load balancing on the training process and loss convergence.", "section": "3.4 INTEGRATE LOAD BALANCING INTO L1 REGULARIZATION"}, {"figure_path": "https://arxiv.org/html/2412.14711/x15.png", "caption": "(b) Average routed tokens ratio of ReMoE w.o. LB", "description": "This figure visualizes the average distribution of tokens routed to each expert in ReMoE without load balancing. Each cell represents an expert, and the color intensity indicates the proportion of tokens routed to that expert.  A darker shade implies a higher proportion of tokens.  White cells represent experts that received very few tokens (less than 1/64 of the total).  The figure is valuable for understanding the expert allocation strategy in ReMoE before the load balancing refinement is applied and helps to visualize potential imbalances in expert usage.", "section": "Load Balancing Ablations"}, {"figure_path": "https://arxiv.org/html/2412.14711/x16.png", "caption": "(c) Average routed tokens ratio of ReMoE w. LB", "description": "This figure visualizes the average number of tokens routed to each expert in ReMoE (Mixture-of-Experts with ReLU routing) when load balancing is applied. Each cell represents the average number of tokens routed to a specific expert across all tokens and layers.  The color intensity reflects the magnitude of the average, with deeper colors representing a higher average number of tokens.  This provides insight into the distribution of computational load across experts and helps to evaluate the effectiveness of the load balancing technique in ensuring a more uniform distribution of computational work among experts.", "section": "Load Balancing in ReMoE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x17.png", "caption": "(d) Sparsity across different layers in ReMoE", "description": "This figure shows the sparsity of the ReLU router's output across different layers in the ReMoE model.  Sparsity, in this context, refers to the proportion of experts that are not activated (i.e., have zero output) for each layer.  The figure illustrates the distribution of sparsity across multiple layers, providing a visualization of how the model dynamically manages the activation of experts during inference.  Higher values indicate higher sparsity (more deactivated experts), while lower values indicate lower sparsity (more activated experts). This visualization aids in understanding the model's behavior and its ability to efficiently allocate computational resources across various layers.", "section": "3.3 CONTROLLING SPARSITY VIA ADAPTIVE L\u2081 REGULARIZATION"}, {"figure_path": "https://arxiv.org/html/2412.14711/x18.png", "caption": "Figure 8: Observations on the role of load balancing in MoE and ReMoE. White squares in (b) represent inactive experts with fewer than 1/64 tokens routed to them.", "description": "Figure 8 demonstrates the effects of load balancing on Mixture-of-Experts (MoE) models, comparing the standard MoE with TopK routing and ReMoE with ReLU routing.  Subfigure (a) shows the training curves for both models with and without load balancing, highlighting the improved convergence of load-balanced models. Subfigure (b) displays a heatmap visualizing the average number of tokens routed to each expert in each layer for ReMoE, both with and without load balancing.  White squares in this heatmap represent inactive experts (those with fewer than 1/64 tokens routed to them). Subfigure (c) shows the average token routing ratios for ReMoE (with and without load balancing) across different layers. Finally, subfigure (d) shows the sparsity in ReMoE models, with and without load balancing, across different layers.", "section": "3.4 INTEGRATE LOAD BALANCING INTO L\u2081 REGULARIZATION"}, {"figure_path": "https://arxiv.org/html/2412.14711/x19.png", "caption": "(a) Domain specialization of MoE", "description": "This figure visualizes the domain specialization of the Mixture-of-Experts (MoE) model.  It shows the average proportion of tokens routed to each of the eight experts across twelve layers of the model for six different domains: Arxiv, Books, C4, Github, Stackexchange, and Wikipedia.  The bar chart illustrates the degree to which each expert specializes in particular domains, revealing whether experts focus on a single domain or handle multiple domains more generally.  A uniform distribution across domains would indicate a lack of specialization, while pronounced differences in the bar heights show a strong domain-specific preference for particular experts.", "section": "5 DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2412.14711/x20.png", "caption": "(b) Domain specialization of ReMoE", "description": "This figure shows the average routed tokens ratio for ReMoE across different domains (Arxiv, Books, C4, Github, Stackexchange, Wiki) and layers (0, 5, and 11). Each bar represents the proportion of tokens assigned to a specific expert within a layer and domain. The figure highlights ReMoE's domain specialization, where some experts are strongly activated within specific domains, unlike MoE where experts are more uniformly distributed across domains.", "section": "5.3 Domain Specialization in ReMoE"}, {"figure_path": "https://arxiv.org/html/2412.14711/x21.png", "caption": "Figure 9: Average routed tokens ratio for MoE and ReMoE across 12 layers and 8 experts in different domains. The gray dashed lines indicate uniform distribution. ReMoE shows stronger domain specialization.", "description": "This figure compares the average number of tokens routed to each expert in both MoE and ReMoE models across 12 layers.  The models are evaluated across six different domains (Arxiv, Books, C4, Github, StackExchange, Wiki).  The gray dashed lines represent a uniform distribution, indicating that each expert would receive an equal number of tokens. The figure demonstrates that the ReMoE model exhibits stronger domain specialization than the standard MoE model, meaning that specific experts tend to handle tokens from particular domains more frequently. This visualization helps illustrate the adaptive resource allocation capabilities of ReMoE.", "section": "5 DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2412.14711/x22.png", "caption": "Figure 10: Flip rate and flip count of MoE and ReMoE", "description": "This figure shows the stability of routing in Mixture-of-Experts (MoE) models using two metrics: 'flip rate' and 'flip count'. The flip rate represents the percentage of expert activation states that change (active to inactive or vice versa) in a single update, while the flip count indicates the average number of experts whose activation states change. The results are presented for MoE and ReMoE models with different numbers of experts (E = 8, 16, 32) trained on 10 billion tokens. The figure demonstrates that ReLU routing in ReMoE is more stable than TopK routing in MoE.", "section": "A STABILITY ANALYSIS OF TOPK AND RELU"}, {"figure_path": "https://arxiv.org/html/2412.14711/x23.png", "caption": "Table 3: Valid loss and settling time for different values of \u03bb0subscript\ud835\udf060\\lambda_{0}italic_\u03bb start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT with \u03b1=1.2\ud835\udefc1.2\\alpha=1.2italic_\u03b1 = 1.2.", "description": "This table shows how the performance of the ReMoE model is affected by different values of the hyperparameter \u03bb\u2080 (lambda-zero), which controls the sparsity of the ReLU router's output. The experiment uses a fixed value of \u03b1 (alpha) = 1.2.  The table displays the validation loss achieved and the number of training steps it took for the model to reach a stable state (settling time) for various \u03bb\u2080 values.  A low settling time indicates faster convergence.", "section": "3.3 CONTROLLING SPARSITY VIA ADAPTIVE L\u2081 REGULARIZATION"}]