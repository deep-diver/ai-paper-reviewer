{"references": [{"fullname_first_author": "Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper establishes a foundational method for evaluating LLMs for code generation, which informs the benchmarking process used for KODCODE."}, {"fullname_first_author": "Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-18", "reason": "This paper introduces the APPS benchmark, which serves as a key resource for coding assessment questions, as well as establishes a commonly used pass@k metric for assessing generated solutions."}, {"fullname_first_author": "Zhuo", "paper_title": "Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions", "publication_date": "2024-06-20", "reason": "This work introduces BigCodeBench, an important benchmark for evaluating code generation models with diverse function calls, which is actively used as an important standard benchmark for the fine-tuning process in this paper."}, {"fullname_first_author": "Hendrycks", "paper_title": "Measuring coding challenge competence with apps", "publication_date": "2021-00-00", "reason": "This paper is important for establishing the APPS dataset that consists of a large volume of coding problems, which was leveraged by KODCODE for the generation of coding assessment questions and benchmarks."}, {"fullname_first_author": "Hui", "paper_title": "Qwen2.5-coder technical report", "publication_date": "2024-09-00", "reason": "This technical report introduces Qwen2.5-Coder, a model used as a strong baseline in this work and also used for evaluating the effectiveness of the KODCODE dataset."}]}