[{"figure_path": "https://arxiv.org/html/2502.06772/x1.png", "caption": "Figure 1: Training framework for our ReasonFlux. We train with hierarchical reinforcement learning to enable the model to plan out an optimal and generalizable thought template trajectory for an input problem. Our new inference-scaling framework is in Figure\u00a02.", "description": "The figure illustrates the training process of ReasonFlux, a hierarchical LLM reasoning framework.  ReasonFlux uses hierarchical reinforcement learning to learn an optimal sequence of thought templates for solving problems.  The process begins with input problems that are analyzed and retrieved from a thought template library. These templates are evaluated and scored to create preference trajectory pairs which are then used in the hierarchical reinforcement learning stage. The result is a model that can plan an optimal and generalizable thought template trajectory for a given problem.  A more detailed inference-scaling framework is described in Figure 2 of the paper.", "section": "3. ReasonFlux: Scaling Thought Templates for Hierarchical LLM Reasoning"}, {"figure_path": "https://arxiv.org/html/2502.06772/x2.png", "caption": "Figure 2: New inference scaling system based on hierarchical reasoning. We retrieve a series of high-level thought templates for complex problems, and gradually conduct instantiated reasoning for a sequence of sub-problems.", "description": "ReasonFlux's inference process begins by retrieving a series of high-level thought templates from its template library.  These templates are selected based on the complexity of the input problem. The system then uses these templates to guide a step-by-step reasoning process, breaking down the complex problem into a sequence of simpler sub-problems.  Each sub-problem is solved using an instantiated version of the appropriate template, and the results are fed back into the system. This iterative refinement process continues until the complete solution is obtained. The figure illustrates this hierarchical approach, showcasing the interplay between high-level template selection and instantiated reasoning at each step.", "section": "3. ReasonFlux: Scaling Thought Templates for Hierarchical LLM Reasoning"}, {"figure_path": "https://arxiv.org/html/2502.06772/x3.png", "caption": "Figure 3: Comprasion between o1-mini and ReasonFlux.", "description": "This figure compares the reasoning process of OpenAI's 01-mini model and the ReasonFlux model on a sample mathematical problem. It illustrates the step-by-step approach taken by each model to arrive at the solution.  01-mini's process is shown as a sequence of attempts, some leading to dead ends, highlighting its less directed approach. In contrast, ReasonFlux demonstrates a more efficient and organized approach, utilizing a hierarchical reasoning strategy guided by its structured template library to reach the solution effectively and precisely.  The visualization of the reasoning paths reveals the significant difference in efficiency and effectiveness between the two models.", "section": "4.3 Reasoning Flows over Planned Template Trajectory"}, {"figure_path": "https://arxiv.org/html/2502.06772/extracted/6191929/figs/scaling.png", "caption": "Figure 4: Inference scaling laws for template-augmented reasoning in ReasonFlux. (a) Scaling interplay rounds between planning and instantiation with increased level of problem complexity. (b) Scaling retrieved templates with increased level of problem complexity.", "description": "Figure 4 illustrates the inference scaling laws observed in the ReasonFlux model.  Specifically, it shows how the number of interplay rounds (planning and instantiation steps) and the number of retrieved templates scale in response to increasing problem complexity.  Panel (a) demonstrates the relationship between interplay rounds and problem complexity, while panel (b) shows the relationship between retrieved templates and problem complexity.  The plots visually represent how the model adapts its reasoning process (number of steps and amount of knowledge used) based on the difficulty of the task.", "section": "4.4. Inference Scaling Laws for Template-Augmented Reasoning"}, {"figure_path": "https://arxiv.org/html/2502.06772/x4.png", "caption": "Figure 5: Exploration-Exploitation Trade-off Comparison between different reasoning strategies. Here we experiment with a diverse set of 200 problems sourced from the AIME competitions spanning 1983 to 2023, divided into four difficulty levels. We test the average exploration cost of ReasonFlux (number of interplay rounds), MCTS (number of reasoning steps) and Best-of-N (number of reasoning trajectories).", "description": "Figure 5 illustrates the exploration-exploitation trade-off comparison of three different reasoning strategies: ReasonFlux, Monte Carlo Tree Search (MCTS), and Best-of-N.  Using 200 problems from the AIME competitions (1983-2023) divided into four difficulty levels, the figure compares the average computational cost (exploration cost) each strategy requires to solve the problems. For ReasonFlux, the exploration cost represents the number of interplay rounds between the planning and instantiation phases. For MCTS, it's the number of reasoning steps. Finally, for Best-of-N, it's the number of reasoning trajectories explored. The figure visually demonstrates how the cost changes across the four difficulty levels for each strategy, allowing for a direct comparison of their efficiency and effectiveness in solving increasingly complex mathematical problems. ", "section": "4.5. Better Exploration-Exploitation Trade-off"}]