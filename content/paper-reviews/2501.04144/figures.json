[{"figure_path": "https://arxiv.org/html/2501.04144/x2.png", "caption": "Figure 1: \nGenerated chirpy 3D birds composed of diverse parts. Our Chirpy3D learns a part latent space from raw 2D images and can generate high-quality creative 3D birds by exploring the part latent space. (Top) Existing species. (Bottom) Novel species.", "description": "This figure showcases the capabilities of Chirpy3D, a novel approach for generating 3D bird models.  The top row displays examples of 3D birds representing existing species, accurately reflecting real-world bird anatomy and features.  The bottom row demonstrates Chirpy3D's ability to create entirely new, never-before-seen bird species by manipulating a learned 'part latent space'. This space allows for creative combinations and variations of bird parts, leading to the generation of diverse and plausible novel bird designs that exhibit realistic details and species-specific features.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.04144/x3.png", "caption": "Figure 2: \nOverall architecture of our Chirpy3D. (Top) During training, we fine-tune a text-to-multi-view diffusion model (e.g., MVDream) with only 2D images of birds. We aim to learn the underlying part information by modeling a continuous part-aware latent space. This is achieved by learning a set of species embeddings \ud835\udc86\ud835\udc86\\bm{e}bold_italic_e, project them into part latents \ud835\udc8d\ud835\udc8d\\bm{l}bold_italic_l through learnable f\ud835\udc53fitalic_f, decode into word embeddings \ud835\udc95\ud835\udc95\\bm{t}bold_italic_t through learnable g\ud835\udc54gitalic_g and insert into text prompt. We train the diffusion model with diffusion loss (Eq.\u00a05) and multiple loss objectives \u2013 \u2112regsubscript\u2112reg\\mathcal{L}_{\\text{reg}}caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT (Eq.\u00a02) to model part latents as Gaussian distribution, \u2112attnsubscript\u2112attn\\mathcal{L}_{\\text{attn}}caligraphic_L start_POSTSUBSCRIPT attn end_POSTSUBSCRIPT (Eq.\u00a06) for part disentanglement, and our proposed \u2112clsubscript\u2112cl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT (Eq.\u00a04) to enhance visual coherency. f\ud835\udc53fitalic_f and g\ud835\udc54gitalic_g are trainable modules. For efficient training, we added LoRA layers into cross-attention layers of the U-Net. (Bottom) During inference, we can first preview multi-view images by selecting desired part latents as condition\nbefore turning them into 3D representations (e.g., NeRF) through SDS loss \u2112SDSsubscript\u2112SDS\\mathcal{L}_{\\text{SDS}}caligraphic_L start_POSTSUBSCRIPT SDS end_POSTSUBSCRIPT.", "description": "This figure illustrates the architecture of Chirpy3D, a system for creative 3D bird generation.  The top half shows the training process.  Chirpy3D fine-tunes a pre-trained text-to-multi-view diffusion model (like MVDream) using 2D bird images. It learns a continuous part-aware latent space by mapping species embeddings to part latents, then decoding these into word embeddings that condition the diffusion model's text prompt.  Multiple loss functions are used during training: a diffusion loss, a regularization loss to ensure part latents follow a Gaussian distribution, an attention loss for part disentanglement, and a novel feature consistency loss to improve visual coherence. The bottom half demonstrates the inference process. Users select desired part latents to preview multi-view images, which can then be converted into 3D representations (e.g., using NeRF) via an SDS loss.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.04144/x4.png", "caption": "Figure 3: As we do not have images of unseen part latents, we use real natural images as our proxy. We extract cross-attention feature maps F\ud835\udc39Fitalic_F of two noised latents, then minimize the discrepancy between the two feature maps. This will encourage the model to compute similar feature maps for any given part latents, which indirectly stabilizes the denoising process for unseen latents.", "description": "This figure illustrates a self-supervised approach to ensure the generation of visually coherent, unseen parts. Since there are no images of unseen part latents available for training, the method uses real natural images as a proxy. By extracting cross-attention feature maps from two noised latents, the model is trained to minimize the discrepancy between these feature maps. This indirect approach encourages the model to generate similar feature maps for any given part latents, regardless of whether the latent represents a seen or unseen part. This ultimately stabilizes the denoising process for unseen parts, leading to more visually coherent novel object generation.", "section": "3.2. Visual Coherency for Novel Generation"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/cls/subject_ti_2x.jpg", "caption": "Figure 4: (a) Seen part selection generation. Unseen part synthesis via (b) novel sampling and (c) interpolation.", "description": "Figure 4 illustrates three different methods for generating bird images using the Chirpy3D model. (a) shows seen part selection, where existing parts from known bird species are combined to create new images.  (b) demonstrates unseen part synthesis through novel sampling, generating entirely new bird parts not seen during training. This is achieved by sampling from a learned continuous latent space representing bird parts. (c) shows unseen part synthesis via interpolation, creating novel bird parts by smoothly blending existing parts from the latent space.  This allows for generating hybrid bird species.", "section": "3.4 Inference and Interpolation of Novel Parts"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/cls/subject_partcraft_2x.jpg", "caption": "(a) Textual Inversion", "description": "This figure shows the results of generating multi-view images of birds using the Textual Inversion method. Textual Inversion is a baseline method that directly uses word embeddings without explicitly modeling part-aware latent space.  The images demonstrate the model's ability to generate birds, but they tend to lack the fine-grained detail and visual consistency observed in other methods presented in the paper.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/cls/subject_ours_2x.jpg", "caption": "(b) PartCraft", "description": "PartCraft is a method for generating images by replacing parts of existing images with corresponding parts from other images. The figure shows the result of this method applied to the task of bird image generation.  Specifically, it demonstrates the linear interpolation of parts between two different bird species, illustrating the system's ability to generate novel combinations of existing parts.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/x5.png", "caption": "(c) Chirpy3D (Ours)", "description": "This figure shows 3D bird models generated by the Chirpy3D model. The models showcase the model's ability to generate high-quality, detailed birds with species-specific features. The birds shown represent a variety of species, both existing and novel, demonstrating the model's capacity for both realistic and creative generation.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/interpolation/interpolate_ti.jpg", "caption": "Figure 5: Subject generation of 2 different species -blue jay, white pelican.", "description": "This figure showcases the model's ability to generate images of different bird species.  Two species are highlighted here: the Blue Jay and the White Pelican.  The images demonstrate the model's capacity to generate realistic and detailed depictions of these birds, capturing fine-grained features such as plumage patterns and beak shapes. This highlights the effectiveness of the model in generating high-quality, species-specific images from a diverse range of birds.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/interpolation/interpolate_partcraft.jpg", "caption": "Figure 6: Visual comparison of part composition. A,B,C,D,E,F\ud835\udc34\ud835\udc35\ud835\udc36\ud835\udc37\ud835\udc38\ud835\udc39A,B,C,D,E,Fitalic_A , italic_B , italic_C , italic_D , italic_E , italic_F represent cardinal, wilson warbler, least auklet, california gull, horned lark, and song sparrow respectively. Red circles indicate changed parts. All generated (including sources & targets) by the same seed.", "description": "This figure displays a visual comparison of part composition using six bird species: cardinal, Wilson warbler, least auklet, California gull, horned lark, and song sparrow.  Each row shows a target image where one part (head, body, or wings) has been replaced with a corresponding part from a different source bird species. All images, both source and target, in each row share the same random seed to highlight how the model generates and recombines components. Red circles clearly mark the parts that have been swapped.", "section": "Part Composition"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/interpolation/interpolate_ours.jpg", "caption": "(a) Textual Inversion", "description": "This figure shows the results of multi-view image generation using the Textual Inversion method.  It displays several generated images of birds, showcasing the method's ability (or inability) to generate images from text prompts.  Differences in image quality, realism, and adherence to the prompt can be observed. The purpose is to provide a visual comparison of this method's performance compared to other methods discussed in the paper (PartCraft and Chirpy3D).", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/tsne_part_ti.png", "caption": "(b) PartCraft", "description": "The figure shows linear interpolation results between two bird species using PartCraft.  PartCraft, unlike Chirpy3D, exhibits an abrupt switch in the generated images after a certain interpolation step, resulting in less smooth transitions between the two bird species.", "section": "4.1 Multi-view Subject Generation"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/tsne_partcraft.png", "caption": "(c) Chirpy3D (Ours)", "description": "This figure shows 3D bird models generated by the Chirpy3D model.  The images demonstrate the model's ability to generate high-quality, novel 3D bird models with diverse parts, illustrating its capacity for creative 3D object generation. The birds depicted showcase intricate details and a range of species, highlighting the model's capability in fine-grained 3D generation.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/tsne_ours_kl0001.png", "caption": "Figure 7: \nLinear interpolation of all part latents between two different species \u2013 blue jay and cardinal.\nOnly one view is shown.\nOur Chirpy3D achieves much smoother interpolation, unlike PartCraft exhibits an abrupt switch phenomenon after a certain step (red box).", "description": "This figure shows a comparison of linear interpolation results between two bird species (blue jay and cardinal) using three different methods: Textual Inversion, PartCraft, and Chirpy3D.  The x-axis implicitly represents the interpolation parameter, smoothly transitioning from one bird species to the other.  The y-axis represents the visual features of the generated bird.  Chirpy3D demonstrates a smooth, continuous transition between the two species' features throughout the interpolation range. In contrast, PartCraft's interpolation shows an abrupt, discontinuous change in bird features at a specific point in the interpolation, indicated by a red box in the figure. This highlights Chirpy3D's ability to generate a more natural and continuous transition between species' features due to its modeling of continuous part latents.", "section": "3.4. Inference and Interpolation of Novel Parts"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/random_ti.jpg", "caption": "(a) Textual Inversion", "description": "Figure 5(a) shows the results of generating images of two bird species (blue jay and white pelican) using the Textual Inversion method. The Textual Inversion method directly uses text embeddings to generate images, without incorporating a mechanism to represent and manipulate individual parts of the bird. This may result in less control over the fine-grained details of the bird's features and less ability to generate novel variations.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/consistency/consistency_partcraft.jpg", "caption": "(b) PartCraft", "description": "Figure 7(b) shows the results of linear interpolation of part latents between two bird species using PartCraft.  PartCraft, unlike Chirpy3D, exhibits an abrupt, non-smooth transition between the two species during the interpolation, as indicated by the red box.", "section": "3.4. Inference and Interpolation of Novel Parts"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/random/random_ours.jpg", "caption": "(c) Chirpy3D (Ours)", "description": "Chirpy3D generates high-quality 3D bird images with diverse parts by exploring a continuous part latent space.  The images show novel bird species created through interpolation and sampling of the part latents, demonstrating the system's ability to generate creative and plausible 3D bird models.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/x6.png", "caption": "Figure 8: t-SNE embeddings of DINO features of generated images. Blue represents images of subject reconstruction; Orange represents images of novel generation.", "description": "This figure visualizes the results of t-SNE dimensionality reduction applied to DINO (DETR-like image-level representations) features extracted from generated images.  The images were generated using the Chirpy3D model, a method for creative 3D bird generation described in the paper.  Points colored blue represent images generated from known bird species (subject reconstruction), while those colored orange represent images of novel bird species created by the model (novel generation). The plot shows how these images cluster together based on their visual similarity as captured by the DINO features. Clustering patterns provide insight into the model's ability to generate both realistic representations of known bird species and creative novel variations.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/consistency/consistency_baseline.jpg", "caption": "(a) Textual Inversion", "description": "This figure displays the results of 3D object generation using the Textual Inversion method. It showcases the limitations of the Textual Inversion method in terms of generating high-quality, detailed, and visually consistent 3D objects, particularly when dealing with fine-grained details and novel object creation.  The generated objects lack the fidelity and coherence seen in other methods, highlighting the challenges of translating 2D fine-grained understanding to the 3D domain.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/consistency/consistency_ours.jpg", "caption": "(b) PartCraft", "description": "This figure shows the results of part composition using the PartCraft method.  Part composition involves replacing one part of a target bird image (e.g., the head) with the corresponding part from a different bird image.  The results demonstrate the model's ability to combine parts from different bird species to create novel bird images. The figure likely shows multiple examples of these part-swapped bird images, illustrating the accuracy and visual quality of the resulting creations compared to the source birds and the target bird.", "section": "Part Composition"}, {"figure_path": "https://arxiv.org/html/2501.04144/x7.png", "caption": "(c) Chirpy3D (Ours)", "description": "This figure shows the results of 3D bird generation using the Chirpy3D method.  It showcases the model's ability to generate high-quality, multi-view images of birds, including both existing and novel species.  The birds are shown from various angles and exhibit realistic details and textures.  The results highlight Chirpy3D's capacity for generating diverse and creative outputs.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/x8.png", "caption": "Figure 9: Generated images with random sampled latents/embeddings. Textual Inversion often produces images with artifacts due to the direct interpolation of word embeddings. PartCraft can generate images with fewer artifacts but lacks consistency. In contrast, our Chirpy3D generates novel images with greater diversity.", "description": "Figure 9 displays the results of generating images using random samples from the latent spaces of three different models: Textual Inversion, PartCraft, and Chirpy3D.  Textual Inversion, due to its method of directly interpolating word embeddings, frequently produces images with noticeable artifacts. PartCraft generates images with fewer artifacts, but the resulting images lack overall consistency. In contrast, Chirpy3D consistently produces novel and diverse images, demonstrating its superior ability in generating creative and high-quality outputs.", "section": "G. Multi-view generation on novel species (random sampling)"}, {"figure_path": "https://arxiv.org/html/2501.04144/x9.png", "caption": "Figure 10: NeRF rendering of learned 3D objects.", "description": "This figure showcases 3D renderings of birds generated using NeRF (Neural Radiance Fields).  It demonstrates the capability of the Chirpy3D model to generate high-quality, detailed 3D models of birds from various angles. The images showcase both existing bird species and novel, synthetic birds created through the model's creative capabilities.  The diversity of generated bird poses and appearances highlights the model's effectiveness in 3D object generation.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/fg3d_dog.jpg", "caption": "(a) Ours without \u2112clsubscript\u2112cl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT", "description": "This figure displays a comparison of 3D bird generation results obtained with and without the feature consistency loss (\u2112cl). The left column shows generations without the feature consistency loss, exhibiting a lack of visual coherence and various artifacts.  The right column shows results obtained with the feature consistency loss, demonstrating significantly improved visual coherence and a reduction in artifacts, thereby emphasizing the importance of this loss term in producing high-quality, consistent 3D bird models.", "section": "4.3 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/fg3d_animal.jpg", "caption": "(b) Ours with \u2112clsubscript\u2112cl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT", "description": "This figure shows a qualitative comparison of visual coherency before and after applying feature consistency loss (\u2112cl). The left column (a) displays bird images generated before applying the loss, while the right column (b) shows the results after applying the loss. The results demonstrate that the feature consistency loss improves visual coherency and reduces artifacts.", "section": "I. Qualitative comparisons of visual coherency before and after applying feature consistency loss"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/reason_gs75_bird_10.jpg", "caption": "Figure 11: \nAll images are generated with the same camera pose but with different seeds on unseen latent. (a) Without our feature consistency loss \u2112clsubscript\u2112cl\\mathcal{L}_{\\text{cl}}caligraphic_L start_POSTSUBSCRIPT cl end_POSTSUBSCRIPT, the generated images lack consistency (e.g., less artifact, and inconsistent visual feature) compared to (b).", "description": "This figure demonstrates the effect of the feature consistency loss (\u2112cl) on the visual consistency of generated images.  Two sets of images are shown, each generated with the same camera pose but different random seeds.  The images in (a) were generated without the \u2112cl loss, resulting in inconsistencies such as artifacts and variations in visual features between the images. The images in (b) were generated with the \u2112cl loss, showcasing improved visual consistency and reduced artifacts.", "section": "4.3 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/reason_gs75_cardinal_10.jpg", "caption": "Figure 12: A hybrid (middle) between siberian husky (left) and papillon (right), trained with Stanford Dogs [27].", "description": "This figure demonstrates the capability of the model to generate novel animal breeds by interpolating between existing breeds.  The image shows a hybrid dog (center) that blends features of a Siberian Husky (left) and a Papillon (right).  The model was trained on the Stanford Dogs dataset, indicating its ability to learn and creatively combine fine-grained characteristics of different dog breeds to produce plausible novel variations.", "section": "C. Other domains"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_ti.jpg", "caption": "Figure 13: Optimization-based 3D generation with NeRF or 3DGS.", "description": "This figure displays the results of 3D object generation using two different methods: NeRF (Neural Radiance Fields) and 3DGS (a method for generating 3D shapes using Gaussian Splatting).  It showcases examples of generated 3D bird models, highlighting the quality and detail achievable with these optimization-based approaches.  The figure shows multiple views of the generated 3D birds from various angles, enabling a full 3D understanding of the model's capabilities.", "section": "B. 3D Generation"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_partcraft.jpg", "caption": "Figure 14: Image-to-3D using front view and side view of generated object.", "description": "This figure demonstrates the image-to-3D generation pipeline using InstantMesh.  The input is a single front view image of a bird, which is then processed to generate a 3D model of the bird. InstantMesh employs an intermediate step of generating multiple views (in this case, a side view is also shown). The final output is a 3D mesh representation ready for rendering or further manipulation.", "section": "B.3 InstantMesh"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_ours_d32_kl001.jpg", "caption": "(a) Mixing chow with golden retriever, pomeranian with pug.", "description": "This figure shows examples of dog breed hybrids generated by the model.  The top row shows a mix between a Chow Chow and a Golden Retriever, and a Pomeranian and a Pug. These images demonstrate the model's ability to generate plausible combinations of existing dog breeds, showcasing fine-grained control over the generation process.", "section": "C. Other domains"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_cls_target.jpg", "caption": "(b) Hamster-cat, Hamster-horse, Elephant-horse", "description": "This figure shows examples of novel animal generation from the Chirpy3D model.  The images depict the results of combining features from disparate animals.  Specifically, it shows three hybrid creatures: a hamster-cat hybrid, a hamster-horse hybrid, and an elephant-horse hybrid. These novel combinations showcase the model's ability to generate plausible and creative combinations of animal features beyond those seen during training.", "section": "C. Other domains"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_random_ti.jpg", "caption": "Figure 15: (a) Dog generation. (b) Animal generation.", "description": "Figure 15 showcases the model's ability to generate images beyond birds.  (a) demonstrates the generation of various dog breeds and hybrids, highlighting the model's capacity for creating novel variations within a specific species. (b) shows the generation of diverse animal combinations, indicating that the model's framework is applicable to a broader range of animal categories.", "section": "C. Other domains"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_random_partcraft.jpg", "caption": "(a) a bird, 3d asset.", "description": "This figure shows the results of generating 3D bird models from the text prompt \"a bird, 3d asset\".  The image showcases multiple generated bird models, each rendered from a slightly different viewpoint to illustrate the multi-view capability of the model.  The variation in the generated birds demonstrates the model's ability to produce diverse outputs even with a simple prompt.", "section": "E. Multi-view generation on common token and fine-grained token"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_random_ours_d32_kl001.jpg", "caption": "(b) a cardinal, 3d asset.", "description": "This figure shows a multi-view rendering of a 3D model of a cardinal generated using the Chirpy3D method.  Multiple views are presented to demonstrate the quality and realism of the generated 3D bird model. The fine-grained details are visible, highlighting the model's ability to capture intricate features. The background is plain, focusing attention on the 3D bird model itself.", "section": "E. Multi-view generation on common token and fine-grained token"}, {"figure_path": "https://arxiv.org/html/2501.04144/x10.png", "caption": "Figure 16: Multi-view generation with text prompt through MVDream [52]. The guidance scale is 7.5. Each row is a different seed. (a) The generation varies for different seeds for the token \u201cbird\u201d. (b) The generation with a fine-grained token \u201ccardinal\u201d. As highly similar objects are generated for each seed, we can use a lower guidance scale for SDS loss and enable 3D generation without oversaturated effect.", "description": "This figure shows the results of multi-view generation using the MVDream model with different text prompts. The guidance scale was set to 7.5.  Subfigure (a) uses the general prompt \"bird\", resulting in varied outputs from different random seeds, demonstrating the model's ability to generate diverse bird-like forms. Subfigure (b) employs the more specific prompt \"cardinal\", leading to consistently similar outputs across different seeds. This consistency with the specific prompt allows for a lower guidance scale during 3D generation using the Score Distillation Sampling (SDS) loss, preventing oversaturation of details and producing higher-quality 3D models.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/x11.png", "caption": "(a) Textual Inversion", "description": "Figure 5(a) shows the results of multi-view generation using the Textual Inversion method.  Textual Inversion, unlike other methods described in the paper (PartCraft and Chirpy3D), directly uses word embeddings for image generation. This is a baseline method compared against more sophisticated approaches that employ part-level manipulation for greater creative control. The image showcases a variety of bird species, highlighting the method's ability to generate different bird types, but potentially at the cost of less detailed rendering and consistency compared to PartCraft or Chirpy3D.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/x12.png", "caption": "(b) PartCraft", "description": "Figure 7 shows the results of linear interpolation between two bird species (blue jay and cardinal) using three different methods: Textual Inversion, PartCraft, and Chirpy3D.  The x-axis represents the interpolation step, ranging from one species to the other. The y-axis implicitly represents the generated image. Textual Inversion produces noisy and discontinuous transitions between the bird species. PartCraft shows an abrupt shift in the generated image at a certain point during interpolation. In contrast, Chirpy3D produces smooth and coherent transitions, showcasing a more natural and seamless morphing process between the two species.", "section": "3.4. Inference and Interpolation of Novel Parts"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_consistency_random_baseline.jpg", "caption": "(c) Chirpy3D (Ours)", "description": "This figure displays multi-view generation results from the Chirpy3D model on novel bird species created through interpolation.  The images showcase a smooth transition between different bird species, highlighting the model's capacity to generate plausible and visually coherent novel bird variations.", "section": "H. Multi-view generation on novel species (interpolation)"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/supp_consistency_random_ours_d4.jpg", "caption": "(d)", "description": "This figure shows multi-view generation results on existing bird species.  The three columns represent the outputs from Textual Inversion, PartCraft, and Chirpy3D respectively. Each row displays a different bird species, illustrating the models' ability to generate multi-view images of known bird types. The goal is to compare the visual fidelity and consistency of the generated images across different methods. Note that (d) shows one of the actual training images for comparison.  Chirpy3D shows improved visual quality and consistency compared to the baseline methods.", "section": "F. Multi-view generation on existing species"}, {"figure_path": "https://arxiv.org/html/2501.04144/x13.png", "caption": "Figure 17: Multi-view generation on existing species, trained with respective methods (a, b, c). (d) One of the training images of the species. Not only our Chirpy3D (c) can reconstruct well in multi-view perspective comparing to Textual Inversion (a) and PartCraft (b), but our generated images are also consistent in terms of orientation and cleaner background.", "description": "Figure 17 presents a comparison of multi-view bird image generation results from three different methods: Textual Inversion, PartCraft, and the authors' proposed Chirpy3D.  Each column (a, b, c) displays images generated by each method for several existing bird species. Column (d) shows an example of a real training image for reference. The figure highlights that Chirpy3D produces images with superior multi-view consistency and cleaner backgrounds compared to the baseline methods.", "section": "F. Multi-view generation on existing species"}, {"figure_path": "https://arxiv.org/html/2501.04144/x14.png", "caption": "(a) Textual Inversion", "description": "This figure shows the results of generating multi-view images of birds using the Textual Inversion method.  It highlights that this method struggles to generate high-quality and consistent multi-view images of birds,  likely due to limitations in representing and manipulating the intricate visual details of bird species. The images appear less realistic and lack the fine-grained detail achievable by other more advanced methods like Chirpy3D.  The figure serves as a visual comparison to highlight the differences between Textual Inversion and other approaches in generating high-quality, detailed bird images.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/x15.png", "caption": "(b) PartCraft", "description": "PartCraft is a method for creative 3D object generation that uses a part-aware approach.  This image shows the results of PartCraft's object generation, specifically focusing on its ability to compose and recombine existing parts to create novel objects. The visual comparison allows assessment of the quality and creativity of the generated objects, showcasing the method's strengths and limitations in generating fine-grained details and plausible combinations.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/scale1.jpg", "caption": "(c) Chirpy3D (Ours)", "description": "This figure shows multi-view generation results on novel bird species created using interpolation by Chirpy3D.  Chirpy3D generates smooth transitions between bird species by blending latent representations of different bird parts.  Compared to methods like Textual Inversion and PartCraft, Chirpy3D produces more visually coherent and natural-looking results, demonstrating the effectiveness of its continuous part-aware latent space in creating novel, plausible bird variations.", "section": "H. Multi-view generation on novel species (interpolation)"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/scale01.jpg", "caption": "Figure 18: Multi-view generation on novel species (random sampling), trained with respective methods. All were generated with the same seed but with different sampled part latents. (a) Trained with Textual Inversion, the generated images are often incomprehensible, indicating that direct sampling from word embedding space is insufficient to generate novel species. (b) PartCraft has a non-linear projector to project word embeddings, while able to generate comprehensible objects, but lacking diversity since it is not trained to have a continuous distribution of part latents. (c) Our Chirpy3D not only can generate images of diverse species, also stable in terms of bird pose.", "description": "This figure compares the results of generating novel bird species using three different methods: Textual Inversion, PartCraft, and Chirpy3D.  Each method used the same random seed, but with different randomly sampled part latents.  Textual Inversion resulted in incomprehensible images because it directly samples from the word embedding space, which is not sufficient for generating novel species.  PartCraft, while generating comprehensible birds, lacked diversity due to its discrete part representation. In contrast, Chirpy3D successfully generated a variety of novel bird species with consistent poses and consistent quality, showcasing the benefits of its continuous part latent space.", "section": "G. Multi-view generation on novel species (random sampling)"}, {"figure_path": "https://arxiv.org/html/2501.04144/extracted/6117322/figs/supp/scale001.jpg", "caption": "(a) Textual Inversion", "description": "This figure shows the results of multi-view generation using the Textual Inversion method.  The method directly uses textual embeddings without leveraging a continuous latent space. The images generated demonstrate the limitations of this approach, producing images with noticeable artifacts and a lack of diversity compared to other methods.", "section": "4. Experiment"}]