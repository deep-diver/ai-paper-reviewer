[{"content": "| Dataset | Language | #Fill-In-the-Blank | #Problem-Solving | #Questions |\n|---|---|---|---|---| \n| CNMO | en & cn | - | 18\u00d72 | 18\u00d72 |\n| CCEE | en & cn | 13\u00d72 | 31\u00d72 | 44\u00d72 |\n| AMC | en & cn | - | 46\u00d72 | 46\u00d72 |\n| WLPMC | en & cn | - | 11\u00d72 | 11\u00d72 |\n| ALL | en & cn | 13\u00d72 | 106\u00d72 | 119\u00d72 |", "caption": "Table 1: Statistics of LiveMathBench", "description": "This table presents the statistics of the LiveMathBench dataset, including the number of fill-in-the-blank and problem-solving questions for each subset (CNMO, CCEE, AMC, WLPMC) in both English and Chinese.", "section": "4.1.2 Benchmark Statistics"}, {"content": "| *LLMs* | *CCEE* | \u2198 Greedy | \u2198 G-Pass@16\u2081.\u2080 | *WLPMC* | G-Pass@16 \u2192 0 | \u2198 Greedy | \u2198 G-Pass@16\u2081.\u2080 |\n|---|---|---|---|---|---|---| \n| *Llama-3.3-70B-Instruct* | 68.3 | 56.8\u2193\u2081\u2087.\u2080 | 31.8\u2193\u2084\u2084.\u2080 | 41.0 | 36.0\u2193\u2081\u2082.\u2082 | 9.2\u2193\u2087\u2084.\u2084 | | \n| *Mistral-Large-Instruct-2411* | 63.3 | 54.6\u2193\u2081\u2083.\u2087 | 42.5\u2193\u2082\u2082.\u2081 | 21.1 | 18.2\u2193\u2081\u2083.\u2087 | 6.1\u2193\u2086\u2086.\u2085 | | \n| *DeepSeek-V2.5-1210* | 74.3 | 56.8\u2193\u2082\u2083.\u2085 | 26.9\u2193\u2085\u2082.\u2086 | 65.9 | 9.1\u2193\u2088\u2086.\u2080 | 4.0\u2193\u2085\u2086.\u2081 | | \n| *Qwen2.5-72B-Instruct* | 74.4 | 56.8\u2193\u2082\u2083.\u2087 | 50.3\u2193\u2081\u2081.\u2080 | 54.0 | 27.3\u2193\u2084\u2089.\u2084 | 0.3\u2193\u2089\u2088.\u2089 | | \n| *Gemini-1.5-Pro-Latest* | 76.9 | 59.1\u2193\u2082\u2083.\u2081 | 45.3\u2193\u2082\u2083.\u2084 | 60.0 | 36.4\u2193\u2084\u2080.\u2080 | 4.3\u2193\u2088\u2088.\u2082 | |\n| *GPT-4o* | 73.7 | 52.3\u2193\u2082\u2089.\u2080 | 34.0\u2193\u2083\u2085.\u2080 | 29.9 | 18.2\u2193\u2083\u2089.\u2081 | 4.0\u2193\u2087\u2088.\u2080 | |\n| *Deepseek-Math-7B-RL* | 61.7 | 43.2\u2193\u2083\u2080.\u2080 | 17.3\u2193\u2086\u2080.\u2080 | 12.5 | 9.1\u2193\u2082\u2087.\u2082 | ~0.0\u2193\u2081\u2080\u2080.\u2080 | |\n| *Qwen2.5-Math-7B-Instruct* | 68.5 | 56.8\u2193\u2081\u2087.\u2081 | 43.6\u2193\u2082\u2083.\u2082 | 52.5 | 27.3\u2193\u2084\u2088.\u2080 | 9.1\u2193\u2086\u2086.\u2087 | |\n| *Qwen2.5-Math-72B-Instruct* | 74.1 | 68.2\u2193\u2088.\u2080 | 50.2\u2193\u2082\u2086.\u2084 | 48.3 | 27.3\u2193\u2084\u2083.\u2085 | 18.2\u2193\u2083\u2083.\u2083 | |\n| *QwQ-32B-Preview* | 85.8 | 70.5\u2193\u2081\u2085.\u2083 | 40.5\u2193\u2083\u2080.\u2080 | 89.3 | 27.3\u2193\u2086\u2089.\u2084 | 18.2\u2193\u2083\u2083.\u2083 | |", "caption": "Table 2: Performance of models on LiveMathBench. We perform 48484848 runs and report results of greedy accuracy, and G-Pass@16{0.5,0.75,1.0}subscript160.50.751.016_{\\{0.5,0.75,1.0\\}}16 start_POSTSUBSCRIPT { 0.5 , 0.75 , 1.0 } end_POSTSUBSCRIPT and mG-Pass@16161616. A more detailed performance can be found in Table\u00a06 at Section\u00a0A.5.1.", "description": "This table presents the performance of various large language models (LLMs) on the LiveMathBench dataset.  It evaluates their mathematical reasoning capabilities using metrics that consider both accuracy and consistency across multiple runs. Specifically, it reports the greedy accuracy, G-Pass@16 (with thresholds of 0.5, 0.75, and 1.0), and mG-Pass@16. The table categorizes models into three groups: General LLMs, Mathematical Reasoning LLMs, and o1-like Reasoning LLMs.  This allows for comparison of performance across different model types and sizes, providing insight into the impact of specialized training and architecture on mathematical reasoning abilities.  It uses 48 samples per question.", "section": "4.3 LiveMathBench & Public Benchmark Performance"}, {"content": "| Models Need to Judge | Agreement | Disagreement | Accuracy (%) |\n|---|---|---|---| \n| *Deepseek-Math-7B-RL* | 296 | 4 | 98.7 |\n| *Qwen2.5-32B-Instruct* | 282 | 18 | 94.0 |\n| *Qwen2.5-Math-72B-Instruct* | 287 | 13 | 95.7 |\n| *Mistral-Large-Instruct-2411* | 285 | 15 | 95.0 |\n| *QwQ-32B-Preview* | 290 | 10 | 96.7 |", "caption": "Table 3: Performance of models on MATH500 and AIME2024. Aligning with experiments on LiveMathBench, we also perform 48484848 runs and report results of greedy accuracy, G-Pass@16{0.5,0.75,1.0}subscript160.50.751.016_{\\{0.5,0.75,1.0\\}}16 start_POSTSUBSCRIPT { 0.5 , 0.75 , 1.0 } end_POSTSUBSCRIPT, and mG-Pass@16161616. More detailed results are available in Table\u00a07 at Section\u00a0A.5.2.", "description": "This table presents the evaluation results of various large language models (LLMs) on two mathematical reasoning benchmarks: MATH500-L5 and AIME2024-45.  The evaluation metrics used are Greedy Accuracy, Pass@16 (equivalent to G-Pass@16 with \u03c4\u21920), G-Pass@16 with varying thresholds (\u03c4 = 0.5, 0.75, and 1.0), and mG-Pass@16. These metrics assess different aspects of reasoning performance, including accuracy, stability, and potential. The table provides a comprehensive comparison of LLM capabilities in solving mathematical problems of varying complexity.", "section": "4.3 LiveMathBench & Public Benchmark Performance"}]