[{"figure_path": "https://arxiv.org/html/2501.13826/x2.png", "caption": "Figure 1: An illustration of Video-MMMU: Evaluating the knowledge acquisition capability from videos through three cognitive stages: 1) Perception:  if models can identify key information related to knowledge; 2) Comprehension:  if models can interpret the underlying concepts; 3) Adaptation: if models can adapt the knowledge from videos to novel scenarios.", "description": "Figure 1 illustrates the three stages of knowledge acquisition in Video-MMMU:  Perception, Comprehension, and Adaptation.  Perception focuses on whether a model can extract key pieces of information from a video.  Comprehension assesses the model's ability to understand the core concepts presented in the video. Finally, Adaptation tests the model's capacity to apply this learned knowledge to solve new and related problems not directly shown in the video.  Each stage involves a different level of cognitive complexity, progressing from simple information retrieval to complex problem solving.", "section": "3. Video-MMMU Dataset"}, {"figure_path": "https://arxiv.org/html/2501.13826/x3.png", "caption": "Figure 2: Sampled Video-MMMU examples across 6 academic disciplines and 3 tracks. The examples are organized in two rows based on distinct video types: (1) Concept-Introduction videos (top row) focus on teaching factual knowledge, fundamental concepts, and theories through explanatory content, while (2) Problem-Solving videos (bottom row) demonstrate step-by-step solutions to an example question.", "description": "Figure 2 presents examples from the Video-MMMU dataset, showcasing the diversity of its content.  The dataset includes videos from six academic disciplines (Art, Business, Humanities, Medicine, Science, and Engineering) and three tracks of questions (Perception, Comprehension, and Adaptation) designed to evaluate different levels of knowledge acquisition. The figure is organized into two rows. The top row displays Concept-Introduction videos, which primarily focus on explaining core concepts and theories using explanatory content. The bottom row shows Problem-Solving videos, which demonstrate step-by-step solutions to example questions, often illustrating practical applications of the concepts presented.", "section": "3. Video-MMMU Dataset"}, {"figure_path": "https://arxiv.org/html/2501.13826/x4.png", "caption": "Figure 3: Taxonomy of QA types and video disciplines.", "description": "Figure 3 presents a taxonomy of the question and answer (QA) pairs used in the Video-MMMU dataset, categorized by cognitive level and video discipline.  (a) shows a pie chart illustrating the distribution of videos across six disciplines: Art, Business, Humanities, Medicine, Science, and Engineering. (b) displays a donut chart visualizing the distribution of QA pairs across the three cognitive levels: Perception, Comprehension, and Adaptation.  Finally, (c) presents bar charts depicting the number of questions that do or do not utilize audio for each discipline.", "section": "3. Video-MMMU Dataset"}, {"figure_path": "https://arxiv.org/html/2501.13826/x5.png", "caption": "Figure 4: Performance comparison across tracks before and after adding audio transcripts.", "description": "This figure shows the performance comparison of different large multimodal models (LLMs) across three knowledge acquisition tracks (Perception, Comprehension, and Adaptation) before and after adding audio transcripts to the video inputs.  The x-axis represents the different tasks, while the y-axis shows the accuracy in percentage.  The chart visually demonstrates the impact of including audio information on the models' ability to acquire knowledge from videos.  Specifically, it highlights whether the additional audio context improves or hinders the models' performance on each type of task.  We can compare the model's performance on the same task with or without audio to better understand the role of audio in video-based learning.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.13826/x6.png", "caption": "(a) Comparison of \u0394knowledgesubscript\u0394knowledge\\Delta_{\\text{knowledge}}roman_\u0394 start_POSTSUBSCRIPT knowledge end_POSTSUBSCRIPT (performance improvement in the Adaptation track after watching the video compared to before).", "description": "The figure shows a bar chart comparing the knowledge gain (\u0394knowledge) of different large language models (LLMs) and humans after watching videos in the Adaptation track of the Video-MMMU benchmark.  \u0394knowledge represents the percentage improvement in the models\u2019 performance on post-video questions compared to their pre-video performance on the same questions.  Higher values indicate a greater knowledge gain from the videos. The chart visually demonstrates the significant difference in knowledge acquisition between human subjects and the various LLMs, highlighting the challenge LLMs face in adapting and applying knowledge from videos to new scenarios.", "section": "5. Knowledge Acquisition in Adaptation Track"}, {"figure_path": "https://arxiv.org/html/2501.13826/x7.png", "caption": "(b) Comparison of Wrong-to-Right Rate (the percentage of Adaptation track questions that were initially answered incorrectly without the video but correctly after watching the video) and Right-to-Wrong Rate (vice versa).", "description": "This figure shows two bar charts visualizing the performance of different large language models (LLMs) on the Adaptation track of the Video-MMMU benchmark. The left chart displays the \"Wrong-to-Right Rate,\" which represents the percentage of questions initially answered incorrectly by the model without watching the video but correctly after watching the video. The right chart displays the \"Right-to-Wrong Rate,\" which represents the percentage of questions initially answered correctly by the model but incorrectly after watching the video.  Both charts help illustrate how the LLMs' knowledge changes (or doesn't change) after they view a video, offering insights into their ability to acquire and apply knowledge from video content. The comparison highlights the effectiveness of video-based learning for different models and the challenges these models face in adapting their knowledge.", "section": "5. Knowledge Acquisition in Adaptation Track"}, {"figure_path": "https://arxiv.org/html/2501.13826/x8.png", "caption": "Figure 5: Key findings in the experiment of \u0394knowledgesubscript\u0394knowledge\\Delta_{\\text{knowledge}}roman_\u0394 start_POSTSUBSCRIPT knowledge end_POSTSUBSCRIPT.", "description": "Figure 5 presents key results from the experiment measuring knowledge gain (\u0394knowledge).  The left subplot (a) compares the knowledge gain of humans versus various large language models (LLMs) after watching educational videos. It shows the percentage improvement in performance on a post-video assessment compared to pre-video assessment performance. The right subplot (b) further analyzes LLM responses to questions. It displays two metrics: the Wrong-to-Right Rate (percentage of initially incorrect answers that were corrected after video exposure) and the Right-to-Wrong Rate (percentage of initially correct answers that became incorrect after video exposure).  These metrics illustrate how LLMs change their answers after viewing the educational videos, revealing insights into their knowledge acquisition process.", "section": "5. Knowledge Acquisition in Adaptation Track"}, {"figure_path": "https://arxiv.org/html/2501.13826/x9.png", "caption": "Figure 6: A Case of Method Adaptation Error. The model can recall the correct knowledge from the video but fails to adapt the method to a new scenario. More error cases are analyzed in the Appendix.", "description": "This figure demonstrates a case where a large language model (LLM) fails to adapt a method learned from a video to a new, slightly different scenario.  The model correctly recalls the knowledge presented in the video about Depth-First Search (DFS) and its use in graph traversal, specifically concerning the concept of \"discovery/finishing\" timestamps. However, when presented with a graph that contains cycles (not shown in the video), the model cannot correctly apply the algorithm. The example highlights the model's limitation in adapting previously learned knowledge to new, more complex situations where subtle differences in the context significantly alter the procedure.", "section": "3.3 Comparison with Existing Benchmarks"}, {"figure_path": "https://arxiv.org/html/2501.13826/x10.png", "caption": "Figure 7: Distribution of the 100 human-annotated errors in Claude-3.5-Sonnet.", "description": "This figure shows a breakdown of the 100 errors that were manually analyzed by human annotators in the responses generated by the Claude-3.5-Sonnet model on the adaptation tasks of the Video-MMMU benchmark. The errors are categorized into five main types: Method Adaptation Error (64%), Question Misreading Error (15%), Method Selection Error (8%), Annotation Error (4%), and Refuse to Answer (4%).  The Method Adaptation Error is the most frequent, indicating that the model often struggles to apply knowledge acquired from the video to novel, yet similar problems.  Question Misreading suggests the model misunderstood aspects of the questions themselves.", "section": "3.3 Comparison with Existing Benchmarks"}, {"figure_path": "https://arxiv.org/html/2501.13826/x11.png", "caption": "Figure 8: Prompt for Adaptation track.", "description": "The prompt shown in Figure 8 is used for the Adaptation track in the Video-MMMU benchmark.  It instructs the large multimodal model (LMM) to watch and learn from the provided video and then use that knowledge to answer a question. Importantly, the image related to the question is presented only at the end of the video, forcing the LMM to fully process the video content before attempting to answer. This setup specifically tests the LMM's ability to adapt previously learned knowledge to a novel scenario.", "section": "3. Video-MMMU Dataset"}, {"figure_path": "https://arxiv.org/html/2501.13826/x12.png", "caption": "Figure 9: Prompt for determining the helpfulness of audio.", "description": "This figure shows the prompt used to instruct an AI model to determine whether audio information is necessary to answer a question based on a video.  The model is given a video clip, a question, and the corresponding answer. It must then determine if audio was required to answer the question, providing a detailed explanation of its reasoning and outputting a JSON object with a 'reason' field (containing the explanation) and a 'use_audio' field (a boolean indicating true or false). The JSON output format is also specified in the prompt.", "section": "3. Comparison with Existing Benchmarks"}, {"figure_path": "https://arxiv.org/html/2501.13826/x13.png", "caption": "Figure 10: An illustration of the dataset curation pipeline.", "description": "The figure illustrates the detailed steps involved in creating the Video-MMMU dataset.  It starts with selecting topics and curating videos from YouTube using the YouTube Data API and GPT-40. A three-tier quality assurance process is then applied, involving annotators, GPT-40, and domain experts. Next, question-answer pairs (QA) are annotated, categorizing questions into three cognitive levels (Perception, Comprehension, and Adaptation). Each level has different question types (e.g., OCR, ASR, CC, PSC, CSA, PSA). Finally, the audio processing step uses Gemini 1.5 Pro to analyze each video-question pair and assess the potential helpfulness of audio to the models.", "section": "3. Video-MMMU Dataset"}, {"figure_path": "https://arxiv.org/html/2501.13826/x14.png", "caption": "Figure 11: A sample error case in the Adaptation track: Method Selection Error by Claude-3.5-Sonnet.", "description": "This figure shows a sample error case from the Adaptation track of the Video-MMMU benchmark.  The model, Claude-3.5-Sonnet, is presented with a circuit problem where a switch is added to a parallel circuit. The model fails to correctly adapt the quantitative analysis method of calculating currents from the video example, leading to a wrong answer. The human response correctly applies the method to solve the new problem. This illustrates a Method Selection Error where the model chooses the wrong approach rather than failing to understand the concepts.", "section": "3.3 Comparison with Existing Benchmarks"}, {"figure_path": "https://arxiv.org/html/2501.13826/x15.png", "caption": "Figure 12: A sample error case in the Adaptation track: Question Misreading Error by Claude-3.5-Sonnet.", "description": "This figure showcases a case where Claude-3.5-Sonnet, a large language model, made an error in solving an adaptation task due to misinterpreting the question.  The task involves analyzing a Howe bridge truss structure under a specified load, specifically calculating the maximum force in bar CG. The model correctly identifies the necessary steps (calculating reactions, applying the method of sections) but misinterprets a key detail in the problem statement: it incorrectly assumes that bar CG is a vertical member, leading to inaccurate calculations. In contrast, a human expert correctly identifies CG as a diagonal member and performs the calculations accurately, resulting in a different answer. This highlights a limitation of the model in precisely extracting information and applying it correctly to problem solving within the context of the presented information.", "section": "3.2.2 Annotation and Quality Control"}, {"figure_path": "https://arxiv.org/html/2501.13826/x16.png", "caption": "Figure 13: A sample error case in the Adaptation track: Method Adaptation Error by GPT-4o.", "description": "This figure presents a case where GPT-40, a large language model, demonstrates a method adaptation error during an adaptation task within the Video-MMMU benchmark.  The task involves using knowledge from a video lecture on calculating displacement from velocity-time graphs. The question asks to determine the time at which an object returns to its initial position given a velocity-time graph showing both positive and negative velocities.  GPT-40 initially displays some understanding of the concept of area under the curve representing displacement, but fails to correctly adapt this knowledge to a scenario with both positive and negative areas. In contrast, a human expert correctly solves the problem by calculating the net displacement, demonstrating the model's limitation in adapting learned methods to new, similar situations.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.13826/x17.png", "caption": "Figure 14: A sample error case in the Adaptation track: Question Misreading Error by GPT-4o.", "description": "This figure shows a case where GPT-40 made an error in the Adaptation track due to misreading the question.  The video lecture covered the photoelectric effect, and the question asked for the approximate work function of a material, given a graph of maximum kinetic energy of photoelectrons versus the frequency of photons.  GPT-40 initially attempted to solve this using formulas and substituting values. However, after reviewing the video, GPT-40 correctly identified the need to find the y-intercept of the graph to find the answer. Yet, GPT-40 misread the y-intercept from the graph, resulting in an incorrect answer. This demonstrates that while the model may have learned the correct method after seeing the video, its performance was hampered by an error in visual interpretation. The human response correctly identifies the y-intercept and calculates the correct answer.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.13826/x18.png", "caption": "Figure 15: A sample error case in the Perception track.", "description": "This figure shows a sample error case in the Perception track.  The model is asked to identify three different types of muscle tissue in images from a video.  The question is designed to test the model's ability to identify the muscle tissue based on visual cues. The model fails to identify the muscles correctly in this instance, showing a weakness in its ability to perform accurate visual perception and recognition tasks. This error highlights that the model may not correctly perceive spatial arrangements in visual content, relying instead on a habitual reading order (left to right, top to bottom).", "section": "3. Video-MMMU Dataset"}, {"figure_path": "https://arxiv.org/html/2501.13826/x19.png", "caption": "Figure 16: A sample error case in the Perception track.", "description": "This figure showcases a typical error in the perception track, specifically highlighting how the model incorrectly identifies the ion channels in a neuron based on their color-coding in a video animation.  The model's response demonstrates a failure to correctly associate the colors (blue, orange/yellow, green) with the respective ion channels (sodium, mechanically-gated, potassium) as accurately depicted in the video's visual elements. The model's response indicates confusion in understanding the visual cues and their correlation to the correct labels.", "section": "3.2. QA Annotation"}, {"figure_path": "https://arxiv.org/html/2501.13826/x20.png", "caption": "Figure 17: A sample error case in the Comprehension track.", "description": "This figure showcases a specific instance where a large language model (LLM) exhibited an error in the comprehension track of the Video-MMMU benchmark.  The task involved understanding and applying a breadth-first search (BFS) algorithm to construct a spanning tree within a graph. The model correctly identified that a BFS involves horizontal exploration before vertical exploration. However, the model made mistakes when the starting node (root node) was changed from node A to node F. The model did not correctly identify which nodes were directly connected to the new root node and, as a result, incorrectly determined the nodes located at level 3 of the BFS spanning tree. This error highlights the challenges faced by LLMs in transferring their understanding of algorithms and problem-solving strategies to novel scenarios, even with minor input changes.", "section": "5.2 Findings"}, {"figure_path": "https://arxiv.org/html/2501.13826/x21.png", "caption": "Figure 18: A sample error case in the Comprehension track.", "description": "The figure showcases a comprehension error made by Claude-3.5-Sonnet. The model incorrectly uses the formula  `t = (2 * Voy) / g` to calculate the time of flight for a projectile launched from a cliff at an angle of 30 degrees. This formula is only valid for projectiles launched from ground level and returning to it.  The correct approach, demonstrated in the video lecture, involves using the quadratic equation `y(t) = Yo + Voy * t - (1/2) * g * t\u00b2` to account for the different launch and landing heights. This highlights the model's failure to fully grasp the problem-solving strategy and adapt it to a slightly modified scenario.", "section": "5.2 Findings"}, {"figure_path": "https://arxiv.org/html/2501.13826/x22.png", "caption": "Figure 19: A Wrong-to-Right example of Claude-3.5-Sonnet in the Adaptation track.", "description": "This figure showcases a successful instance of knowledge acquisition from a video lecture, demonstrated by the Claude-3.5-Sonnet model. Initially, the model incorrectly calculated the maximum flow in a network using the Ford-Fulkerson algorithm. However, after viewing a video explaining the algorithm and its steps, the model revised its approach. It correctly identified augmenting paths, respected capacity constraints, and applied flow conservation rules to arrive at the accurate solution. This highlights the model's ability to learn and utilize the information presented in the video, thereby improving its performance in solving network flow problems.", "section": "5.2 Findings"}, {"figure_path": "https://arxiv.org/html/2501.13826/x23.png", "caption": "Figure 20: A Wrong-to-Right example of Claude-3.5-Sonnet in the Adaptation track.", "description": "This figure shows a successful knowledge acquisition example from a video lecture on thin film interference.  Claude-3.5-Sonnet initially incorrectly calculates the film thickness needed for maximum constructive interference, due to a misunderstanding of phase shifts at the boundaries between materials with different refractive indices. After watching the video, however, the model correctly identifies the phase shift based on the relative refractive indices and applies the correct formula for constructive interference to determine the proper film thickness. This demonstrates the model's ability to learn from video and correct its understanding of complex physics principles.", "section": "3.3 Comparison with Existing Benchmarks"}]