[{"content": "|               | 5 turns                                      | 8 turns                                      |\n| :------------- | :-------------------------------------------- | :-------------------------------------------- |\n|               | train | test | test-noisy | train | test | test-noisy |\n| # sessions    | 13584 | 2017 | 2017       | 8730  | 1325 | 1325       |\n| # unique turns | 21092 | 3069 | 3069       | 16990 | 2480 | 2480       |\n| # words in utterance | 18.87  | 20.28 | 20.28      | 18.71  | 20.42 | 20.42      |\n| # speakers in each session | 2.83   | 2.85 | 2.85       | 3.43   | 3.47 | 3.47       |\n| # face tracks per clip | 2.41   | 3.12 | 2.50       | 2.39   | 3.14 | 2.52       |\n| avg. secs per face track | 2.31   | 2.71 | 2.72       | 2.30   | 2.74 | 2.73       |\n| % speakers not in current clip | 13.43  | 1.03 | 19.26      | 13.51  | 1.10 | 18.93      |\n| % speakers not in all clips | 6.13   | 0.17 | 1.13       | 5.57   | 0.14 | 0.44       |\n| # faces per frame | 1.61   | 2.20 | 1.76       | 1.60   | 2.21 | 1.78       |\n| % speakers not in current frame | 24.05  | 6.52 | 25.64      | 24.15  | 6.42 | 25.30      |\n| % speakers not in all frames | 9.53   | 1.01 | 3.32       | 7.45   | 0.42 | 1.37       |", "caption": "Table 1: Dataset Statistics of Friends-MMC. We provide a train set, a test set and a more challenging test-noisy set.", "description": "Table 1 presents a statistical overview of the Friends-MMC dataset. It breaks down the dataset into three subsets: a training set, a standard test set, and a more challenging 'test-noisy' set.  For each set, it provides key statistics including the number of sessions, unique turns, words per utterance, average number of speakers per session, the percentage of speakers whose faces are not detected in the current frame or clip, and the number of face tracks per clip.  These statistics offer valuable insights into the dataset's composition and complexity, highlighting the potential challenges for modeling multi-modal multi-party conversations.", "section": "Friends-MMC Dataset"}, {"content": "|               |                                          | 5 turns | 5 turns | 8 turns | 8 turns |\n| :------------ | :--------------------------------------- | :------ | :------ | :------ | :------ |\n|               |                                          |         | noisy   |         | noisy   |\n| 0             | random                                   | 31.82   | 32.61   | 28.54   | 29.03   |\n|               | (std.dev.)                              | (0.25)  | (0.47)  | (0.49)  | (0.27)  |\n| *Frame Only*  |                                          |         |         |         |         |\n| 1             | $M_{1}$(CNN)                             | 72.88   | 63.72   | 72.90   | 62.51   |\n| *Video Only* |                                          |         |         |         |         |\n| 2             | $M_{1}$(TalkNet)                          | 80.89   | 70.91   | 81.00   | 70.50   |\n| *Text Only*  |                                          |         |         |         |         |\n| 3             | $M_{2}$                                  | 33.24   | 33.85   | 29.09   | 29.33   |\n| 4             | GPT 3.5 (3-shot)                         | 37.21   | 37.24   | 33.35   | 32.81   |\n| *Use image and text modality* |                     |         |         |         |         |\n| 5             | Violet                                   | 32.66   | 33.16   | 27.73   | 28.86   |\n| 6             | LLaVA v1.5-13B                           | 46.30   | 42.39   | 45.73   | 41.41   |\n| 7             | Emu-14B                                 | 61.76   | 58.23   | 60.96   | 56.46   |\n| 8             | $M_{1}$(CNN) + $M_{2}$                   | 75.81   | 68.61   | 74.53   | 67.21   |\n| 9             | $M_{1}$(CNN) + $M_{2}^{\normalsize \u2020}$     | 84.90   | 78.01   | 90.80   | 83.93   |\n| 10            | GPT-4o (0-shot)                          | 66.36   | 65.60   | 63.64   | 61.02   |\n| 11            | Human                                    | 82.25   | -       | 84.49   | -       |\n| *Use video and text modality* |                     |         |         |         |         |\n| 12            | $M_{1}$(TalkNet) + $M_{2}$               | 83.21   | 74.12   | 83.60   | 75.00   |\n| 13            | $M_{1}$(TalkNet) + $M_{2}^{\normalsize \u2020}$ | 90.88   | 83.09   | 95.10   | 89.69   |", "caption": "Table 2: Accuracy on the test and test-noisy set of Friends-MMC. M1subscript\ud835\udc401M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and M2subscript\ud835\udc402M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT denote the visual and textual model of our baseline method, respectively. For M1subscript\ud835\udc401M_{1}italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, we use CNN or TalkNet to take image or video as input. \u2020\u00a0indicates that we use ground truths instead of textual model outputs (M2subscript\ud835\udc402M_{2}italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT) to serve as upper bounds.", "description": "Table 2 presents the accuracy results of the proposed baseline model for conversation speaker identification on the Friends-MMC dataset.  It compares performance using different combinations of modalities: image-only (CNN), video-only (TalkNet), text-only (M2 and GPT-3.5), image+text, and video+text.  The table also shows results from various pre-trained models such as Violet, LLaVA, and Emu for comparison, and includes a human performance baseline.  The 'test-noisy' set is a more challenging subset of the test set with 20% of face tracks randomly removed. The results show how using both visual and textual information improves accuracy significantly over using a single modality, and that the proposed baseline outperforms pre-trained models.", "section": "Conversation Speaker Identification"}, {"content": "| Model | Speaker | 5 turns | 8 turns |\n|---|---|---|---| \n| Llama2-7B | No | 30.69 | 36.98 |\n|  | Random | 31.23 | 43.32 |\n|  | Random History | 31.63 | 43.40 |\n|  | Shuffled | 35.20 | 48.60 |\n|  | Ground truth | 36.89 | 49.36 |\n|  | $M_{1}$(CNN) + $M_{2}$ | 34.16 | 45.81 |\n|  | $M_{1}$(TalkNet) + $M_{2}$ | 34.56 | 46.64 |\n| Emu-14B | No | 30.49 | 31.09 |\n|  | Random | 29.35 | 31.55 |\n|  | Random History | 29.45 | 31.25 |\n|  | Shuffled | 33.02 | 35.17 |\n|  | Ground truth | 34.06 | 36.30 |\n|  | $M_{1}$(CNN) + $M_{2}$ | 31.98 | 33.89 |\n|  | $M_{1}$(TalkNet) + $M_{2}$ | 32.97 | 34.64 |", "caption": "Table 3: Accuracy of conversation response prediction by selecting one from a set of ten utterances as candidates.", "description": "This table presents the accuracy of conversation response prediction results.  The model predicts the most likely final utterance from a set of ten candidate utterances, given the visual context and preceding utterances.  Results are broken down by different experimental conditions, including whether the model was trained with ground truth speaker information, random speaker information, shuffled speaker names, or speaker information predicted by the model. The accuracy is shown for both 5-turn and 8-turn conversations.", "section": "Conversation Response Prediction"}]