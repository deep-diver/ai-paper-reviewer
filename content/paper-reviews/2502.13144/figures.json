[{"figure_path": "https://arxiv.org/html/2502.13144/x1.png", "caption": "Figure 1: Different training paradigms of end-to-end autonomous driving (AD).", "description": "This figure compares three different training approaches for end-to-end autonomous driving models:  Imitation Learning (IL), simulator-based Reinforcement Learning (RL), and the proposed 3DGS-based RL method.  The IL approach uses human driving demonstrations to train the model, but suffers from causal confusion and an open-loop gap. The simulator-based RL approach addresses these issues by training in a simulator, but it introduces the Sim2Real gap and may not accurately model real-world conditions. The 3DGS-based RL approach combines the strengths of both methods, using a highly realistic digital replica of the real world to train the model via RL, while incorporating IL for better alignment with human driving behavior. This approach offers improved safety and addresses the limitations of the other two methods.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13144/x2.png", "caption": "Figure 2: Overall framework of RAD. RAD takes a three-stage training paradigm.\nIn the perception pre-training, ground-truths of map and agent are used to guide instance-level tokens to encode corresponding information. In the planning pre-training stage, large-scale driving demonstrations are used to initialize the action distribution. In the reinforced post-training stage, RL and IL synergistically fine-tune the AD policy.", "description": "RAD's training process consists of three stages.  First, perception pre-training uses ground truth map and agent data to train instance-level token encoders for scene understanding.  Second, planning pre-training leverages large-scale driving demonstrations to initialize the action distribution of the autonomous driving policy. Finally, reinforced post-training uses reinforcement learning (RL) and imitation learning (IL) together to fine-tune the policy, combining the strengths of both approaches to achieve optimal performance.  RL handles the causal relationships and the open-loop problem, while IL maintains human-like driving behavior.", "section": "3. RAD"}, {"figure_path": "https://arxiv.org/html/2502.13144/x3.png", "caption": "Figure 3: Post-training. N\ud835\udc41Nitalic_N workers parallelly run. The generated rollout data (st,at,rt+1,st+1,\u2026)subscript\ud835\udc60\ud835\udc61subscript\ud835\udc4e\ud835\udc61subscript\ud835\udc5f\ud835\udc611subscript\ud835\udc60\ud835\udc611\u2026(s_{t},a_{t},r_{t+1},s_{t+1},...)( italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_a start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT , \u2026 ) are recorded in a rollout buffer. Rollout data and human driving demonstrations are used in RL- and IL-training steps to fine-tune the AD policy synergistically.", "description": "The figure illustrates the post-training stage of the RAD model, where N parallel workers simultaneously interact with the 3DGS environment. Each worker generates a sequence of data, including states (s), actions (a), rewards (r), and next states (s), which is then collected in a buffer.  This data, along with human driving demonstrations, are used to refine the driving policy using both reinforcement learning (RL) and imitation learning (IL) techniques, combining their strengths for improved performance.", "section": "3. RAD"}, {"figure_path": "https://arxiv.org/html/2502.13144/x4.png", "caption": "Figure 4: Example diagram of four types of reward sources. (1): Collision with a dynamic obstacle ahead triggers a reward rdcsubscript\ud835\udc5fdcr_{\\text{dc}}italic_r start_POSTSUBSCRIPT dc end_POSTSUBSCRIPT. (2): Hitting a static roadside obstacle incurs a reward rscsubscript\ud835\udc5fscr_{\\text{sc}}italic_r start_POSTSUBSCRIPT sc end_POSTSUBSCRIPT. (3): Moving onto the curb exceeds the positional deviation threshold dmaxsubscript\ud835\udc51maxd_{\\text{max}}italic_d start_POSTSUBSCRIPT max end_POSTSUBSCRIPT, triggering a reward rpdsubscript\ud835\udc5fpdr_{\\text{pd}}italic_r start_POSTSUBSCRIPT pd end_POSTSUBSCRIPT. (4): Drifting toward the adjacent lane exceeds the heading deviation threshold \u03c8maxsubscript\ud835\udf13max\\psi_{\\text{max}}italic_\u03c8 start_POSTSUBSCRIPT max end_POSTSUBSCRIPT, triggering a reward rhdsubscript\ud835\udc5fhdr_{\\text{hd}}italic_r start_POSTSUBSCRIPT hd end_POSTSUBSCRIPT.", "description": "Figure 4 illustrates the four reward components used in the reinforcement learning process of the autonomous driving system.  Each component penalizes specific unsafe behaviors:\n\n1.  **Dynamic Collision (\ud835\udc5fdcr_{\n  dc})**: A negative reward is given if the vehicle collides with a moving obstacle (e.g., another car or pedestrian).\n2.  **Static Collision (\ud835\udc5fscr_{\n  sc})**: A negative reward is given if the vehicle collides with a stationary obstacle (e.g., a curb or barrier).\n3.  **Positional Deviation (\ud835\udc5fpdr_{\n  pd})**: A negative reward is given if the vehicle deviates from the optimal trajectory by more than a threshold distance (dmax).\n4.  **Heading Deviation (\ud835\udc5fhdr_{\n  hd})**: A negative reward is given if the vehicle's heading angle deviates from the optimal trajectory by more than a threshold angle (\u03c8max).", "section": "3. RAD"}, {"figure_path": "https://arxiv.org/html/2502.13144/x5.png", "caption": "Figure 5: Closed-loop qualitative comparisons between the IL-only policy and RAD in 3DGS environments. Rows 1-2 correspond to Yield to Pedestrians. Rows 3-4 correspond to Unprotected Left-turn.", "description": "This figure presents a qualitative comparison of the performance of an Imitation Learning (IL)-only policy versus the proposed RAD policy in handling complex driving scenarios within a 3DGS (3D Gaussian Splatting) simulated environment.  The scenarios depicted showcase the challenges of yielding to pedestrians (Rows 1-2) and executing unprotected left turns (Rows 3-4). By visually comparing the behaviors of both policies under these conditions, the figure highlights RAD's superior ability to navigate these complex situations safely and smoothly, while the IL-only approach demonstrates limitations in handling such challenging conditions.", "section": "4. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.13144/x6.png", "caption": "Figure A1: \nMore Qualitative Results. Comparison between the IL-only policy and RAD in various driving scenarios: Detour (Rows 1-2), Crawl in Dense Traffic (Rows 3-4), Traffic Congestion (Rows 5-6), and U-turn(Rows 7-8).", "description": "Figure A1 presents a qualitative comparison of the performance of the IL-only policy and RAD across diverse driving scenarios.  The scenarios are displayed in rows, with each row showcasing a different driving situation.  Rows 1-2 illustrate a detour maneuver; Rows 3-4 show navigation through dense traffic conditions; Rows 5-6 depict maneuvering through congested traffic; and Rows 7-8 demonstrate successful U-turn execution. For each scenario, the figure displays a sequence of images, allowing visual comparison between the IL-only policy's and RAD's performance in terms of smoothness, collision avoidance, and adherence to the driving lanes.", "section": "A. Appendix"}]