[{"content": "| Our testset |\n|---|---| \n| **Models** | **en-in** |\n| NLLB | 30.73 / 57.62 |\n| LoRA FT | **48.98 / 71.99** |\n| IT2 | 39.66 / 66.49 |", "caption": "Table 1: \nExample translations from English to Hindi in the Scientific/Technical domain. \nSentences marked as \u2020 are in-domain, while \u2021 are out-of-domain. \nThe words in blue are terms with multiple meanings, that tend to get translated incorrectly. \nThe words in green represent the correct, expected translation by the model for the blue word in the given context. The words in red represent incorrect translations.", "description": "This table showcases example translations from English to Hindi, focusing on scientific and technical terminology.  It highlights the challenges of machine translation in this domain by comparing the output of a model to professional translations.  In-domain and out-of-domain sentences are indicated, and the table analyzes words with multiple meanings, showing where the model translates correctly (green), incorrectly (red), or with an alternative meaning (blue).", "section": "Where does Present-day MT fail?"}, {"content": "| **Flores+**   |\n|---|---| \n| **Models** | **en-in** |\n|---|---| \n| NLLB  | 19.73 / 54.27 |\n| LoRA FT | 22.04 / 57.33 |\n| IT2 | **24.08 / 59.45** |", "caption": "Table 2: Results are in the form <bleu>/<chrf++>. \nThese scores represent the average of all 8 languages covered by the dataset. \nAll models were evaluated without using beam-search or sampling.", "description": "Table 2 presents the BLEU and chrF++ scores for different machine translation models on the Flores+ benchmark.  The scores are averages across all eight Indian languages included in the Shiksha dataset.  Crucially, these evaluation metrics were calculated without using beam search or sampling, which ensures a fair comparison between models and highlights the inherent translation quality of each.", "section": "5 Evaluation"}, {"content": "| Parameter | Setting |\n|---|---| \n| peft-type | LORA |\n| rank | 256 |\n| lora alpha | 256 |\n| lora dropout | 0.1 |\n| rslora | True |\n| target modules | all-linear |\n| learning rate | 4e-5 |\n| optimizer | adafactor |\n| data-type | BF-16 |\n| epochs | 1 |", "caption": "Table 3: Hyperparameters for our 3rd approach. \nFirst approach was trained for 10 epochs and second for 4 epochs seperately", "description": "Table 3 presents the hyperparameters used for the third training approach of the Shiksha model.  The first and second approaches used different training durations: 10 epochs for the first and 4 epochs for the second.  The table details the settings for parameters like the type of PEFT (Parameter-Efficient Fine Tuning) method employed (LoRA), the rank, alpha, dropout rate for LoRA, whether relative LoRA (rslora) was used, which model modules were targeted, the learning rate, optimizer used (Adafactor), data type (BF-16), and the number of training epochs.", "section": "5.2 Training"}]