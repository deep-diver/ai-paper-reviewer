[{"figure_path": "https://arxiv.org/html/2412.09025/x1.png", "caption": "Figure 1: Translation Pair Counts (in thousands)", "description": "This figure shows the number of translation pairs (in thousands) in the Shiksha dataset. The dataset includes translations between 8 Indian languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Tamil, Telugu) and English. The heatmap shows the number of translation pairs between each pair of languages. For example, the top left cell (0) represents the number of Bengali-to-Bengali pairs. The numbers increase along the diagonal, indicating that more translation pairs exist between a language and itself.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.09025/x3.png", "caption": "Figure 2: Average LABSE score across language pairs", "description": "This figure shows the average LABSE (Language-Agnostic BERT Sentence Embedding) similarity scores for all language pairs in the dataset.  Higher scores indicate greater similarity, suggesting higher confidence in the quality of the aligned sentence pairs used to create the parallel corpus. The x-axis represents different language pairs, and the y-axis represents the average LABSE similarity score for each pair. The plot visually demonstrates the consistency of alignment quality across various language combinations, with scores generally above 0.75.", "section": "4.5 Data Analysis"}, {"figure_path": "https://arxiv.org/html/2412.09025/x4.png", "caption": "Figure 3: A sample page from a bilingual document", "description": "This figure shows a sample page from a bilingual document used in the dataset creation.  It demonstrates the alternating English and Indic text within the transcriptions, interspersed with timestamps and video snapshots. This highlights the format of the raw data before cleaning and extraction steps detailed in Section 4.2, Data Cleaning and Extraction.", "section": "4 The Dataset"}, {"figure_path": "https://arxiv.org/html/2412.09025/x5.png", "caption": "Figure 4: Chrf++ comparison between NLLB, IT2 and our model across all Indian languages. \nThe size of the bubble represents the population of the speakers.", "description": "Figure 4 presents a comparison of chrF++ scores for three machine translation models across eight Indian languages: NLLB, IndicTrans2 (IT2), and the model proposed in the paper.  The chart displays the performance of each model on each language, enabling a direct comparison. The size of each bubble plotted correlates with the number of speakers of that language, visually representing the scale of each language's user base.", "section": "5.3 Evaluation"}, {"figure_path": "https://arxiv.org/html/2412.09025/extracted/6063910/assets/screenshot.png", "caption": "Figure 5: A screenshot from the Translingua tool", "description": "This figure is a screenshot of the Translingua tool in action.  It showcases the tool's user interface, which displays a video lecture in English alongside its translation in Hindi. The interface also shows timestamps, speech-to-text transcriptions of the audio, and the possibility of text-to-speech functionality. The overall design aims to aid users in the accurate and efficient translation of video lectures.", "section": "7 Conclusion"}, {"figure_path": "https://arxiv.org/html/2412.09025/x6.png", "caption": "Figure 6: Feedback on Translation Quality from a subset of Users", "description": "A pie chart summarizing user feedback on the quality of machine translations produced by the Translingua tool.  The majority of respondents (76.5%) rated the translations as \"Excellent\", with a smaller portion rating them as \"Good\" (17.6%). The remaining responses fell into the categories of \"Average\", \"Poor\", and \"Very Poor\", indicating a high level of user satisfaction with the translation quality.", "section": "7 Conclusion"}]