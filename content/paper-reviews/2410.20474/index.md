---
title: "GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation"
summary: "GrounDiT achieves precise spatial grounding in text-to-image generation using a novel training-free approach that transplants denoised image patches into specified regions, significantly improving spa..."
categories: ["AI Generated"]
tags: ["üîñ 24-10-27", "ü§ó 24-10-29"]
showSummary: true
date: 2024-10-27
draft: false
---

{{< keyword >}} 2410.20474 {{< /keyword >}}

### TL;DR


{{< lead >}}

Current training-free approaches for spatially grounding text-to-image models often struggle with accurate object placement within bounding boxes. They typically rely on noisy image updates via backpropagation from custom loss functions, resulting in imprecise control.  This limits the flexibility and precision of user-guided image generation.



This paper introduces GROUNDIT, a novel training-free technique that utilizes the flexibility of Diffusion Transformers. GROUNDIT employs a two-stage approach: global update for initial refinement and local update for fine-grained control. The core innovation is a **noisy patch transplantation mechanism** which cultivates and transplants denoised patches into designated bounding box regions. This approach enables robust spatial grounding, achieving **state-of-the-art performance** on benchmark datasets, particularly when handling complex scenarios with multiple or small bounding boxes.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.20474" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.20474" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is important because it presents **GROUNDIT**, a novel training-free method for precise spatial control in text-to-image generation. This addresses a key challenge in the field, improving the accuracy and controllability of AI image generation.  It leverages the unique properties of diffusion transformers, opening new avenues for research in training-free spatial grounding and high-quality image synthesis. The findings are significant for researchers aiming to enhance user control in AI-powered image generation systems.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} GROUNDIT offers precise spatial control in text-to-image generation without requiring model retraining. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The method leverages a novel noisy patch transplantation technique enabled by the semantic sharing property of Diffusion Transformers. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} GROUNDIT outperforms existing training-free spatial grounding approaches on benchmark datasets. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](https://ai-paper-reviewer.com/2410.20474/figures_1_0.png)

> üîº Figure 1 shows spatially grounded images generated by the proposed GROUNDIT model, highlighting its ability to precisely place objects within designated bounding boxes compared to existing methods.
> <details>
> <summary>read the caption</summary>
> Figure 1: Spatially grounded images generated by our GROUNDIT. Each image is generated based on a text prompt along with bounding boxes, which are displayed in the upper right corner of each image. Compared to existing methods that often struggle to accurately place objects within their designated bounding boxes, our GROUNDIT enables more precise spatial control through a novel noisy patch transplantation mechanism.
> </details>





![](https://ai-paper-reviewer.com/2410.20474/charts_17_0.png)

> üîº The chart shows that as the amount of joint token denoising increases, the LPIPS score between two generated images decreases, indicating increased similarity.
> <details>
> <summary>read the caption</summary>
> Figure 7: LPIPS score between two generated images with varying Œ≥ value. A gradual decrease in LPIPS [52] indicates that joint token denoising progressively enhances the similarity between the generated images.
> </details>





{{< table-caption >}}
<table id='7' style='font-size:14px'><tr><td>Method</td><td>Spatial (%)</td><td>HRS Size (%)</td><td>Color (%)</td><td>DrawBench Spatial (%)</td></tr><tr><td colspan="5">Backbone: Stable Diffusion 41</td></tr><tr><td>Stable Diffusion 41</td><td>8.48</td><td>9.18</td><td>12.61</td><td>12.50</td></tr><tr><td>PixArt-a 8</td><td>17.86</td><td>11.82</td><td>19.10</td><td>20.00</td></tr><tr><td>Layout-Guidance</td><td>16.47</td><td>12.38</td><td>14.39</td><td>36.50</td></tr><tr><td>Attention-Refocusing 38</td><td>24.45</td><td>16.97</td><td>23.54</td><td>43.50</td></tr><tr><td>BoxDiff 48</td><td>16.31</td><td>11.02</td><td>13.23</td><td>30.00</td></tr><tr><td>R&B 47</td><td>30.14</td><td>26.74</td><td>32.04</td><td>55.00</td></tr><tr><td colspan="5">Backbone: PixArt-ÔøΩ 8</td></tr><tr><td>PixArt-R&B</td><td>37.13</td><td>20.76</td><td>29.07</td><td>60.00</td></tr><tr><td>GROUNDIT (Ours)</td><td>45.01</td><td>27.75</td><td>35.67</td><td>60.00</td></tr></table>{{< /table-caption >}}

> üîº Table 1 quantitatively compares the spatial grounding accuracy of different methods on the HRS and DrawBench datasets, highlighting GROUNDIT's superior performance.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](https://ai-paper-reviewer.com/2410.20474/figures_4_0.png)

> üîº The figure illustrates the two-stage denoising process in GROUNDIT, showing how global and local updates are performed using cross-attention maps and noisy patch transplantation.
> <details>
> <summary>read the caption</summary>
> Figure 2: A single denoising step of GROUNDIT consists of two stages. Stage 1 (Sec. 5.1) performs Global Update, which updates the noisy image xt using a custom loss function and obtains t. Stage 2 (Sec. 5.3) performs Local Update, providing fine-grained control over individual bounding boxes through a novel noisy patch cultivation-transplantation technique.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_6_0.png)

> üîº This figure illustrates the process of joint token denoising in diffusion transformers and demonstrates the phenomenon of semantic sharing, where denoising two images jointly leads to semantically similar outputs.
> <details>
> <summary>read the caption</summary>
> Figure 3: (A) Joint Token Denoising (Alg. 1). Two different noisy images, xt and yt, are each assigned positional embeddings based on their respective sizes. The two sets of image tokens are then merged and passed through DiT for a denoising step. Afterward, the denoised tokens are split back into xt‚àí1 and yt‚àí1. (B), (C) Semantic Sharing. Denoising two noisy images using joint token denoising results in semantically correlated content between the generated images. Here, y indicates that joint token denoising is during the initial 100% of the timesteps, after which the images are denoised for the remaining steps.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_9_0.png)

> üîº Figure 4 qualitatively compares the spatial grounding performance of GROUNDIT against several baselines across various text prompts and bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparisons between our GROUNDIT and baselines. Leftmost column shows the input bounding boxes, and columns 2‚Äì6 include the baseline results. The rightmost column includes the results of our GROUNDIT.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_14_0.png)

> üîº The figure shows spatially grounded images generated by the proposed method, GROUNDIT, showcasing its ability to accurately place objects within designated bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 1: Spatially grounded images generated by our GROUNDIT. Each image is generated based on a text prompt along with bounding boxes, which are displayed in the upper right corner of each image. Compared to existing methods that often struggle to accurately place objects within their designated bounding boxes, our GROUNDIT enables more precise spatial control through a novel noisy patch transplantation mechanism.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_14_1.png)

> üîº Figure 5 shows example images generated by GrounDiT with varying aspect ratios and sizes, each based on a text prompt and bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GrounDiT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_17_0.png)

> üîº Figure 5 shows example images generated by the proposed GROUNDIT model with varying aspect ratios and sizes, demonstrating its ability to accurately place objects within specified bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_17_1.png)

> üîº Figure 5 shows examples of images generated by GrounDiT with varying aspect ratios and sizes, demonstrating the model's ability to accurately place objects within specified bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GrounDiT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_17_2.png)

> üîº Figure 5 shows examples of images generated by the proposed GROUNDIT model with varying aspect ratios and sizes, each based on a text prompt and bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_17_3.png)

> üîº Figure 5 shows examples of images generated by the GROUNDIT model with different aspect ratios and bounding boxes, demonstrating the model's ability to generate images that accurately reflect the given text prompts and spatial constraints.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_17_4.png)

> üîº Figure 5 shows examples of images generated by the proposed GROUNDIT model, demonstrating its ability to generate images with varying aspect ratios and sizes while maintaining accurate spatial grounding.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_19_0.png)

> üîº Figure 4 presents a qualitative comparison of GROUNDIT against several baseline methods for spatially grounded image generation, showcasing GROUNDIT's superior performance in handling complex grounding conditions.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparisons between our GROUNDIT and baselines. Leftmost column shows the input bounding boxes, and columns 2-6 include the baseline results. The rightmost column includes the results of our GROUNDIT.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_0.png)

> üîº Figure 5 shows examples of images generated by the GrounDiT model, demonstrating its ability to generate images with varying aspect ratios and sizes while accurately placing objects within specified bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GrounDiT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_1.png)

> üîº Figure 1 shows spatially grounded images generated by the proposed GROUNDIT model, highlighting its ability to accurately place objects within specified bounding boxes compared to existing methods.
> <details>
> <summary>read the caption</summary>
> Figure 1: Spatially grounded images generated by our GROUNDIT. Each image is generated based on a text prompt along with bounding boxes, which are displayed in the upper right corner of each image. Compared to existing methods that often struggle to accurately place objects within their designated bounding boxes, our GROUNDIT enables more precise spatial control through a novel noisy patch transplantation mechanism.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_2.png)

> üîº The figure illustrates the two-stage denoising process in the GROUNDIT framework, showing the global update stage using cross-attention maps and the local update stage using noisy patch transplantation for fine-grained spatial control.
> <details>
> <summary>read the caption</summary>
> Figure 2: A single denoising step of GROUNDIT consists of two stages. Stage 1 (Sec. 5.1) performs Global Update, which updates the noisy image xt using a custom loss function and obtains t. Stage 2 (Sec. 5.3) performs Local Update, providing fine-grained control over individual bounding boxes through a novel noisy patch cultivation-transplantation technique.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_3.png)

> üîº The figure illustrates the two-stage denoising process in the proposed GROUNDIT model, showing the global update stage and the local update stage with noisy patch cultivation and transplantation.
> <details>
> <summary>read the caption</summary>
> Figure 2: A single denoising step of GROUNDIT consists of two stages. Stage 1 (Sec. 5.1) performs Global Update, which updates the noisy image xt using a custom loss function and obtains t. Stage 2 (Sec. 5.3) performs Local Update, providing fine-grained control over individual bounding boxes through a novel noisy patch cultivation-transplantation technique.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_4.png)

> üîº Figure 5 shows example images generated by the proposed GROUNDIT model, highlighting its ability to generate images with diverse aspect ratios and sizes according to the given bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 5: Spatially grounded images generated by our GROUNDIT with varying aspect ratios and sizes. Each image is generated based on a text prompt along with bounding boxes, which are displayed next to (or below) each image.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_5.png)

> üîº The figure illustrates the two-stage denoising process of the GROUNDIT model, showing global and local updates for fine-grained spatial control.
> <details>
> <summary>read the caption</summary>
> Figure 2: A single denoising step of GROUNDIT consists of two stages. Stage 1 (Sec. 5.1) performs Global Update, which updates the noisy image xt using a custom loss function and obtains t. Stage 2 (Sec. 5.3) performs Local Update, providing fine-grained control over individual bounding boxes through a novel noisy patch cultivation-transplantation technique.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_6.png)

> üîº Figure 9 shows additional examples of images generated by the GROUNDIT model, demonstrating its ability to accurately place objects within their corresponding bounding boxes, even in more complex scenes.
> <details>
> <summary>read the caption</summary>
> Figure 9: Additional spatially grounded images generated by out GROUNDIT.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_7.png)

> üîº The figure shows examples of spatially grounded images generated by the proposed GROUNDIT model, highlighting its ability to accurately place objects within specified bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 1: Spatially grounded images generated by our GROUNDIT. Each image is generated based on a text prompt along with bounding boxes, which are displayed in the upper right corner of each image. Compared to existing methods that often struggle to accurately place objects within their designated bounding boxes, our GROUNDIT enables more precise spatial control through a novel noisy patch transplantation mechanism.
> </details>



![](https://ai-paper-reviewer.com/2410.20474/figures_20_8.png)

> üîº Figure 9 shows additional examples of images generated by the proposed GROUNDIT model, demonstrating its ability to generate images with multiple objects precisely placed within their designated bounding boxes.
> <details>
> <summary>read the caption</summary>
> Figure 9: Additional spatially grounded images generated by out GROUNDIT.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table id='4' style='font-size:14px'><tr><td>Method</td><td>CLIP score ‚Üë</td><td>ImageReward ‚Üë</td><td>PickScore ‚Üë</td></tr><tr><td>PixArt-R&B</td><td>33.49</td><td>0.28</td><td>0.52</td></tr><tr><td>GROUNDIT (Ours)</td><td>33.63</td><td>0.44</td><td>0.48</td></tr></table>{{< /table-caption >}}
> üîº Table 2 quantitatively compares the prompt fidelity of images generated by PixArt-R&B and GROUNDIT using three metrics: CLIP score, ImageReward, and PickScore.
> <details>
> <summary>read the caption</summary>
> Table 2: Quantitative comparisons on prompt fidelity on HRS benchmark [3]. Bold represents the best method.
> </details>

{{< table-caption >}}
<table id='0' style='font-size:14px'><tr><td>[20]</td><td>Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In CVPR, 2024.</td></tr><tr><td>[21]</td><td>Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: a reference-free evaluation metric for image captioning. In EMNLP, 2021.</td></tr><tr><td>[22]</td><td>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.</td></tr><tr><td>[23]</td><td>Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using text-to-image diffusion models. In ICLR, 2024.</td></tr><tr><td>[24]</td><td>Gwanghyun Kim, Hayeon Kim, Hoigi Seo, Dong Un Kang, and Se Young Chun. Beyondscene: Higher- resolution human-centric scene generation with pretrained diffusion. arXiv preprint arXiv:2404.04544, 2024.</td></tr><tr><td>[25]</td><td>Jaihoon Kim, Juil Koo, Kyeongmin Yeo, and Minhyuk Sung. Synctweedies: A general generative framework based on synchronized diffusions. arXiv preprint arXiv:2403.14370, 2024.</td></tr><tr><td>[26]</td><td>Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung- Woo Ha, and Jun- Yan Zhu. Dense text-to-image generation with attention modulation. In ICCV, 2023.</td></tr><tr><td>[27]</td><td>Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.</td></tr><tr><td>[28]</td><td>Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. In NeurIPS, 2023.</td></tr><tr><td>[29]</td><td>Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. SyncDiffusion: Coherent montage via synchronized joint diffusions. In NeurIPS, 2023.</td></tr><tr><td>[30]</td><td>Yuseung Lee and Minhyuk Sung. Reground: Improving textual and spatial grounding at no cost. arXiv preprint arXiv:2403.13589, 2024.</td></tr><tr><td>[31]</td><td>Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023.</td></tr><tr><td>[32]</td><td>Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. TMLR, 2024.</td></tr><tr><td>[33]</td><td>Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr DollÔøΩr. Microsoft coco: Common objects in context, 2015.</td></tr><tr><td>[34]</td><td>Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. arXiv preprint arXiv:2311.12891, 2023.</td></tr><tr><td>[35]</td><td>Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022.</td></tr><tr><td>[36]</td><td>Wan-Duo Kurt Ma, Avisek Lahiri, JP Lewis, Thomas Leung, and W Bastiaan Kleijn. Directed diffusion: Direct control of object placement through attention guidance. In AAAI, 2024.</td></tr><tr><td>[37]</td><td>William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023.</td></tr><tr><td>[38]</td><td>Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. arXiv preprint arXiv:2306.05427, 2023.</td></tr><tr><td>[39]</td><td>Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÔøΩller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.</td></tr><tr><td>[40]</td><td>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.</td></tr><tr><td>[41]</td><td>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.</td></tr><tr><td>[42]</td><td>Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, 2015.</td></tr><tr><td>[43]</td><td>Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022.</td></tr><tr><td>[44]</td><td>Takahiro Shirakawa and Seiichi Uchida. Noisecollage: A layout-aware text-to-image diffusion model based on noise cropping and merging. In CVPR, 2024.</td></tr></table>{{< /table-caption >}}
> üîº Table 1 quantitatively compares the spatial grounding performance of GROUNDIT against several baselines across two benchmark datasets, HRS and DrawBench, using three evaluation criteria: spatial, size, and color.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.
> </details>

{{< table-caption >}}
<table id='6' style='font-size:14px'><tr><td>Dataset</td><td>Subset of MS-COCO-2014 33</td><td>HRS-Spatial</td><td>Custom Dataset</td></tr><tr><td>Avg. # of Bounding Boxes</td><td>2.06</td><td>3.11</td><td>4.48</td></tr></table>{{< /table-caption >}}
> üîº Table 1 quantitatively compares the grounding accuracy (spatial, size, and color) of different methods on the HRS and DrawBench datasets, highlighting the superior performance of GROUNDIT.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.
> </details>

{{< table-caption >}}
<table id='0' style='font-size:14px'><tr><td>Method</td><td>Subset of MS-COCO-2014 33</td><td>HRS-Spatial</td><td>Custom Dataset</td></tr><tr><td colspan="4">Backbone: Stable Diffusion 41</td></tr><tr><td>Stable Diffusion 41</td><td>0.176</td><td>0.068</td><td>0.030</td></tr><tr><td>PixArt-a 8</td><td>0.233</td><td>0.085</td><td>0.036</td></tr><tr><td>Layout-Guidance</td><td>0.307</td><td>0.199</td><td>0.122</td></tr><tr><td>Attention-Refocusing 38</td><td>0.254</td><td>0.145</td><td>0.078</td></tr><tr><td>BoxDiff 48</td><td>0.324</td><td>0.164</td><td>0.106</td></tr><tr><td>R&B 47</td><td>0.411</td><td>0.326</td><td>0.198</td></tr><tr><td colspan="4">Backbone: PixArt-ÔøΩ 8</td></tr><tr><td>PixArt-R&B</td><td>0.418</td><td>0.334</td><td>0.206</td></tr><tr><td>GROUNDIT (Ours)</td><td>0.432</td><td>0.372</td><td>0.250</td></tr></table>{{< /table-caption >}}
> üîº Table 4 quantitatively compares the mean Intersection over Union (mIoU) scores achieved by GROUNDIT and baseline methods across three datasets with varying numbers of bounding boxes, demonstrating GROUNDIT's superior performance in spatial grounding.
> <details>
> <summary>read the caption</summary>
> Table 4: Quantitative comparisons of mIoU (‚Üë) on a subset of MS-COCO-2014 [33], HRS-Spatial [3], and our custom dataset. Bold represents the best, and underline represents the second best method.
> </details>

{{< table-caption >}}
<table id='2' style='font-size:14px'><tr><td>Method</td><td>CLIP score ‚Üë</td><td>ImageReward ‚Üë</td><td>PickScore ‚Üë (Ours - Baseline)</td></tr><tr><td colspan="4">Backbone: Stable Diffusion 41</td></tr><tr><td>Layout-Guidance</td><td>32.48</td><td>-0.401</td><td>+0.30</td></tr><tr><td>Attention-Refocusing 38</td><td>31.36</td><td>-0.508</td><td>+0.22</td></tr><tr><td>BoxDiff 48</td><td>32.57</td><td>-0.199</td><td>+0.30</td></tr><tr><td>R&B 47</td><td>33.16</td><td>-0.021</td><td>+0.26</td></tr><tr><td colspan="4">Backbone: PixArt-ÔøΩ 8</td></tr><tr><td>PixArt-R&B</td><td>33.49</td><td>0.280</td><td>-0.04</td></tr><tr><td>GROUNDIT (Ours)</td><td>33.63</td><td>0.444</td><td>-</td></tr></table>{{< /table-caption >}}
> üîº Table 2 quantitatively compares the prompt fidelity of images generated by GROUNDIT and PixArt-R&B using three metrics: CLIP score, ImageReward, and PickScore.
> <details>
> <summary>read the caption</summary>
> Table 2: Quantitative comparisons on prompt fidelity on HRS benchmark [3]. Bold represents the best method.
> </details>

{{< table-caption >}}
<br><table id='11' style='font-size:18px'><tr><td>16</td><td>Xt ‚Üê GlobalUpdate (x‚åÄ, t, C, G) ; // Global update (Sec.</td><td>5.1</td></tr><tr><td>17</td><td>Xt-1, {ui,t-1}~N-1 ‚Üê LocalUpdate(ÔøΩt, {ui,t}}N-1 , t,c, G) : // Local update (Sec.</td><td>5.3</td></tr><tr><td>18</td><td>return Xt-1, {ui,t-1}ÔøΩÔøΩ01;</td><td></td></tr></table>{{< /table-caption >}}
> üîº Table 1 quantitatively compares the grounding accuracy (spatial, size, and color) of different methods on the HRS and DrawBench datasets, showing GROUNDIT's superior performance.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparisons of grounding accuracy on HRS [3] and DrawBench [43] benchmarks. Bold represents the best, and underline represents the second best method.
> </details>

{{< table-caption >}}
<table id='14' style='font-size:16px'><tr><td># of bounding boxes</td><td>3</td><td>4</td><td>5</td><td>6</td></tr><tr><td>R&B 47</td><td>37.52</td><td>38.96</td><td>39.03</td><td>39.15</td></tr><tr><td>PixArt-R&B</td><td>28.31</td><td>28.67</td><td>29.04</td><td>29.15</td></tr><tr><td>GROUNDIT (Ours)</td><td>37.71</td><td>41.10</td><td>47.83</td><td>55.30</td></tr></table>{{< /table-caption >}}
> üîº This table shows the average execution time of different models (R&B, PixArt-R&B, and GROUNDIT) for varying numbers of bounding boxes in an image, demonstrating the computational cost increase with more bounding boxes.
> <details>
> <summary>read the caption</summary>
> Table 6: Comparison of average execution time based on the number of bounding boxes. Values in the table are given in seconds
> </details>

</details>


### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2410.20474/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.20474/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}