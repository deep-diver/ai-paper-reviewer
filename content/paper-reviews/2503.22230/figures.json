[{"figure_path": "https://arxiv.org/html/2503.22230/x1.png", "caption": "Figure 1: Overview of the RLHF Training Framework. Our proposed pipeline consists of two sequential phases: (1) Reward Model Training, where we construct three complementary reward models\u2014namely, the Bradley-Terry (BT) model, the Generative Reward Model (GenRM), and Reasoning Task Verifiers (RTV). Specifically, the BT model is trained on pairwise comparisons to capture human preferences, while the GenRM assigns explicit reward scores aligned with these preferences using either ground-truth solutions (for reasoning tasks) or the best-of-N selections identified by the BT model (for general tasks). The RTV component implements specialized validators tailored to specific task requirements, such as code-execution sandboxes for evaluating programming tasks; and (2) Reinforcement Learning Optimization, in which the language model is iteratively optimized using PPO under guidance from both GenRM and RTV. This stage leverages carefully selected training prompts identified through our Pre-PPO prompt-selection method and employs strategic optimization techniques to robustly enhance model performance and alignment.", "description": "This figure illustrates the RLHF training framework, which involves two main stages. The first stage is Reward Model Training, where three reward models are trained: the Bradley-Terry (BT) model (trained on pairwise comparisons of human preferences), the Generative Reward Model (GenRM) (assigns reward scores based on ground truth or BT model's best-of-N selections), and Reasoning Task Verifiers (RTV) (specialized validators for specific tasks, such as code execution for programming tasks). The second stage is Reinforcement Learning Optimization, where the language model is iteratively optimized using PPO, guided by GenRM and RTV.  Pre-PPO prompt selection is employed to identify the most challenging prompts, leading to robust performance and alignment enhancements.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/train_all.png", "caption": "Figure 2: Overall test scores from the initial run using an expanded dataset combining newly collected data (six million prompts) with the original dataset (one million prompts). Despite increasing dataset size substantially, RLHF did not yield improvements in performance. Additionally, the best performance was observed at around the 3,500-step mark, after which test scores gradually declined.", "description": "This figure displays the overall test scores obtained during the initial RLHF training experiment.  Two datasets were used: an original dataset of one million prompts and a newly collected dataset of six million prompts, combined for a total of seven million prompts.  Despite the substantial increase in data size, the results show that RLHF training did not lead to improved performance as measured by the overall test score. In fact, the model achieved its peak performance around training step 3500, after which the performance gradually decreased. This suggests that simply increasing the amount of training data may not guarantee improved performance in RLHF, implying the importance of data quality over quantity.", "section": "3.2 Pre-PPO for Training Prompts Selection"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/QRM.png", "caption": "Figure 3: Distribution of reward scores for newly collected prompts. The x-axis shows the percentage of prompts. The y-axis represents the reward score range from 0 to 1, with 0.5 indicating parity with the reference. Approximately 90% of prompts received scores above 0.5 for both small-size and large-size models, suggesting apparent superiority over reference outputs. However, manual inspection revealed that many high-scoring outputs exhibited reward hacking behavior and were qualitatively inferior to the original best-selected outcomes.", "description": "This figure shows the distribution of reward scores obtained from a newly collected dataset of prompts. The x-axis displays the percentage of prompts, while the y-axis represents the reward score, ranging from 0 to 1. A score of 0.5 indicates that the model's output is comparable to the reference output. The figure reveals that approximately 90% of the prompts received scores exceeding 0.5 for both small and large language models, seemingly outperforming the reference outputs. However, a closer manual inspection uncovered that a significant number of high-scoring outputs exhibited reward hacking behaviors and were qualitatively inferior to the initially selected best outputs.", "section": "3.2 Pre-PPO for Training Prompts Selection"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/FRAC.png", "caption": "Figure 4: The distribution of prompts across both math and coding task during the training phases", "description": "This figure shows the proportion of math and coding prompts used during each training step of the Reinforcement Learning from Human Feedback (RLHF) process.  It visually represents how the focus on math and coding tasks changes over the course of training, illustrating the strategy of prioritizing these task types early in the training pipeline before incorporating other kinds of prompts. The x-axis represents the training step, and the y-axis represents the fraction of prompts dedicated to either math or coding tasks.", "section": "3.3 Early-stage RLHF: Prioritizing Mathematical and Coding Tasks"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/AblationStudy.png", "caption": "Figure 5: Ablation study on small-size model. We do the ablation study to demonstrate the effectiveness of each strategy. Early Training Emphasis refers to early training emphasis on mathematical and coding tasks", "description": "This ablation study uses a small-sized language model to evaluate the individual contributions of three different RLHF training strategies: 1) Pre-PPO prompt selection, which prioritizes more challenging prompts, 2) Early Training Emphasis, which focuses on mathematical and coding tasks in the initial training phase, and 3) a combination of both Pre-PPO and Early Training Emphasis.  The graph likely shows the overall performance of the model across various tasks as a function of training steps, allowing comparison of the performance achieved using each strategy against the baseline (no additional strategies). This visualization helps determine the effectiveness and potential synergy between the proposed strategies in improving RLHF performance.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/codemath_first.png", "caption": "Figure 6: Early emphasis on mathematical and coding tasks significantly improves RLHF performance in both coding and STEM areas on Testset-V1.0. Notably, the coding performance with this approach surpasses the baseline within just 1000 training steps.", "description": "This figure displays the results of an ablation study that demonstrates the impact of prioritizing mathematical and coding tasks during the early stages of Reinforcement Learning from Human Feedback (RLHF) training.  Two line graphs show the performance of a model trained with this early emphasis strategy versus a baseline model across different training steps, with performance measured in terms of test scores on coding and STEM tasks. The graph clearly shows that the model that prioritizes these tasks outperforms the baseline, achieving comparable results much faster. In particular, the model that prioritizes coding and math tasks significantly surpasses the baseline model in coding performance within 1000 training steps.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/RM_CMP.png", "caption": "Figure 7: Comparison of Reward Hacking Susceptibility and Performance Trends for RTV, GenRM, and BT Reward Models During RLHF Training", "description": "This figure displays the reward hacking susceptibility and performance trends observed during Reinforcement Learning from Human Feedback (RLHF) training for three different reward models: Reasoning Task Verifiers (RTV), Generative Reward Model (GenRM), and Bradley-Terry Reward Model (BT).  It illustrates how each model's performance and resistance to reward hacking changes over the course of RLHF training.  This allows for a comparison of the effectiveness and robustness of the various reward models in guiding the model's learning process and avoiding reward hacking behavior.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/datascale_preppo.png", "caption": "Figure 8: Impact of data scaling on Pre-PPO strategy performance. The graph shows the overall RLHF performance as the percentage of newly collected training data increases from 10% to 20% and 50%. Counter-intuitively, increasing the amount of training data leads to a noticeable degradation in performance, suggesting that high-quality training prompts are scarce in real-world settings and that simply scaling data quantity does not guarantee improvement.", "description": "This figure displays the results of an experiment investigating the effect of increasing the amount of training data on the performance of the RLHF model using the Pre-PPO prompt selection strategy. The x-axis represents the percentage of newly collected prompts added to the original dataset (10%, 20%, and 50%), while the y-axis shows the overall RLHF performance.  The results show a counter-intuitive trend: increasing the size of the training dataset, even with the Pre-PPO strategy, does not improve performance.  Instead, it leads to a decrease in performance.  This suggests that the quality of the training prompts is more important than the sheer quantity.  In other words, simply adding more data may not improve the performance, and it may even hurt the performance if the additional data are low quality. This highlights the scarcity of high-quality training prompts in real-world scenarios and the need for carefully curated datasets.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Pre-PPO-BOOST.png", "caption": "Figure 9: Data Scale method boost both math and code performance.", "description": "Figure 9 presents a comparison of the performance of coding and mathematical tasks between the baseline RLHF approach and the proposed 'Data Scale' method.  The graph shows that the 'Data Scale' method, which combines Pre-PPO prompt selection and early emphasis on mathematical and coding tasks, leads to significantly improved performance on both task types compared to the baseline.  Specifically, the 'Data Scale' method demonstrates faster and more substantial gains in performance over the training steps.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/QRM_Score.png", "caption": "Figure 10: Comparison of Reward Model Scores across Different Edit Distance Bins for GenRM with and without Ground Truth.", "description": "This figure shows the relationship between reward model scores and the diversity of model responses.  It compares the performance of two versions of the Generative Reward Model (GenRM): one trained with ground truth and one without. The x-axis represents the maximum edit distance among five model responses for a given prompt. A higher edit distance indicates less diversity in the responses. The y-axis displays the average normalized reward model score for each edit distance bin. The figure demonstrates that the GenRM trained with ground truth is more sensitive to fine-grained response variations (lower edit distances) and assigns higher scores when responses show higher diversity. In contrast, the GenRM without ground truth shows a less pronounced relationship between diversity and score, reflecting a lower sensitivity to finer distinctions between responses.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Score_Diff.png", "caption": "Figure 11: Comparison of Score Difference across Different Edit Distance Bins for GenRM with and without Ground Truth, and RTV.", "description": "This figure compares the differences in reward scores assigned by three different reward models (GenRM with ground truth, GenRM without ground truth, and RTV) across various levels of response diversity.  The x-axis represents bins of maximum edit distances between responses, indicating the granularity of response differences (smaller distances mean finer-grained distinctions). The y-axis shows the normalized score differences within each bin.  The figure aims to illustrate how each reward model's sensitivity to fine-grained response variations differs and whether that impacts the effectiveness of RLHF.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/entropy.png", "caption": "(a) Response entropy change during the RLHF training process", "description": "This figure illustrates the change in response entropy throughout the reinforcement learning from human feedback (RLHF) training process.  Response entropy is a measure of the diversity of the model's generated responses. A higher entropy indicates greater response diversity, while a lower entropy suggests less diversity, with the model tending to produce more similar responses. The x-axis represents the training steps, and the y-axis shows the response entropy at each step.  The plot reveals how the diversity of the model's responses changes over the course of RLHF training.  This is a key metric to assess whether the training process is successfully improving the model's ability to generate a wide range of responses or is narrowing its output over time.", "section": "3.2 Pre-PPO for Training Prompts Selection"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_Creation.png", "caption": "(b) The comparison of response entropy change during the RLHF training process", "description": "Figure 12(b) displays the change in response entropy during RLHF training for models using GenRM without ground truth.  It compares the entropy of responses generated by the baseline RLHF model and the model trained using the improved method (DataScale). The x-axis represents training steps, while the y-axis represents the mean entropy across various responses. The plot visually demonstrates the impact of the proposed methods on maintaining response diversity throughout the training process.", "section": "3.2 Pre-PPO for Training Prompts Selection"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_Math.png", "caption": "(c) The comparison of response entropy change during the RLHF training process", "description": "This figure compares the response entropy change during RLHF training between the baseline method and the proposed method. The response entropy is calculated separately for three different types of reward models: GenRM with ground truth, GenRM without ground truth, and RTV. The figure helps to analyze the impact of the proposed method on the response diversity of the model during training and how this method affects different types of reward models differently.", "section": "3.3 Early-stage RLHF: Prioritizing Mathematical and Coding Tasks"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Entropy_RTV.png", "caption": "(d) The comparison of response entropy change during the RLHF training process", "description": "Figure 12(d) displays the change in response entropy over the course of RLHF training, specifically focusing on the performance of the RTV reward model. The graph visually represents how the diversity of model responses evolves as training progresses. It allows for a comparison between the baseline model's response diversity and the response diversity achieved by the model using the improved strategy. This comparison helps in evaluating the effectiveness of the proposed techniques in maintaining response diversity throughout the RLHF training process. The x-axis of the graph represents the training steps, while the y-axis corresponds to the mean response entropy.", "section": "3.2 Pre-PPO for Training Prompts Selection"}, {"figure_path": "https://arxiv.org/html/2503.22230/extracted/6317381/seed/img/Big_Dist.png", "caption": "Figure 12: The comparison of response entropy change during the RLHF training process", "description": "Figure 12 presents a detailed analysis of how response diversity changes during the Reinforcement Learning from Human Feedback (RLHF) training process.  It visualizes the entropy of model responses across various task categories, offering insights into the impact of different reward models and training strategies. Subfigures (a) to (d) show the change in response entropy over training steps for tasks supervised by distinct reward methods: Generative Reward Model (GenRM) with ground truth, GenRM without ground truth, and Reasoning Task Verifiers (RTV). These subfigures provide a granular view of how response diversity evolves under different reward mechanisms and highlight the effects of the proposed data selection and prioritization strategies on response diversity across different task types. The figure illustrates a decline in overall response diversity during RLHF training but highlights the importance of careful data curation and strategic training to maintain diversity.", "section": "3.2 Pre-PPO for Training Prompts Selection"}]