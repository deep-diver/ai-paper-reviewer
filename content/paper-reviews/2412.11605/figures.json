[{"figure_path": "https://arxiv.org/html/2412.11605/x1.png", "caption": "Figure 1: An example of the interfering factors (story content) in independently sampled multiple responses (Left). Refined response pairs exclude these factors, highlight the key difference (ending sentence), and lead to improved performance on iteratively trained LLaMA3-8B-Instruct (Right).", "description": "The figure shows an example of multiple responses generated from an LLM for the prompt \"Write a story and end it with 'The devil is in the details.'\"  It highlights how independently sampled responses can vary in content, such as different story titles (e.g., Hansel and Gretel vs. Little Red Riding Hood) which interferes with preference learning.  It then shows how refined responses maintain consistent story content and focus on the key requirement of the prompt, which is the ending sentence.  A bar graph on the right illustrates the improved performance achieved by using refined response pairs during the iterative training of a LLaMA3-8B-Instruct model.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.11605/x2.png", "caption": "Figure 2: SPaR iterative training framework. At iteration t\ud835\udc61titalic_t, the refiner Rtsubscript\ud835\udc45\ud835\udc61R_{t}italic_R start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT first judges the generated responses from the actor Mtsubscript\ud835\udc40\ud835\udc61M_{t}italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to collect negative data. Next, a tree-search algorithm is employed to refine these imperfect responses. Finally, using the data from the above steps, we can optimize the actor and refiner for the next iteration, aiming for continuous self-improvement.", "description": "The figure illustrates the iterative training process of SPaR.  At each iteration *t*, there's an actor model (M_t) and a refiner model (R_t), both initialized from the same base model.  The actor generates responses to instructions, and the refiner critiques these responses, identifying negative (incorrect) examples. The refiner then uses a tree-search algorithm to refine these negative responses into correct ones, creating refined response pairs. These pairs, along with the refiner's judgments, are used to train the next iteration's actor (M_{t+1}) and refiner (R_{t+1}) via DPO and RFT respectively.  This iterative process fosters continuous self-improvement in both models, leading to enhanced instruction following capabilities.", "section": "2. METHOD"}, {"figure_path": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/baseline.png", "caption": "Figure 3: Comparison with baseline methods across iterations (Cf. Figure 9 for SPaR-7B). SPaR-8B consistently surpasses all baselines.", "description": "This figure compares the performance of SPAR-8B with other baseline methods (AutoIF, SELF, Self-Rewarding, and Meta-Rewarding) on the IFEval benchmark across three iterations of training. The x-axis represents the training iteration, while the y-axis represents the average score on IFEval.  The results demonstrate that SPAR-8B consistently outperforms all baseline methods in each iteration and improves with each training iteration. The performance of GPT-4-Turbo is also included as a reference point.", "section": "3.3 ACTOR EVALUATION RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/decoding.png", "caption": "Figure 4: Synthetic data experiment results: Character Sequence Generation (left) and Start/End Story Generation (right).\nFor Character Sequence Generation, interfering pairs show rapid learning of the uppercase ratio (interfering factor) but perform worse than refinement pairs. In the Start/End Story Generation task, refinement pairs outperform interfering pairs, which even underperform the original model at step 0.", "description": "This figure presents the results of a synthetic data experiment designed to isolate the impact of interfering factors in preference learning. Two tasks are used: Character Sequence Generation and Start/End Story Generation. In the Character Sequence Generation task, the model is prompted to generate a sequence of characters with length constraints. Interfering pairs are created by introducing variation in character case. Results show that the model quickly learns the uppercase ratio in interfering pairs but performs worse on the primary instruction following objective, as compared to training with refined pairs. The Start/End Story Generation task prompts the model to generate a story with a specified beginning and ending sentence.  Here, interfering pairs contain variations in the story's middle section, which is irrelevant to the given instruction. Results show that refinement pairs outperform interfering pairs significantly. Notably, training with interfering pairs leads to worse performance than the initial model.", "section": "3.5 ABLATIONS AND ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2412.11605/extracted/6072122/figures/taxonomy.png", "caption": "Table 4: Ablation study on the actor.", "description": "This table presents an ablation study conducted on the actor model within the SPAR framework. The study examines the impact of removing key components of SPAR, specifically Tree Search and Refinement data, on the actor's performance on the IFEval and FollowBench (SSR) benchmarks. The purpose is to demonstrate the importance of these components for enhancing instruction-following capabilities. The evaluation metrics include prompt-level strict accuracy and instruction-level strict accuracy on IFEval, and the average score on FollowBench (SSR). The results show a performance drop when these components are removed, highlighting their contribution to the effectiveness of the SPAR framework.", "section": "3.5 ABLATIONS AND ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2412.11605/x3.png", "caption": "Table 5: Ablation study on the refiner.", "description": "This table presents an ablation study conducted on the refiner model, exploring the impact of different training components on its performance. It assesses how removing certain aspects like tree-search, refinement data, or iterative training affects the refiner's ability to evaluate instruction-following responses accurately, as measured by accuracy (Acc.) and F1-score on both Natural and Adversarial subsets of the LLMBar benchmark. The 'Average' columns represents the average scores across both subsets.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.11605/x4.png", "caption": "Figure 5: Comparison of decoding strategies. Model performance improves with increased inference times.", "description": "This figure compares different decoding strategies during inference for SPAR-8B-DPO-iter3 on IFEval benchmark, including greedy decoding, best-of-N, Breadth-First Search (BFS), and Depth-First Search (DFS). X-axis represents inference times, measured by the number of response generations. Y-axis is the average score on IFEval. The results demonstrate increased inference times enhance model performance. While tree search refinement (BFS and DFS)'s performance growth is slower, it ultimately gets superior results than best-of-N. Refinement could be more suitable for scaling test-time compute for the instruction-following task.", "section": "3. Experiments"}]