[{"figure_path": "https://arxiv.org/html/2412.15213/x2.png", "caption": "Figure 1: We propose CrossFlow, a general and simple framework that directly evolves one modality to another using flow matching with no additional conditioning.\nThis is enabled using a vanilla transformer without cross-attention, achieving comparable performance with state-of-the-art models on (a) text-to-image generation, and (b) various other tasks, without requiring task specific architectures.", "description": "The figure illustrates the CrossFlow framework, a novel approach for cross-modality evolution.  It directly transforms data from one modality (e.g., text) to another (e.g., image) using flow matching, without the need for additional conditioning mechanisms like cross-attention.  This is achieved through a simple vanilla transformer architecture, resulting in comparable performance to state-of-the-art models on various tasks, including text-to-image generation and other cross-modal transformations. The figure highlights the framework's simplicity and generalizability across diverse applications.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15213/x3.png", "caption": "Figure 2: CrossFlow Architecture.\nCrossFlow enables direct evolution between two different modalities.\nTaking text-to-image generation as an example, our T2I model comprises two main components: a Text Variational Encoder and a standard flow matching model.\nAt inference time, we utilize the Text Variational Encoder to extract the text latent z0\u2208\u211dh\u00d7w\u00d7csubscript\ud835\udc670superscript\u211d\u210e\ud835\udc64\ud835\udc50z_{0}\\in\\mathbb{R}^{h\\times w\\times c}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h \u00d7 italic_w \u00d7 italic_c end_POSTSUPERSCRIPT from text embedding x\u2208\u211dn\u00d7d\ud835\udc65superscript\u211d\ud835\udc5b\ud835\udc51x\\in\\mathbb{R}^{n\\times d}italic_x \u2208 blackboard_R start_POSTSUPERSCRIPT italic_n \u00d7 italic_d end_POSTSUPERSCRIPT produced by any language model.\nThen we directly evolve this text latent into the image space to generate image latent z1\u2208\u211dh\u00d7w\u00d7csubscript\ud835\udc671superscript\u211d\u210e\ud835\udc64\ud835\udc50z_{1}\\in\\mathbb{R}^{h\\times w\\times c}italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_h \u00d7 italic_w \u00d7 italic_c end_POSTSUPERSCRIPT.", "description": "CrossFlow is a framework for cross-modal generation that directly maps one modality to another using flow matching without additional conditioning.  The figure illustrates the architecture of CrossFlow for text-to-image generation. It uses a Text Variational Encoder (VE) to convert text embeddings from a language model into a latent representation (z0). This latent representation is then evolved using a standard flow matching model into an image latent space (z1) from which the final image is generated.  This process bypasses the need for noise distributions and explicit conditioning mechanisms typically found in diffusion or flow-matching models.", "section": "4. CrossFlow"}, {"figure_path": "https://arxiv.org/html/2412.15213/x4.png", "caption": "Figure 3: Performance vs. Model Parameters and Iterations. We compare the baseline of starting from noise with text cross-attention with CrossFlow, while controlling for data, model size and training steps. Left: Larger models are able to exploit the cross-modality connection better. Right: CrossFlow needs more steps to converge, but converges to better final performance. Overall, CrossFlow scales better than the baseline and can serve as the framework for future media generation models.", "description": "This figure compares the performance of CrossFlow and a standard flow matching model (baseline) for text-to-image generation.  The experiment controls for the amount of training data used, model size (number of parameters), and number of training steps.  The left panel shows how performance (measured by FID-30K) improves with larger model sizes for both methods, but CrossFlow demonstrates better scaling and higher performance at larger sizes. The right panel shows how performance changes as a function of training steps. CrossFlow requires more training steps to converge, but ultimately achieves better performance than the baseline model, again demonstrating superior scaling properties.  In summary, this figure illustrates that CrossFlow is more efficient in terms of scaling than the baseline method.", "section": "5.1 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2412.15213/x5.png", "caption": "Figure 4: CrossFlow provides visually smooth interpolations in the latent space.\nWe show images generated by linear interpolation between the first (left) and second (right) text latents.\nCrossFlow enables visually smooth transformations of object direction, composite colors, shapes, background scenes, and even object categories.\nPlease zoom in for better visualization.\nFor brevity, we display only 7 interpolating images here; additional interpolating images can be found in Appendix\u00a0C (Fig.\u00a010 and Fig.\u00a011).", "description": "This figure demonstrates the smooth transitions achievable with CrossFlow when linearly interpolating between two text-based latent representations.  The interpolation showcases seamless changes in various aspects of the generated images, including object orientation, color composition, shapes, backgrounds, and even object types.  The visual effect highlights the model's ability to navigate the latent space smoothly and predictably. Note that due to space constraints, only a subset of the interpolations are displayed here; the full set is included in Appendix C (Fig. 10 and Fig. 11).", "section": "4. Flowing from Text to Image"}, {"figure_path": "https://arxiv.org/html/2412.15213/x6.png", "caption": "Figure 5: CrossFlow allows arithmetic in text latent space.\nUsing the Text Variational Encoder (VE), we first map the input text into the latent space z0subscript\ud835\udc670z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT.\nArithmetic operations are then performed in this latent space, and the resulting latent representation is used to generate the corresponding image.\nThe latent code z0subscript\ud835\udc670z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT used to generate each image is provided at the bottom.", "description": "This figure demonstrates the novel capability of CrossFlow to perform arithmetic operations directly within the latent space of text embeddings.  A Variational Encoder first maps input text into a latent representation (z0). Then, arithmetic operations (addition and subtraction) are performed on these latent vectors. The results of these operations are then used to generate new images, which reflect the semantic changes induced by the arithmetic. The images generated, along with their corresponding latent codes (z0), are shown to illustrate how simple arithmetic manipulations in the latent space lead to meaningful alterations in the generated images.", "section": "4. CrossFlow"}, {"figure_path": "https://arxiv.org/html/2412.15213/x7.png", "caption": "(a)", "description": "This figure shows how CrossFlow directly evolves text into images for text-to-image generation.  It contrasts with traditional methods that learn a complex mapping from Gaussian noise to the target image distribution. CrossFlow learns a direct mapping from the distribution of one modality (text) to the distribution of another (image), simplifying the process and eliminating the need for noise or additional conditioning mechanisms.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15213/x8.png", "caption": "(b)", "description": "This figure shows the various tasks that CrossFlow can perform, highlighting its versatility and generalizability.  It demonstrates the ability of CrossFlow to directly evolve one modality (e.g., text) into another (e.g., image) without requiring task-specific architectures. The various tasks shown include text-to-image generation, image captioning, monocular depth estimation, and image super-resolution.  Each example visually showcases the successful transformation between modalities using the CrossFlow framework.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15213/x9.png", "caption": "(c)", "description": "This figure shows the CrossFlow architecture.  It is a general framework for cross-modal flow matching. The figure illustrates how CrossFlow directly evolves one modality (e.g., text) to another (e.g., image) using flow matching, without the need for cross-attention or a conditioning mechanism. A variational encoder processes the input data, ensuring the source distribution is compatible with the target distribution.  A standard flow matching model, using a vanilla transformer, is then used to transform the source modality into the target modality.  This architecture is shown to be efficient and effective across a range of tasks.", "section": "4. CrossFlow"}, {"figure_path": "https://arxiv.org/html/2412.15213/x10.png", "caption": "(d)", "description": "This figure demonstrates the generalizability of the CrossFlow framework by showcasing its performance on various cross-modal and intra-modal tasks.  These include text-to-image generation, image captioning, depth estimation, and image super-resolution.  Each subfigure shows example results, illustrating that CrossFlow can handle diverse media types and achieve state-of-the-art or comparable performance without requiring task-specific architectures.", "section": "4. CrossFlow"}, {"figure_path": "https://arxiv.org/html/2412.15213/x11.png", "caption": "(e)", "description": "This figure demonstrates the impact of different training strategies on the performance of the model.  It compares three approaches: (1) jointly training the variational encoder (VE) and flow matching model from the start; (2) pre-training the VE and then training the flow matching model; and (3) pre-training the VE and then jointly fine-tuning both models.  The results show that jointly training both components from the beginning yields the best performance, highlighted by the lowest FID score and highest CLIP score, indicating the best balance between visual fidelity and semantic alignment.", "section": "4.2. Training CrossFlow"}]