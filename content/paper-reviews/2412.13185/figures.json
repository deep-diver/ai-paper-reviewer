[{"figure_path": "https://arxiv.org/html/2412.13185/x1.png", "caption": "Figure 1: 2D-conditioned human motion generation. Given an image representing the target scene and a text prompt describing the desired motion, we generate a motion sequence that aligns with the text description and projects naturally onto the scene image.\nThis generated motion then serves as the control signal for the subsequent video generation tasks.", "description": "The figure illustrates the process of 2D-conditioned human motion generation.  It starts with a scene image (e.g., a beach) and a text prompt specifying the desired motion (e.g., \"a person is dancing\").  The system then generates a human motion sequence which is consistent with both the scene and the text prompt. This motion sequence is then used to control a video generation model, which creates a realistic video of a person performing the specified motion in the specified scene.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.13185/x2.png", "caption": "Figure 2: Overview. The text prompt and background scene image are encoded by the CLIP and DINO encoders, and incorporated into the model via in-context conditioning. The AdaLN layer receives the diffusion timestep as input. Our multi-conditional transformer model then generates a human motion sequence through a diffusion denoising process, aligning the generated motion with both input conditions.", "description": "This figure illustrates the architecture of the proposed model for 2D-conditioned human motion generation.  It shows how text prompts and background scene images are processed. First, CLIP and DINO encoders convert the text and image, respectively, into a shared representation.  This information is then integrated into the model using an 'in-context conditioning' approach.  A diffusion timestep is processed via an AdaLN layer, which normalizes the data based on the diffusion step. Our multi-conditional transformer model uses this combined information (text, image, timestep) to generate a human motion sequence.  The process utilizes a diffusion denoising model, iteratively refining the initial noisy motion until a clean, realistic motion sequence is produced that aligns with both the text description and the background scene. ", "section": "4. Approach"}, {"figure_path": "https://arxiv.org/html/2412.13185/x3.png", "caption": "Figure 3: Affordance-aware human generation. Our model generates human poses consistent with both text prompts and scene context, such as standing on a cliff. It also supports complex human-scene interactions, including activities like petting a dog.", "description": "Figure 3 showcases the model's ability to generate human poses that are consistent with both the text prompt and the scene's context.  Examples include a person standing on a cliff, sitting on a chair, surfing, or petting a dog, demonstrating the model's understanding of scene affordances and its capability to handle intricate human-environment interactions.", "section": "5. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.13185/x4.png", "caption": "Figure 4: Motion generation with large dynamics. Our results show motion sequences that are accurately placed and move within scenes, such as playing tennis, enabling the generation of complex human activities that are challenging for video generation models.", "description": "Figure 4 showcases the model's ability to generate human motion sequences with significant dynamic range and accurate placement within diverse scenes.  Examples include tennis, basketball, jumping rope, lunges, trampoline jumping, swinging, and cycling. These complex activities are challenging for video generation models to reproduce faithfully. The figure highlights the model's capacity to create realistic and dynamic human movement that is integrated seamlessly into the environment.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.13185/x5.png", "caption": "Figure 5: Comparison to state-of-the-art. MDM and SceneDiff produces implausible poses, MLD generates mismatched motion with the scene, and HUMANISE generates static poses.\nOur method generates coherent motion aligned with both the scene and text prompts.", "description": "Figure 5 compares the proposed method's performance to several state-of-the-art models for human motion generation.  The comparison highlights the strengths of the new approach. MDM and SceneDiff, which are other methods, produce unrealistic or implausible human poses. MLD generates motion that does not match the context of the scene. HUMANISE, yet another alternative method, generates motion that is essentially static and unchanging. In contrast, the proposed method successfully generates coherent and realistic human motions that are consistent with both the background scene and the textual description of the desired action.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.13185/x6.png", "caption": "Figure 6: Motion-guided human video generation.\nOur approach generates scene-compatible motion sequences from a scene image and text prompt, which are then used to animate a reference human using Champ\u00a0[60] or Gen-3\u00a0[11]. The generated motion ensures accurate human shapes and smooth motion in the resulting videos, outperforming SVD\u00a0[5] in preserving human geometry and motion consistency.", "description": "This figure demonstrates the application of the proposed method for generating human motion in video generation.  The model takes a scene image and a text prompt as input and generates a motion sequence compatible with the scene.  This motion sequence is then used to animate a reference human model in existing video generation frameworks, Champ and Gen-3. The results showcase the capability of the method to produce videos with accurate human shapes and smooth movements, surpassing the performance of the baseline method, SVD, in preserving human geometry and motion consistency.", "section": "5.3 Motion-guided Human Video Generation"}]