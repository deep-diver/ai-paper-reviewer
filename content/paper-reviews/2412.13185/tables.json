[{"content": "| Dataset | Motions | Texts | Scenes | Scene Representation | Scene Type |\n|---|---|---|---|---|---| \n| KIT [42] | 3.9k | 6.2k | No | No | Indoor |\n| HumanML3D [16] | 14.6k | 44.9k | No | No | Indoor |\n| HUMANISE [49] | 19.6k | 19.6k | 643 | RGBD | Indoor |\n| PROX [18] | 28k | No | 12 | RGBD | Indoor |\n| LaserHuman [10] | 3.5k | 12.3k | 11 | RGBD | Indoor/Outdoor |\n| Motion-X [32] | 81.1k | 81.1k | 81.1k | Video | Indoor/Outdoor |\n| HiC-Motion | 300k | 300k | 300k | Video | Indoor/Outdoor |", "caption": "Table 1: \nDataset statistics. HiC-Motion is the largest dataset comprising motions, text, and diverse indoor and outdoor scenes.", "description": "Table 1 presents a comparison of various human motion datasets, highlighting the number of motion sequences, text descriptions, scenes, and the type of scene representation used.  It emphasizes that the HiC-Motion dataset, introduced in this paper, is significantly larger than existing datasets and includes diverse indoor and outdoor scenes, making it suitable for training models that generate human motions conditioned on 2D scenes.", "section": "3. Humans-in-Context Motion Dataset"}, {"content": "| Methods | FID (\u2193) | Accuracy (\u2191) | Diversity (\u2191) | Multimodality (\u2191) |\n|---|---|---|---|---|\n| MDM [45] | 164.595 | 0.325 | 24.758 | 18.924 |\n| MLD [8] | 85.913 | 0.322 | 25.119 | 19.464 |\n| SceneDiff [24] | 543.769 | 0.203 | 4.217 | 3.861 |\n| HUMANISE [49] | 159.935 | 0.225 | 23.287 | 19.956 |\n| MDM+ [45] | 46.035 | 0.620 | 23.002 | 17.627 |\n| Ours-scene | 46.458 | 0.482 | 24.968 | **21.320** |\n| Ours | **44.639** | **0.661** | **26.027** | 20.130 |", "caption": "Table 2: Quantitative results. Our method achieves better quality and diversity scores compared to state-of-the-art text-conditioned, scene-conditioned, and multimodal motion generation models.", "description": "This table presents a quantitative comparison of different human motion generation models.  The models are evaluated using four metrics: FID (Frechet Inception Distance), Accuracy, Diversity, and Multimodality. Lower FID scores indicate better quality, while higher scores for Accuracy, Diversity, and Multimodality reflect better performance in those respective areas. The table shows that the proposed method ('Ours') outperforms state-of-the-art models in terms of motion quality and diversity, surpassing both text-conditioned, scene-conditioned, and multimodal approaches.", "section": "5. Experiments"}, {"content": "| Methods | Scene-Align (\u2191) | Text-Align (\u2191) | Quality (\u2191) | Total (\u2191) |\n|---|---|---|---|---|\n| MDM [45] | 2.25 | 1.35 | 1.50 | 5.10 |\n| MLD [8] | 2.85 | 1.95 | 1.90 | 6.70 |\n| SceneDiff [24] | 2.05 | 1.20 | 1.20 | 4.45 |\n| HUMANISE [49] | 2.20 | 1.45 | 1.30 | 4.95 |\n| MDM+ [45] | 2.57 | 1.73 | 1.94 | 6.24 |\n| Ours-scene | 2.90 | 2.00 | 1.95 | 6.85 |\n| Ours | **3.55** | **2.70** | **2.85** | **9.10** |", "caption": "Table 3: Automated evaluation. We report average VLM scores (0-5) for generated motions, assessing alignment with scene, text, and pose quality. Our method outperforms all evaluated baselines.", "description": "This table presents a quantitative evaluation of different human motion generation models using a Vision-Language Model (VLM).  The VLM assigns scores from 0 to 5 for three aspects: how well the generated motion aligns with the background scene, how well it aligns with the text description of the motion, and the overall quality of the generated human poses. The table compares the proposed method's performance to several state-of-the-art baselines, demonstrating its superiority in generating high-quality, scene-consistent human motions that match the given text prompts.", "section": "5. Experiments"}, {"content": "| Timestep | Text | Scene | FID (\u2193) | Accuracy (\u2191) |\n|---|---|---|---|---|\n| AdaLN | In-Context | In-Context | 44.639 | 0.661 |\n| AdaLN | In-Context | Cross-Attn | 47.656 | 0.567 |\n| In-Context | In-Context | In-Context | 62.927 | 0.554 |\n| In-Context | In-Context | Cross-Attn | 66.827 | 0.519 |", "caption": "Table 4: Ablation study. We study different transformer block designs, and choose AdaLN for timestep conditioning and In-Context for text and scene conditions as our main configuration.", "description": "This ablation study investigates the impact of different transformer block designs and conditioning methods on the performance of the model.  Specifically, it compares using AdaLN (Adaptive Layer Normalization) versus in-context conditioning for timestep information; and it compares different methods for incorporating text and scene condition information into the transformer. The goal is to determine the optimal configuration for generating human motion that aligns well with both text and scene inputs, leading to superior motion quality and accuracy.", "section": "5 Experiments"}]