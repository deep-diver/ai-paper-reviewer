[{"figure_path": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/video_driven.png", "caption": "Figure 1: \nIllustration of real-time portrait video generation. Given a portrait image and audio sequence as input, our model can generate high-fidelity animation results from full head to upper-body interaction with diverse facial expressions and style control.", "description": "This figure demonstrates the real-time portrait video generation capabilities of the ChatAnyone model.  The input consists of a single portrait image and an audio sequence. The output is a high-fidelity video of a full head and upper body avatar, exhibiting realistic and diverse facial expressions. The model allows for control over the style of the generated video.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/audio2motion.png", "caption": "Figure 2: Pipeline of upper-body video generation with hybrid control fusion, which takes both explicit facial keypoints and implicit body keypoints to conduct feature warping, while rendered hand image further inject into generator for improving the quality of hand generation.", "description": "This figure illustrates the pipeline for generating upper-body videos.  It starts with a source image which undergoes feature extraction. Simultaneously, facial keypoints (explicit control) and body keypoints (implicit control) are extracted from the source image.  These keypoints, along with rendered hand images (providing additional detailed hand control), are then used to guide a warping module. This warping module distorts the appearance features based on the motion information extracted from the keypoints.  The warped features are fed into a generator which produces the final upper-body video output, leveraging both explicit and implicit control signals for refined control over facial expressions and body movements.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/face_offset.png", "caption": "Figure 3: Illustration of hierarchical audio2motion diffusion model, including facial motion prediction with style control at bottom, and upper-body motion prediction with hands at top.", "description": "Figure 3 illustrates the hierarchical audio2motion diffusion model.  The bottom part shows facial motion prediction, which includes style control mechanisms such as reference sequence injection for style transfer. The top section shows upper-body motion prediction, driven by the output of the facial motion prediction module and audio input.  This upper-body module also incorporates hand motion generation using hand coefficients from a MANO template.  The model uses a combination of cross-attention mechanisms and adaptive layer normalization to effectively combine audio features and motion information at each level.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.21144/extracted/6313754/fig/visualization_main.png", "caption": "Figure 4: Illustration of face refine network, the left of figure shows the architecture, while the right demonstrates that more precise facial keypoints are located by adding implicit offset.", "description": "Figure 4 illustrates the architecture of a face refinement network used to enhance the realism of generated facial expressions.  The network takes as input both explicit and implicit facial keypoints. Explicit keypoints provide coarse control, while implicit offsets refine the position of the keypoints, leading to more precise and natural facial expressions.  The right side of the figure visually demonstrates how the addition of implicit offsets leads to a more accurate representation of facial keypoint locations compared to using only explicit keypoints.", "section": "3. Method"}]