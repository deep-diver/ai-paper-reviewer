{"importance": "This paper is crucial for researchers aiming to enhance MLLMs' spatial reasoning for real-world tasks like robotics. **It highlights current limitations and provides a benchmark for future developments**, spurring innovation in multimodal AI.", "summary": "MLLMs still struggle with spatial reasoning! LEGO-Puzzles benchmark reveals critical deficiencies, paving the way for AI advancement.", "takeaways": ["LEGO-Puzzles is introduced as a new benchmark to evaluate spatial reasoning in MLLMs, offering enhanced visual richness and scalability.", "Current MLLMs show limitations in spatial understanding and sequential reasoning, with performance lagging behind human capabilities.", "The benchmark includes visual question answering and image generation tasks, revealing deficiencies in MLLMs' ability to follow instructions and maintain spatial coherence."], "tldr": "Multi-step spatial reasoning is crucial for real-world applications but current Multimodal Large Language Models (MLLMs) struggle with it. Existing evaluations are often simplistic or lack scalability, failing to address multi-step aspects. To solve this problem, the authors introduce **LEGO-Puzzles, a scalable benchmark designed to evaluate spatial understanding and sequential reasoning in MLLMs through LEGO-based tasks**. The dataset consists of 1,100 VQA samples spanning 11 tasks.\n\nBy creating this benchmark, the authors conduct a comprehensive evaluation of MLLMs. The results show that even the most powerful MLLMs struggle to follow assembly instructions with a high accuracy. **LEGO-Puzzles effectively exposes deficiencies in existing MLLMs' spatial understanding, underscoring the need for further advancements in multimodal spatial reasoning**. The framework is also designed to evaluate spatially grounded image generation.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2503.19990/podcast.wav"}