[{"content": "| Training Parameters |  | \n|---|---| \n| **Learning Rate** | 1e-4 | \n| **Optimizer** | AdamW | \n| **Weight Decay** | 0.01 | \n| **Adam eps** | 1e-8 | \n| **Batch Size** | 256 | ", "caption": "Table 1: Details of Training Parameters used for the training of Samba-ASR", "description": "This table details the hyperparameters employed during the training phase of the Samba-ASR model.  It includes the optimizer used (AdamW), the learning rate, weight decay, Adam epsilon value, and batch size. These parameters were carefully selected to balance model performance, stability and training efficiency.", "section": "6 Training Details"}, {"content": "| Model | Average WER | Gigaspeech | LS Clean | LS Other | SPGISpeech |\n|---|---|---|---|---|---| \n| Samba-ASR (SandLogic) | 3.65 | 9.12 | 1.17 | 2.48 | 1.84 |\n| nvidia/canary-1b | 4.15 | 10.12 | 1.48 | 2.93 | 2.06 |\n| nyrahealth/CrisperWhisper | 4.69 | 10.24 | 1.82 | 4.00 | 2.7 |\n| nvidia/parakeet-tdt-1.1b | 7.01 | 9.52 | 1.40 | 2.60 | 3.16 |\n| openai/whisper-large-v3 | 7.44 | 10.02 | 2.01 | 3.91 | 2.94 |", "caption": "Table 2: Model Performance Comparison Across Various Datasets", "description": "This table presents a comparison of the Word Error Rate (WER) achieved by different Automatic Speech Recognition (ASR) models on four benchmark datasets: GigaSpeech, LibriSpeech Clean, LibriSpeech Other, and SPGISpeech.  The WER, a common metric for evaluating ASR accuracy, represents the percentage of words incorrectly transcribed.  Lower WER values indicate higher accuracy. The table allows for a direct comparison of Samba-ASR's performance against state-of-the-art models, showcasing its superior accuracy and efficiency across diverse datasets and speech characteristics.", "section": "7 Evaluation and Results"}]