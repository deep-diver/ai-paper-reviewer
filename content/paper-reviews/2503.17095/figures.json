[{"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/pretrain.png", "caption": "Figure 1: Results of FFaceNeRF. With few-shot training, our method can edit 3D-aware images from desired layouts.", "description": "Figure 1 showcases the results produced by the FFaceNeRF model.  It demonstrates the model's ability to perform few-shot 3D face editing. The figure shows a source image with a typical mask overlayed.  Next to it are examples of 'desired layouts', which represent the user-specified edits to be made to the facial features. Then, an 'edited mask' shows the refined mask created by the model to guide the editing process. Finally, the 'fine-edit results' images display the final edited images produced by FFaceNeRF, reflecting the changes based on the 'desired layouts'. This illustrates how the model effectively incorporates user input to produce high-quality, 3D-aware face edits.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/overview_pfnerf.png", "caption": "Figure 2: Pretraining stage of FFaceNeRF following EG3D\u00a0[4] and NeRFFaceEditing\u00a0[18] for disentangled representation.", "description": "This figure illustrates the pretraining phase of the FFaceNeRF model. It details how the model leverages the EG3D and NeRFFaceEditing architectures to achieve disentangled representations of face appearance and geometry.  The process begins with an appearance decoder (\u03a8app) generating a face volume from a latent code. This face volume is then rendered into an image. Simultaneously, a geometry decoder (\u03a8geo) using a pre-trained segmentation network processes a normalized tri-plane feature (F'tri) to output a corresponding segmentation mask volume.  The combination of these two networks allows for the generation of a face image and its associated segmentation mask.  Importantly, the method allows adaptation of the geometry decoder to work with customized segmentation mask layouts rather than being limited to fixed layouts.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_05_pfnerf_data.jpg", "caption": "Figure 3: Overview of FFaceNeRF. LMTA is conducted during the training of \u03a6g\u2062e\u2062osubscript\u03a6\ud835\udc54\ud835\udc52\ud835\udc5c\\Phi_{geo}roman_\u03a6 start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT. \u03a6g\u2062e\u2062osubscript\u03a6\ud835\udc54\ud835\udc52\ud835\udc5c\\Phi_{geo}roman_\u03a6 start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT takes as input the concatenation of normalized tri-plane feature F\u2032^t\u2062r\u2062isubscript^superscript\ud835\udc39\u2032\ud835\udc61\ud835\udc5f\ud835\udc56\\hat{F^{\\prime}}_{tri}over^ start_ARG italic_F start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT end_ARG start_POSTSUBSCRIPT italic_t italic_r italic_i end_POSTSUBSCRIPT (yellow box), view direction vdsubscript\ud835\udc63\ud835\udc51v_{d}italic_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT (white box), outputs of \u03a8g\u2062e\u2062osubscript\u03a8\ud835\udc54\ud835\udc52\ud835\udc5c\\Psi_{geo}roman_\u03a8 start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT, which are segmentation labels S\u2062e\u2062g\ud835\udc46\ud835\udc52\ud835\udc54Segitalic_S italic_e italic_g (blue box), and density \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 (red box). Density \u03c3\ud835\udf0e\\sigmaitalic_\u03c3 is directly used from the output of \u03a8g\u2062e\u2062osubscript\u03a8\ud835\udc54\ud835\udc52\ud835\udc5c\\Psi_{geo}roman_\u03a8 start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT, without further training using \u03a6g\u2062e\u2062osubscript\u03a6\ud835\udc54\ud835\udc52\ud835\udc5c\\Phi_{geo}roman_\u03a6 start_POSTSUBSCRIPT italic_g italic_e italic_o end_POSTSUBSCRIPT.", "description": "Figure 3 illustrates the architecture of FFaceNeRF, focusing on the geometry adapter (\u03a6geo).  The adapter receives the concatenated inputs of the normalized tri-plane feature (F'tri), view direction (vd), and outputs from the geometry decoder (\u03a8geo), which include segmentation labels (Seg) and density (\u03c3).  The density (\u03c3) is directly used from the \u03a8geo output without additional training of \u03a6geo. Notably, the Latent Mixing for Triplane Augmentation (LMTA) is applied during the training of \u03a6geo to enhance the model's adaptability to varied segmentation masks.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_01_augment2.png", "caption": "Figure 4: Examples of dataset with different segmentation layouts. Green boxes are close-up views of eye regions while red boxes are close-up views of nose regions.", "description": "Figure 4 shows examples from the dataset used in the FFaceNeRF model training.  The dataset includes images with three different segmentation mask layouts: Base, Eyes, and Nose. The Base layout provides a general segmentation of facial features. The Eyes and Nose layouts offer more detailed segmentations focusing on eye and nose areas respectively. The figure highlights these differences by displaying example images from each layout type.  Green boxes show zoomed-in views of the eye region, while red boxes focus on the nose area.", "section": "4.2. Dataset"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_04_mixing_vis_b.png", "caption": "Figure 5: Semantics-augmentation tradeoff: When mixing earlier layers, semantics and tri-plane feature information change largely (high L1, low mIoU). On the other hand, when mixing later layers, semantics and augmentation change little (low L1, high mIoU).", "description": "This figure illustrates the effect of latent mixing on the balance between preserving semantic information and introducing diversity during training.  The x-axis represents the layer number in a style-based generator, where earlier layers contain more coarse geometric information and later layers contain finer details. The y-axis shows two metrics: L1 distance (measuring the change in tri-plane features after mixing) and mIoU (measuring the similarity of segmentation masks before and after mixing).  The plot shows that mixing earlier layers leads to large changes in both semantics and tri-plane features (high L1, low mIoU), indicating significant augmentation but potential loss of semantic consistency. Conversely, mixing later layers results in smaller changes (low L1, high mIoU), suggesting less augmentation but better preservation of original semantic information.", "section": "4.3 Latent Mixing for Triplane Augmentation"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/4_03_mixing_vis_a.png", "caption": "Figure 6: Visualization of tri-plane mixing N layers with the top L1 metrics. The geometry and the semantic information such as the outline of chin and beard are changed even when only 2 layers are mixed.", "description": "This figure visualizes the effects of latent mixing on tri-plane features during training.  Latent mixing involves blending different latent codes at specific layers to create augmented tri-plane features.  The experiment shows that even when mixing only two layers (N=2), selected based on having the highest L1 metric (which measures the magnitude of change), both geometry and semantic information such as the chin and beard outlines are visibly altered in the generated face images.  This demonstrates the ability of latent mixing to successfully introduce changes across different levels of detail in the generated output.", "section": "4.3. Latent Mixing for Triplane Augmentation"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/mixing_triplane3.png", "caption": "Figure 7: Visualization of tri-plane mixing N layers based on the top mIoU metrics. The geometry and the semantic information are not changed while colors are changed.", "description": "This figure visualizes the results of applying latent mixing to the tri-plane features in a neural radiance field (NeRF)-based face editing model.  Latent mixing combines different latent codes from a style-based generator, affecting the color and texture information in the generated image.  The 'top mIoU metrics' selection criteria indicate that the layers were chosen to maximize the intersection over union (IoU) between the generated and ground truth segmentation masks. The experiment aims to demonstrate that latent mixing primarily affects the color information (such as changing hair color or skin tone), while maintaining the geometry and semantic features (such as facial shape and structure) of the face.  The different images shown in the figure represent variations with different numbers (N) of layers undergoing latent mixing.", "section": "4.3. Latent Mixing for Triplane Augmentation"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/ours.png", "caption": "Figure 8: Results of mixing multiple tri-planes. Left: Mixing top N layers based on mIoU, Right: Mixing top N layers based on L1.", "description": "This figure visualizes the effects of mixing multiple tri-plane layers in the FFaceNeRF model during training. The left panel shows the results when selecting the top N layers based on the mean Intersection over Union (mIoU) metric, focusing on preserving semantic consistency. The right panel shows the results when selecting the top N layers based on the L1 distance metric, emphasizing the amount of change introduced by the augmentation.  Both panels illustrate a trade-off between preserving semantic information (mIoU) and introducing diversity in the training data (L1).  The graphs plot mIoU and L1 values against the number of layers (N) to show this relationship.", "section": "4.3 Latent Mixing for Triplane Augmentation"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/comparison.png", "caption": "Figure 9: Examples of multi-view images edited using our method. Edited regions are indicated with red arrows in target masks.", "description": "This figure showcases the results of FFaceNeRF's ability to perform multi-view face editing.  It presents three columns: the original source image with its initial mask, the corresponding target mask indicating the desired edits (with red arrows highlighting the specific regions to be modified), and the final edited images viewed from multiple angles. This demonstrates the model's capacity to consistently apply edits across different viewpoints, maintaining a realistic and coherent 3D representation of the edited face.", "section": "4.4.1 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/facenerfab.png", "caption": "Figure 10: Examples of our results and those of baseline methods in editing tasks. The results of our method faithfully reflect the edited regions.", "description": "Figure 10 presents a qualitative comparison of face editing results between the proposed FFaceNeRF model and two baseline methods: NeRFFaceEditing and IDE-3D.  Each row shows the source image, the source mask, the target mask (indicating the desired edits), and the edited results from each of the three methods. The figure highlights that FFaceNeRF achieves the most accurate and faithful editing results, closely reflecting the edits specified in the target mask, while the baseline methods show less accurate editing and sometimes fail to preserve the original facial identity.", "section": "4.4.2. Baseline Comparisons"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/fewnerfab3.png", "caption": "Figure 11: Ablation study results. Our full model best followed the target masks while preserving the original identity. The color shifted when trained without feature injection, hair was not faithfully generated when trained without LMTA, and the source identity changed when trained with mixing all layers.", "description": "Figure 11 displays a comparison of face editing results using FFaceNeRF with different components removed to show their individual contributions.  The top row shows the original source images and the target masks, which specify the desired edits. The subsequent rows show results from models trained without feature injection, without latent mixing for triplane augmentation (LMTA), and with all layers mixed during training, along with the results from the complete FFaceNeRF model.  The complete model shows accurate reproduction of the target mask while maintaining the source face's identity.  Removing components led to problems such as color shifts (without feature injection), inaccurate hair generation (without LMTA), and alterations to the underlying facial features (mixing all layers).", "section": "4.4.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.17095/extracted/6299363/figures/app_ffaceGAN.jpg", "caption": "Figure 12: Comparisons with percentage-based optimization on enlarging eyes show that our method more faithfully follows the desired eye size modifications.", "description": "Figure 12 shows a comparison between FFaceNeRF's overlap-based optimization and a traditional percentage-based approach for eye enlargement.  The image demonstrates that FFaceNeRF more accurately reflects the desired changes in eye size specified by the user's edited mask, achieving more faithful results compared to the percentage-based method.", "section": "4.4.3 Ablation Study"}]