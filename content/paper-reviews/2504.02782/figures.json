[{"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/Structure.jpg", "caption": "Figure 1: Commonly used pipelines for unified image generation and understanding, and potential decoder architectures of GPT4o\u2019s image generation choice. The complete speculation architectures can be seen in the Figure\u00a07.", "description": "This figure illustrates three common architectures for multimodal models capable of image generation and understanding.  These architectures differ in how they combine text and vision information: (a) uses an autoregressive (AR) model followed by a diffusion model, (b) uses a native AR model, and (c) combines a stitched or concatenated AR approach with a diffusion model. The figure also speculates on GPT-40's potential internal architecture, highlighting that it might utilize one of these or a variation thereof.  The complete speculation of GPT-40's architecture is detailed in Figure 7.", "section": "Potential Architectures Behind GPT40"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/pipeline.jpg", "caption": "Figure 2: The overall workflow of our GPT-ImgEval, consisting the GPT-4o Image generation, Evaluation, and Analysis.", "description": "This figure illustrates the end-to-end workflow of the GPT-ImgEval benchmark.  It starts with GPT-40 image generation, using three core datasets: GenEval for image generation, Reason-Edit for image editing, and WISE for world knowledge-informed semantic synthesis. The generated images are then evaluated using these datasets' respective metrics. Finally, the results undergo a comprehensive analysis, including an investigation into GPT-40's architecture, identification of weaknesses, and a comparative analysis against Gemini 2.0 Flash.  The analysis provides insights into GPT-40's strengths, limitations, and potential architectural choices.", "section": "2 GPT-ImgEval Evaluation Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/GenEval_cases.jpg", "caption": "Figure 3: Examples of generation results of GPT4o using GenEval\u00a0[17], covering single object, two objects, counting, colors, position, and attribute binding.", "description": "Figure 3 showcases GPT-40's image generation capabilities using the GenEval benchmark.  It presents six categories of example images.  These categories demonstrate GPT-40's ability to generate images with varying levels of complexity, correctly interpreting aspects such as single objects, multiple objects, counting objects, color specification, positional relationships between objects, and attribute binding. For example, the figure displays the model's ability to accurately generate the requested number of objects (e.g., \"three sports balls\"), specify object colors (e.g., \"a blue TV\"),  and precisely place objects relative to each other (e.g., \"a carrot left of an orange\").  This visually demonstrates GPT-40's nuanced understanding of textual prompts and the ability to translate them into complex, visually accurate image outputs.", "section": "2.2 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/EvalScore_bar.jpg", "caption": "Figure 4: \nQuantitative results of model editing under the Reason-Edit benchmark\u00a0[21]. We compare the performance of GPT4o with seven other SOTA image editing models. We see that GPT4o significantly outperforms other models.", "description": "This bar chart displays a quantitative comparison of the performance of eight different image editing models on the Reason-Edit benchmark.  The models are evaluated based on a metric that considers both the accuracy of edits requested and the consistency of the unchanged regions in the edited image. GPT-40 shows significantly better performance compared to the other seven state-of-the-art (SOTA) models.", "section": "2.3 Image Editing"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/SmartEdit_case.jpg", "caption": "Figure 5: Examples of model editing results. We visualize the qualitative results of GPT4o with the other four SOTA editing generation methods. We use the Reason-Edit\u00a0[21] benchmark for evaluation.", "description": "This figure displays a qualitative comparison of GPT-40's image editing capabilities against four other state-of-the-art (SOTA) models using the Reason-Edit benchmark.  Each row shows the results for a different editing task, presenting the original image, the results from each model (including GPT-40), and highlighting the successful and unsuccessful edits. The figure demonstrates GPT-40's superior performance in generating semantically accurate, visually coherent, and contextually aware edits compared to the other models.", "section": "2.3 Image Editing"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/WISE.jpg", "caption": "Figure 6: Visual examples of generation results on the WISE benchmark\u00a0[31]. We visualize the qualitative results of GPT4o under different evaluation scenarios, following the WISE benchmark.", "description": "Figure 6 showcases example outputs from GPT-4o on the WISE benchmark [31], which tests the model's ability to generate images grounded in world knowledge.  The figure visually demonstrates GPT-4o's performance across various subdomains of the WISE benchmark.  Each section shows the prompt given to the model and the resulting image generation, highlighting the model's ability to understand and incorporate factual information and nuanced contextual details when creating images.", "section": "2.4 World knowledge-Informed Semantic Synthesis"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/complete_archi.png", "caption": "Figure 7: We present a complete architectural speculation, proposing four possible candidates that differ in their choice of visual encoder while all share a diffusion-based head for image decoding.", "description": "Figure 7 explores potential architectural designs for GPT-40's image generation capabilities.  It proposes four variations, each differing in the type of visual encoder used (semantic-only, pixel-only, pixel and segmentation, or a combination of pixel and segmentation encoders).  However, all four proposed architectures share a common feature: a diffusion-based decoder for generating the final image. This highlights the researchers' focus on the decoder component and its potential reliance on diffusion models while acknowledging the uncertainty surrounding the visual encoder.", "section": "Potential Architectures Behind GPT40"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/oai_cd.png", "caption": "Figure 8: An \"easter-egg\" example officially provided by the OpenAI, which aligns the potential architecture-(a) in Figure\u00a01.", "description": "Figure 8 shows an image from OpenAI's official documentation that illustrates their GPT-40 model's image generation pipeline.  It visually depicts the process as a sequence: tokens are converted to an intermediate representation by a transformer, then fed into a diffusion model, which produces the final pixel output (image). This directly supports Hypothesis 2 in the paper, which proposes that GPT-40 utilizes a hybrid architecture combining a transformer with a diffusion-based head for image generation.", "section": "Potential Architectures Behind GPT40"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/GPT_Structure_Detection.jpg", "caption": "Figure 9: The overall workflow of the proposed model-based discrimination method.", "description": "This figure illustrates the process of using a model-based approach to differentiate between autoregressive (AR) and diffusion-based image generation models.  It starts with prompts from the GenEval benchmark that are used to generate images using both an AR model and a diffusion model. These generated images are then used to train a binary classifier to distinguish between the two types of generation methods. Finally, the trained classifier is applied to images generated by GPT-40 to determine whether its internal image generation mechanism is based on an AR or diffusion approach.  This helps in understanding GPT-40's underlying architecture.", "section": "Which Architectures are behind GPT40?"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/bad-cases.jpg", "caption": "Figure 10: Failure Cases and Limitations of GPT-4o. We identify several scenarios in which GPT-4o may fail, along with common artifacts present in its generated images.", "description": "Figure 10 showcases instances where GPT-40's image generation capabilities fall short.  It highlights several key limitations and common artifacts observed in the generated outputs.  These include inconsistencies in image generation where minor unintended changes appear even when no edits are requested; issues with high-resolution and over-refinement, showing the model's tendency to enhance detail even when a less detailed image is requested; limitations of the brush tool, where edits don't remain localized and affect the entire image; difficulties generating complex scenes with multiple people or objects interacting naturally; and finally, limitations in generating text in non-English languages, specifically Chinese characters which are often incorrectly rendered.", "section": "4 Weakness Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/multi-turn2.jpg", "caption": "Figure 11: Multi-round generation comparison between GPT-4o and Gemini-2.0 Flash.", "description": "This figure displays a comparison of multi-round image generation between GPT-40 and Gemini 2.0 Flash.  It shows an input image and then the results of three consecutive editing rounds using prompts given to each model. The prompts aim to alter different aspects of the image (changing object color and presence).  The figure highlights the differences in the models' abilities to consistently apply edits and correctly understand instructions across multiple rounds.  The color-coded boxes indicate correctly and incorrectly edited images.", "section": "5.1 GPT-40 vs. Gemini 2.0 Flash: Comparative Analysis of Multi-round Generation"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/SmartEdit_Fig1.jpg", "caption": "Figure 12: Examples of generation results of GPT4o using Reason-Edit\u00a0[21].", "description": "This figure showcases qualitative examples of GPT-40's image editing capabilities using the Reason-Edit benchmark.  It demonstrates the model's ability to perform various editing operations such as replacing objects, removing objects, and changing object attributes. Each row presents a different editing task, showing the input image and the results generated by GPT-40. This provides a visual demonstration of the model's performance on complex image editing instructions.", "section": "2.3 Image Editing"}, {"figure_path": "https://arxiv.org/html/2504.02782/extracted/6334205/figures/multi-turn.jpg", "caption": "Figure 13: Multi-round generation comparison between GPT-4o and Gemini-2.0 Flash.", "description": "This figure displays a comparison of multi-round image generation between GPT-40 and Gemini 2.0 Flash.  It shows an initial image and then three rounds of edits applied to that image by both models.  Each round presents a new instruction, and the resulting images from both models are shown side-by-side, allowing for a visual comparison of their ability to maintain consistency across multiple editing rounds and their understanding and execution of the provided instructions. The edits are varied, showing examples of adding elements, removing elements, and modifying existing elements.", "section": "5.1 GPT-40 vs. Gemini 2.0 Flash: Comparative Analysis of Multi-round Generation"}]