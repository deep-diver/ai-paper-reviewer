[{"figure_path": "https://arxiv.org/html/2504.01848/x1.png", "caption": "Figure 1: PaperBench is a benchmark for evaluating AI agents\u2019 abilities to replicate AI research. Each sample includes a research paper and a grading rubric that specifies the assessment criteria for a complete replication. Agents create a codebase from scratch as their submission (1), which is then executed to verify result reproduction (2) and graded against the rubric by an LLM-based judge (3).", "description": "PaperBench is a benchmark that evaluates AI agents' ability to reproduce AI research papers.  The process involves three stages: (1) An AI agent receives a research paper and rubric; (2) The agent generates a codebase from scratch and submits it for execution; and (3) An LLM-based judge grades the results of the executed code against the rubric's criteria.  This figure visually depicts the workflow, showing the agent, the research paper, the rubric, the submission of the agent, the reproduction process, and the final LLM-based grading process.", "section": "PaperBench"}, {"figure_path": "https://arxiv.org/html/2504.01848/x2.png", "caption": "Figure 2: Rubrics hierarchically decompose the replication task into a tree of increasingly granular requirements. Leaf nodes are graded for binary pass/fail criteria, and a parent\u2019s score is the weighted average of its children. In the example above, the final Replication Score is 55%.", "description": "The figure illustrates how a rubric is structured to evaluate the replication of a research paper. The rubric uses a hierarchical tree structure, breaking down the replication task into smaller, more specific subtasks. Each leaf node (lowest level) represents a single, clearly defined criterion that can be assessed as either passed or failed. A parent node's score is calculated as a weighted average of its children's scores. This hierarchical structure allows for a granular assessment of the replication attempt, providing a more nuanced evaluation than a simple pass/fail judgment. The example in the figure shows a final Replication Score of 55%.", "section": "PaperBench"}, {"figure_path": "https://arxiv.org/html/2504.01848/", "caption": "Figure 3: Comparing human versus agent performance on a 4-paper subset of PaperBench. o1 initially outperforms the human baseline but plateaus after the first hour, leading it to fall behind the humans by the end. Note that the human attempt for test-time-model-adaptation ends at the 24 hour mark and is thus excluded from the \u20183-paper subset\u2019 discussed elsewhere in the paper. Error bars on model performance is SEM over 3 repeats.", "description": "This figure compares the performance of human researchers and the o1 AI model on a subset of four papers from the PaperBench benchmark.  The x-axis represents the time spent working on the replication tasks (in hours), and the y-axis shows the achieved replication score.  The results indicate that the o1 model initially outperforms human participants, but this advantage is short-lived;  its performance plateaus after the first hour, and the human researchers surpass it by the end of the experiment.  The test-time-model-adaptation task's human data is incomplete (ending at 24 hours), and therefore, it's excluded from the analysis of the 3-paper subset that is mentioned in the main paper.  Error bars show the standard error of the mean (SEM) based on three repetitions of each experiment.", "section": "5.4 Human Baseline Performance"}, {"figure_path": "https://arxiv.org/html/2504.01848/extracted/6328448/assets/rubric_excerpt_json.png", "caption": "Figure 4: An excerpt of the rubric for one of the papers in PaperBench. Shown in the underlying JSON (left) and in a GUI (right).", "description": "This figure shows an example of a rubric from PaperBench, a benchmark for evaluating AI's ability to replicate AI research.  The left side displays the rubric in its underlying JSON format, which is a hierarchical tree structure defining the requirements for successful replication. Each node represents a specific criterion, with leaf nodes representing individual pass/fail requirements.  The right side shows the same rubric in a graphical user interface (GUI), providing a more user-friendly visual representation of the hierarchical structure and the weights assigned to each criterion. This highlights the complexity and granularity involved in assessing AI's replication capabilities.", "section": "2. PaperBench"}, {"figure_path": "https://arxiv.org/html/2504.01848/extracted/6328448/assets/rubric_excerpt_gui.png", "caption": "Figure 5: Performance on JudgeEval vs average cost per paper in JudgeEval for various model backends in SimpleJudge. Model cost measured in terms of input+output tokens multiplied by their respective cost-per-token on the OpenAI API. Human cost estimated at 12 hours of work at an hourly rate of $100 USD/hr. Reasoning models are run with with reasoning effort set to \u201chigh\u201d.", "description": "This figure compares the performance of different large language models (LLMs) used as automated judges on the JudgeEval benchmark. The x-axis represents the average cost per paper in USD, which is calculated based on the number of input and output tokens consumed by each model during the evaluation.  The y-axis represents the F1 score, a common metric for evaluating the accuracy of binary classification.  The higher F1 score and lower cost, the better the model's performance.  The plot includes results from several OpenAI models (GPT-4-mini, GPT-4, 01-mini, 01, 03-mini), as well as a random baseline and human performance. The human cost is estimated at $1200 USD per paper, based on 12 hours of work at $100/hour.  The reasoning effort for the LLMs was set to \"high\". The figure aims to showcase the trade-off between cost and accuracy when selecting an automated judge for the PaperBench evaluation.", "section": "4.2. Evaluating Judges with JudgeEval"}, {"figure_path": "https://arxiv.org/html/2504.01848/x4.png", "caption": "Figure 6: The replication score assigned by o3-mini SimpleJudge to the \u2018rice/0\u2019 submission in JudgeEval, over different depths of pruning. Pruning at depth 100 is equivalent to not pruning for this paper. We plot the ground truth human Judge grade in red. Error bars are standard error of the mean over 3 repeats.", "description": "This figure displays the impact of rubric pruning on the accuracy of an automated judge.  The x-axis shows different depths of pruning applied to the rubric.  A depth of 100 means no pruning. The y-axis shows the replication score.  The automated judge's scores (using OpenAI's o3-mini model) are shown with error bars representing the standard error of the mean across three repetitions. The ground truth score from a human judge is also plotted in red. This experiment helps determine the trade-off between the cost of judging (which decreases with more pruning) and the accuracy of the automated judging process.", "section": "Evaluating Judges with JudgeEval"}, {"figure_path": "https://arxiv.org/html/2504.01848/x5.png", "caption": "Figure 7: Judge file ranking prompt. Variables like tree_structure\u2014 are replaced with their value at runtime.", "description": "This figure shows the prompt used by the automated judge (SimpleJudge) to rank the files from a submission. The prompt instructs the judge to identify and list the most relevant files in descending order of importance for assessing the submission against a specific criterion (leaf node) from the rubric.  The prompt includes context from the paper, the addendum, the rubric, and the directory structure of the submission.  Variables within the prompt, such as {tree_structure}, are dynamically replaced with their values during runtime, making the prompt adaptable to different submissions and criteria.  The goal is to provide the judge with the most relevant information to make an accurate assessment.", "section": "2.3 Grading"}]