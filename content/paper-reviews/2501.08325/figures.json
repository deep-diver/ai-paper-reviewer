[{"figure_path": "https://arxiv.org/html/2501.08325/x3.png", "caption": "Figure 1: We propose GameFactory, a framework that leverages the powerful generative capabilities of pre-trained video models for the creation of new games. By learning action control from a small-scale first-person Minecraft dataset, this framework can transfer these control abilities to open-domain videos, ultimately allowing the creation of new games within open-domain scenes. As illustrated in (1)-(4), GameFactory supports action control across diverse, newly generated scenes in open domains, paving the way for the development of entirely new game experiences. The yellow buttons indicate pressed keys, and the arrows represent the direction of mouse movements.", "description": "GameFactory is a framework that uses pre-trained video models to generate new games.  It learns action controls from a small Minecraft dataset and applies them to open-domain videos. This allows for the creation of games in various, realistic settings. The figure shows four examples of this, demonstrating action control (indicated by yellow buttons for key presses and arrows for mouse movements) in diverse generated scenes.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.08325/x4.png", "caption": "Figure 2: A schematic of our GameFactory creating new games based on pre-trained large video generation models. The upper blue section shows the generative capabilities of the pre-trained model in an open-domain, while the lower green section demonstrates how the action control module, learned from a small amount of game data, can be plugged in to create new games.", "description": "GameFactory uses a pre-trained large video generation model to create new games. The model's open-domain generative capabilities are shown in the upper blue section.  The lower green section illustrates how a smaller action control module, trained on a small game dataset, is added to control the generation process, allowing creation of new games within various scenes. This demonstrates the transfer of action control abilities from a known dataset to open-domain videos, resulting in novel game experiences.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.08325/x5.png", "caption": "Figure 3: \n(a) Integration of Action Control Module into transformer blocks of the video diffusion model.\n(b) Different control mechanisms for continuous mouse signals and discrete keyboard signals (detailed analysis in Sec\u00a03.3).\n(c) Due to temporal compression (compression ratio r=4\ud835\udc5f4r=4italic_r = 4), the number of latent features differs from the number of actions, causing granularity mismatch during fusion. Grouping aligns these sequences for fusion. Additionally, the i\ud835\udc56iitalic_i-th latent feature can fuse with action groups within a previous window (window size w=3\ud835\udc643w=3italic_w = 3), accounting for delayed action effects (e.g., \u2018jump\u2019 key affects several subsequent frames).", "description": "Figure 3 illustrates how the action control module is integrated into a video diffusion model for generating action-controlled videos.  Panel (a) shows the module's placement within the transformer blocks of the video diffusion model. Panel (b) details the different control mechanisms used for continuous mouse movements and discrete keyboard inputs; a key difference is that continuous signals are concatenated with latent video features while discrete signals use a cross-attention mechanism. Panel (c) addresses a problem caused by temporal compression of the input video.  Because the video is compressed, the number of latent features is not equal to the number of actions.  A grouping mechanism with a sliding window is used to align the features and actions so they can be correctly fused.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.08325/x6.png", "caption": "Figure 4: \nStyle bias in video game generation. The model tuned on Minecraft data inherits its distinctive pixelated block style, creating a domain gap from the original parameters. This motivates decoupling action control learning from data style learning through specialized training strategies.", "description": "The figure shows that fine-tuning a pre-trained video generation model directly on Minecraft data results in the generated videos inheriting the blocky style of Minecraft. This creates a problem because the goal is to generate videos in various styles and not just the Minecraft style.  To address this issue, the authors propose a multi-phase training approach that separates learning the game's style from learning action control. This allows the model to generalize to new scenes and styles while maintaining the ability to respond to user actions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.08325/x7.png", "caption": "Figure 5: \nPhase #0: pretraining a video generation model on open-domain data.\nPhase #1: finetuning with LoRA for game video data.\nPhase #2: training the action control module while fixing other parameters.\nPhase #3: inference for action-controlled open-domain generation.\nTo decouple style learning from action control, Phase #1 learns game-specific style while Phase #2 focuses on style-independent action control. This design preserves the open-domain capabilities from Phase #0, enabling generalization in Phase #3.", "description": "This figure illustrates the multi-phase training strategy employed in GameFactory. Phase #0 involves pretraining a video generation model on a large, open-domain dataset.  Phase #1 fine-tunes this model using LoRA (Low-Rank Adaptation) on game-specific video data to learn the style of the target game without affecting the action control. Phase #2 focuses exclusively on training the action control module while keeping the pre-trained model and LoRA parameters frozen. This decoupling ensures style-independent action control.  Finally, Phase #3 demonstrates the system's ability to generate action-controlled videos in open domains, leveraging the generalized action control from Phase #2 and the open-domain scene generation capabilities from Phase #0.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.08325/x8.png", "caption": "Figure 6: \nIllustration of autoregressive video generation. The frames from index 00 to k\ud835\udc58kitalic_k serve as conditional frames, while the remaining N\u2212k\ud835\udc41\ud835\udc58N-kitalic_N - italic_k frames are for prediction, with k\ud835\udc58kitalic_k randomly selected. (a) Training stage: Loss computation and optimization focus only on the noise of predicted frames. (b) Inference stage: The model iteratively selects the latest k+1\ud835\udc581k+1italic_k + 1 frames as conditions to generate N\u2212k\ud835\udc41\ud835\udc58N-kitalic_N - italic_k new frames, enabling autoregressive generation.", "description": "Figure 6 illustrates the autoregressive video generation process.  In the training stage (a), a sequence of video frames (from 0 to N) is used, with a random subset (k frames) serving as conditioning.  The model learns to predict the noise in the remaining (N-k) frames. Only the noise prediction loss for these frames contributes to the training objective. The inference stage (b) leverages this learned ability to generate longer videos iteratively.  Starting with k+1 initial frames, the model predicts the next N-k frames, appends them to the initial sequence, and then uses the most recent k+1 frames to predict further frames, continuing this process until the desired video length is achieved. This allows for the creation of arbitrarily long videos.", "section": "3.6. Autoregressive Generation of Long Action-Controllable Game Videos"}, {"figure_path": "https://arxiv.org/html/2501.08325/x9.png", "caption": "Figure 7: Demonstration of action control capabilities in the Minecraft domain. The model has successfully learned basic atomic actions (WASD keys) and mouse-based yaw and pitch controls. Additionally, it can combine these atomic actions to execute more complex movements. Note that the text below each video frame is a descriptive label of the content, not a text prompt provided to the model.", "description": "This figure showcases the model's ability to control actions within the Minecraft game environment.  The model successfully learned fundamental atomic actions, such as moving forward, backward, left, and right, using the WASD keys, as well as controlling the camera's yaw and pitch using the mouse.  Importantly, the model can combine these basic actions to perform more complex movements. The text under each video frame provides a descriptive label explaining the action, rather than being a text prompt used to generate the video.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08325/x10.png", "caption": "Figure 8: Demonstration of the learned response to collision, one of the most common interactions in Minecraft navigation. Note that the text below each video frame is a descriptive label of the content, not a text prompt provided to the model.", "description": "Figure 8 showcases the model's ability to handle collisions, a frequent event in Minecraft navigation.  The figure displays two example videos demonstrating the model's response to encountering obstacles: one where the agent stops upon hitting a wall, and another where it stops when encountering an obstacle. Importantly, the text under each video frame serves only as a descriptive label; it was not given to the model as a prompt.", "section": "4.3 Scene Generalization"}, {"figure_path": "https://arxiv.org/html/2501.08325/x11.png", "caption": "Figure 9: Our model demonstrates the ability to generalize to a different game type, a racing game. Interestingly, the yaw control learned in Minecraft seamlessly transfers to steering control in the racing game, while unrelated actions, such as moving backward, left, or right, and pitch angle adjustments, automatically diminish.", "description": "The figure showcases the model's ability to generalize to a new game type, a racing game, using knowledge learned from the Minecraft domain.  It highlights that the 'yaw' control (rotation around the vertical axis), learned in Minecraft, seamlessly transfers to steering control in the racing game.  Conversely, actions not directly related to steering, such as moving backward, left, or right, and adjustments to the pitch angle (rotation around the horizontal axis), are automatically reduced or eliminated in the racing game context. This demonstrates that the model effectively distinguishes and transfers relevant skills while suppressing irrelevant ones.", "section": "4.3. Scene Generalization"}, {"figure_path": "https://arxiv.org/html/2501.08325/x12.png", "caption": "Figure 10: An example of video clip annotation, where words describing scenes and objects are highlighted in red and bolded.", "description": "This figure shows an example from the GF-Minecraft dataset.  A video clip from the game is presented along with its corresponding textual annotation. The annotation uses natural language to describe the scene's visual elements, including the terrain, plants, and other objects (such as a zombie-like figure). Key words used in the description are highlighted in red and boldface for emphasis. This demonstrates how the dataset includes both video data and associated textual descriptions, which are used to train the model in understanding and generating game scenes with associated descriptions.", "section": "GF-Minecraft Dataset"}, {"figure_path": "https://arxiv.org/html/2501.08325/x13.png", "caption": "Figure 11: Qualitative comparison of key input control performance. It can be observed that cross-attention significantly outperforms concatenation in handling discrete key input signals, while concatenation may fail to respond to the key input. The yellow buttons indicate pressed keys.", "description": "This figure compares the performance of cross-attention and concatenation methods in handling discrete key inputs for action control.  The results show that cross-attention is significantly better at correctly responding to key presses than concatenation. In some cases, the concatenation method fails entirely to register the key presses, while cross-attention consistently reflects the intended actions. The yellow buttons in the images represent the keys pressed during the actions.", "section": "3.3 Action Control Module"}]