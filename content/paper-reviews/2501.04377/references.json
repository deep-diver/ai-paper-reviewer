{"references": [{"fullname_first_author": "Keyu Tian", "paper_title": "Visual autoregressive modeling: Scalable image generation via next-scale prediction", "publication_date": "2024-04-05", "reason": "This paper introduces the Visual Autoregressive (VAR) model, the central subject of the complexity analysis in the current paper."}, {"fullname_first_author": "Josh Alman", "paper_title": "Fast attention requires bounded entries", "publication_date": "2023-12-12", "reason": "This paper provides crucial Lemma 4.3, which establishes the hardness of approximate attention computation under the Strong Exponential Time Hypothesis (SETH), a cornerstone of the current paper's computational limits analysis."}, {"fullname_first_author": "Russell Impagliazzo", "paper_title": "On the complexity of k-sat", "publication_date": "2001-06-01", "reason": "This foundational paper introduces the Strong Exponential Time Hypothesis (SETH), the primary complexity-theoretic assumption underlying many of the lower bound arguments in the current paper."}, {"fullname_first_author": "Josh Alman", "paper_title": "The fine-grained complexity of gradient computation for training large language models", "publication_date": "2024-12-12", "reason": "This paper offers important insights into the computational complexity of training large language models, which is directly relevant to the computational efficiency of VAR models."}, {"fullname_first_author": "Yingyu Liang", "paper_title": "Conv-basis: A new paradigm for efficient attention inference and gradient computation in transformers", "publication_date": "2024-05-20", "reason": "This paper introduces a novel method for accelerating attention mechanisms, a key component of VAR models, which is closely related to the efficient algorithms developed in the current paper."}]}