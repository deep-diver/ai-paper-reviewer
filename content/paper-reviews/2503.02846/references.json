{"references": [{"fullname_first_author": "Yuntao Bai", "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback", "publication_date": "2022-04-01", "reason": "This is one of the foundational papers on Reinforcement Learning from Human Feedback (RLHF), the approach that this paper builds upon."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "A neural probabilistic language model", "publication_date": "2000-01-01", "reason": "This classic paper introduces the Neural Probabilistic Language Model, establishing a fundamental concept for modern language modeling."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "This paper proposes Direct Preference Optimization (DPO), which is the core method that Mask-DPO modifies and improves upon."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-01", "reason": "This paper introduces Proximal Policy Optimization (PPO), which is a representative work for Reinforcement Learning from Human Feedback (RLHF)."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper trains language models to follow instructions with human feedback, which is a powerful alignment method."}]}