[{"figure_path": "https://arxiv.org/html/2503.02846/extracted/6251826/figures/teaser.png", "caption": "Figure 1: Comparison between DPO and Mask-DPO.\nVanilla DPO (a) inadvertently encourages and penalizes all the content in the preferred and non-preferred samples, respectively, regardless of their correctness. Instead, Mask-DPO (b) incorporates sentence-level facticity into the mask signal, preventing incorrect reward signal, which resolves ambiguity in preference learning.", "description": "This figure illustrates the core difference between the vanilla Direct Preference Optimization (DPO) method and the proposed Mask-DPO method for factuality alignment in LLMs. Vanilla DPO uses response-level factuality, leading to the model inadvertently encouraging incorrect information in preferred samples and penalizing correct information in non-preferred samples.  Mask-DPO addresses this issue by incorporating sentence-level factuality as a mask signal during training. This allows Mask-DPO to learn only from factually correct sentences in the preferred samples, thus resolving ambiguity and enhancing the effectiveness of factuality alignment.", "section": "2 MASK-DPO"}, {"figure_path": "https://arxiv.org/html/2503.02846/extracted/6251826/figures/method.png", "caption": "Figure 2: The overview of Mask-DPO.\nFirst, we sample K candidate responses for each question from the policy model.\nThen, we use a fine-grained hallucination annotator to perform a sentence-level factuality annotation on each response. We use the proportion of correct sentences out of the total number of sentences as the factuality score. We select the responses with the highest and lowest scores as preferred and non-preferred samples, respectively.\nFinally, we perform fine-grained factuality alignment on the policy model using such fine-grained preference data, where the reward signals to the sentences, i.e., incorrect sentences in the preferred samples and correct sentences in the non-preferred samples, would be ignored.", "description": "This figure illustrates the Mask-DPO process.  First, multiple responses are generated for a given question using a language model (the 'policy model'). Each response is then analyzed sentence-by-sentence by a 'fine-grained hallucination annotator' to determine the factuality of each sentence. A factuality score is calculated for each response based on the proportion of correct sentences. The responses with the highest and lowest factuality scores are selected as 'preferred' and 'non-preferred' samples, respectively. Finally, these samples are used in a fine-grained preference learning step to adjust the policy model.  Crucially, during this learning phase, feedback signals from incorrect sentences in preferred samples and correct sentences in non-preferred samples are ignored. This process focuses the model's learning on factual information and reduces the impact of conflicting information within a single response.", "section": "2 MASK-DPO"}]