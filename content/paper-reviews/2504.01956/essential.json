{"importance": "This paper is important as it presents an **efficient video distillation framework, VideoScene,** for generating 3D scenes from sparse views. This method **addresses the limitations of existing approaches** and opens new avenues for real-time 3D reconstruction, game design, and virtual reality.", "summary": "VideoScene: One-step 3D scene generation from sparse views by distilling video diffusion models for faster, structurally consistent results.", "takeaways": ["VideoScene enables one-step 3D scene generation with strong structural consistency from just two input images.", "A 3D-aware leap flow distillation strategy leaps over time-consuming redundant information and focuses on crucial details.", "A dynamic denoising policy network adapts to the optimal leap timestep during inference, maximizing 3D prior usage."], "tldr": "Recovering 3D scenes from sparse views is challenging due to its ill-posed nature. Conventional methods use geometry regularization or feed-forward models but suffer with minimal overlap and insufficient visual information. Recent video generative models show promise, capable of generating plausible 3D structures. Pioneering research explores video generative prior and creates 3D scenes. However, they are limited by slow inference time and lack of 3D constraints, leading to inefficiencies and reconstruction artifacts.\n\nThis paper introduces **VideoScene**, a novel video distillation model to generate 3D scenes in one step. It uses a 3D-aware leap flow distillation strategy to leap over time-consuming steps and train a dynamic denoising policy network for optimal timestep. Extensive experiments show that VideoScene achieves faster and superior 3D scene generation results than previous video diffusion models, enhancing its potential as an efficient tool for future applications.", "affiliation": "Tsinghua University", "categories": {"main_category": "Computer Vision", "sub_category": "3D Vision"}, "podcast_path": "2504.01956/podcast.wav"}