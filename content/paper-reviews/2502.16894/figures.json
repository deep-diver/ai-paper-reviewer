[{"figure_path": "https://arxiv.org/html/2502.16894/x1.png", "caption": "Figure 1: The effect of initializations from different SVD segments (ui,\u03c3i,vi\u22a4)subscript\ud835\udc62\ud835\udc56subscript\ud835\udf0e\ud835\udc56superscriptsubscript\ud835\udc63\ud835\udc56top(u_{i},\\sigma_{i},v_{i}^{\\top})( italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_\u03c3 start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT \u22a4 end_POSTSUPERSCRIPT ) for rank 32 and 128. The performance normalized by min-max scaling.", "description": "This figure visualizes the impact of different SVD segment initializations on LoRA's performance, specifically focusing on the utilization of various singular value segments (Ui, \u03a3i, ViT) in initializing the low-rank adapters (B and A).  The experiment is conducted for two different ranks (32 and 128) to assess the effect across different dimensionality of the adapter matrices.  Performance is evaluated across multiple datasets and normalized by min-max scaling for easier comparison.  The x-axis shows the different SVD segments (index i), indicating which part of the pre-trained weight matrix SVD is used for initialization, and the y-axis represents the normalized performance. This helps illustrate the optimal SVD segment to utilize for initialization, demonstrating that using only the principal or minor singular values is suboptimal and that using the middle segments is more effective.", "section": "2. Background and Motivation"}, {"figure_path": "https://arxiv.org/html/2502.16894/x2.png", "caption": "Figure 2: SVD initialization vs.\u00a0scaling s\ud835\udc60sitalic_s and rank r\ud835\udc5fritalic_r", "description": "This figure visualizes the impact of scaling factor (s) and rank (r) on the performance of SVD-based Low-Rank Adaptation (LoRA).  The left panel shows training loss curves, illustrating how different scaling factors affect convergence speed.  The right panel displays the gradient norm, demonstrating the relationship between scaling, rank, and gradient magnitude.  The figure highlights the trade-off between these factors to improve efficiency and performance of the method. For instance, with low rank (e.g., r=1), the gradient norm is small, while applying proper scaling (s=16) increases it, reducing the performance gap.", "section": "2. Rethinking Scaling Factor"}, {"figure_path": "https://arxiv.org/html/2502.16894/x3.png", "caption": "Figure 3: \nIllustration of Our Method.\nSingle Low-Rank Adaptation: LoRA reduces trainable parameters by reparameterizing W\ud835\udc4aWitalic_W as W=W0+s\u2062B\u2062A\ud835\udc4asubscript\ud835\udc4a0\ud835\udc60\ud835\udc35\ud835\udc34W=W_{0}+sBAitalic_W = italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_s italic_B italic_A, with B\ud835\udc35Bitalic_B and A\ud835\udc34Aitalic_A as low-rank matrices.\nMoE Fine-tuning: Full MoE fine-tuning, where experts W1superscript\ud835\udc4a1W^{1}italic_W start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and WEsuperscript\ud835\udc4a\ud835\udc38W^{E}italic_W start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT are selected by the router in this moment.\nSubfigure (I): Our method replaces the single pair B,A\ud835\udc35\ud835\udc34B,Aitalic_B , italic_A with multiple pairs {Bi,Ai}i=1Esuperscriptsubscriptsuperscript\ud835\udc35\ud835\udc56superscript\ud835\udc34\ud835\udc56\ud835\udc561\ud835\udc38\\{B^{i},A^{i}\\}_{i=1}^{E}{ italic_B start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_E end_POSTSUPERSCRIPT, initialized from different segments of the SVD of W0subscript\ud835\udc4a0W_{0}italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and adaptively selected by the router.\nSubfigure (II): We align optimization with SVD-structured MoE by separately aligning each expert. Wressubscript\ud835\udc4aresW_{\\text{res}}italic_W start_POSTSUBSCRIPT res end_POSTSUBSCRIPT ensures the equivalent weight equals W0subscript\ud835\udc4a0W_{0}italic_W start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT before optimization, and we scale each expert\u2019s equivalent gradient to closely approximate full MoE fine-tuning.", "description": "Figure 3 illustrates the core idea of the GOAT method, comparing it to standard LoRA and full MoE fine-tuning.  Single Low-Rank Adaptation shows the basic LoRA method where a weight matrix is reparameterized using low-rank matrices B and A.  MoE Fine-tuning depicts a full MoE architecture with multiple experts selected by a router. Subfigure (I) details the GOAT approach: replacing the single (B, A) pair with multiple (Bi, Ai) pairs.  These pairs are initialized using different segments of the SVD decomposition of the pre-trained weights (W0) and selected adaptively by the router. Subfigure (II) explains how GOAT aligns optimization with an SVD-structured MoE by individually aligning each expert's weight and scaling gradients to match full MoE fine-tuning.  Wres ensures that the equivalent weight matches the pre-trained weight before optimization begins.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2502.16894/x4.png", "caption": "Figure 4: Training loss curves of Different LoRA methods and Full Fine-tuning MoE on Cars. The balance loss is excluded in the MoE baselines for a fair comparison with single LoRA baselines.", "description": "This figure displays the training loss curves for various parameter-efficient fine-tuning (PEFT) methods and the full fine-tuning Mixture-of-Experts (MoE) model on the Cars dataset.  The x-axis represents the training steps, and the y-axis shows the training loss.  The curves demonstrate the convergence speed and final loss achieved by each method.  The MoE baselines have their balance loss excluded to allow for a more direct comparison to the single LoRA methods. This visualization allows for a comparison of the training efficiency and effectiveness of different PEFT approaches relative to the full fine-tuning model.", "section": "4.5. Convergence Speed"}, {"figure_path": "https://arxiv.org/html/2502.16894/x5.png", "caption": "Figure 5: Performance of different methods across ranks.", "description": "This figure shows how the performance of different parameter-efficient fine-tuning (PEFT) methods changes as the rank of the low-rank matrices used in the methods increases.  The x-axis represents the rank, ranging from 8 to 128. The y-axis represents the performance, likely measured as accuracy or a similar metric. The figure demonstrates the scalability of the GOAT method, showing how its performance narrows the gap with the full fine-tuning (FT) method as the rank increases.  Other methods, such as MoLoRA and HydraLoRA, are also included for comparison.", "section": "4.5. Convergence Speed"}, {"figure_path": "https://arxiv.org/html/2502.16894/x6.png", "caption": "Figure 6: Performance vs.\u00a0number of experts and activation ratio (total rank=32). \u201c2 in 8\u201d means activating 2 out of 8 experts.", "description": "This figure demonstrates the impact of the number of experts and the activation ratio on model performance, while keeping the total rank fixed at 32.  The x-axis represents different configurations, such as activating 2 out of 8 experts ('2 in 8'), 4 out of 8, etc., demonstrating how adjusting the number of active experts and their proportion affects overall performance.  The y-axis represents the model's performance, likely measured by accuracy or a similar metric. The graph shows the performance of different methods (likely GOAT and baselines) under various expert configurations, allowing for a comparison of their relative performance under different sparsity levels. The results help to determine an optimal balance between the number of experts and the activation ratio for improved performance and efficiency.", "section": "4.5. Convergence Speed"}, {"figure_path": "https://arxiv.org/html/2502.16894/x7.png", "caption": "Figure 7: Expert Load Distribution across different tasks. We illustrate the fraction of tokens assigned to each expert {ei}i=18superscriptsubscriptsubscript\ud835\udc52\ud835\udc56\ud835\udc5618\\{e_{i}\\}_{i=1}^{8}{ italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 8 end_POSTSUPERSCRIPT", "description": "This figure visualizes the distribution of tokens processed by each of the eight experts in a Mixture-of-Experts (MoE) model across various downstream tasks.  It shows the proportion of tokens handled by each expert for each task. The goal is to illustrate whether the expert load is balanced across the different tasks or if there's any skewing towards specific experts for particular task types (e.g., image classification vs. natural language generation).  Uniform distribution across experts suggests effective utilization of the MoE architecture and appropriate allocation of tasks among experts.", "section": "4.7 Routing Analysis"}]