{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-05-28", "reason": "This paper is a foundational work in large language models, demonstrating the ability of these models to perform few-shot learning."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-18", "reason": "This paper describes LLaMA 2, a popular open-source large language model, which is a key baseline for the experiments in this paper."}, {"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2022-04-06", "reason": "This paper presents PaLM, a large language model that experienced loss spikes during training, motivating the need for solutions like the one proposed in the current paper."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-07", "reason": "This paper introduced the Transformer architecture, which is the basis for many modern large language models."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper introduces LLaMA, an open and efficient foundation language model which provided the basis for LLaMA2."}]}