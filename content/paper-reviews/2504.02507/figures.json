[{"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/zclip3.png", "caption": "Figure 1: Training loss graph comparing 1) training without clipping, 2) clipping with fixed threshold 1.0, and 3) ZClip for a LLaMA 1B model.\nThe learning rate for all three experiments is 3.0\u00d710\u221233.0superscript1033.0\\times 10^{-3}3.0 \u00d7 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT.\nWhile both \u201cno clipping\u201d and \u201cconstant clipping\u201d exhibit spiky behavior and diverge early, ZClip (with zthressubscript\ud835\udc67thresz_{\\text{thres}}italic_z start_POSTSUBSCRIPT thres end_POSTSUBSCRIPT=2.5 and \u03b1=0.97\ud835\udefc0.97\\alpha=0.97italic_\u03b1 = 0.97) remains stable and continues to optimize effectively throughout training.\nDetails on the model configuration and other training hyperparameters are presented in Appendix Section\u00a06.1.", "description": "Figure 1 presents a comparison of training loss curves for three different gradient clipping methods applied to a 1B parameter LLaMA language model.  The x-axis represents the number of tokens processed during training, while the y-axis displays the training loss. The three curves represent: 1) training without any gradient clipping, 2) training with a fixed threshold gradient clipping method where the threshold is set to 1.0, and 3) training with the proposed ZClip adaptive gradient clipping method.  The learning rate was held constant at 3.0 x 10^-3 for all three experiments. The figure clearly shows that both training without clipping and training with fixed threshold clipping exhibit substantial instability, indicated by sharp spikes and eventual divergence. In contrast, ZClip maintains stability and consistently decreases the training loss, demonstrating its effectiveness in mitigating gradient instability during training. The hyperparameters used in ZClip for this experiment are z_thres = 2.5 and alpha = 0.97.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/1.png", "caption": "Figure 2: \nTraining loss graph for a LLaMA 1B model trained with fixed-threshold clipping c = 1.0.\nGradient norm spikes persist due to a mismatch between the static threshold and the running distribution.\nThis reveals a key limitation of fixed-threshold clipping in dynamically changing training regimes.", "description": "This figure shows the training loss for a 1B parameter LLaMA model using fixed-threshold gradient clipping with a threshold of 1.0.  Despite the clipping, large spikes in the gradient norm persist throughout training. This is because the fixed threshold doesn't adapt to the changing distribution of the gradient norms during training. The graph highlights a major weakness of fixed-threshold clipping: its inability to handle the dynamic nature of gradient behavior in large language model training.", "section": "2 Gradient Clipping Methods"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/zt_zstar_2_5.png", "caption": "Figure 3: \nThree possible choices for the z-score adjustment function \u03be\u2062(zt)\ud835\udf09subscript\ud835\udc67\ud835\udc61\\xi(z_{t})italic_\u03be ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) illustrated for zthres=2.5subscript\ud835\udc67thres2.5z_{\\text{thres}}=2.5italic_z start_POSTSUBSCRIPT thres end_POSTSUBSCRIPT = 2.5.\nNote the discontinuity for \u03be\u2062(zt)=0\ud835\udf09subscript\ud835\udc67\ud835\udc610\\xi(z_{t})=0italic_\u03be ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = 0, and the reciprocal nature of \u03be\u2062(zt)=zthres2/zt\ud835\udf09subscript\ud835\udc67\ud835\udc61superscriptsubscript\ud835\udc67thres2subscript\ud835\udc67\ud835\udc61\\xi(z_{t})=\\nicefrac{{z_{\\text{thres}}^{2}}}{{z_{t}}}italic_\u03be ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = / start_ARG italic_z start_POSTSUBSCRIPT thres end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG leading to more aggressive clipping for more extreme outliers.", "description": "Figure 3 displays three alternative functions for adjusting the z-score, which is a measure of how many standard deviations the current gradient norm is from the running mean.  The x-axis represents the z-score (zt), and the y-axis represents the adjusted z-score (\u03be(zt)). The three functions shown are:\n\n1.  **Clipping to mean:**  Sets the adjusted z-score to 0, effectively clipping the gradient norm to the running mean. This creates a discontinuity at the threshold.\n2.  **Clipping to max:** Sets the adjusted z-score to the threshold (zthres). This maintains continuity at the threshold but results in a less aggressive form of clipping than reciprocal clipping.\n3.  **Reciprocal clipping:**  The adjusted z-score is calculated as zthres\u00b2/zt. This function is continuous at the threshold and exhibits more aggressive clipping for larger z-scores (i.e., more extreme outliers).  This approach is used in the paper's proposed ZClip algorithm.", "section": "2.3 Z-score based Gradient Adjustment"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/low_high.png", "caption": "Figure 4: Comparison of Test Loss between ZClip (lr=3e-3) and a baseline model (lr=5e-4) on a 50B token corpus.\nZClip achieved the same final loss as the baseline while requiring 18.6B fewer tokens to get there.\nThe plot uses log-scaled training loss for visibility, and smoothing has been applied to reduce noise.\nZClip allows for faster convergence without compromising on final loss value.", "description": "Figure 4 presents a comparison of the test loss achieved by two different training methods: ZClip with a learning rate of 3e-3 and a baseline model with a learning rate of 5e-4.  Both models were trained on a 50-billion token corpus. The key finding is that ZClip reached the same final test loss as the baseline model but required 18.6 billion fewer tokens to reach that point. The graph shows training loss over time, using a logarithmic scale for better visualization. Smoothing was applied to the loss curves to minimize the impact of noise, enabling clearer comparison of the overall trends. This demonstrates that ZClip achieves faster convergence to a comparable final test loss without sacrificing performance. ", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/5e3.png", "caption": "((a))", "description": "Figure 7(a) shows the gradient norms before clipping during the training process for both ZClip and fixed-threshold clipping methods, using a learning rate of 1e-3.  It highlights that in early training, ZClip exhibits only small and transient spikes in the gradient norms, whereas the fixed-threshold clipping method experiences larger and more frequent deviations from the mean. This visualization emphasizes the adaptive nature of ZClip in handling fluctuations.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_5e3_before.png", "caption": "((b))", "description": "Figure 7(b) shows the gradient norms after clipping for both ZClip and fixed-threshold clipping methods.  The graph demonstrates that ZClip effectively suppresses the fluctuations of the gradient norms, making them smooth and stable throughout the training process.  This is in contrast to the fixed-threshold clipping method, which fails to adapt to the changing distribution of gradient norms and results in persistent spikes after clipping, indicating instability.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_5e3_after.png", "caption": "((c))", "description": "This figure shows the comparison of gradient norms before and after clipping for ZClip and fixed-threshold clipping, trained with a learning rate of 1e-3.  The left panel shows the gradient norms before clipping, where ZClip exhibits small, transient spikes early in training, while fixed-threshold clipping shows larger and more frequent deviations.  The right panel shows the gradient norms after clipping, demonstrating that ZClip effectively suppresses fluctuations, maintaining smooth and stable norms, while fixed-threshold clipping fails to adapt, resulting in persistent spikes and instability.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/3e3.png", "caption": "((d))", "description": "This figure is not present in the provided document.  Therefore, no description or section can be provided.", "section": "N/A"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_3e3_before.png", "caption": "((e))", "description": "Figure 7 shows a comparison of gradient norms for ZClip and fixed-threshold clipping.  Subfigure (a) displays the gradient norms before clipping, demonstrating that ZClip exhibits small, transient spikes early in training, whereas fixed-threshold clipping shows larger and more frequent deviations. Subfigure (b) shows the gradient norms after clipping, revealing ZClip effectively suppresses fluctuations and maintains smooth, stable norms, unlike fixed-threshold clipping, which fails to adapt and results in persistent spikes and instability.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_3e3_after.png", "caption": "((f))", "description": "Figure 7(a) displays gradient norms before clipping, illustrating that fixed-threshold clipping has larger and more frequent deviations compared to ZClip. Figure 7(b) shows gradient norms after clipping, where ZClip effectively suppresses fluctuations, maintaining smooth and stable norms, unlike fixed-threshold clipping which has persistent post-clipping spikes.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/1e3.png", "caption": "Figure 5: ZClip and fixed-threshold clipping at higher learning rates.\nEach row shows training loss (left), gradient norm before clipping (middle), and after clipping (right).\nFor 3.0\u00d710\u221233.0superscript1033.0\\times 10^{-3}3.0 \u00d7 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT ZClip stabilized gradients and reduces post-clipping spikes, unlike fixed-threshold clipping which accumulates instability.\nFor 5.0\u00d710\u221235.0superscript1035.0\\times 10^{-3}5.0 \u00d7 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT both clipping methods saturated.", "description": "This figure compares the performance of ZClip and fixed-threshold gradient clipping methods under high learning rates (3.0e-3 and 5.0e-3).  The figure is structured in rows, each row representing a different learning rate. Each row contains three subplots: training loss, gradient norm before clipping, and gradient norm after clipping.  At the lower learning rate (3.0e-3), ZClip effectively stabilizes the gradients and reduces the post-clipping spikes, demonstrating its adaptive nature. In contrast, the fixed-threshold method shows accumulating instability as it fails to adapt dynamically. At the higher learning rate (5.0e-3), both methods exhibit saturation, indicating that even adaptive clipping may not always prevent divergence under excessively high learning rates.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_1e3_before.png", "caption": "((a))", "description": "The figure shows the comparison of the gradient norms before and after clipping for different learning rates. The left column displays training loss, the middle column shows gradient norms before clipping, and the right column shows gradient norms after clipping.  In each row, a different learning rate is used, showing how ZClip (in blue) stabilizes the gradients and reduces post-clipping spikes, unlike fixed-threshold clipping (in orange) which can accumulate instability, especially at higher learning rates.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_1e3_after.png", "caption": "((b))", "description": "Figure 7(b) shows the gradient norms *after* clipping has been applied for both ZClip and fixed-threshold clipping methods, trained with a learning rate of 1e-3.  The plot highlights that ZClip effectively suppresses fluctuations in the gradient norms, maintaining smooth and stable values. In contrast, the fixed-threshold method fails to adapt to the changing gradient norm distribution, leading to persistent spikes and instability after clipping.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/7e4.png", "caption": "((c))", "description": "Figure 7 shows a comparison of gradient norms before and after clipping for both ZClip and fixed-threshold clipping methods.  Panel (c) is not included in the provided document.  Panel (a) shows that before clipping, ZClip exhibits small, transient spikes early in training, while the fixed-threshold method displays larger and more frequent deviations.  Panel (b) demonstrates that after clipping, ZClip effectively suppresses fluctuations, maintaining smooth and stable gradient norms, unlike the fixed-threshold method which fails to adapt and exhibits persistent spikes.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_7e4_before.png", "caption": "((d))", "description": "Figure 7(d) visualizes the distribution of gradient norms after applying clipping.  It shows that, unlike the fixed-threshold clipping method which exhibits frequent spikes even after clipping, ZClip effectively suppresses large gradient norm fluctuations, maintaining a smooth and stable distribution. This highlights ZClip's ability to adapt to the dynamic nature of training while simultaneously ensuring stability.", "section": "4 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_7e4_after.png", "caption": "((e))", "description": "Figure 5 shows the training loss, gradient norms before clipping, and gradient norms after clipping for a LLaMA 1B model trained with two different gradient clipping methods (ZClip and fixed-threshold clipping) at two different learning rates (5.0e-3 and 3.0e-3).  At the higher learning rate (3.0e-3), ZClip successfully stabilized gradients and prevented post-clipping spikes, unlike the fixed-threshold clipping which resulted in instability. At the higher learning rate (5.0e-3), both methods saturated, indicating that even the adaptive approach of ZClip cannot handle excessively high learning rates.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/5e4.png", "caption": "((f))", "description": "This figure shows the comparison of gradient norms before and after clipping for ZClip and fixed-threshold clipping at various learning rates.  The left column displays the training loss, the middle column shows the gradient norms before clipping, and the right column shows the gradient norms after clipping.  For higher learning rates, ZClip stabilizes gradients and significantly reduces post-clipping spikes, while fixed-threshold clipping results in accumulated instability.  At lower learning rates, both methods show more stability, but ZClip generally maintains smoother gradient behavior.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_5e4_before.png", "caption": "((g))", "description": "Figure 7 visualizes the gradient norms for ZClip and fixed-threshold clipping methods (threshold = 1.0) during training with a learning rate of 1e-3.  Subfigure (a) shows the gradient norms *before* clipping. ZClip initially displays small, transient spikes early in training, while fixed-threshold clipping exhibits larger and more frequent deviations from the mean. Subfigure (b) presents the gradient norms *after* clipping. ZClip effectively suppresses fluctuations, maintaining smooth and stable norms. In contrast, fixed-threshold clipping fails to adapt to the changing distribution, resulting in persistent post-clipping spikes and instability. This highlights ZClip's superior ability to maintain stable gradients and handle variations in gradient norm distributions during training.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_5e4_after.png", "caption": "((h))", "description": "This figure shows the training loss curves for three different z-score adjustment functions used in ZClip: clipping to mean (red), clipping to max (green), and reciprocal clipping (purple).  The x-axis represents the z-score (zt), which measures how many standard deviations the current gradient norm is from the running mean, and the y-axis represents the adjusted z-score (z't) after applying the clipping function. The reciprocal clipping function provides a continuous and more balanced adjustment compared to the others, avoiding sharp discontinuities while still effectively controlling extreme gradients.", "section": "2.3 ZClip"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/3e4.png", "caption": "((i))", "description": "This figure shows the distribution of gradient norms during early training (steps 500-635) and mid-training (steps 3050-3185).  The black curve represents the best-fit normal distribution.  Early training shows a mild skewness and heavier tails, while mid-training displays improved symmetry and tighter variance, indicating a better approximation to a normal distribution as training progresses.  This is visual evidence supporting the assumption in ZClip that gradient norms approximately follow a normal distribution in short time windows.", "section": "6.4 Normality Assumption of Gradient Norms"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_3e4_before.png", "caption": "((j))", "description": "This figure compares the training loss and gradient norms (before and after clipping) for ZClip and fixed-threshold clipping across different learning rates. It shows that ZClip effectively stabilizes training, especially at higher learning rates, preventing spikes and maintaining smoother convergence. In contrast, fixed-threshold clipping struggles to adapt to the changing dynamics of gradient norms, leading to instability and persistent spikes, especially at higher learning rates.", "section": "4 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_3e4_after.png", "caption": "((k))", "description": "This figure compares the training loss for three different methods: training without gradient clipping, using fixed-threshold gradient clipping, and using the proposed ZClip method.  The x-axis represents the number of tokens processed during training, and the y-axis represents the training loss.  The graph shows that training without clipping results in unstable training with large loss spikes and divergence. Fixed-threshold clipping helps to stabilize training somewhat but still leads to some divergence. In contrast, ZClip shows smooth and stable loss, demonstrating its effectiveness at mitigating loss spikes.", "section": "2 Gradient Clipping Methods"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/1e4.png", "caption": "((l))", "description": "This figure compares the training loss, gradient norm before clipping, and gradient norm after clipping for different learning rates using both ZClip and fixed-threshold clipping.  It demonstrates ZClip's effectiveness in stabilizing training even at higher learning rates where fixed-threshold clipping fails, due to ZClip's ability to adapt dynamically to the evolving gradient norm distribution.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_1e4_before.png", "caption": "((m))", "description": "The figure displays training loss graphs for three different training scenarios using a LLaMA 1B model: training without any clipping, using a constant clipping threshold of 1.0, and employing ZClip.  It visually demonstrates how ZClip helps prevent the sharp, unstable loss spikes observed in the other two methods, allowing for smoother, more efficient training.  The x-axis represents the total number of tokens processed during training, while the y-axis represents the training loss.", "section": "3 Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/lr_1e4_after.png", "caption": "((n))", "description": "This figure compares the training loss curves for three different approaches: training without gradient clipping, training with a fixed threshold gradient clipping method, and training with the proposed ZClip method.  The x-axis represents the number of tokens processed during training (in billions), and the y-axis represents the training loss. The plot visually demonstrates that ZClip effectively prevents the large loss spikes that are observed with the other two methods, while maintaining comparable convergence speed.", "section": "2 Gradient Clipping Methods"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/grad_norm_before3.png", "caption": "((o))", "description": "Figure 7 visualizes the comparison of gradient norms between ZClip and fixed-threshold clipping with a threshold of 1.0, trained using a learning rate of 1e-3.  The subfigure (a) presents the gradient norms before clipping, demonstrating that ZClip exhibits small, transient spikes only in the initial stages of training, while fixed-threshold clipping shows larger and more frequent deviations. Subfigure (b) displays the gradient norms after clipping, illustrating that ZClip effectively suppresses these fluctuations, maintaining smooth and stable norms. In contrast, fixed-threshold clipping fails to adapt to the evolving distribution, resulting in persistent post-clipping spikes and instability. This highlights ZClip's adaptive nature, making it superior for maintaining gradient stability during training.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/grad_norm_after3.png", "caption": "Figure 6: ZClip and fixed-threshold clipping at lower learning rates. Each row shows training loss (left), gradient norm before clipping (middle), and after clipping (right).\nZClip preserves stability and convergence also at smaller learning rates, while fixed-threshold still struggles with (benign) spikes.", "description": "Figure 6 presents a comparison of ZClip and fixed-threshold gradient clipping methods under lower learning rates.  It's organized into rows, each displaying three plots for a specific learning rate:  training loss, gradient norm before clipping, and gradient norm after clipping.  The plots visually demonstrate that ZClip maintains stability and convergence even with lower learning rates, preventing the benign spikes observed with the traditional fixed-threshold approach.  This suggests ZClip's effectiveness in handling minor fluctuations without excessive regularization.", "section": "4.2 Performance at Lower Learning Rates"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/clip_algo2.png", "caption": "((a))", "description": "Figure 7(a) presents a comparison of the gradient norms before clipping for ZClip and fixed-threshold clipping methods.  The figure shows that, early in the training process, ZClip exhibits small, transient spikes in gradient norms, while the fixed-threshold method demonstrates larger and more frequent deviations. This highlights the adaptive nature of ZClip and its ability to effectively manage smaller fluctuations while the fixed-threshold method does not adapt to the evolving distribution and overreacts to even small changes.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/nd1.png", "caption": "((b))", "description": "Figure 7(b) displays the gradient norms after clipping is applied.  It visually demonstrates the effectiveness of ZClip in stabilizing gradient norms compared to fixed-threshold clipping. ZClip maintains smooth, stable norms throughout training, while fixed-threshold clipping continues to exhibit fluctuations and larger deviations post-clipping, showing its inability to adapt to the dynamically changing gradient distribution during training.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/nd2.png", "caption": "Figure 7: Comparison of gradient norms for ZClip and fixed-threshold clipping with threshold value of 1.0, trained with a learning rate of 1e-3.\n(a) Before clipping: ZClip exhibits small, transient spikes early in training, while fixed-threshold clipping shows larger and more frequent deviations.\n(b) After clipping: ZClip effectively suppresses these fluctuations, maintaining smooth and stable norms.\nIn contrast, fixed-threshold clipping fails to adapt to the evolving distribution, resulting in persistent post-clipping spikes and instability.", "description": "Figure 7 presents a comparison of gradient norms before and after applying ZClip and fixed-threshold clipping methods during training.  The comparison is performed using a learning rate of 1e-3. Panel (a) shows the gradient norms *before* clipping. ZClip displays small, temporary spikes in the early phase of training, whereas the fixed-threshold approach exhibits noticeably larger and more frequent deviations.  Panel (b) presents the gradient norms *after* clipping has been applied. ZClip effectively reduces the fluctuations to smooth, stable levels; however, the fixed-threshold method fails to adapt to the changes in data distribution over time, leading to persistent, large spikes even after clipping.  This visualization demonstrates ZClip\u2019s superiority in maintaining stable training dynamics by dynamically adjusting to the evolving gradient norm distribution, unlike the fixed-threshold method which uses a static threshold ill-suited for changing distributions.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/through.png", "caption": "Figure 8: Training loss for three ZClip variants\u2014max, reciprocal, and mean.\nThe learning rate for all experiments is 1.0\u00d710\u221231.0superscript1031.0\\times 10^{-3}1.0 \u00d7 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT.", "description": "This figure compares the training loss curves for three variations of the ZClip algorithm: max, reciprocal, and mean.  Each variation uses a different approach for adjusting the gradient norm when a spike is detected.  The experiment is conducted with a fixed learning rate of 1.0 x 10^-3, allowing for a comparison of how effectively each approach handles gradient instabilities during training.", "section": "6.3 ZClip - \"Clipping to mean and max\""}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/clip_percent.png", "caption": "((a))", "description": "Figure 7(a) shows the comparison of gradient norms before clipping for ZClip and fixed-threshold clipping methods.  It visualizes the gradient norms for both methods throughout the training process.  This comparison reveals that ZClip, unlike the fixed-threshold method, exhibits only small, short-lived spikes early in the training. The fixed-threshold method displays significantly larger and more frequent deviations of gradient norms. This difference highlights ZClip's ability to maintain smoother and more stable gradient norms compared to the traditional fixed-threshold method.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/mean.png", "caption": "((b))", "description": "Figure 7(b) displays the gradient norms *after* clipping has been applied.  It shows the impact of ZClip and standard fixed-threshold clipping methods.  ZClip's results show stable, smooth gradient norms, indicating its effective suppression of sudden, large gradient increases. In contrast, the fixed-threshold approach demonstrates ongoing, frequent spikes even after clipping, highlighting its inability to maintain stable gradients.", "section": "Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02507/extracted/6297715/data/std.png", "caption": "Figure 9: Distribution of gradient norms in (a) Early training (steps 500\u2013635) and (b) Mid training (steps 3050\u20133185). The black curve shows the best-fit normal distribution. Early training exhibits mild skewness and heavier tails, while mid training displays improved symmetry and tighter variance.", "description": "Figure 9 presents the distributions of gradient norms during early (steps 500-635) and mid (steps 3050-3185) training phases.  Histograms are shown alongside the best-fit normal distributions.  The early training phase shows a distribution with mild skewness (a slight asymmetry) and heavier tails (more data points far from the mean) compared to a perfect normal distribution. In contrast, the mid-training phase displays a distribution that is more symmetrical and has a tighter variance (data points are more concentrated around the mean). This visual comparison illustrates how the distribution of gradient norms evolves during the training process, becoming more normally distributed as training progresses. This is important because ZClip's anomaly detection algorithm relies on the assumption that gradient norms approximately follow a normal distribution.", "section": "6.4 Normality Assumption of Gradient Norms"}]