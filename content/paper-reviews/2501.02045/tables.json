[{"content": "| Model Details | METAGENE-1 | \n|---|---| \n| Architecture | Llama-2-7B | \n| Embedding Size | 4096 | \n| Intermediate Size | 11008 | \n| Number of Attention Heads | 32 | \n| Number of Hidden Layers | 32 | \n| Vocabulary Size | 1024 | \n| Sequence Length | 512 | \n| Normalization | RMSNorm | \n| Regularization | z-loss | \n| Position Embedding | Rotary | \n| Bias | None | \n| Warmup Steps | 2000 | \n| Batch Size | 30720 | \n| Weight Decay | 0.1 | \n| Learning Rate Schedule | Cosine Decay | \n| Initial Learning Rate | 6 \u00d7 10<sup>-4</sup> | \n| \u03b2<sub>1</sub>, \u03b2<sub>2</sub> | 0.9, 0.95 |", "caption": "Table 1: METAGENE-1 architecture details.", "description": "This table provides detailed specifications of the METAGENE-1 architecture, including the model type (autoregressive transformer), embedding size, number of attention heads, hidden layer size, normalization method, regularization type, positional embedding technique, bias usage, warmup steps, batch size, weight decay, learning rate schedule, initial learning rate, and beta parameters (\u03b21, \u03b22).  It compares these hyperparameters to those of the Llama-2-7B model for context and shows the overall model size.", "section": "3. METAGENE-1: Metagenomic Foundation Model"}, {"content": "|               | **DNABERT-2** | **DNABERT-S** | **NT-2.5b-Multi** | **NT-2.5b-1000g** | **METAGENE-1** |\n| :------------ | :------------: | :------------: | :---------------: | :---------------: | :------------: |\n| **Pathogen-Detect (avg.)** | 87.92          | 87.02          | 82.43             | 79.02             | **92.96**          |\n| **Pathogen-Detect-1** | 86.73          | 85.43          | 83.80             | 77.52             | **92.14**          |\n| **Pathogen-Detect-2** | 86.90          | 85.23          | 83.53             | 80.38             | **90.91**          |\n| **Pathogen-Detect-3** | 88.30          | 89.01          | 82.48             | 79.83             | **93.70**          |\n| **Pathogen-Detect-4** | 89.77          | 88.41          | 79.91             | 78.37             | **95.10**          |", "caption": "Table 2: Results on the Pathogen Detection benchmark. The metric used for all evaluations is MCC. The header row reports macro-averaged performance metrics. See Section\u00a05.2 for details.", "description": "This table presents the results of the pathogen detection benchmark.  Four different datasets (PATHOGEN-DETECT-1 through 4), each derived from separate wastewater sequencing deliveries and excluding data used for model pretraining, were used for evaluation. The model's performance on each dataset is evaluated using the Matthews Correlation Coefficient (MCC), a measure commonly used for evaluating classification model performance.  The header row displays the macro-averaged MCC across all four datasets, providing a single summary metric.  Details on how these datasets were constructed and the specifics of the evaluation methodology are provided in Section 5.2 of the paper. ", "section": "5.2. Pathogen Detection Benchmark"}, {"content": "|               | **DNABERT-2** | **DNABERT-S** | **NT-2.5b-Multi** | **NT-2.5b-1000g** | **METAGENE-1** |\n| :------------- | :-------------: | :-------------: | :----------------: | :----------------: | :-------------: |\n| **Human-Virus (avg.)** | 0.564          | 0.570          | 0.675              | 0.710              | **0.775**          |\n| **Human-Virus-1** | 0.594          | 0.605          | 0.671              | 0.721              | **0.828**          |\n| **Human-Virus-2** | 0.507          | 0.510          | 0.652              | 0.624              | **0.742**          |\n| **Human-Virus-3** | 0.606          | 0.612          | 0.758              | 0.740              | **0.835**          |\n| **Human-Virus-4** | 0.550          | 0.551          | 0.620              | **0.755**          | 0.697          |\n| **HMPD (avg.)** | 0.397          | 0.403          | 0.449              | 0.451              | **0.465**          |\n| **HMPD-single** | 0.292          | 0.293          | 0.285              | 0.292              | **0.297**          |\n| **HMPD-disease** | 0.480          | 0.486          | 0.498              | 0.489              | **0.542**          |\n| **HMPD-sex**    | 0.366          | 0.367          | 0.487              | 0.476              | **0.495**          |\n| **HMPD-source** | 0.451          | 0.465          | 0.523              | **0.545**          | 0.526          |\n| **HVR (avg.)**  | 0.479          | 0.479          | 0.546              | 0.524              | **0.550**          |\n| **HVR-p2p**    | 0.548          | 0.550          | 0.559              | **0.650**          | 0.466          |\n| **HVR-s2s-align** | 0.243          | 0.241          | 0.266              | **0.293**          | 0.267          |\n| **HVR-s2s-small** | 0.373          | 0.372          | 0.357              | 0.371              | **0.467**          |\n| **HVR-s2s-tiny** | 0.753          | 0.753          | 1.000              | 0.782              | **1.000**          |\n| **HMPR (avg.)** | 0.347          | 0.351          | 0.348              | 0.403              | **0.476**          |\n| **HMPR-p2p**   | 0.566          | **0.580**          | 0.471              | 0.543              | 0.479          |\n| **HMPR-s2s-align** | 0.127          | 0.129          | 0.144              | **0.219**          | 0.140          |\n| **HMPR-s2s-small** | 0.419          | 0.421          | 0.443              | **0.459**          | 0.432          |\n| **HMPR-s2s-tiny** | 0.274          | 0.274          | 0.332              | 0.391              | **0.855**          |\n| **Global Average** | 0.475          | 0.479          | 0.525              | 0.545              | **0.590**          |", "caption": "Table 3: Results on the Genomic Embedding (Gene-MTEB) benchmark. See Section\u00a05.3 for details.", "description": "Table 3 presents the results of the Gene-MTEB benchmark, a newly introduced evaluation metric designed to assess the quality of genomic sequence embeddings generated by various foundation models.  It evaluates the models' zero-shot performance on eight classification and eight clustering tasks using data from the Human Microbiome Project and a held-out portion of the METAGENE-1 metagenomic dataset. The table compares the performance of METAGENE-1 against several other state-of-the-art genomic models, specifically DNABERT-2, DNABERT-S, and two variants of Nucleotide Transformer (NT-2.5b-Multi and NT-2.5b-1000g).  The results highlight METAGENE-1's performance across different genomic tasks.", "section": "5.3. Genomic Embedding Benchmark"}, {"content": "|       | CNN | HyenaDNA | DNABERT | NT-2.5B-Multi | DNABERT-2 | METAGENE-1 |\n| :---: | :-: | :-: | :-: | :-: | :-: | :-: |\n| **TF-Mouse (avg.)** | 45.3 | 51.0 | 57.7 | 67.0 | 68.0 | **71.4** |\n| **0** | 31.1 | 35.6 | 42.3 | **63.3** | 56.8 | 61.5 |\n| **1** | 59.7 | 80.5 | 79.1 | 83.8 | **84.8** | 83.7 |\n| **2** | 63.2 | 65.3 | 69.9 | 71.5 | 79.3 | **83.0** |\n| **3** | 45.5 | 54.2 | 55.4 | 69.4 | 66.5 | **82.2** |\n| **4** | 27.2 | 19.2 | 42.0 | 47.1 | **52.7** | 46.6 |\n| **TF-Human (avg.)** | 50.7 | 56.0 | 64.4 | 62.6 | **70.1** | 68.3 |\n| **0** | 54.0 | 62.3 | 68.0 | 66.6 | **72.0** | 68.9 |\n| **1** | 63.2 | 67.9 | 70.9 | 66.6 | **76.1** | 70.8 |\n| **2** | 45.2 | 46.9 | 60.5 | 58.7 | **66.5** | 65.9 |\n| **3** | 29.8 | 41.8 | 53.0 | 51.7 | **58.5** | 58.1 |\n| **4** | 61.5 | 61.2 | 69.8 | 69.3 | 77.4 | **77.9** |\n| **EMP (avg.)** | 37.6 | 44.9 | 49.5 | 58.1 | 56.0 | **66.0** |\n| **H3** | 61.5 | 67.2 | 74.2 | 78.8 | 78.3 | **80.2** |\n| **H3K14ac** | 29.7 | 32.0 | 42.1 | 56.2 | 52.6 | **64.9** |\n| **H3K36me3** | 38.6 | 48.3 | 48.5 | 62.0 | 56.9 | **66.7** |\n| **H3K4me1** | 26.1 | 35.8 | 43.0 | 55.3 | 50.5 | **55.3** |\n| **H3K4me2** | 25.8 | 25.8 | 31.3 | 36.5 | 31.1 | **51.2** |\n| **H3K4me3** | 20.5 | 23.1 | 28.9 | 40.3 | 36.3 | **58.5** |\n| **H3K79me3** | 46.3 | 54.1 | 60.1 | 64.7 | 67.4 | **73.0** |\n| **H3K9ac** | 40.0 | 50.8 | 50.5 | 56.0 | 55.6 | **65.5** |\n| **H4** | 62.3 | 73.7 | 78.3 | 81.7 | 80.7 | **82.7** |\n| **H4ac** | 25.5 | 38.4 | 38.6 | 49.1 | 50.4 | **61.7** |\n| **PD (avg.)** | 77.1 | 35.0 | 84.6 | **88.1** | 84.2 | 82.3 |\n| **All** | 75.8 | 47.4 | 90.4 | **91.0** | 86.8 | 86.0 |\n| **No-TATA** | 85.1 | 52.2 | 93.6 | 94.0 | **94.3** | 93.7 |\n| **TATA** | 70.3 | 5.3 | 69.8 | **79.4** | 71.6 | 67.4 |\n| **CPD (avg.)** | 62.5 | 48.4 | **73.0** | 71.6 | 70.5 | 69.9 |\n| **All** | 58.1 | 37.0 | **70.9** | 70.3 | 69.4 | 66.4 |\n| **No-TATA** | 60.1 | 35.4 | 69.8 | **71.6** | 68.0 | 68.3 |\n| **TATA** | 69.3 | 72.9 | **78.2** | 73.0 | 74.2 | 75.1 |\n| **SSD** | 76.8 | 72.7 | 84.1 | **89.3** | 85.0 | 87.8 |\n| **COVID** | 22.2 | 23.3 | 62.2 | **73.0** | 71.9 | 72.5 |\n| **Global Win %** | 0.0 | 0.0 | 7.1 | 21.4 | 25.0 | **46.4** |", "caption": "Table 4: Results on the Genome Understanding Evaluation (GUE) benchmark. Non-METAGENE-1 results are adapted from Zhou et\u00a0al. (2023). The metric used for all evaluations is MCC, except for the COVID task, which uses F1 score. The header rows report macro-averaged performance metrics. The final row shows Global Win %, i.e., the percentage of tasks in which a given method achieves top score under the associated metric.", "description": "Table 4 presents the results of the Genome Understanding Evaluation (GUE) benchmark, comparing METAGENE-1's performance against other state-of-the-art genomic foundation models.  The GUE benchmark consists of 28 sequence-level classification tasks focused on various aspects of genomics.  The primary evaluation metric is the Matthews Correlation Coefficient (MCC), except for the COVID-19 task which uses the F1 score. The table displays the MCC or F1 score for each model on each task and then shows the macro-averaged performance across all tasks.  The final row shows the 'Global Win %', indicating the percentage of tasks where each model achieved the highest score. Results for models other than METAGENE-1 are taken from Zhou et al. (2023). This provides a comprehensive comparison of METAGENE-1's performance across a broad range of genomic tasks.", "section": "5.4. Genome Understanding Evaluation Benchmark"}, {"content": "| Group | F1 | Loss (Std. Err) | Tokenized Seq Len (Std. Dev) |\n|---|---|---|---| \n| **Metagenomics** | - | 1.24 (1.31) | 24.91 (3.35) |\n| **Random** | 0.91 | 5.83 (0.29) | 27.16 (1.32) |\n| **Human** | 0.94 | 5.22 (0.22) | 27.29 (1.33) |\n| **Mouse** | 0.91 | 5.38 (0.54) | 27.2 (1.34) |", "caption": "Table 5: OOD detection performance between metagenomics sequences and other data sources.", "description": "This table presents the out-of-distribution (OOD) detection performance of the METAGENE-1 model.  It compares the model's ability to distinguish metagenomic sequences from other data sources, including randomly generated sequences and sequences from human and mouse genomes.  The evaluation metric used is F1 score, and the table shows the F1 scores and standard errors for each data source, along with the average sequence length for each data source. The results demonstrate METAGENE-1's effectiveness in identifying metagenomic sequences as in-distribution data and other types of sequences as out-of-distribution data.", "section": "5.5. Anomaly Detection from Wastewater"}, {"content": "| Model | Setting |\n|---|---| \n| DNABERT-<span class=\"ltx_Math\">\u22c6</span> | Full Model |\n| NT-<span class=\"ltx_Math\">\u22c6</span> | LoRA |\n| METAGENE-1 | LoRA |\n| LoRA Modules | query, key, value, dense |\n| LoRA Rank | 8 |\n| LoRA <span class=\"ltx_Math\">\u03b1</span> | 16 |\n| LoRA Dropout | 0.1 |\n| Optimizer | AdamW |\n| Optimizer Momentum | <span class=\"ltx_Math\">\u03b2<sub>1</sub></span>, <span class=\"ltx_Math\">\u03b2<sub>2</sub></span> = 0.9, 0.999 |\n| Learning Rate | 1e-4<sup>\u039b</sup> |\n| LR Scheduler | Linear Warmup + Constant LR |\n| Warmup Steps | 50 |\n| Weight Decay | 0.01 |\n| Denominator <span class=\"ltx_Math\">\u03f5</span> | 1e-8 |\n| Precision | BF16-mixed |\n| Batch Size | 32 |\n| Epochs | 10 |\n| Hardware | NVIDIA A100 80GB |", "caption": "Table 6: Hyperparameter settings for the Pathogen Detection fine-tuning experiments. \u039b\u039b\\Lambdaroman_\u039b: for DNABERT-S, we halve the learning to 5e-5 as we observe clear oscillation behavior in the training loss.", "description": "This table details the hyperparameters used for fine-tuning various models on the pathogen detection task.  It lists the model (DNABERT-2, DNABERT-S, Nucleotide Transformer variants, and METAGENE-1), the type of fine-tuning (full model or LoRA), the LoRA modules used (if applicable), LoRA rank, alpha, and dropout rate. Optimizer parameters (optimizer, momentum, learning rate, learning rate scheduler, warmup steps, weight decay, denominator epsilon), precision, batch size, and number of epochs are also provided.  A note indicates that for DNABERT-S, the learning rate was halved due to observed oscillations in training loss.", "section": "5.2. Pathogen Detection Benchmark"}, {"content": "| LoRA Modules | query, key, value, dense<sup>\u039b</sup> | \n|---|---| \n| LoRA Rank | 8 | \n| LoRA \u03b1 | 16 | \n| LoRA Dropout | 0.1 | \n| Optimizer | AdamW | \n| Optimizer Momentum (\u03b2<sub>1</sub>, \u03b2<sub>2</sub>) | 0.9, 0.999 | \n| Learning Rate | {1e-4\u22ef1e-3}<sup>\u03a9</sup> | \n| LR Scheduler | Linear Warmup + Constant LR | \n| Warmup Steps | 50 | \n| Weight Decay | 0.01 | \n| Denominator \u03f5 | 1e-8 | \n| Precision | BF16-mixed | \n| Batch Size | 32 | \n| Epochs | 10 | \n| Hardware | NVIDIA A100 80GB | ", "caption": "Table 7: Hyperparameter settings for the GUE fine-tuning experiments. \u039b\u039b\\Lambdaroman_\u039b: LoRA is applied to query-value or query-key-value-dense modules. \u03a9\u03a9\\Omegaroman_\u03a9: learning rates are tuned over a equally-spaced grid of 1e-4, 2e-4, \u22ef\u22ef\\cdots\u22ef, 1e-3. All hyperparameters are selected according to performances on validation sets.", "description": "Table 7 details the hyperparameters used during fine-tuning experiments for the Genome Understanding Evaluation (GUE) benchmark.  It specifies that Low-Rank Adaptation (LoRA) was applied to either query-value or query-key-value-dense modules, with a rank of 8 and alpha of 16.  A dropout rate of 0.1 was used.  The AdamW optimizer was employed with a learning rate tuned across an equally-spaced grid from 1e-4 to 1e-3, along with other standard hyperparameter settings. All hyperparameters were selected based on their performance on validation sets.", "section": "C.2. Additional Details for the GUE Benchmark"}]