[{"figure_path": "https://arxiv.org/html/2504.05118/extracted/6342259/fig/score.png", "caption": "Figure 1: AIME 2024 scores of VAPO on the Qwen2.5-32B base model, demonstrates significant superiority over the previous state-of-the-art (SOTA) method DAPO, achieving this with notably fewer training stepso. The x-axis denotes the gradient update steps.", "description": "This figure displays the performance of the VAPO model on the AIME 2024 benchmark dataset.  The VAPO model, based on the Qwen-2.5-32B language model, significantly outperforms the previous state-of-the-art (SOTA) method, DAPO.  The y-axis represents the accuracy achieved on the AIME 2024 dataset, while the x-axis shows the number of gradient update steps during training.  The graph clearly illustrates VAPO's superior performance and faster convergence, achieving high accuracy with far fewer training steps than DAPO.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.05118/extracted/6342259/fig/length.png", "caption": "(a) Mean response length.", "description": "This figure shows the mean response length over training steps for both VAPO and DAPO.  It illustrates how the average length of the model's generated responses changes as the model is trained using reinforcement learning.  The plot helps visualize the effects of the algorithms (VAPO and DAPO) on response length and allows for a comparison of their training dynamics in terms of response generation.", "section": "5.3 Training Dynamics"}, {"figure_path": "https://arxiv.org/html/2504.05118/extracted/6342259/fig/reward.png", "caption": "(b) Reward score.", "description": "This plot displays the reward score during the training process of the VAPO and DAPO models. The reward score is a crucial metric reflecting model performance on the reasoning task, where a higher score indicates better performance. The x-axis represents the training steps, showing the progression of training over time. The y-axis displays the reward score achieved by each model at each step. The plot visualizes how the reward score changes for both models across training steps, illustrating the learning progress and relative performance improvement of VAPO compared to DAPO.", "section": "5.3 Training Dynamics"}, {"figure_path": "https://arxiv.org/html/2504.05118/extracted/6342259/fig/entropy.png", "caption": "(c) Generation entropy.", "description": "Figure 2(c) displays the generation entropy over training steps for both VAPO and DAPO. Generation entropy measures the uncertainty or randomness in the model's output. Lower entropy indicates less uncertainty and more focused generation, while higher entropy indicates more exploration.  The plot shows how the entropy changes during training, reflecting the balance between exploration and exploitation strategies employed by the models. The comparison between VAPO and DAPO's entropy curves provides insight into their training stability and convergence behaviors.", "section": "5.3 Training Dynamics"}]