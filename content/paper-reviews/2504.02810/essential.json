{"importance": "KUMO offers a **robust and scalable benchmark** to evaluate LLMs' reasoning abilities, addressing contamination issues and **opening new avenues for genuine LLM assessment.** The finding of the **importance of domain diversity** will open a new way for future study.", "summary": "KUMO: A generative framework that dynamically evaluates complex reasoning in LLMs, revealing genuine generalization abilities beyond memorization.", "takeaways": ["Many LLMs have surpassed university-level performance on simple reasoning tasks.", "Reasoning-scaled LLMs achieve university-level performance on complex reasoning tasks.", "LLM performance on KUMO correlates strongly with real-world reasoning benchmarks."], "tldr": "LLMs have shown reasoning skills, but it's unclear if they truly reason or just memorize training data. Public benchmarks become unreliable due to contamination from being included in LLM training sets. Addressing these issues, this paper introduces a new evaluation framework, designed to assess reasoning in LLMs. This project combines LLMs with symbolic engines, generating multi-turn tasks, partially observable, and adjustable in difficulty. \n\nThe authors evaluated 23 LLMs on 5,000 tasks across 100 domains created. This new method benchmarks their reasoning abilities against university students. The results show that many LLMs outperform university-level performance on easy reasoning tasks. Also, LLM performance on tasks correlates with real-world reasoning benchmarks, indicating its value as an assessment tool.", "affiliation": "Peking University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.02810/podcast.wav"}