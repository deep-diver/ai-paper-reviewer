[{"Alex": "Hey podcast listeners, get ready to have your minds BLOWN! We're diving into some seriously fascinating stuff today. Forget everything you thought you knew about how AI 'sees' the world because we're about to uncover a secret weapon for making tech accessible to everyone!", "Jamie": "Wow, Alex, that's quite an intro! You've definitely piqued my interest. So, accessibility in AI... where do we even start?"}, {"Alex": "Alright Jamie, buckle up! We're talking about a new way to build datasets \u2013 the lifeblood of AI \u2013 specifically for blind and low-vision (BLV) users. The secret? Incorporating sighted user feedback to diagram descriptions to get sighted user insights.", "Jamie": "Diagrams, huh? So not just regular images. What makes diagrams so different, and why focus on BLV users in particular?"}, {"Alex": "Great question! Diagrams are information-dense. Think flowcharts, schematics\u2026 things that require careful interpretation. BLV users often rely on text descriptions to understand these, and current AI isn't always great at providing useful ones. What we found that blind instructors are amazing at using VLM to describe this diagrams", "Jamie": "Okay, that makes sense. So existing methods aren't cutting it. What's the core issue that your research tackles?"}, {"Alex": "The big problem is a 'preference misalignment.' The people creating these descriptions \u2013 sighted annotators \u2013 often don't understand what information is actually *useful* or *accessible* to someone who can't see the diagram. What we want is to get blind instructors give us the instructions.", "Jamie": "Hmm, so it's like translating a joke and it just doesn't land. You get the words right, but miss the humor. How did you bridge this gap?"}, {"Alex": "Exactly! We created a new dataset called SIGHTATION. Instead of having sighted people *write* descriptions, we had them *assess* descriptions generated by AI, but guided by something called 'latent supervision'.", "Jamie": "Latent supervision? Sounds fancy. Could you break that down for me and our listeners?"}, {"Alex": "It is a bit techy! Basically, we used a VLM to generate question-answer pairs about the diagram. This acts as a 'guide' for the second VLM, which then creates the actual description, in favor of blind user requirements. Sighted assessors can then rate the VLM-generated description by giving a score.", "Jamie": "So the AI is teaching the AI, with humans giving the final stamp of approval, by giving a ranking. That's a neat approach! What kind of feedback were these assessors providing?"}, {"Alex": "They were rating the descriptions on things like factuality, informativeness, succinctness, diversity - are they telling what is happening and not adding extra details - and overall usefulness. We even had BLV educators specifically assess how helpful the descriptions were for *their* students.", "Jamie": "Ah, getting input from the end-users themselves. That's smart! I imagine there were some surprising insights?"}, {"Alex": "Definitely. We found that descriptions considered highly useful by sighted individuals weren't always the best for BLV users, due to the different preferences. This validated our hypothesis that those users are key and it is worth the effort to get their input.", "Jamie": "So it's not just about 'seeing' all the details, but curating the information in a way that's meaningful. What are some real-world applications of this kind of dataset?"}, {"Alex": "Think accessible textbooks, educational resources, even workplace tools. Imagine a blind student being able to independently understand a complex scientific diagram, thanks to an AI-powered description tailored to their needs. We even used the dataset to fine tune models and they performed better!", "Jamie": "That sounds incredibly empowering. It goes beyond just providing information to fostering true understanding. Ummm, so, what's next? Are there limitations of the current study?"}, {"Alex": "Absolutely. We primarily relied on question-answer pairs to guide the description generation. Exploring alternative forms of supervision could be interesting to see what happens. Also, diagrams can get super complex. We might need even more sophisticated techniques to handle intricate details.", "Jamie": ""}, {"Alex": "We need even more sophisticated techniques to handle intricate details. But our work shows that involving the target audience in dataset creation \u2013 the BLV community \u2013 is crucial for building truly accessible AI.", "Jamie": "That's a powerful message, Alex. It's a reminder that AI shouldn't be a one-size-fits-all solution. So many fields out there are making this push towards personalization. It's great that the accessibility conversation is happening in a similar direction"}, {"Alex": "Alright Jamie, buckle up! We're talking about a new way to build datasets \u2013 the lifeblood of AI \u2013 specifically for blind and low-vision (BLV) users. The secret? Involving the people to make this data great", "Jamie": "Diagrams, huh? So not just regular images. What makes diagrams so different, and why focus on BLV users in particular?"}, {"Alex": "Great question! Diagrams are information-dense. Think flowcharts, schematics\u2026 things that require careful interpretation. BLV users often rely on text descriptions to understand these, and current AI isn't always great at providing useful ones.", "Jamie": "Okay, that makes sense. So existing methods aren't cutting it. What's the core issue that your research tackles?"}, {"Alex": "The big problem is a 'preference misalignment.' The people creating these descriptions \u2013 sighted annotators \u2013 often don't understand what information is actually *useful* or *accessible* to someone who can't see the diagram.", "Jamie": "Hmm, so it's like translating a joke and it just doesn't land. You get the words right, but miss the humor. How did you bridge this gap?"}, {"Alex": "Exactly! We created a new dataset called SIGHTATION. Instead of having sighted people *write* descriptions, we had them *assess* descriptions generated by AI, but guided by something called 'latent supervision'.", "Jamie": "Latent supervision? Sounds fancy. Could you break that down for me and our listeners?"}, {"Alex": "It is a bit techy! Basically, we used a VLM to generate question-answer pairs about the diagram. This acts as a 'guide' for the second VLM, which then creates the actual description.", "Jamie": "So the AI is teaching the AI, with humans giving the final stamp of approval. That's a neat approach! What kind of feedback were these assessors providing?"}, {"Alex": "They were rating the descriptions on things like factuality, informativeness, succinctness, diversity, and overall usefulness. We even had BLV educators specifically assess how helpful the descriptions were for *their* students.", "Jamie": "Ah, getting input from the end-users themselves. That's smart! I imagine there were some surprising insights?"}, {"Alex": "Definitely. We found that descriptions considered highly useful by sighted individuals weren't always the best for BLV users. Some sighted assessors were too wordy in descriptions when blind users needed succinct points", "Jamie": "So it's not just about 'seeing' all the details, but curating the information in a way that's meaningful. What are some real-world applications of this kind of dataset?"}, {"Alex": "Think accessible textbooks, educational resources, even workplace tools. Imagine a blind student being able to independently understand a complex scientific diagram, thanks to an AI-powered description tailored to their needs.", "Jamie": "That sounds incredibly empowering. It goes beyond just providing information to fostering true understanding. Ummm, so, what's next? Are there limitations of the current study?"}, {"Alex": "Absolutely. We primarily relied on question-answer pairs to guide the description generation. Exploring alternative forms of supervision could be interesting. But our work highlights the need to take a step back and get the users' input", "Jamie": ""}, {"Alex": "And that's the real takeaway: accessible AI isn't just about building better algorithms; it's about building *inclusive* processes.", "Jamie": "Such an important point. Alex, thanks so much for sharing your insights with us today! It\u2019s given me a lot to think about when I build my data models. Looking forward to seeing what happens next in this area!"}]