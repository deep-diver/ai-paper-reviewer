[{"figure_path": "https://arxiv.org/html/2504.13173/extracted/6367629/MIRAS.png", "caption": "Figure 1: The overview of Miras framework. Miras is based on four critical choices of (1) memory architecture, (2) attentional bias, (3) retention gate, and (4) memory learning algorithm. In this framework, the memory architecture determines the model capacity to memorize; attentional bias is responsible for modeling the underlying mapping patterns; retention gate determines how to balance learning new concepts and the retention of previously learned concepts; and memory learning algorithm is responsible for memory management.", "description": "The MIRAS framework is a general framework for designing sequence models based on four key choices: memory architecture, attentional bias, retention gate, and memory learning algorithm.  The memory architecture determines the model's capacity to store information. The attentional bias defines how the model learns the relationships between inputs (keys) and outputs (values).  The retention gate controls the balance between learning new information and retaining previously learned information. Finally, the memory learning algorithm specifies how the model updates its memory.  Different combinations of these four components lead to models with various strengths and weaknesses.", "section": "Associative Memory, Attentional Bias, and Retention"}, {"figure_path": "https://arxiv.org/html/2504.13173/extracted/6367629/MIRAS-Block.png", "caption": "Figure 2: Visualization of the Miras\u2019s variant architecture, their hybrid counterpart with SWA, and block design of Miras layer.", "description": "This figure visualizes the architecture of three novel sequence models (MONETA, YAAD, and MEMORA) based on the MIRAS framework.  It shows the individual components of a MIRAS layer, including the MLP, normalization, SwiGLU activation, and residual connections. It also illustrates how these layers are combined in both a purely MIRAS-based architecture and a hybrid architecture that incorporates Sliding Window Attention (SWA). The hybrid architecture combines the strengths of both MIRAS and SWA, potentially offering a superior balance of long-context modeling capabilities and parallelizable training.", "section": "4 MIRAS: Learning to Memorize with Robust and Expressive Memory"}]