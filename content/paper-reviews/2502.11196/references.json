{"references": [{"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-12-06", "reason": "This paper introduces a novel approach to training generalized multi-query transformer models, which is relevant to the study of knowledge circuits in LLMs."}, {"fullname_first_author": "Zeyuan Allen-Zhu", "paper_title": "Physics of language models: Part 3.1, knowledge storage and extraction", "publication_date": "2024-07-21", "reason": "This paper provides a theoretical framework for understanding knowledge storage and extraction in LLMs, which is crucial for analyzing the evolution of knowledge circuits."}, {"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-06", "reason": "This foundational paper demonstrates the few-shot learning capabilities of LLMs, which are leveraged in the continual pre-training strategies examined in the study."}, {"fullname_first_author": "Damai Dai", "paper_title": "Knowledge neurons in pretrained transformers", "publication_date": "2022-05-22", "reason": "This paper introduces the concept of knowledge neurons, which are relevant to the study of knowledge circuits as a means of understanding knowledge representation in LLMs."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-07", "reason": "This paper introduces the LLaMa 3 family of models, which are used in this study to provide consistent findings on different architectures and help in analysing the evolution of knowledge circuits."}]}