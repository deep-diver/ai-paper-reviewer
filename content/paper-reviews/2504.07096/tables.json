[{"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S2.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S2.T1.1.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.1.1\">Stage</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S2.T1.1.1.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.2.1\">Dataset</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.3.1\"># Docs</span></th>\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\" id=\"S2.T1.1.1.1.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.1.1.4.1\"># Tokens</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.1.2.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">pre-training</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S2.T1.1.1.2.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/allenai/olmo-mix-1124\" title=\"\">allenai/olmo-mix-1124</a></th>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.1.2.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3081 M</td>\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S2.T1.1.1.2.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">4575 B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T1.1.1.3.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">mid-training</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T1.1.1.3.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/allenai/dolmino-mix-1124\" title=\"\">allenai/dolmino-mix-1124</a></th>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.1.3.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">81 M</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.1.3.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">34 B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T1.1.1.4.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">post-training</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S2.T1.1.1.4.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/allenai/tulu-3-sft-olmo-2-mixture-0225\" title=\"\">SFT</a> &amp; <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/allenai/olmo-2-0325-32b-preference-mix\" title=\"\">DPO</a> &amp; <a class=\"ltx_ref ltx_href\" href=\"https://huggingface.co/datasets/allenai/RLVR-GSM-MATH-IF-Mixed-Constraints\" title=\"\">RLVR</a>\n</th>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.1.4.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.7 M</td>\n<td class=\"ltx_td ltx_align_right\" id=\"S2.T1.1.1.4.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.6 B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.5.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.5.4.1.1\">Total</span></th>\n<th class=\"ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.5.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"></th>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.5.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.5.4.3.1\">3164 M</span></td>\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S2.T1.1.1.5.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.1.5.4.4.1\">4611 B</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: \nThe full training data of OLMo-2-32B-Instruct, which OLMoTrace matches against. For mid-training data, we excluded sources that already appeared in the pre-training data, from both the statistics and the index.", "description": "This table details the composition of the training data used for the OLMo-2-32B-Instruct language model, specifically showing the number of documents and tokens in the pre-training, mid-training, and post-training datasets. Note that for the mid-training data, any sources that were already present in the pre-training data were excluded from the statistics and the index used for matching by OLMOTrace.  The total number of documents and tokens across all stages is also provided, showing the massive scale of the model's training data.", "section": "2 System Description"}, {"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"A3.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A3.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A3.T2.1.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T2.1.1.1.1.1.1\">Score</span></th>\n<th class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt\" id=\"A3.T2.1.1.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T2.1.1.1.1.2.1\">\n<span class=\"ltx_p\" id=\"A3.T2.1.1.1.1.2.1.1\" style=\"width:320.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T2.1.1.1.1.2.1.1.1\">Description</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A3.T2.1.1.2.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T2.1.1.2.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T2.1.1.2.1.2.1\">\n<span class=\"ltx_p\" id=\"A3.T2.1.1.2.1.2.1.1\" style=\"width:320.0pt;\">The snippet or context of the snippet is about a different topic than the query and model response (though possibly semantically similar): \n<br class=\"ltx_break\"/>For example, for the query breast cancer symptoms, give a 0 to: \n<br class=\"ltx_break\"/>\u2003A snippet about heart attack symptoms \u2013 wrong topic \n<br class=\"ltx_break\"/>\u2003A snippet about brain cancer symptoms \u2013 may not necessarily apply to breast cancer symptoms</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A3.T2.1.1.3.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T2.1.1.3.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T2.1.1.3.2.2.1\">\n<span class=\"ltx_p\" id=\"A3.T2.1.1.3.2.2.1.1\" style=\"width:320.0pt;\">The snippet or context of the snippet is about a broader topic than the query and model response, or is potentially relevant but there\u2019s not enough information: \n<br class=\"ltx_break\"/>For example, for the query breast cancer symptoms, give a 1 to: \n<br class=\"ltx_break\"/>\u2003A snippet about cancer in general \u2013 missing key specifics of symptoms</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A3.T2.1.1.4.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T2.1.1.4.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T2.1.1.4.3.2.1\">\n<span class=\"ltx_p\" id=\"A3.T2.1.1.4.3.2.1.1\" style=\"width:320.0pt;\">The snippet or context of the snippet is on the right topic of the query and model response, but is in a slightly different context or is too specific to fit the exact query: \n<br class=\"ltx_break\"/>For example, for the query breast cancer symptoms, give a 2 to: \n<br class=\"ltx_break\"/>\u2003A snippet referring a breast cancer treatment side effect</span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"A3.T2.1.1.5.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">3</th>\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t\" id=\"A3.T2.1.1.5.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T2.1.1.5.4.2.1\">\n<span class=\"ltx_p\" id=\"A3.T2.1.1.5.4.2.1.1\" style=\"width:320.0pt;\">The snippet or context of the snippet is about a subject that is a direct match, in topic and scope, of the most likely user intent for the query and model response: \n<br class=\"ltx_break\"/>For example, for the query breast cancer symptoms, give a 3 to: \n<br class=\"ltx_break\"/>\u2003A snippet discussing a symptom specific to breast cancer</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: \nLeft: Rubrics for document relevance evaluation.\nRight: Prompt for automatically evaluating document relevance with LLM-as-a-Judge.", "description": "This table presents two parts: the first part details the rubric used for human evaluation of document relevance, providing a Likert scale (0-3) with descriptions for each score level to ensure consistent judgment.  The second part shows the prompt used for automatically evaluating document relevance using an LLM (Large Language Model) as a judge, outlining the scoring criteria and instructions to be used by the LLM.  This combined approach allows for both human judgment and automated assessment of document relevance, enhancing the objectivity and efficiency of the evaluation process. ", "section": "4.2 Evaluating Document Relevance"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A3.T2.2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A3.T2.2.1.1.1\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_tt\" id=\"A3.T2.2.1.1.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T2.2.1.1.1.1.1\">\n<span class=\"ltx_p\" id=\"A3.T2.2.1.1.1.1.1.1\" style=\"width:230.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T2.2.1.1.1.1.1.1.1\">LLM-as-a-Judge Prompt</span></span>\n</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T2.2.1.2.2\">\n<td class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_t\" id=\"A3.T2.2.1.2.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">\n<span class=\"ltx_inline-block ltx_align_top\" id=\"A3.T2.2.1.2.2.1.1\">\n<span class=\"ltx_p\" id=\"A3.T2.2.1.2.2.1.1.1\" style=\"width:230.0pt;\">You will be given a user prompt, a model\u2019s response to the prompt, and a retrieved document. Please rate how relevant the document is to the prompt and model response. Rate on a scale of 0 (not relevant) to 3 (very relevant). Respond with a single number, and do not include any other text in your response.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Rubric for rating:\n\n<br class=\"ltx_break\"/>0: The document is about a different topic than the prompt and model response.\n\n<br class=\"ltx_break\"/>1. The document is about a broader topic than the prompt and model response, or is potentially relevant but there\u2019s not enough information.\n\n<br class=\"ltx_break\"/>2. The document is on the right topic of the prompt and model response, but is in a slightly different context or is too specific.\n\n<br class=\"ltx_break\"/>3. The document is about a subject that is a direct match, in topic and scope, of the most likely user intent for the prompt and model response.\n\n<br class=\"ltx_break\"/>\n<br class=\"ltx_break\"/>Prompt: {prompt}\n\n<br class=\"ltx_break\"/>Model response: {response}\n\n<br class=\"ltx_break\"/>Retrieved document: {document}</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: \nEvaluating the relevance level of top documents displayed by OLMoTrace.\nAvg score is on a likert scale of 0-3, where 0 is \u201cunrelated\u201d and 3 is \u201chighly relevant\u201d.\nFor % relevant, we consider a document as relevant if it gets a score of 2 or 3.\nWe use LLM-as-a-Judge with gpt-4o-2024-08-06, except in the last row where we collect annotation from a human expert.", "description": "This table presents the results of an evaluation measuring the relevance of the top documents retrieved by the OLMoTrace system.  Relevance is scored on a Likert scale (0-3), where 0 indicates completely unrelated and 3 indicates highly relevant.  The percentage of relevant documents (those with a score of 2 or 3) is also shown.  The evaluation uses an LLM-as-a-Judge (gpt-4o-2024-08-06) for most results; however, the last row shows scores from human evaluators, allowing for a comparison between automated and human assessment of relevance.", "section": "4.2 Evaluating Document Relevance"}]