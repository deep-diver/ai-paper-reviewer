[{"Alex": "Hey podcast listeners, buckle up! Today, we're diving into some seriously cool AI detective work. Forget CSI; we're talking 'Computational Source Investigation'! We're unraveling how language models spit out their answers, tracing them back to the *trillions* of words they've devoured. It\u2019s like finding the exact book and page where an AI learned a specific fact\u2026 or maybe, just maybe, made something up! Joining me is Jamie, ready to grill me on all the juicy details.", "Jamie": "Wow, that sounds intense! Trillions of words? Where do we even begin? What exactly are we calling this 'AI detective' tool?"}, {"Alex": "We're talking about OLMOTRACE, Jamie. It's a system designed to trace the outputs of Language Models (LMs) back to their training data. Basically, when an AI like, say, OLMo, generates a text response, OLMOTRACE finds and shows you verbatim matches \u2013 exact chunks of text \u2013 from the documents it was trained on that appear in the LM's response.", "Jamie": "Okay, verbatim matches... so it's like a super-powered search function, but for an AI's brain? Umm, why is this important? I mean, AI says stuff, right? Why do we need to know where it got it from?"}, {"Alex": "That's the million-dollar question! Think about it: LMs are being used in high-stakes scenarios \u2013 medical diagnoses, legal advice, even writing code. We *need* to understand why they generate certain responses. Is it based on solid, reliable training data, or some weird corner of the internet? OLMOTRACE helps us with fact-checking, identifying potential hallucinations, and even understanding the creative process of these models.", "Jamie": "Hallucinations? So, you're saying AI, like, makes stuff up? Hmm, so this is all about trust, making sure AI is reliable. I guess I can see that. So, what does OLMOTRACE actually do? How does it dig through trillions of words so fast?"}, {"Alex": "That's where the magic happens. OLMOTRACE uses something called an infini-gram, which is basically a highly optimized index of all the training data. It's like a super-organized library catalogue that allows us to quickly locate exact matches. And we developed a novel parallel algorithm to speed up the computation of matching spans.", "Jamie": "Okay, 'infini-gram'... Got it, magic words for 'super-fast search'. What's this 'algorithm' thing? Sounds complicated. And what are 'matching spans'?"}, {"Alex": "The algorithm is the secret sauce that lets us break down the LM's output into smaller chunks, and then search for those chunks in the infini-gram really quickly, and in parallel. We call these chunks 'matching spans' because they're spans of text in the LM's output that exactly match text in the training data.", "Jamie": "So it's like breaking down a sentence into phrases and searching for those phrases in a giant database? Ummm, how does it decide which phrases to search for? I mean, can't be every single word, right?"}, {"Alex": "Exactly! OLMOTRACE is smart about it. It looks for 'maximal matching spans' \u2013 the longest possible chunks of text that appear verbatim in the training data. And it filters out short, common phrases that aren't particularly informative. It wants the long, unique spans because they are more like to reflect significant connections between the model's output and its training data.", "Jamie": "Okay, longer is better. Makes sense. So after it finds the matching spans in the training data, what happens then? Does it just dump a huge list of documents on you?"}, {"Alex": "Not at all! OLMOTRACE retrieves the documents that contain those matching spans and displays them to the user. It shows you a snippet of the document with the matching span highlighted and also provides access to the full document.", "Jamie": "Ah, so you can actually read the surrounding text. That sounds helpful. Hmmm, So you get to read the source document. So it highlights where the AI got its information."}, {"Alex": "Yes, exactly! But it doesn't stop there. OLMOTRACE also reranks the documents by relevance, using a BM25 scoring algorithm. This helps prioritize the documents that are most closely related to the user's query and the LM's response.", "Jamie": "BM25\u2026 another magic word! So, it sorts the results to show the best matches first? Okay, that makes the list useful and not overwhelming. I imagine it's not as simple as just finding the information, though. Are there challenges in making this work well?"}, {"Alex": "Definitely! One major challenge is dealing with the sheer scale of the training data. Trillions of tokens require a lot of computing power and efficient indexing techniques. Another challenge is filtering out irrelevant or redundant matches to avoid overwhelming the user with too much information.", "Jamie": "Trillions of tokens\u2026 that is insane. So, what kind of computing power are we talking about? Like, can I run OLMOTRACE on my laptop?"}, {"Alex": "Haha, probably not! We host the inference pipeline on a CPU-only node in the Google Cloud Platform, with 64 vCPUs and 40TB of SSD disks for storing the index files. Each query takes an average of about 4.5 seconds, which is pretty impressive considering the amount of data we're searching through.", "Jamie": "Okay, so it's a serious operation! I guess my laptop will stick to playing cat videos. Does OLMOTRACE work on all sorts of AI models?"}, {"Alex": "OLMOTRACE is currently designed to work with the OLMo family of models, specifically OLMo-2-32B-Instruct and others. The cool thing is the core part of the system is open-sourced under the Apache 2.0 license, so you can adapt it to be applied to any LM as long as you have access to its full training data.", "Jamie": "Okay, open-source is always good. Can anyone use it? What are the real-world applications?"}, {"Alex": "Absolutely! OLMOTRACE is available in the Ai2 Playground for anyone to try out. The use cases are vast and varied. We've identified three primary examples of use cases. Those include fact-checking, tracing math capabilities and tracing the LM's generated 'creative' expressions.", "Jamie": "So, is this like checking to see how creative the model is?"}, {"Alex": "For creative tracing, OLMOTRACE reveals the potential sources of what seems like brand new things from an LM by finding snippets in the training data that align with its responses. It also helps understand how LMs learn to carry out arithmetic operations. It's more like verifying if the LM is regurgitating someone else's creativity than being creative itself.", "Jamie": "Interesting. So not only does it help fact-check, it also can reveal influences on the AI's output... Okay, that opens up a can of worms. Are there downsides to all of this traceability?"}, {"Alex": "It's a fair point. One potential downside is that OLMOTRACE could make potentially problematic contents in the LM training data more easily exposed, including copyright infringement, PII (personal identifiable information), and toxicity. We need to be mindful of these ethical considerations as we develop and deploy these tools.", "Jamie": "Yeah, exposing all of that training data... you could open up a Pandora's Box of problems. But at least it allows for the issues to be addressed."}, {"Alex": "Exactly. It helps provide transparency where there usually is none, in addition to opening the door to future work. What are the next steps in that research to refine it?", "Jamie": "Yeah, tell me more! What do you see as the future direction of this line of research?"}, {"Alex": "The next step is to start focusing on causal relationships between the LM and source data, as well as working on non-verbatim source tracing. This can lead to new types of tracing that work to improve how well current RAG (retrieval-augmented generation) applications perform and make it easier to inspect them. ", "Jamie": "Interesting. So, tracing the exact information is just the first step?"}, {"Alex": "That's just the beginning! It allows us to see how well LMs can adapt things they've learned to other facets. For example, with non-verbatim source tracing, it'll be easy to see if there is some correlation between bias and the sources the LMs trained with to create these biases.", "Jamie": "Wow. If that happens it'll become far easier to check models and verify them against these biases. Thank you for sharing this!"}, {"Alex": "Of course! I think the impact of this research can extend past just tracing the information itself. We can apply it to better understand the data and how to best curate the data to remove issues.", "Jamie": "You mean, like, cleaning up the data *before* training the AI, rather than just checking the AI afterward?"}, {"Alex": "Precisely! It's about creating better building blocks for AI. In short, OLMOTRACE represents a significant step towards understanding and controlling the behavior of large language models by linking their outputs back to their training data. It paves the way for more transparent, reliable, and trustworthy AI systems.", "Jamie": "Well, that was fascinating. Thank you, Alex, for making this research so accessible. I have to admit, I thought it would be way over my head, but you explained it perfectly!"}, {"Alex": "My pleasure, Jamie! Thanks for joining me. And to our listeners, I hope you enjoyed this dive into the world of AI detectives. Keep exploring, keep questioning, and keep pushing the boundaries of what's possible!", "Jamie": "Goodbye everyone!"}]