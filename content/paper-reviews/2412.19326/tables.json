[{"content": "| Task | Samples | Datasets |\n|---|---|---|\n| Segmentation | 114.6K | MeViS [18], SAMv2 [69] |\n| Temporal Grounding | 116.5K | ActivityNet [7], TACoS [70], QVHighlight [39], DiDeMo [27], QuerYD [63], HiREST [107], NLQ [25] |\n| Spatial Grounding | 540.0K | Allseeing-V2 [85], Visual Genome [37], RefCOCO [103], RefCOCO+ [103], RefCOCOg [59] |\n| Conversation | 3M | YouCook2 [17], ActivityNet [7], VideoChat2-IT [48], ShareGPT-4o [14], LLaVA-Hound-DPO [113], ShareGPT4V [10] |", "caption": "Table 1: Overview of Datasets Used in TPO for Various Tasks.", "description": "This table provides an overview of the datasets used to train the various task heads within the Task Preference Optimization (TPO) framework.  It details the specific datasets and the number of samples used for each of the three main visual tasks addressed by TPO: Segmentation, Temporal Grounding, and Spatial Grounding.  This information is crucial in understanding the scope and scale of the model's training data and how this data is used to improve the model's performance on various vision tasks.", "section": "4. Experiment"}, {"content": "| Model | LLM | Params | Frames | MVBench [48] | VideoMME [22] Overall | VideoMME [22] Short | VideoMME [22] Medium | VideoMME [22] Long | MLVU [117] | M-AVG |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| TimeChat [72] | 7B | 96 | 38.5 | 34.3 | 36.9 | 39.1 | 43.1 | 31.8 | 33.9 | 32.1 | 33.6 | 30.9 |\n| Video-LLAVA [49] | 7B | 8 | 43.0 | 41.1 | 41.9 | 46.9 | 47.3 | 38.7 | 40.4 | 37.8 | 37.9 | 47.3 |\n| ShareGPT4Video [11] | 7B | 16 | 51.2 | 39.9 | 43.6 | 48.3 | 53.6 | 36.3 | 39.3 | 35.0 | 37.9 | 46.4 |\n| LLaVA-Next-Video [115] | 7B | 16 | 44.0 | 38.0 | 40.8 | 44.6 | 47.4 | 37.7 | 39.4 | 31.9 | 35.6 | 39.3 |\n| ST-LLM [52] | 7B | 64 | 54.9 | 37.9 | 42.3 | 45.7 | 48.4 | 36.8 | 41.4 | 31.3 | 36.9 | - |\n| PLLaVA-34B [97] | 34B | 16 | 58.1 | 40.0 | 35.0 | 47.2 | 36.2 | 38.2 | 35.9 | 34.7 | 32.9 | 53.6 |\n| Chat-UniVi [33] | 7B | 64 | 40.8 | 40.6 | 45.9 | 45.7 | 51.2 | 41.3 | 47.3 | 39.1 | 43.4 | - |\n| VideoChat2 (baseline) [48] | 7B | 16 | 60.4 | 39.5 | 43.8 | 48.3 | 52.8 | 37.0 | 39.4 | 33.2 | 39.2 | 44.5 |\n| VideoChat-TPO | 7B | 16 | **66.8** (+6.4) | **48.8** (+9.3) | **53.8** (+10.0) | 58.8 | 64.9 | 46.7 | 50.0 | 41.0 | **46.4** | **54.7** (+10.2) |", "caption": "Table 2: Performance on Multimodal Video Understanding.\nWe compare our model to others using LLMs of the same generation or 16-frame input. w/o s. indicates without subtitle, while w s. indicates with subtitle. M-AVG refers to the mean average of MLVU.", "description": "This table presents a comparison of the performance of various models on multimodal video understanding benchmarks.  The benchmarks include MVBench (Overall, w/o subtitle, w/ subtitle), VideoMME (Short, Medium, Long, AVG, w/o subtitle, w/ subtitle), and MLVU (M-AVG).  Models are compared based on their performance using LLMs of the same generation or with 16 input frames.  The 'w/o s.' and 'w/ s.' columns indicate results without and with subtitles, respectively.  The M-AVG column shows the mean average score across the MLVU benchmark.", "section": "4.1 General Understanding Evaluation"}, {"content": "| Model | Acc@IoP | Acc@GQA | mIoP | IoP@0.3 | IoP@0.5 | mIoU | IoU@0.3 | IoU@0.5 |\n|---|---|---|---|---|---|---|---|---|\n| VIOLETv2 [23] | 54.9 | 12.8 | 23.6 | 25.1 | 23.3 | 3.1 | 4.3 | 1.3 |\n| SeViLA [105] | 72.5 | 16.6 | 29.5 | 34.7 | 22.9 | 21.7 | 29.2 | 13.8 |\n| LangRepo [34] | 59.6 | 17.1 | 31.3 | - | 28.7 | 18.5 | - | 12.2 |\n| FrozenBiLM NG+ [100] | 73.8 | 17.5 | 24.2 | 28.5 | 23.7 | 9.6 | 13.5 | 6.1 |\n| VideoStreaming [65] | 57.4 | 17.8 | 32.2 | - | 31.0 | 19.3 | - | 13.3 |\n| LLoVi [110] | 65.9 | 24.3 | **37.3** | - | **36.9** | 20.0 | - | 15.3 |\n| HawkEye [90] | - | - | - | - | - | 25.7 | 37.0 | 19.5 |\n| VideoChat-TPO | **77.7** | **25.5** | 35.6 | **47.5** | 32.8 | **27.7** | **41.2** | **23.4** |", "caption": "Table 11: Ablation of Reasoning Data and Head Performance.", "description": "This table presents an ablation study analyzing the impact of different components and data on the performance of the Task Preference Optimization (TPO) model.  Specifically, it investigates the contribution of various visual task heads (temporal, region, mask) and the inclusion of reasoning data on key metrics. The results show the effectiveness of the combined model components and data in achieving high performance.", "section": "4.3. Ablation Studies"}, {"content": "| Model | MM IU [60] | SEED2<sub>I</sub> [41] | SEED2<sub>M</sub> [41] |\n|---|---|---|---|\n| LLaVA-v1.5 [51] | 19.2 | 58.3 | 39.2 |\n| ShareGPT4V [10] | 18.5 | - | - |\n| OpenFlamingo [2] | 22.3 | 36.6 | 43.5 |\n| LLaVA-Interleave [43] | 32.4 | - | - |\n| VideoChat2 [48] | 35.0 | 26.5 | 27.6 |\n| VideoChatGPT [57] | - | 38.3 | 49.8 |\n| InternLM-XComposer [19] | 21.9 | 65.4 | 49.8 |\n| VideoChat-TPO | **40.2** (+5.2) | **67.3** (+40.8) | **70.0** (+42.4) |", "caption": "Table 12: Impact of TPO Components and Data. T, R, M, and C denote temporal head, region head, mask head, and conversation data respectively. R1@0.5 means R1@0.5 in Charades-STA, Acc@0.5 represents the mean of Acc@0.5 in all COCO datasets, \ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J&\u2131\u2131\\mathcal{F}caligraphic_F means \ud835\udca5\ud835\udca5\\mathcal{J}caligraphic_J&\u2131\u2131\\mathcal{F}caligraphic_F in Ref-YouTube-VOS.", "description": "This table presents an ablation study analyzing the impact of different components and data on the performance of the Task Preference Optimization (TPO) method.  It shows the results of various model configurations, systematically removing or adding components such as the temporal head (T), region head (R), and mask head (M), as well as including or excluding conversation data (C). The performance is measured using three metrics derived from different datasets: R1@0.5 from Charades-STA (a moment retrieval dataset); Acc@0.5 (average accuracy at IoU threshold of 0.5) across COCO datasets (evaluating region-based tasks); and J&F (Jaccard & F1-score) from Ref-YouTube-VOS (a referring video object segmentation dataset).  Each row represents a specific model configuration, indicating which components were included during training. This table helps to understand the contribution of each component and the dataset to the overall model performance.", "section": "4.3. Ablation Studies"}, {"content": "| Model | Charades-STA [24] |  |  |  |  |\n|---|---|---|---|---|---| \n|  | R@0.3 | R@0.5 | R@0.7 | mIoU |  |\n|---|---|---|---|---|---| \n| UniVTG [50] | 44.1 | 25.2 | 10.0 | 27.1 |  |\n| VideoChat2 [48] | 38.0 | 14.3 | 3.8 | 24.6 |  |\n| VTimeLLM [29] | 51.0 | 27.5 | 11.4 | 31.2 |  |\n| TimeChat [72] | - | 32.2 | 13.4 | - |  |\n| HawkEye [90] | 50.6 | 31.4 | 14.5 | 33.7 |  |\n| ChatVTG [66] | 52.7 | 33.0 | 15.9 | 34.9 |  |\n| VideoChat-TPO | **58.3** | **40.2** | **18.4** | **38.1** |  |", "caption": "Table 13: Results on MVBench Multi-choice Question Answering.", "description": "This table presents the performance of various models on the MVBench benchmark, a challenging dataset designed for evaluating multimodal video understanding capabilities. It showcases the accuracy scores of different models across multiple fine-grained video tasks.  The results highlight the strengths and weaknesses of various approaches in terms of temporal perception and reasoning abilities.  These scores provide a comprehensive evaluation of the models' capacity to understand and reason about complex video scenarios.", "section": "4.1 General Understanding Evaluation"}, {"content": "| Model | Charades-STA [24] |  |  |  | QVHighlight [39] |  |  |\n|---|---|---|---|---|---|---|---| \n|  | R@0.3 | R@0.5 | R@0.7 | mIoU | mAP | HIT@1 |\n|---|---|---|---|---|---|---|---| \n| M-DETR [39] | 65.8 | 52.1 | 30.6 | 45.5 | 35.7 | 55.6 |\n| QD-DETR [62] | - | 57.3 | 32.6 | - | 38.9 | 62.4 |\n| UniVTG [50] | 72.6 | 60.2 | 38.6 | 52.2 | 40.5 | 66.3 |\n| TimeChat [72] | - | 46.7 | 23.7 | - | 21.7 | 37.9 |\n| HawkEye [90] | 72.5 | 58.3 | 28.8 | - | - | - |\n| VideoChat-TPO | **77.0** | **65.0** | **40.7** | **55.0** | 38.8 | 66.2 |", "caption": "Table 14: Quantitative results of MMIU\u00a0[60]. Accuracy is the metric, and the Overall score is computed across all tasks.", "description": "Table 14 presents a quantitative analysis of the Multimodal Multi-Image Understanding (MMIU) benchmark [60].  It evaluates the performance of various models across multiple image understanding tasks. The 'Overall' score represents the average performance across all tasks, while other columns likely detail performance on specific sub-tasks within the MMIU benchmark. Accuracy serves as the primary evaluation metric for the table.", "section": "4.1 General Understanding Evaluation"}, {"content": "| Methods | RefCOCO [103] |  |  |  | \n|---|---|---|---|---|\n|  | val | testA | testB |  | \n| MAttNet \u2605 [104] | 76.4 | 80.4 | 69.3 |  | \n| OFA-L [82] | 80.0 | 83.7 | 76.4 |  | \n| G-DINO-L \u2605 [53] | 90.6 | 93.2 | 88.2 |  | \n| VisionLLM-H [84] | - | 86.7 | - |  | \n| Shikra-7B [8] | 87.0 | 90.6 | 80.2 |  | \n| NExT-Chat-7B [109] | 85.5 | 90.0 | 77.9 |  | \n| VideoChat-TPO | 85.9 | 90.8 | 81.3 |  | ", "caption": "Table 15: Ablation task datasets.", "description": "This table presents the ablation study on the impact of different vision tasks included in the training process of the Task Preference Optimization (TPO) method. It shows the performance of the model when trained with various combinations of tasks: temporal grounding, spatial grounding, and segmentation. The results demonstrate the effect of each task on the overall performance and the synergistic effect when multiple tasks are combined.", "section": "4.3. Ablation Studies"}, {"content": "| Model | LaSOT [21] |  |  | GOT-10k [30] |  |  |\n|---|---|---|---|---|---|---|\n|  | Success | P<sub>norm</sub> | P | Overlap | SR0.5 | SR0.75 |\n|---|---|---|---|---|---|---|\n| SiamFC [5] | 33.6 | 42.0 | 33.9 | 34.8 | 35.3 | 9.8 |\n| ATOM [16] | 51.5 | - | - | 55.6 | 63.4 | 40.2 |\n| SiamRPN++ [40] | 49.6 | 56.9 | 49.1 | 51.8 | 61.8 | 32.5 |\n| SiamFC++ [98] | 54.4 | 62.3 | 54.7 | 59.5 | 69.5 | 47.9 |\n| LLaVA-1.5 [51] | 19.4 | 16.5 | 12.8 | 23.5 | 20.2 | 9.7 |\n| Merlin [102] | 39.8 | 40.2 | 38.1 | 51.4 | 55.9 | 42.8 |\n| VideoChat-TPO | **69.4** | **80.1** | **76.9** | **70.6** | **79.8** | **66.0** |", "caption": "Table 16: Training Settings of VideoChat-TPO. Con. means conversation data and LR means learning rate.", "description": "This table details the hyperparameters and training settings used in the Task Preference Optimization (TPO) method, specifically for the VideoChat2 model.  It breaks down the configuration across different stages of the training process, outlining learning rates (LR) for various components like the vision encoder, connector, task heads (temporal, region, mask), task tokens, and the Large Language Model (LLM) fine-tuned with LoRA.  The table also shows the optimizers used (AdamW), weight decay, input resolution, frame count, LoRA rank and alpha for the LLM, warmup ratio, batch size, epochs, and the numerical precision (DeepSpeed bf16). The different stages reflect the incremental introduction and training of these elements: Stage 1 focuses on task assignment and LoRA training of the LLM; Stage 2 fine-tunes task heads and tokens; and Stage 3 jointly trains all components, including multimodal conversation data.", "section": "4. Experiment"}, {"content": "| Method | Ref-YouTube-VOS [74] |  |  | MeViS [18] |  |  | \n|---|---|---|---|---|---|---|\n|  | \ud835\udca5 | \u2131 |  | \ud835\udca5 | \u2131 |  |\n| ReferFormer [93] | 62.9 | 61.3 | 64.6 | 31.0 | 29.8 | 32.2 |\n| OnlineRefer [92] | 62.9 | 61.0 | 64.7 | - | - | - |\n| LISA [38] | 52.6 | 52.1 | 53.0 | - | - | - |\n| VideoLISA [4] | 63.7 | **61.7** | 63.7 | 44.4 | 41.3 | 47.6 |\n| VideoChat-TPO | **63.9** | 52.3 | **75.4** | **47.0** | **42.6** | **51.3** |", "caption": "Table 17: Training Datasets. The temporal grounding includes two subtasks: moment retrieval and highlight detection.", "description": "This table details the datasets used for training the Task Preference Optimization (TPO) model.  It's broken down by training stage (1, 2, and 3), with each stage focusing on different aspects of model training.  Stage 1 focuses on task assignment, using smaller datasets to teach the model to identify the task type. Stage 2 concentrates on training the visual task heads, utilizing significantly larger datasets for each task. Stage 3 involves multi-task training, combining large datasets for all tasks and incorporating a large multimodal conversation dataset to optimize overall performance and alignment between tasks. Note that temporal grounding is composed of moment retrieval and highlight detection subtasks.", "section": "4. Experiment"}]