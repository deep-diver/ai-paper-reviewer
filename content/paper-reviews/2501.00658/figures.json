[{"figure_path": "https://arxiv.org/html/2501.00658/x1.png", "caption": "Figure 1: Visualization of log influential scores log\u2061|\u2202\ud835\udc9at/\u2202\ud835\udc99s|subscript\ud835\udc9a\ud835\udc61subscript\ud835\udc99\ud835\udc60\\log|\\partial\\bm{y}_{t}/\\partial\\bm{x}_{s}|roman_log | \u2202 bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT / \u2202 bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | versus distance (t\u2212s)\ud835\udc61\ud835\udc60(t-s)( italic_t - italic_s ).", "description": "The figure visualizes the relationship between the logarithmic influence scores (log|\u2202yt/\u2202xs|) and the relative distance (t-s) between tokens in a sequence processed by a state space model (SSM).  The influence score quantifies how much a given input token (xs) impacts a specific output token (yt). The plot shows how this influence decays exponentially with distance, illustrating the recency bias inherent in SSMs.  Different lines represent different pre-trained SSM models (with varying sizes), and all show a consistent linear decay rate. This suggests the recency bias in SSMs is a property of the model architecture rather than merely a reflection of data statistics.", "section": "3.1 SSMS ARE LOCALLY BIASED"}, {"figure_path": "https://arxiv.org/html/2501.00658/x4.png", "caption": "Figure 2: Comparison between SSM and Transformer on the \u201cNeedle in a Haystack\" benchmark. The left figure shows the retrieval accuracy of the Mamba-Codestral-7B model, while the right figure presents the retrieval accuracy of the Mistral-7B model.\nWe present a heatmap where \"full context length\" refers to the total length of the document, and \"needle position\" denotes the relative position of the statement to be retrieved within the context. See more fine-grained visualization in Appendix E.2.", "description": "This figure compares the performance of State Space Models (SSMs) and Transformers on the \"Needle in a Haystack\" benchmark, a task designed to evaluate the ability of language models to retrieve information from long contexts.  The benchmark involves embedding a statement within a long document and assessing the model's ability to retrieve that statement. The left heatmap shows the results for the Mamba-Codestral-7B SSM, while the right heatmap shows the results for the Mistral-7B Transformer model.  The heatmaps illustrate retrieval accuracy across different document lengths (\"full context length\") and statement positions within the document (\"needle position\").  The results reveal a recency bias in the SSM, where accuracy is higher when the statement is closer to the end of the document. The Transformer model shows more consistent performance regardless of statement position.", "section": "3.2 LOST IN THE DISTANCE: LONG-CONTEXT RETRIEVAL TEST"}, {"figure_path": "https://arxiv.org/html/2501.00658/x5.png", "caption": "(a) Attack ratio = 256/1024\u2062(25.00%)2561024percent25.00256/1024~{}~{}(25.00\\%)256 / 1024 ( 25.00 % )", "description": "This figure shows the results of a target attack experiment on the CIFAR-10 dataset, focusing on the impact of attacking different regions of the input sequences on model performance.  Two attack ratios are shown: 25.00% and 46.875%. Each bar represents the attack success rate for a specific model (H3, Transformer, RWKV, Mamba) and attack region (leading or trailing tokens). Lower success rates indicate higher robustness against that type of attack.", "section": "3.3 POTENTIAL RISK ON MODEL ROBUSTNESS"}, {"figure_path": "https://arxiv.org/html/2501.00658/x8.png", "caption": "(b) Attack ratio = 480/1024\u2062(46.875%)4801024percent46.875480/1024~{}~{}(46.875\\%)480 / 1024 ( 46.875 % )", "description": "This figure shows the results of a target attack experiment on the CIFAR-10 dataset.  Specifically, it shows the attack success rates for different models (H3, Transformer, RWKV, Mamba) when a significant portion (46.875%) of the input sequence is replaced with pixels from the target class. This is done separately for leading and trailing tokens, illustrating model vulnerability to targeted attacks. Lower success rates indicate higher robustness.", "section": "3.3 POTENTIAL RISK ON MODEL ROBUSTNESS"}, {"figure_path": "https://arxiv.org/html/2501.00658/x9.png", "caption": "Figure 3: Results of target attack experiments on CIFAR-10, where \u201chorse\u201d is the target class. (a) and (b) present target attack success rates under two attack ratios. Lower success rates suggest higher robustness in the corresponding attack regions.", "description": "This figure displays the results of targeted attack experiments conducted on the CIFAR-10 dataset, focusing on the robustness of different models against adversarial attacks.  The target class for these attacks was 'horse'. Two attack scenarios were tested, each with different ratios of corrupted data: (a) 25% and (b) 47%. The attacks involved replacing pixels in either the beginning or end of the image sequences with pixels from images of the target class.  The success rate of the attack (how often the model misclassified a non-horse image as a horse) is shown for four models: H3, Mamba, RWKV and the Transformer baseline. Lower success rates indicate greater robustness of the model to that type of attack. The figure shows that SSMs (H3, Mamba, and RWKV) are significantly less robust to attacks targeting trailing tokens (the end of the sequence), while the Transformer is less susceptible to either leading or trailing token attacks.", "section": "3.3 POTENTIAL RISK ON MODEL ROBUSTNESS"}, {"figure_path": "https://arxiv.org/html/2501.00658/x10.png", "caption": "Figure 4: We empirically observe that deeper models become increasingly advantageous as the context length grows. However, beyond a certain depth, the performance of SSMs begins to plateau and eventually declines.", "description": "This figure displays the validation loss of the Mamba model across different numbers of layers and model sizes (number of parameters) for two different context lengths (2048 and 8192).  It demonstrates that increasing model depth improves performance, particularly as context length increases. However, beyond a certain number of layers (depth), the performance plateaus and then starts to decrease, indicating a scalability bottleneck in SSMs at greater depths.", "section": "4.1 Necessity and Limits of Depth Scaling"}, {"figure_path": "https://arxiv.org/html/2501.00658/x11.png", "caption": "(a) \ud835\udc83tsubscript\ud835\udc83\ud835\udc61\\bm{b}_{t}bold_italic_b start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and \ud835\udc89tsubscript\ud835\udc89\ud835\udc61\\bm{h}_{t}bold_italic_h start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.", "description": "The figure visualizes the feature smoothness across layers in pre-trained Mamba and Pythia models.  Panel (a) shows the sharpness of the input features (\ud835\udc4f\ud835\udc61) and the hidden states (\u210e\ud835\udc61). Panel (b) focuses only on the mixer module output, while panel (c) shows the output of the entire block, encompassing all components (mixer, MLP, etc.). The y-axis represents the average pairwise difference between tokens; a lower value signifies greater feature smoothness and thus more oversmoothing.", "section": "Unveiling over-smoothing in SSMs"}, {"figure_path": "https://arxiv.org/html/2501.00658/x12.png", "caption": "(b) Mixer output.", "description": "This figure shows the sharpness of features in the output of the mixer module in Mamba and Pythia models.  Sharpness is a measure of how distinguishable the token representations are; lower sharpness indicates over-smoothing, where token representations become too similar.", "section": "Unveiling over-smoothing in SSMs"}, {"figure_path": "https://arxiv.org/html/2501.00658/x13.png", "caption": "(c) Block output.", "description": "This figure visualizes the feature smoothness across layers in pre-trained Mamba and Pythia models.  The y-axis represents the average pairwise differences among tokens. The figure displays smoothness for three different output types: (a) the input tokens (bt) and hidden state (ht) vectors, (b) the output of the attention module (mixer), and (c) the output of the entire block (block), which encompasses all components within a single block of the model architecture.  Comparing across these output types allows for analysis of how smoothness changes as information flows through different stages of the model.", "section": "Unveiling Over-smoothing in SSMs"}, {"figure_path": "https://arxiv.org/html/2501.00658/x14.png", "caption": "Figure 5: Visualization of feature smoothness across layers in pre-trained Mamba and Pythia. The y-axis represents the average pairwise differences among tokens. Mixer outputs (b) solely consider the Mamba or attention module, while Block outputs (c) include all other components (e.g., MLP).", "description": "This figure visualizes how feature smoothness changes across different layers of pre-trained Mamba and Pythia language models.  Feature smoothness is measured as the average pairwise difference between token representations. The figure includes three sub-figures. (a) shows the smoothness of the input token embeddings (bt) and the hidden state representations (ht) at each layer. (b) focuses on the smoothness of the output from the Mamba or attention module (mixer) only. (c) shows the overall smoothness of the output of the entire block, including all components like MLPs in addition to the attention/Mamba module. This allows for a comparison of smoothness within the attention module and the overall effect of the entire block on the features. This helps to understand the impact of various model components on feature over-smoothing.", "section": "Unveiling Over-smoothing in SSMs"}, {"figure_path": "https://arxiv.org/html/2501.00658/x17.png", "caption": "Figure 6: Cumulative histogram of (Am\u2062a\u2062x\u2212Am\u2062i\u2062n)subscript\ud835\udc34\ud835\udc5a\ud835\udc4e\ud835\udc65subscript\ud835\udc34\ud835\udc5a\ud835\udc56\ud835\udc5b(A_{max}-A_{min})( italic_A start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT - italic_A start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT ). The height of each bin represents the cumulative proportion of (Am\u2062a\u2062x\u2212Am\u2062i\u2062n)subscript\ud835\udc34\ud835\udc5a\ud835\udc4e\ud835\udc65subscript\ud835\udc34\ud835\udc5a\ud835\udc56\ud835\udc5b(A_{max}-A_{min})( italic_A start_POSTSUBSCRIPT italic_m italic_a italic_x end_POSTSUBSCRIPT - italic_A start_POSTSUBSCRIPT italic_m italic_i italic_n end_POSTSUBSCRIPT ) less than or equal to the corresponding value on the x-axis.", "description": "This figure shows the cumulative distribution of the difference between the maximum and minimum values of the diagonal elements of the state transition matrix (A) across different channels in the SSM model.  The x-axis represents the threshold for the difference (A_max - A_min), while the y-axis shows the cumulative percentage of channels where the difference is less than or equal to the threshold.  This distribution reveals that a significant proportion of channels do not exhibit a large difference between maximum and minimum values, which is an indicator that simultaneous mitigation of recency bias and over-smoothing is challenging.", "section": "4.2 UNVEILING OVER-SMOOTHING IN SSMS"}]