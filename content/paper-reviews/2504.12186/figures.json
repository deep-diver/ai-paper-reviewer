[{"figure_path": "https://arxiv.org/html/2504.12186/extracted/6367151/img/Overview.jpg", "caption": "Figure 1: CoMotion\u00a0tracks 3D poses online from monocular RGB video. Rather than detect new poses in each frame and associate them to existing tracks, CoMotion\u00a0updates tracks directly from incoming image features. As a result, CoMotion\u00a0keeps track of distinct individuals as they overlap in the camera frame (top) and occlude each other (bottom).\nArrows highlight some points of interest.", "description": "Figure 1 presents a visual comparison of CoMotion's performance against a baseline method (4DHumans) on tracking 3D human poses from a monocular video stream.  The top row showcases a scenario with individuals overlapping in the camera frame, while the bottom row demonstrates a situation involving occlusion. CoMotion successfully maintains consistent tracking despite these challenges because it directly updates pose estimates from incoming image features, rather than relying on independent per-frame detections and data association. This contrasts with the baseline, which struggles to maintain individual identity when occlusion or overlap occurs. The arrows in the figure highlight specific areas where the advantages of CoMotion are readily apparent.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.12186/extracted/6367151/img/comparison_4D.png", "caption": "Figure 2: \nOverview.\nCoMotion\u00a0estimates 3D poses for all people in a frame. An image encoder produces image features Ftsuperscript\ud835\udc39\ud835\udc61F^{t}italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, which are passed through the detection module to identify potential new tracks. In parallel, the pose update module attends to Ftsuperscript\ud835\udc39\ud835\udc61F^{t}italic_F start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT to update the existing tracks from the previous timestep. Both outputs are compared to each other to decide whether to instantiate or remove any tracks. If a detection is flagged as a new track, it is passed through the update module before being added to the final output tracks for the current frame. The inset details the pose update module.", "description": "CoMotion processes each frame of a video to estimate 3D poses of all people present.  The system uses an image encoder to extract features from the current frame (Ft). These features are fed into two parallel modules: a detection module (to identify potential new people) and a pose update module (to refine existing poses from the previous frame). Both modules process Ft independently and their outputs are compared to determine whether to start tracking new individuals, update existing tracks, or remove stale tracks. Any newly detected individuals are passed to the pose update module before being integrated into the final output.  The figure inset highlights the internal workings of the pose update module.", "section": "3 COMOTION"}, {"figure_path": "https://arxiv.org/html/2504.12186/extracted/6367151/img/posetrack18_annot.png", "caption": "Figure 3: We compare predictions made by CoMotion and 4D Humans unrolled through time on a sample from PoseTrack. Due to making independent predictions per frame, we observe that 4D Humans occasionally makes abrupt changes to the estimated pose (see green track on the right).", "description": "Figure 3 presents a comparison of 3D human pose tracking results from CoMotion and 4D Humans on a video sequence from the PoseTrack dataset.  The figure visually demonstrates the temporal coherence of CoMotion's pose estimations compared to 4D Humans.  Specifically, the green track in the figure highlights a situation where the 4D Humans model exhibits abrupt and inconsistent pose predictions across frames, while CoMotion's pose estimations show smoother and more consistent tracking. This difference arises because CoMotion updates poses based on the entire image context and a recurrent model that maintains temporal consistency, whereas 4D Humans makes independent predictions for each frame.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2504.12186/extracted/6367151/img/posetrack21_annot.png", "caption": "Figure 4: Incorrect handling of missing annotations in PoseTrack18.\nDue to incomplete annotations in PoseTrack18, predicted tracks may be incorrectly regarded as \u201cfalse positives\u201d. We show representative samples where annotations are green and \u201cfalse positives\u201d are red.", "description": "This figure demonstrates how incomplete annotations in the PoseTrack18 dataset lead to inaccurate evaluation results.  Specifically, it showcases instances where the evaluation algorithm incorrectly labels correctly predicted object tracks as \"false positives\" because ground truth annotations are missing for those tracks in certain frames.  The figure visually highlights this issue by displaying sample images with correct annotations shown in green, and those incorrectly identified as false positives in red. This illustrates the challenges of evaluating pose estimation and tracking algorithms when dealing with incomplete or noisy datasets.", "section": "A.1 TRACKING EVALUATION DETAILS"}, {"figure_path": "https://arxiv.org/html/2504.12186/x1.png", "caption": "Figure 5: Incorrect handling of missing annotations in PoseTrack 21. PoseTrack21 addresses the incompleteness of PoseTrack18 annotations by providing \u2018ignore\u2019 regions to accompany the annotated tracks. For the frame on the left, the center image illustrates the annotation of the person in the center (shown in green) and a polygon defining the \u2018ignore\u2019 region in blue. The right image shows predicted tracks in red, which are still penalized as false positives by the PoseTrack21 evaluation code despite being contained in the \u2018ignore region\u2018. This is a bug that we fix.", "description": "PoseTrack21 attempts to improve upon PoseTrack18 by incorporating 'ignore' regions to account for missing annotations in the dataset.  However, a bug in the evaluation code causes predicted tracks that fall within these 'ignore' regions to be incorrectly penalized as false positives.  Figure 5 illustrates this issue: the leftmost image shows the original frame; the center image highlights a ground truth annotation (green) and the corresponding 'ignore' region (blue); and the rightmost image shows the incorrectly flagged predicted tracks (red). The authors of this paper corrected this bug in their evaluation.", "section": "A.1 TRACKING EVALUATION DETAILS"}, {"figure_path": "https://arxiv.org/html/2504.12186/x2.png", "caption": "Figure 6: Impact of the assumed focal length. To investigate how the focal length of the intrinsics matrix affects performance we run our model on PoseTrack videos (for which we do not have ground truth camera calibration) and report 2D PCK accuracy. We adjust the assumed focal length and observe that the network\u2019s 2D keypoint accuracy is consistent as long as we remain within a realm of values which correspond to what one would typically find with most camera hardware apart from extremely wide-angle options such as a fish-eye lens. In the above figure, fx (the x-axis) is normalized by the image width.", "description": "This figure demonstrates the robustness of the CoMotion model to variations in the assumed focal length of the camera.  The experiment involved running the model on PoseTrack videos (where ground truth camera calibration wasn't available).  The x-axis represents the assumed focal length (fx), normalized by the image width. The y-axis shows the resulting 2D Percentage of Correct Keypoints (PCK) accuracy. The plot shows that the model maintains high accuracy across a wide range of focal lengths, indicating its resilience to errors or uncertainty in this parameter.  The only exception is extremely wide-angle lenses like fish-eye lenses.", "section": "5.1 CONTROLLED EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2504.12186/x3.png", "caption": "Figure 7: Comparing the per-frame runtime of CoMotion\u00a0with prior work on the PoseTrack21 validation set. All measurements were made on a V100 GPU using the code released by the respective authors. CoMotion\u00a0is significantly faster than prior work. Specifically, CoMotion\u00a0is approximately 1.4x faster than PARE (176ms vs 258ms) and 12x faster than 4D Humans (176ms vs 2163ms) on average.", "description": "This figure compares the processing speed (runtime) of CoMotion with three other state-of-the-art multi-person 3D pose tracking methods on the PoseTrack21 validation set. All methods were run on the same hardware (a V100 GPU) using the code provided by the respective authors. The box plot shows that CoMotion is significantly faster than the other methods. Quantitatively, CoMotion is about 1.4 times faster than PARE (176 milliseconds per frame versus 258 milliseconds per frame) and 12 times faster than 4D Humans (176ms/frame versus 2163ms/frame).", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.12186/extracted/6367151/img/pseudo_compare_02.png", "caption": "Figure 8: Pseudocode for the image encoder.", "description": "This image shows the pseudocode for the image encoder of the CoMotion model. The image encoder takes an image as input and produces image features. The encoder uses the ConvNeXtV2 architecture. It consists of four stages which progressively lower the resolution of the image. At each stage, it produces a feature map. These features are then linearly projected to the same resolution to combine early and late features. The features are then normalized and returned as the final image features.", "section": "A.3.1 IMAGE ENCODER"}]