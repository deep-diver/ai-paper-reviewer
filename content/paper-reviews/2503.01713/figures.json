[{"figure_path": "https://arxiv.org/html/2503.01713/x1.png", "caption": "Figure 1: Three motivational examples illustrating the current limitations of precise retrieval for RAG.", "description": "This figure showcases three common problems in the precise retrieval phase of Retrieval-Augmented Generation (RAG) systems.  Panel A demonstrates \"Ineffective Corpus Segmentation,\" where the crucial segment containing the answer is improperly divided, hindering retrieval. Panel B illustrates \"Noisy Retrieval,\" where the retrieval process returns both relevant and irrelevant segments, leading to an ambiguous and potentially inaccurate LLM response. Finally, Panel C shows \"Missing Retrieval,\" where a critical segment containing the answer is missed, resulting in an incorrect LLM response. Each panel shows the question, the relevant segment, the retrieved segments, and the resulting LLM response, highlighting the failures that result from poor segmentation and retrieval strategies.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2503.01713/x2.png", "caption": "Figure 2: Workflow of SAGE, where the \u21e2\u21e2\\dashrightarrow\u21e2 inidcates the pipelines of self-feedback.", "description": "This figure illustrates the workflow of the SAGE framework for precise retrieval in RAG.  It begins with corpus segmentation, where the corpus is divided into semantically complete chunks using a trained model.  These chunks are then converted into vector embeddings and stored in a vector database.  When a question arrives, it is embedded and used to query the database, retrieving the top N most similar chunks. A gradient-based reranking model then dynamically selects the most relevant chunks (Top K). These top K chunks, along with the original question, are provided as input to a large language model (LLM) for answer generation. Finally, a self-feedback loop is implemented: the LLM's generated answer and the context are assessed to determine if additional or fewer chunks are needed for a more accurate response. If the answer quality is not sufficient, the system iteratively refines chunk selection and LLM input until the quality reaches the set threshold or the iteration limit is reached.", "section": "III. OVERVIEW"}, {"figure_path": "https://arxiv.org/html/2503.01713/x3.png", "caption": "Figure 3: Motivation of corpus segmentation. The number in  1 means the chunk ID.", "description": "This figure illustrates the motivation behind choosing a suitable corpus segmentation method for retrieval-augmented generation (RAG). It compares four different approaches: partitioning by a small fixed length, partitioning by whole sentences using a small fixed length, partitioning by whole sentences using a large fixed length, and using a semantics-based segmentation model. The examples highlight how fixed-length segmentation often leads to semantically incomplete or incoherent chunks, whereas a semantic approach ensures the retrieved context is relevant and complete. The numbers within the figure represent chunk IDs.", "section": "IV. Semantic Segmentation"}, {"figure_path": "https://arxiv.org/html/2503.01713/x4.png", "caption": "Figure 4: Corpus segmentation model.", "description": "This figure illustrates the architecture of the corpus segmentation model used in the SAGE framework.  The model takes pairs of sentences as input, embedding them using a pre-trained embedding model.  Feature augmentation then calculates the difference and product of the sentence embeddings.  Finally, a Multi-Layer Perceptron (MLP) processes these features (embeddings, difference, and product) to produce a score indicating whether the two sentences should belong to the same semantically coherent chunk or be separated.  A threshold on this score determines the final segmentation decision.", "section": "IV. Semantic Segmentation"}, {"figure_path": "https://arxiv.org/html/2503.01713/x5.png", "caption": "Figure 5: Two general cases in relevance scores of retrieved segmentations.", "description": "This figure illustrates two common patterns observed in the relevance scores assigned to retrieved document segments by a reranking model in a RAG system.  The x-axis represents the ID of each retrieved segment, and the y-axis shows its relevance score.  The first pattern shows a sharp drop in relevance scores after the top few segments, indicating a clear separation between highly relevant and less relevant content.  The second pattern shows a more gradual decrease in relevance, suggesting a less distinct boundary between highly relevant and less relevant segments.  These patterns highlight the challenge of dynamically selecting the optimal number of relevant segments for input to the language model.", "section": "V. GRADIENT-BASED CHUNK SELECTION"}, {"figure_path": "https://arxiv.org/html/2503.01713/x6.png", "caption": "Figure 6: Prompt of Self-Feedback.", "description": "This figure shows the prompt template used for the LLM self-feedback mechanism in the SAGE framework.  The prompt includes the original question, the LLM's generated answer, and a request for the LLM to evaluate the answer's quality (on a scale of 1-10) and assess whether the provided context was sufficient or excessive (+1/-1). This feedback is crucial for dynamically adjusting the number of retrieved chunks in subsequent iterations, improving the accuracy and efficiency of the RAG system.", "section": "VI. LLM SELF-FEEDBACK"}, {"figure_path": "https://arxiv.org/html/2503.01713/x7.png", "caption": "Figure 7: Segmentation Overhead Evaluation.", "description": "Figure 7 presents a bar chart comparing the time overhead introduced by the semantic segmentation module in SAGE across three different datasets: QUALITY, NarrativeQA, and QASPER.  It illustrates the additional time required for SAGE's segmentation process, comparing it to the time taken by a typical LLM approach.  The chart visually demonstrates whether the time savings from using a lightweight segmentation model in SAGE outweighs the overhead added by the extra steps in SAGE's process. The use of different LLMs (GPT-4 and GPT-4-o-mini) for different parts of the experiment are also shown in the graph.", "section": "VII. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.01713/x8.png", "caption": "Figure 8: A case of noisy retrieval.", "description": "This figure demonstrates a real-world example from the QUALITY dataset where noisy retrieval negatively impacts question answering.  The question asks for the genetic definition of 'kin'.  While the correct answer is 'full siblings', numerous noisy chunks, containing information supporting alternative answers like 'all humans', are retrieved along with the correct answer.  The chart illustrates how the score assigned to each chunk by the model varies. Crucially, it shows that if the number of retrieved chunks (K) is small (2 \u2264 K \u2264 10), the correct answer ('full siblings') is selected. However, as K increases (11 \u2264 K \u2264 13), incorrect responses arise due to the noisy chunks' influence, and when K reaches 14, the LLM selects the completely wrong answer 'all humans'. This highlights the issue of noisy retrieval in RAG systems and the need for a precise retrieval mechanism.", "section": "VIII. INSIGHTS OF RAG TASKS"}, {"figure_path": "https://arxiv.org/html/2503.01713/x9.png", "caption": "Figure 9: A case of missing retrieval.", "description": "This figure demonstrates a scenario where crucial contextual information, necessary for accurate question answering, is absent from the retrieved text.  The question is about which technology has not been developed on Venus. The correct answer (Option 3: Creating fire) requires the presence of a specific chunk of text among those retrieved. However, when the number (K) of retrieved chunks is set to a value less than or equal to 6, this essential chunk is missing, leading to an incorrect answer (Option 4: Metallurgy).  Only when K is between 7 and 14 does the model have access to the necessary information and provide the correct answer.  Increasing K to 15 leads to another incorrect answer.", "section": "VIII. INSIGHTS OF RAG TASKS"}, {"figure_path": "https://arxiv.org/html/2503.01713/x10.png", "caption": "Figure 10: A case of ineffective corpus segmentation.", "description": "This figure shows an example from the QUALITY dataset where ineffective corpus segmentation leads to an incorrect answer.  The question is: \"Who asked Gavir to sing a tribal song?\" The correct answer is \"The moderator.\" However, the corpus is segmented such that the two sentences containing the answer (\"Well, enough of that!\" the moderator said briskly.  \"How about singing one of your tribal songs for us?\" Gavir said, \"I will sing the Song of Going to Hunt.\") are split into separate chunks.  As a result, the model cannot connect the moderator's request with Gavir's response, making it impossible to determine the correct answer.", "section": "VIII. INSIGHTS OF RAG TASKS"}]