[{"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T1.3\">\n<tr class=\"ltx_tr\" id=\"S2.T1.3.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S2.T1.3.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S2.T1.3.4.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.3.4.2\">Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.3.4.3\">Coding</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S2.T1.3.4.4\">Science</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.3.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.5.1\">(AIME 2024)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.5.2\">(LiveCodeBench 24.08-25.02)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.5.3\">(GPQA-Diamond)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S2.T1.1.1.1\">DeepSeek-R1-Distill-Qwen-32B<sup class=\"ltx_sup\" id=\"S2.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.1.1.1.1.1\">\u2020</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.2\">72.6 (9.6k Tokens)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.3\">57.2 (10.1k Tokens)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.4\">62.1 (5.3k Tokens)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.2.2.1\">DeepSeek-R1-Distill-Llama-70B<sup class=\"ltx_sup\" id=\"S2.T1.2.2.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.2.2.1.1.1\">\u2020</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.2\">70.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.3\">57.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.2.2.4\">65.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.3.3.1\">DeepSeek-R1<sup class=\"ltx_sup\" id=\"S2.T1.3.3.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S2.T1.3.3.1.1.1\">\u2020</span></sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.2\">79.8 (9.6k Tokens)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.3\">65.9 (10.4k Tokens)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.3.3.4\">71.5 (5.3k Tokens)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.3.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S2.T1.3.6.1\">TinyR1-32B-Preview (Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.3.6.2\">78.1 (11.8k Tokens)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.3.6.3\">61.6 (12.4k Tokens)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S2.T1.3.6.4\">65.0 (8.6k Tokens)</td>\n</tr>\n</table>", "caption": "Table 1: Performance comparison on benchmark datasets. All scores are reported as pass@1. Scores reported from DeepSeek-R1 paper DeepSeek-AI (2025) are noted with \u2020. The number in parentheses represents the average output token length (including the chain of thought), obtained from our testing.", "description": "This table compares the performance of different large language models (LLMs) on three benchmark datasets: AIME 2024 (Mathematics), LiveCodeBench (Coding), and GPQA-Diamond (Science).  The models compared include DeepSeek-R1 and its distilled versions (DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B), along with the authors' new model, TinyR1-32B-Preview.  Performance is measured by pass@1 (the percentage of correct answers for each dataset). The table also shows the average output token length (including chain-of-thought reasoning) produced by each model, giving an indication of computational cost. Scores from the DeepSeek-R1 paper are marked with a \u2020.", "section": "3.2 Main Results"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T2.1.1.1.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.1.1.2\">Math</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.1.1.3\">Coding</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.1.1.4\">Science</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S3.T2.1.1.5\">Merging Time</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.1\">(AIME 2024)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.2\">(LiveCodeBench)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.3\">(GPQA-Diamond)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.2.4\">(GPU Hours)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.3.1\">Math Expert</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.3.2\">73.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.3.3\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.3.4\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.3.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.4.1\">Coding Expert</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.4.3\">63.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.4.4\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.4.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.1\">Science Expert</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.5.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.5.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.5.4\">64.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.5.5\">-</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.6.1\">Data Mixture</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.6.2\">75.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.6.3\">61.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.6.4.1\">65.7</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.6.5\">740 h</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.7.1\">Merging: (Math &amp; Coding) &amp; Science</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.7.2\">77.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.7.3.1\">63.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.7.4\">64.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.7.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.7.5.1\">4h</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.8.1\">Merging: (Math &amp; Science) &amp; Coding</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.8.2.1\">78.1</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.8.3\">61.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.8.4\">65.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.8.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.8.5.1\">4h</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T2.1.9.1\">TinyR1-32B-Preview</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.9.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.9.2.1\">78.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.9.3\">61.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.9.4\">65.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.9.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.9.5.1\">4h</span></td>\n</tr>\n</table>", "caption": "Table 2: Performance comparison between backbone experts, the data-mixture model, and merged model. All scores are reported as pass@1. LiveCodeBench here refers to the 24.08-25.02 subset of full LiveCodeBench.", "description": "Table 2 presents a performance comparison of different large language models (LLMs) on three benchmark datasets: AIME 2024 for mathematics, LiveCodeBench (subset 24.08-25.02) for coding, and GPQA-Diamond for science.  The models compared include three specialized \"expert\" models (one each for math, coding, and science), a \"Data Mixture\" model trained on combined data from all three domains, and the proposed \"merged model\" which combines the knowledge from the expert models using a novel merging technique.  The results show the pass@1 accuracy (percentage of correctly answered questions) for each model on each benchmark. The table also includes the time spent in the model merging phase (in GPU hours). This allows for a direct comparison of the performance and efficiency gains achieved by the proposed merging technique compared to training separate models or simply mixing the data.", "section": "3.3 Ablation Study"}]