[{"content": "| Judge Model | Realization | Aggregation | Agreement (\u03c4) with Gold Ranking |\n|---|---|---|---| \n| Qwen2.5-72B-Instruct | Likert | Win-Rate | .83 |\n| URM-LLaMa-3.1-8B | Reward | Mean | .82 |\n| GPT-4o-2024-11-20 | Anchor | Mean | .82 |\n| Llama-3-1-405b-instruct-fp8 | Numeric | Mean | .81 |\n| Mistral-large-instruct-2407 | Likert | BT | .81 |\n| GPT-4o-mini-2024-07-18 | Numeric | Win-Rate | .81 |\n| ArmoRM-Llama3-8B-v0.1 | Reward | Mean | .80 |\n| Llama-3-1-70b-instruct | Numeric | Win-Rate | .80 |\n| Skywork-Llama-3.1-8B-v0.2 | Reward | Mean | .79 |\n| Llama-3.1-8B-Instruct | TokenProbs | Mean | .78 |", "caption": "Table 1: Top 10 judges by ranking performance. Judges are sorted by the Kendall\u2019s Tau correlation between their overall system ranking and the gold ranking from Chatbot Arena (\u00a74.4). For every judge model, only the best-performing realization and aggregation method is shown. For the full results, refer to Appendix Table\u00a0LABEL:tab:leaderboard_full.", "description": "This table presents the top 10 performing Large Language Models (LLMs) and reward models, ranked by their accuracy in generating system rankings that align with human-generated rankings from the Chatbot Arena dataset. The ranking is determined using Kendall's Tau correlation.  For each model, only the single best-performing combination of realization (how the model was used for judgment) and aggregation method (how individual scores were combined to get a system-level score) is shown.  The full results, including the performance of all 48 models and all combinations of realizations and aggregation methods, can be found in Appendix Table LABEL:tab:leaderboard_full.", "section": "4 Judge Performance Results"}, {"content": "Judge Model|Realization|Aggregation|Agreement (\u03c4)\n---|---|---|---\nw/ Gold Ranking| | | |\nQwen2.5-72B-Instruct|Likert|Win-Rate|.827\nURM-LLaMa-3.1-8B|Reward|Mean|.823\nGPT-4o-2024-11-20|Anchor|Mean|.822\nURM-LLaMa-3.1-8B|Reward|BT|.819\nQwen2.5-72B-Instruct|Likert|BT|.817\nURM-LLaMa-3.1-8B|Reward|Win-Rate|.816\nQwen2.5-72B-Instruct|Numeric|BT|.814\nGPT-4o-2024-11-20|Anchor|Win-Rate|.814\nQwen2.5-72B-Instruct|Numeric|Win-Rate|.813\nLlama-3-1-405b-instruct-fp8|Numeric|Mean|.812\nLlama-3-1-405b-instruct-fp8|Numeric|Win-Rate|.812\nMistral-large-instruct-2407|Likert|BT|.811\nGPT-4o-2024-11-20|Anchor|BT|.809\nMistral-large-instruct-2407|Numeric|BT|.809\nURM-LLaMa-3.1-8B|Reward|Median|.809\nGPT-4o-mini-2024-07-18|Numeric|Win-Rate|.807\nLlama-3-1-405b-instruct-fp8|Numeric|BT|.805\nGPT-4o-mini-2024-07-18|Numeric|BT|.804\nMistral-large-instruct-2407|Numeric|Win-Rate|.802\nQwen2.5-72B-Instruct|Likert|Mean|.801\nArmoRM-Llama3-8B-v0.1|Reward|Mean|.800\nQwen2.5-72B-Instruct|Anchor|Mean|.799\nGPT-4o-mini-2024-07-18|Likert|BT|.798\nLlama-3-1-70b-instruct|Numeric|Win-Rate|.798\nLlama-3-1-70b-instruct|Numeric|BT|.798\nMistral-large-instruct-2407|Likert|Win-Rate|.798\nQwen2.5-72B-Instruct|Anchor|BT|.794\nLlama-3-1-405b-instruct-fp8|Likert|Win-Rate|.793\nLlama-3-1-70b-instruct|TokenProbs|Win-Rate|.793\nGPT-4o-mini-2024-07-18|Likert|Win-Rate|.793\nArmoRM-Llama3-8B-v0.1|Reward|Median|.793\nLlama-3-1-405b-instruct-fp8|Likert|BT|.787\nMistral-large-instruct-2407|Anchor|Win-Rate|.786\nSkywork-Llama-3.1-8B-v0.2|Reward|Mean|.786\nQwen2.5-72B-Instruct|Anchor|Win-Rate|.786\nMistral-large-instruct-2407|Likert|Mean|.782\nGPT-4o-mini-2024-07-18|Numeric|Mean|.781\nSkywork-Llama-3.1-8B-v0.2|Reward|Win-Rate|.780\nLlama-3-1-405b-instruct-fp8|Likert|Mean|.780\nSkywork-Llama-3.1-8B-v0.2|Reward|BT|.778\nLlama-3.1-8B-Instruct|TokenProbs|Mean|.778\nQwen2.5-72B-Instruct|TokenProbs|BT|.777\nLlama-3.1-8B-Instruct|TokenProbs|Median|.776\nMixtral-8x22B-instruct-v0.1|Numeric|BT|.776\nLlama-3-1-70b-instruct|TokenProbs|Median|.776\nGPT-4o-2024-11-20|Numeric|BT|.774\nGPT-4o-mini-2024-07-18|Likert|Mean|.773\nQwen2.5-72B-Instruct|Numeric|Mean|.773\nGPT-4o-2024-11-20|Likert|BT|.773\nGPT-4o-2024-11-20|Numeric|Win-Rate|.771\nLlama-3-OffsetBias-RM-8B|Reward|Win-Rate|.765\nLlama-3-1-70b-instruct|TokenProbs|BT|.765\nLlama-3-OffsetBias-RM-8B|Reward|BT|.765\nSkywork-Llama-3.1-8B-v0.2|Reward|Median|.764\nLlama-3-1-70b-instruct|TokenProbs|Mean|.764\nMistral-large-instruct-2407|Anchor|Mean|.764\nLlama-3-1-70b-instruct|Numeric|Mean|.764\nArmoRM-Llama3-8B-v0.1|Reward|BT|.763\nArmoRM-Llama3-8B-v0.1|Reward|Win-Rate|.762\nLlama-3-OffsetBias-RM-8B|Reward|Median|.759\nGPT-4o-mini-2024-07-18|TokenProbs|Win-Rate|.759\nGPT-4o-2024-11-20|Likert|Win-Rate|.758\nLlama-3-OffsetBias-RM-8B|Reward|Mean|.757\nMixtral-8x22B-instruct-v0.1|Numeric|Win-Rate|.756\nGPT-4o-mini-2024-07-18|TokenProbs|BT|.752\nQwen2.5-72B-Instruct|TokenProbs|Median|.752\nMistral-large-instruct-2407|Numeric|Mean|.750\nLlama-3-70b-instruct|Numeric|BT|.749\nQwen2.5-72B-Instruct|TokenProbs|Win-Rate|.748\nLlama-3-1-405b-instruct-fp8|Anchor|Win-Rate|.748\nLlama-3-1-70b-instruct|Likert|Mean|.746\nGPT-4o-2024-11-20|Likert|Mean|.744\nLlama-3.1-8B-Instruct|TokenProbs|Win-Rate|.744\nLlama-3-1-405b-instruct-fp8|Anchor|Mean|.744\nLlama-3.1-8B-Instruct|TokenProbs|BT|.741\nLlama-3-1-405b-instruct-fp8|TokenProbs|Win-Rate|.741\nGPT-4o-mini-2024-07-18|TokenProbs|Mean|.741\nMixtral-8x22B-instruct-v0.1|Likert|BT|.738\nGPT-4o-2024-11-20|Numeric|Mean|.738\nLlama-3-1-405b-instruct-fp8|TokenProbs|Median|.737\nLlama-3.1-8B-Instruct|Likert|Mean|.736\nLlama-3-70b-instruct|Numeric|Win-Rate|.733\nLlama-3-1-405b-instruct-fp8|TokenProbs|Mean|.733\nLlama-3-1-70b-instruct|Likert|Win-Rate|.732\nMistral-large-instruct-2407|TokenProbs|Mean|.730\nInternlm2-7b-reward|Reward|Mean|.731\nLlama-3-1-405b-instruct-fp8|Anchor|BT|.730\nInternlm2-20b-reward|Reward|Mean|.728\nMistral-large-instruct-2407|Anchor|BT|.725\nInternlm2-20b-reward|Reward|Median|.724\nGPT-4o-mini-2024-07-18|TokenProbs|Median|.723\nLlama-3.1-8B-Instruct|Likert|BT|.723\nLlama-3-1-70b-instruct|Likert|BT|.722\nInternlm2-7b-reward|Reward|Median|.721\nMixtral-8x22B-instruct-v0.1|Likert|Mean|.719\nInternlm2-7b-reward|Reward|Win-Rate|.717\nInternlm2-20b-reward|Reward|BT|.717\nMixtral-8x22B-instruct-v0.1|TokenProbs|Win-Rate|.717\nLlama-3-1-70b-instruct|Anchor|Win-Rate|.716\nGRM-Llama3.2-3B|Reward|Mean|.716\nInternlm2-20b-reward|Reward|Win-Rate|.716\nMixtral-8x22B-instruct-v0.1|Numeric|Mean|.715\nLlama-3-1-70b-instruct|Anchor|Mean|.714\nGRM-Llama3.2-3B|Reward|Win-Rate|.712\nInternlm2-7b-reward|Reward|BT|.712\nGRM-Llama3.2-3B|Reward|BT|.711\nGRM-Llama3.2-3B|Reward|Median|.706\nGPT-4o-2024-11-20|TokenProbs|Median|.704\nLlama-3-70b-instruct|Numeric|Mean|.704\nMixtral-8x22B-instruct-v0.1|TokenProbs|BT|.702\nGPT-4o-2024-11-20|TokenProbs|Mean|.701\nGPT-4o-2024-11-20|TokenProbs|BT|.700\nLlama-3-70b-instruct|Likert|BT|.698\nLlama-3-70b-instruct|TokenProbs|Win-Rate|.696\nGPT-4o-2024-11-20|TokenProbs|Win-Rate|.696\nLlama-3.1-8B-Instruct|Anchor|Mean|.695\nLlama-3.1-8B-Instruct|Likert|Win-Rate|.694\nLlama-3-1-70b-instruct|Anchor|BT|.688\nLlama-3-70b-instruct|Likert|Win-Rate|.681\nLlama-3.1-8B-Instruct|Numeric|Mean|.680\nLlama-3-70b-instruct|Likert|Mean|.678\nLlama-3.1-8B-Instruct|Anchor|BT|.677\nGPT-4o-mini-2024-07-18|Anchor|Mean|.675\nLlama-3-1-405b-instruct-fp8|TokenProbs|BT|.672\nLlama-3.1-8B-Instruct|Numeric|BT|.668\nGPT-4o-mini-2024-07-18|Anchor|Win-Rate|.668\nLlama-3-70b-instruct|Anchor|Mean|.667\nLlama-3-70b-instruct|TokenProbs|Mean|.666\nMixtral-8x22B-instruct-v0.1|Anchor|Mean|.665\nLlama-3-70b-instruct|TokenProbs|BT|.663\nGPT-4o-mini-2024-07-18|Anchor|BT|.659\nMixtral-8x7B-instruct-v0.1|Numeric|BT|.656\nMixtral-8x7B-instruct-v0.1|Anchor|BT|.655\nMixtral-8x22B-instruct-v0.1|TokenProbs|Mean|.650\nEurus-RM-7b|Reward|Median|.643\nEurus-RM-7b|Reward|Mean|.641\nMixtral-8x22B-instruct-v0.1|Anchor|BT|.641\nLlama-3.1-8B-Instruct|Anchor|Win-Rate|.639\nLlama-3-70b-instruct|Anchor|Win-Rate|.638\nLlama-3-70b-instruct|Anchor|BT|.633\nLlama-3.1-8B-Instruct|Numeric|Win-Rate|.632\nEurus-RM-7b|Reward|Win-Rate|.629\nEurus-RM-7b|Reward|BT|.628\nMixtral-8x7B-instruct-v0.1|Numeric|Win-Rate|.626\nMixtral-8x7B-instruct-v0.1|Numeric|Mean|.626\nMixtral-8x7B-instruct-v0.1|Anchor|Win-Rate|.622\nMixtral-8x22B-instruct-v0.1|Anchor|Win-Rate|.612\nMixtral-8x7B-instruct-v0.1|Anchor|Mean|.610\nMixtral-8x7B-instruct-v0.1|Likert|BT|.590\nMixtral-8x7B-instruct-v0.1|Likert|Mean|.585\nMixtral-8x7B-instruct-v0.1|Likert|Win-Rate|.543\nMixtral-8x7B-instruct-v0.1|TokenProbs|BT|.427\nMistral-large-instruct-2407|TokenProbs|Win-Rate|.417\nMixtral-8x7B-instruct-v0.1|TokenProbs|Mean|.411\nMixtral-8x7B-instruct-v0.1|TokenProbs|Win-Rate|.371\nMistral-large-instruct-2407|TokenProbs|BT|.369\nMistral-large-instruct-2407|TokenProbs|Median|.363", "caption": "Table 2: Judges by ranking performance. The judges are sorted by the Kendall\u2019s Tau correlation between their overall system ranking and the gold ranking from Chatbot Arena (\u00a74.4).", "description": "This table presents the top-performing large language models (LLMs) and reward models, ranked by their ability to accurately rank different systems based on their overall quality.  The ranking is determined by calculating the Kendall's Tau correlation between each model's system ranking and a human-generated gold standard ranking from Chatbot Arena.  Higher Kendall's Tau correlation indicates better agreement with the human ranking, signifying a more accurate system-level evaluation by the model.", "section": "5 JuStRank - Judge Performance Results"}, {"content": "| Judge | Self-bias | Significance p-value |\n|---|---|---|\n| GPT-4o-mini-2024-07-18 (Anchor) | -0.05 | \u2013 |\n| GPT-4o-mini-2024-07-18 (Likert) | -0.04 | \u2013 |\n| GPT-4o-mini-2024-07-18 (Numeric) | +0.03 | >0.5 (N.S.) |\n| GPT-4o-mini-2024-07-18 (TokenProbs) | +0.06 | 0.13 (N.S.) |\n| Llama-3-1-70b-instruct (Anchor) | -0.05 | \u2013 |\n| Llama-3-1-70b-instruct (Likert) | +0.16 | 7.1e-03 |\n| Llama-3-1-70b-instruct (Numeric) | -0.00 | \u2013 |\n| Llama-3-1-70b-instruct (TokenProbs) | -0.03 | \u2013 |\n| Llama-3-70b-instruct (Anchor) | +0.09 | 4.7e-04 |\n| Llama-3-70b-instruct (Likert) | +0.15 | 8.4e-08 |\n| Llama-3-70b-instruct (Numeric) | +0.14 | 1.8e-13 |\n| Llama-3-70b-instruct (TokenProbs) | -0.01 | \u2013 |\n| Llama-3.1-8B-Instruct (Anchor) | -0.07 | \u2013 |\n| Llama-3.1-8B-Instruct (Likert) | -0.04 | \u2013 |\n| Llama-3.1-8B-Instruct (Numeric) | +0.02 | >0.5 (N.S.) |\n| Llama-3.1-8B-Instruct (TokenProbs) | -0.04 | \u2013 |\n| Mistral-large-instruct-2407 (Anchor) | -0.07 | \u2013 |\n| Mistral-large-instruct-2407 (Likert) | +0.02 | >0.5 (N.S.) |\n| Mistral-large-instruct-2407 (Numeric) | +0.06 | 0.33 (N.S.) |\n| Mistral-large-instruct-2407 (TokenProbs) | +0.01 | >0.5 (N.S.) |", "caption": "Table 3: Judge self-bias. The table shows the self-bias values for LLM judge realizations, i.e., the value of the corrected bias Bsa\u2032psuperscriptsubscriptsuperscript\ud835\udc35\u2032subscript\ud835\udc60\ud835\udc4e\ud835\udc5d{B^{\\prime}_{s_{a}}}^{p}italic_B start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT (\u00a76.2) where the LLM judge p\ud835\udc5dpitalic_p and system sasubscript\ud835\udc60\ud835\udc4es_{a}italic_s start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT correspond to the same underlying LLM. For positive self-bias values we test the statistical significance using paired t-tests (one-sided, with Bonferroni correction). N.S.: non-significant (p>0.05\ud835\udc5d0.05p>0.05italic_p > 0.05).", "description": "This table presents the results of a statistical analysis assessing whether Large Language Models (LLMs) exhibit bias towards systems using the same underlying LLM (self-bias).  Each row represents a specific LLM judge realization, showing the calculated self-bias value, whether it was significantly positive or negative, and the p-value of a statistical test for significance. A positive bias indicates the LLM judge favors systems built on its own underlying model, while a negative bias suggests otherwise.  Non-significant results are noted as N.S.  The statistical test employed is a paired t-test with a Bonferroni correction to account for multiple comparisons.", "section": "6 Judge Behavior"}, {"content": "| Judge Model | Realization | Agreement | Decisiveness | Bias |\n|---|---|---|---|---|\n|  |  | with Gold \u03c4\u2191 | \u03b1\u2191 | \u03b4\u2193 |\n| URM-LLaMa-3.1-8B | Reward | .819 | 1.84 | .085 |\n| Qwen2.5-72B-Instruct | Likert | .817 | 4.76 | .079 |\n| Qwen2.5-72B-Instruct | Numeric | .814 | 4.09 | .079 |\n| Mistral-large-instruct-2407 | Likert | .811 | 5.47 | .086 |\n| GPT-4o-2024-11-20 | Anchor | .809 | 3.07 | .085 |\n| Mistral-large-instruct-2407 | Numeric | .809 | 3.01 | .082 |\n| Llama-3-1-405b-instruct-fp8 | Numeric | .805 | 4.33 | .087 |\n| GPT-4o-mini-2024-07-18 | Numeric | .804 | 2.91 | .077 |\n| GPT-4o-mini-2024-07-18 | Likert | .798 | 4.61 | .087 |\n| Llama-3-1-70b-instruct | Numeric | .798 | 2.69 | .087 |\n| Qwen2.5-72B-Instruct | Anchor | .794 | 2.93 | .090 |\n| Llama-3-1-405b-instruct-fp8 | Likert | .787 | 5.22 | .097 |\n| Skywork-Llama-3.1-8B-v0.2 | Reward | .778 | 2.46 | .100 |\n| Qwen2.5-72B-Instruct | TokenProbs | .777 | 2.69 | .082 |\n| Mixtral-8x22B-instruct-v0.1 | Numeric | .776 | 2.12 | .089 |\n| GPT-4o-2024-11-20 | Numeric | .774 | 2.15 | .077 |\n| GPT-4o-2024-11-20 | Likert | .773 | 5.49 | .089 |\n| Llama-3-1-70b-instruct | TokenProbs | .765 | 1.26 | .070 |\n| Llama-3-OffsetBias-RM-8B | Reward | .765 | 1.39 | .076 |\n| ArmoRM-Llama3-8B-v0.1 | Reward | .763 | 1.84 | .092 |\n| GPT-4o-mini-2024-07-18 | TokenProbs | .752 | 2.10 | .084 |\n| Llama-3-70b-instruct | Numeric | .749 | 1.27 | .084 |\n| Llama-3.1-8B-Instruct | TokenProbs | .741 | .598 | .061 |\n| Mixtral-8x22B-instruct-v0.1 | Likert | .738 | 2.53 | .108 |\n| Llama-3-1-405b-instruct-fp8 | Anchor | .730 | 3.58 | .112 |\n| Mistral-large-instruct-2407 | Anchor | .725 | 2.13 | .111 |\n| Llama-3.1-8B-Instruct | Likert | .723 | .935 | .090 |\n| Llama-3-1-70b-instruct | Likert | .722 | 3.90 | .120 |\n| Internlm2-20b-reward | Reward | .717 | 1.90 | .098 |\n| Internlm2-7b-reward | Reward | .712 | 2.35 | .113 |\n| GRM-Llama3.2-3B | Reward | .711 | 2.30 | .114 |\n| Mixtral-8x22B-instruct-v0.1 | TokenProbs | .702 | 1.85 | .088 |\n| GPT-4o-2024-11-20 | TokenProbs | .700 | 2.22 | .093 |\n| Llama-3-70b-instruct | Likert | .698 | 2.40 | .122 |\n| Llama-3-1-70b-instruct | Anchor | .688 | 2.71 | .126 |\n| Llama-3.1-8B-Instruct | Anchor | .677 | .868 | .085 |\n| Llama-3-1-405b-instruct-fp8 | TokenProbs | .672 | 1.55 | .092 |\n| Llama-3.1-8B-Instruct | Numeric | .668 | 1.20 | .104 |\n| Llama-3-70b-instruct | TokenProbs | .663 | .775 | .071 |\n| GPT-4o-mini-2024-07-18 | Anchor | .659 | 1.41 | .111 |\n| Mixtral-8x7B-instruct-v0.1 | Numeric | .656 | 1.27 | .102 |\n| Mixtral-8x7B-instruct-v0.1 | Anchor | .655 | 1.17 | .102 |\n| Mixtral-8x22B-instruct-v0.1 | Anchor | .641 | 1.50 | .140 |\n| Llama-3-70b-instruct | Anchor | .633 | 1.82 | .132 |\n| Eurus-RM-7b | Reward | .628 | 2.49 | .138 |\n| Mixtral-8x7B-instruct-v0.1 | Likert | .590 | .838 | .110 |\n| Mixtral-8x7B-instruct-v0.1 | TokenProbs | .427 | .739 | .107 |\n| Mistral-large-instruct-2407 | TokenProbs | .369 | 1.17 | .123 |", "caption": "Table 4: Judge characteristics. The table presents three measures for each judge realization: an overall ranking quality \u03c4\ud835\udf0f\\tauitalic_\u03c4 (\u00a75, Kendall\u2019s Tau correlation with the Chatbot Arena gold ranking), a decisiveness score \u03b1\ud835\udefc\\alphaitalic_\u03b1 (\u00a76.1, App.\u00a0F), and its propensity for system-specific biases \u03b4\ud835\udeff\\deltaitalic_\u03b4 (\u00a76.2). Correlations \u03c4\ud835\udf0f\\tauitalic_\u03c4 shown are for the BT aggregation method; \u03b1\ud835\udefc\\alphaitalic_\u03b1 and \u03b4\ud835\udeff\\deltaitalic_\u03b4 are calculated on the judge scores before aggregation. \u2193\u2193\\downarrow\u2193: Lower is better.", "description": "This table presents a comprehensive evaluation of different Large Language Models (LLMs) and reward models used as judges for ranking systems.  For each judge (and its different configurations/realizations), three key metrics are provided:\n\n1. **Overall Ranking Quality (\u03c4):**  Measures the correlation (Kendall's Tau) between the judge's ranking of systems and a human-generated gold standard ranking from Chatbot Arena. Higher values indicate better agreement with human judgment.\n2. **Decisiveness (\u03b1):**  Quantifies the tendency of a judge to produce extreme win-rate predictions (i.e., closer to 0 or 1). Higher \u03b1 values represent more decisive judges, amplifying the difference between strong and weak systems.\n3. **Bias (\u03b4):** Measures the propensity of a judge to exhibit system-specific bias (i.e., consistently favoring or disfavoring particular systems). Lower values indicate less bias.  The analysis is performed using the Bradley-Terry (BT) aggregation method for ranking quality (\u03c4) while decisiveness (\u03b1) and bias (\u03b4) are calculated using raw judge scores before aggregation.", "section": "Judge Performance Results"}]