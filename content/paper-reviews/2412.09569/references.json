{"references": [{"fullname_first_author": "Zheng Cai", "paper_title": "InternLM2 technical report", "publication_date": "2024-03-17", "reason": "This paper introduces InternLM2, a large language model used as one of the judges in the benchmark, and its performance is directly compared against others."}, {"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot Arena: An open platform for evaluating LLMs by human preference", "publication_date": "2024-00-00", "reason": "This paper describes Chatbot Arena, which provides the ground truth ranking used for evaluating the judges' performance and comparing their ranking agreement with human preferences."}, {"fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "publication_date": "2024-00-00", "reason": "This paper introduces RewardBench, a benchmark for evaluating instance-level performance of LLMs as judges; its results are compared with those from JuStRank to highlight differences between instance and system-level evaluations."}, {"fullname_first_author": "Tianle Li", "paper_title": "From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline", "publication_date": "2024-06-11", "reason": "This paper describes the Arena Hard dataset, which provides the system response data used in the proposed JuStRank benchmark, thus establishing the data foundation for the system-level ranking evaluation."}, {"fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena", "publication_date": "2023-00-00", "reason": "This paper discusses the use of LLMs as judges, which is the central theme of the target paper, providing relevant background and context for the proposed system-level benchmarking approach."}]}