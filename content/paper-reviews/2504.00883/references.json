{"references": [{"fullname_first_author": "Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper outlines the R1-Zero framework, which this study adapts for visual-spatial reasoning in MLLMs."}, {"fullname_first_author": "Shao", "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models.", "publication_date": "2024-02-01", "reason": "The research utilizes GRPO and cites the original GRPO paper, underlining the method's importance for improving reasoning."}, {"fullname_first_author": "Wang", "paper_title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.", "publication_date": "2024-09-01", "reason": "This paper is fundamental, as the research focuses on improving the Qwen2-VL model's visual-spatial reasoning capabilities."}, {"fullname_first_author": "Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model.", "publication_date": "2023-01-01", "reason": "This paper is used as the primary baseline for comparison to GRPO for visual-spatial reasoning."}, {"fullname_first_author": "Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.", "publication_date": "2022-01-01", "reason": "This paper is important as the research initially explores Chain of Thought prompting strategies, which are found to be ineffective for small- to medium-sized Qwen2-VL on VSI-bench."}]}