{"references": [{"fullname_first_author": "Wolf", "paper_title": "Transformers: State-of-the-art natural language processing", "publication_date": "2020-10-01", "reason": "This paper introduces the Transformers library, a fundamental tool used throughout the reported work for modeling VLMs."}, {"fullname_first_author": "von Werra", "paper_title": "TRL: Transformer reinforcement learning", "publication_date": "2020-01-01", "reason": "This reference introduces Transformer Reinforcement Learning (TRL), a popular and important baseline that the current framework aims to improve upon."}, {"fullname_first_author": "Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention.", "publication_date": "2023-01-01", "reason": "This reference describes vLLM, a key tool employed in this work for efficient inference when collecting responses from VLMs."}, {"fullname_first_author": "Schulman", "paper_title": "Approximating KL divergence", "publication_date": "2025-01-01", "reason": "This paper is a key reference for KL divergence calculations, a crucial aspect of the RL algorithms applied in this study."}, {"fullname_first_author": "Agarwal", "paper_title": "Deep reinforcement learning at the edge of the statistical precipice", "publication_date": "2021-01-01", "reason": "This work is important because it addresses the challenges of robust and reliable evaluation in RL, a central theme of this work's proposed evaluation scheme."}]}