[{"figure_path": "https://arxiv.org/html/2503.24290/x1.png", "caption": "Figure 1: \nEvaluation performance of Open-Reasoner-Zero-{7B, 32B} on benchmarks (averaged on 16 responses) during training.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.", "description": "This figure displays the training performance of the Open-Reasoner-Zero models with 7 billion and 32 billion parameters on three reasoning benchmarks: AIME2024, MATH500, and GPQA Diamond.  The x-axis represents the number of training steps, and the y-axis represents the accuracy achieved on each benchmark.  The graph shows that the 32B parameter model surpasses the performance of the DeepSeek-R1-Zero-Qwen-32B model (which used as a baseline), reaching a similar level of accuracy with significantly fewer training steps (approximately one-tenth). This highlights the efficiency of the Open-Reasoner-Zero approach.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.24290/x2.png", "caption": "Figure 2: \nTrain-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - {0.5B, 1.5B, 7B, 32B}.\nTrain Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes.\nInterestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.", "description": "Figure 2 presents the results of scaling up the training of the Open-Reasoner-Zero model across different sizes (0.5B, 1.5B, 7B, and 32B parameters).  The graphs show both the training reward and the response length over the course of training. Both metrics exhibit a steady increase across all model sizes, demonstrating that the model's performance and response length scale consistently with increased training compute and model size.  A notable observation is that even with fluctuations in response length for the largest 32B parameter model, the training remains stable, indicating the robustness of the minimalist training approach (vanilla PPO with GAE and a simple rule-based reward) employed.", "section": "2. Scale-up Reinforcement Learning from a Base Model"}, {"figure_path": "https://arxiv.org/html/2503.24290/x3.png", "caption": "Figure 3: The code snippet for verifying the mathematical correctness of generated answers using the Math-Verify library.", "description": "This figure shows a Python code snippet that uses the Math-Verify library to check if the generated answer from the language model is mathematically correct. The code takes two inputs: the ground truth and the model's output, both in a parsed format.  It then uses the `verify` function from the Math-Verify library to compare the two and returns a boolean value indicating whether the model's answer is correct.", "section": "2.1. Basic Settings"}, {"figure_path": "https://arxiv.org/html/2503.24290/x4.png", "caption": "Figure 4: \nPercentage of responses following the reasoning format.\nResults demonstrate rapid adoption of structured reasoning patterns even by the base model using only a simple rule-based reward function.\nOur findings suggest that complicated reward functions are unnecessary for training Reasoner-Zero models.", "description": "This figure shows the percentage of responses that followed the expected reasoning format during training.  The results indicate that even a simple, rule-based reward function, without complex design, quickly trained the base language model to adopt a structured reasoning format. This finding suggests that complicated reward functions aren't necessary for effective Reasoner-Zero model training, simplifying the training process and improving scalability.", "section": "2. Scale-up Reinforcement Learning from a Base Model"}, {"figure_path": "https://arxiv.org/html/2503.24290/x5.png", "caption": "Figure 5: Comparison of training and evaluation reward and average response length for the Open-Reasoner-Zero 7B model. All of benchmarks experience a sudden increase in reward and response length at a certain point, a phenomenon like emergent behavior.", "description": "Figure 5 presents a detailed analysis of the training and evaluation results for the Open-Reasoner-Zero 7B model.  It showcases four subplots, each corresponding to a specific benchmark (Training, MATH500, GPQA Diamond, and AIME2024).  Each subplot displays two key metrics: reward and average response length plotted against the number of training steps. Notably, across all four benchmarks, the reward and response length exhibit a sharp increase at a certain point during training. This phenomenon suggests an emergent behavior where the model suddenly shows significantly improved performance and reasoning capabilities.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.24290/x6.png", "caption": "Figure 6: Reflection patterns in generated responses. The Average Correct Reflection Length consistently exceeds the Average Response Length throughout the training process. A particularly noteworthy phenomenon emerges around step 680, where we observe a simultaneous acceleration in three metrics: Reward in training set, Average Correct Reflection Length, and Average Response Length.", "description": "Figure 6 analyzes the relationship between reflection patterns in model-generated responses and key training metrics.  The average length of correct responses containing reflection patterns consistently surpasses the average length of all generated responses, suggesting that more thoughtful responses lead to higher accuracy.  A significant event occurs around training step 680, marked by a simultaneous and rapid increase in three metrics: the average training reward, the average length of correct responses with reflection patterns, and the average length of all generated responses. This sudden improvement suggests a qualitative shift in model reasoning ability, possibly indicating an emergent behavior.", "section": "3.2 Training Results"}, {"figure_path": "https://arxiv.org/html/2503.24290/x7.png", "caption": "Figure 7: Comparison of different GAE \u03bb\ud835\udf06\\lambdaitalic_\u03bb values. GAE \u03bb=1.0\ud835\udf061.0\\lambda=1.0italic_\u03bb = 1.0 shows better stability and performance compared to \u03bb=0.95\ud835\udf060.95\\lambda=0.95italic_\u03bb = 0.95 for both training reward and response length.", "description": "Figure 7 presents a comparison of the training reward and response length for two different values of the GAE (Generalized Advantage Estimation) lambda (\u03bb) hyperparameter within the Proximal Policy Optimization (PPO) algorithm.  The graph shows the training progress over a series of steps, plotting both reward and response length.  The results show that using a GAE \u03bb of 1.0 leads to more stable training with better performance (both reward and response length) compared to using a \u03bb of 0.95.  The improved stability is evident in the smoother curve of the \u03bb=1.0 line. The superior performance translates to achieving higher rewards and longer response lengths across training.", "section": "3.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.24290/x8.png", "caption": "Figure 8: \nComparisons to applying KL-related regularizations.\nNotably, training without KL constraints demonstrates superior average benchmark performance and length scaling property, compared to models trained with KL Loss and KL Penalty. Performance is evaluated on MATH500, AIME2024, and GPQA DIAMOND benchmarks using pass@1 metric.", "description": "Figure 8 presents an ablation study comparing the performance of models trained with and without KL (Kullback-Leibler) regularization techniques.  The x-axis represents the training steps, while the y-axis displays two key metrics: the average benchmark performance (pass@1 metric) across MATH500, AIME2024, and GPQA Diamond datasets, and the average response length. The results reveal that models trained without KL regularization (no KL loss or KL penalty) achieve superior performance on the benchmarks and exhibit better scaling properties with respect to the response length compared to models using KL-based regularization methods. This finding underscores the effectiveness and stability of a minimalist training approach that does not incorporate KL regularization.", "section": "3.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.24290/x9.png", "caption": "Figure 9: Data scale ablation study. Training data from math train 7.5k to Open-Reasoner-Zero 57k, we observe a consistent increase in both training reward and response length for training and evaluation set, indicating that data scale plays a crucial role in training performance. Performance is evaluated on MATH500 benchmark using pass@1 metric.", "description": "This figure presents an ablation study on the impact of training data size on the performance of the Open-Reasoner-Zero model.  It shows that increasing the amount of training data, from a smaller set of 7.5k samples to a much larger set of 57k samples from the MATH500 benchmark, leads to a consistent improvement in both the training reward and the average response length.  This result is observed for both the training dataset and the evaluation dataset, demonstrating the importance of data scale for achieving better performance in large-scale reinforcement learning.  The evaluation metric used is pass@1, specifically on the MATH500 benchmark.", "section": "3.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.24290/x10.png", "caption": "Figure 10: \nEvaluation performance of Open-Reasoner-Zero-{0.5B, 1.5B}.\nWe report the average accuracy on the benchmark dataset for each question with 16 responses.", "description": "Figure 10 presents the evaluation results for the Open-Reasoner-Zero models with 0.5B and 1.5B parameters.  It shows the average accuracy achieved on four benchmark datasets (AIME2024, AIME2025, MATH500, and GPQA Diamond) across different training steps.  The graph visually demonstrates the performance improvement of both models on these reasoning tasks as training progresses. Each data point represents the average accuracy calculated from 16 responses to each question.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.24290/x11.png", "caption": "Figure 11: \nData Curation Ablation Study. CN represents Chinese data and EN represents English data. Our results demonstrate that the English-only dataset yields superior training stability and final model performance.", "description": "This ablation study investigates the impact of data curation on the training stability and final model performance of the Open-Reasoner-Zero model.  The study compares two different data configurations: one using only English data (EN) and another including both English and Chinese data (CN). The results show that training with the English-only dataset leads to superior stability and ultimately better model performance compared to the dataset containing both languages.  This suggests that the inclusion of Chinese data might introduce complexities or noise that hinder the training process, thus reducing the overall effectiveness of the model.", "section": "3.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.24290/x12.png", "caption": "Figure 12: \nComparison of different Prompt, Rollout, Batch Size combinations.\nU.S. represents Update steps of model parameters in each generation steps.\nOn policy update setting performs better than off policy counterpart on both training reward and response length.", "description": "Figure 12 investigates the impact of different hyperparameters on the performance of reinforcement learning for large language models.  It compares training results using various combinations of prompt numbers, rollout lengths (number of responses generated per prompt), and batch sizes during training.  A key variable is 'U.S.', which represents the number of times model parameters are updated per generation step. The figure shows that an on-policy update strategy (where parameters are updated after each generation) outperforms an off-policy strategy (where updates are less frequent) in terms of both training reward and response length.", "section": "3.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.24290/x13.png", "caption": "Figure 13: Comparison of different KL Loss, KL Penalty, and GAE \u03bb\ud835\udf06\\lambdaitalic_\u03bb values.", "description": "This figure presents an ablation study comparing the effects of different KL loss, KL penalty, and GAE lambda values on reinforcement learning performance.  It likely shows training curves (reward and response length) for various combinations of these hyperparameters, illustrating how each setting affects the stability and final performance of the model. The goal is to determine the optimal combination for stable and high-performing training.", "section": "3.3 Ablation Study"}]