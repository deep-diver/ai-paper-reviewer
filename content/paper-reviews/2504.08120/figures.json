[{"figure_path": "https://arxiv.org/html/2504.08120/x1.png", "caption": "Figure 1: Machine Translation evaluation process with Reasoning LLMs", "description": "The figure illustrates the machine translation evaluation process using reasoning LLMs.  An original text is first translated by a machine translation model. Then, a GEMBA-MQM prompt (including instructions, error categories, and few-shot examples) is fed to a reasoning LLM (such as DeepSeek R1 or 03-mini), along with the machine-generated translation and the original text. The reasoning LLM then evaluates the translation quality by analyzing the translation and generating a response that includes identified errors, assigned error categories, and an overall quality assessment. This response represents the model's reasoning trace in evaluating machine translation quality.", "section": "3 Experimental Setup"}]