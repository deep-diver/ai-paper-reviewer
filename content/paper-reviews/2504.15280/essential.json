{"importance": "This work matters as it spotlights MLLMs' shortcomings in multi-view understanding, urging development of specific architectures that consider the spatial geometry. It will help bridge the gap to improve human-level 3D scene understanding.", "summary": "All-Angles Bench: A new benchmark evaluating MLLMs' multi-view scene understanding, revealing limitations in geometric & cross-view consistency.", "takeaways": ["Current MLLMs struggle with multi-view geometric consistency and cross-view correspondence, especially with partially occluded views and establishing coarse camera poses.", "Domain-specific refinements or modules that embed stronger multi-view awareness are needed to improve MLLMs' 3D scene understanding.", "The All-Angles Bench benchmark offers valuable insights for bridging the gap between MLLMs and human-level multi-view understanding."], "tldr": "Multi-view understanding is a key challenge for MLLMs, which struggle with geometric consistency and cross-view correspondence. Existing benchmarks often fall short in evaluating these capabilities, leading to agent manipulation & navigation errors. The paper tackles this gap by introducing **All-Angles Bench**, a benchmark with over 2,100 question-answer pairs across 90 diverse scenes. It tests multi-view understanding skills like counting, pose estimation, and object manipulation.\n\nThe study evaluates 27 MLLMs, and exposes performance gaps with human ability. The **benchmarks reveal limitations** in handling occluded views and establishing camera poses. Chain-of-thought prompting yields limited improvement. The findings advocate specialized architectural refinements or training that builds multi-view awareness for better 3D perception.", "affiliation": "UC Berkeley", "categories": {"main_category": "Computer Vision", "sub_category": "Scene Understanding"}, "podcast_path": "2504.15280/podcast.wav"}