[{"figure_path": "https://arxiv.org/html/2412.05552/x1.png", "caption": "Figure 1: We consolidate diverse navigation tasks into a unified language-guided navigation framework sorted by language granularity. Previous approaches utilize task-specific designs tailored to address particular types of language instructions, as shown in (a) and (b). In contrast, we propose a versatile system that can interpret and execute arbitrary language instructions as shown in (c).", "description": "This figure illustrates the three different approaches to language-guided visual navigation tasks.  (a) shows the traditional approach of using separate models for each task, each tailored to the specificity of its instructions. This method is not efficient or easily scalable. (b) displays a similar approach of using the same model but with different parameters for each task.  This approach is better, but still not as flexible or versatile. (c) shows the proposed method by the authors, a unified model capable of handling diverse instructions with varying levels of granularity (high-level, coarse-grained, and fine-grained), achieving a more versatile and generalizable navigation system.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.05552/extracted/6049815/figures/pipeline.jpg", "caption": "Figure 2: \nIllustration of MoE position and experts\u2019 routing methods. SAME\u00a0routing based on multimodal features from visual observations and language instructions allows the agent to dynamically adapt to environmental visual changes.", "description": "Figure 2 illustrates different Mixture of Experts (MoE) routing methods and how the State-Adaptive Mixture of Experts (SAME) model uses them.  It shows four variations: task-wise MoE, token-wise MoE, language-conditioned MoE, and SAME.  The core difference is how the routing mechanism selects which expert network(s) to use for processing the input.  Task-wise selects based on the task, token-wise based on each input token, language-conditioned based on the input language, and SAME uniquely uses a combination of multimodal features from visual observations and language instructions to make this selection dynamically.  This dynamic adaptation allows the SAME agent to handle the variations in visual environments encountered during visual navigation.", "section": "3. Mixture of Experts for Versatile Language-guided Visual Navigation"}]