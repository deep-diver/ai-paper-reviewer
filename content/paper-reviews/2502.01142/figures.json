[{"figure_path": "https://arxiv.org/html/2502.01142/x1.png", "caption": "Figure 1: Correspondence between human thinking processes and DeepRAG.\nSpecifically, retrieval narrative ensures a structured and adaptive retrieval flow, generating subqueries informed by previously retrieved information, and\natomic decisions dynamically determines whether to retrieve external knowledge or rely solely on the parametric knowledge for each subquery.", "description": "This figure illustrates the parallel between human reasoning and DeepRAG's approach to retrieval-augmented generation.  The left side shows a human's thought process when answering a complex question: first understanding the question, then breaking it down into smaller, manageable parts, searching for relevant information as needed, and finally combining the gathered information to formulate a complete answer. DeepRAG mimics this process using two key components: the retrieval narrative (ensuring a well-structured and adaptive flow of subqueries, building upon previous retrieval results) and atomic decisions (strategically deciding at each step whether to use external information retrieval or rely only on the model's existing knowledge).  This systematic approach contrasts with less efficient methods that may retrieve excessive information.", "section": "3 Thinking to Retrieval Step by Step"}, {"figure_path": "https://arxiv.org/html/2502.01142/x2.png", "caption": "Figure 2: An overview of DeepRAG, our framework comprises three steps: (1) Binary Tree Search, (2) Imitation Learning, and (3) Chain of Calibration. Given a dataset, we first employ binary tree search to synthesize data for imitation learning, enabling the model to learn retrieval patterns. Subsequently, we use binary tree search to construct preference data for further calibrating the LLM\u2019s awareness of its knowledge boundaries.", "description": "DeepRAG is composed of three stages: Binary Tree Search, Imitation Learning, and Chain of Calibration.  First, a binary tree search method systematically explores different reasoning paths, combining retrieval and parametric knowledge. This generates training data that shows the model how to make decisions about when retrieval is necessary.  Then, Imitation Learning uses this data to teach the model effective retrieval strategies. Finally, Chain of Calibration further refines the model's ability to recognize its knowledge boundaries, leading to more accurate decisions about when to retrieve external information and improving the overall effectiveness of retrieval-augmented reasoning.", "section": "Thinking to Retrieval Step by Step"}, {"figure_path": "https://arxiv.org/html/2502.01142/x3.png", "caption": "Figure 3: (a) Subquery Statistics. (b) Retrieval Statistics.", "description": "This figure presents a visual representation of the distribution of subquery counts and retrieval attempts during the question-answering process using DeepRAG.  Panel (a) shows the number of subqueries generated for each question, indicating the complexity of question decomposition.  Panel (b) illustrates the number of retrieval attempts made for each question, reflecting the frequency of external knowledge retrieval within the DeepRAG framework.  This visualization helps to understand the efficiency and the extent of knowledge base utilization within DeepRAG's multi-step reasoning process.", "section": "Question Decomposition Effectiveness"}]