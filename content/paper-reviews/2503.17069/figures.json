[{"figure_path": "https://arxiv.org/html/2503.17069/x1.png", "caption": "Figure 1: Examples of PVChat\u2019s ability with one-shot learning (e.g., <Nz>and <Ab>). PVChat can answer questions about the personalized information correctly while other models [5, 50] fail.", "description": "Figure 1 showcases PVChat's capacity for one-shot learning.  Two example queries are presented. Each query involves a video featuring specific individuals (<Nz> and <Ab> in the examples).  PVChat successfully answers questions requiring personalized information (understanding of the specific individuals' actions and activities) gleaned from a single reference video.  In contrast, other state-of-the-art models (VideoLLaMA2 and InternVideo2) fail to accurately answer these personalized questions, highlighting PVChat's unique capabilities in personalized video comprehension.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.17069/x2.png", "caption": "Figure 2: The systematic data collection pipeline. For positive data collection, the original videos are processed by DeepFaceLab [36] for high-quality face and InterVideo2 [50] for demographic characteristics, which boost identity preservation. ConsisID [56] and LivePortrait [9] with PhotoMaker [21] utilize the identity information to generate videos of various background or different motion/expression, respectively. For model\u2019s robust perception, hard negative samples are selected from either similar face retrieval to generate negative videos, or sampled from the CelebV-HQ dataset [61]. These negative samples guarantee the model\u2019s accurate recognition of both identity and content.", "description": "This figure illustrates the process of data augmentation for training a personalized video chat model.  Starting with original videos, high-quality facial images are extracted using DeepFaceLab.  InternVideo2 analyzes these images to determine demographic information (age, gender, etc.), which is crucial for identity preservation.  This demographic information, along with the extracted face, is then used by ConsisID and PhotoMaker to synthesize new videos with the same identity but different backgrounds and actions (positive samples). Hard negative samples are generated by retrieving similar faces from Laion-Face-5B and CelebV-HQ, creating a diverse dataset with varying visual similarities to the positive samples. This ensures the model learns to distinguish between genuine and similar-looking individuals, improving accuracy and robustness.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/extracted/6299133/QAgen.png", "caption": "Figure 3: We illustrate the process of automatically generating question-answer pairs using InternVideo2 [50] and ChatGPT [1]. A positive and a negative sample are shown at the bottom.", "description": "This figure details the automated pipeline for generating question-answer pairs used in training the PVChat model.  InternVideo2 [50], a video question answering model, initially generates the question-answer pairs. ChatGPT [1] then refines these pairs, ensuring natural language and consistency. The bottom of the figure shows examples of both a positive (correctly answered) and a negative (incorrectly answered) question-answer pair, illustrating the types of data used in model training.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/extracted/6299133/training.png", "caption": "Figure 4: (a) The training pipeline of our method. (b) The proposed ReMoH technique for better specialized characteristics learning.", "description": "Figure 4 illustrates two key aspects of the PVChat model. (a) presents the overall training pipeline, showcasing a two-stage process: initial image-based training for static feature learning followed by video-based fine-tuning to incorporate dynamic aspects and refine subject-specific understanding. (b) details the ReLU Routing Mixture-of-Heads (ReMoH) attention mechanism, a core component of the model designed for efficient and adaptive learning of personalized features. ReMoH is depicted as a routing system that dynamically selects relevant attention heads, enhancing model performance and understanding of individual characteristics.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/x3.png", "caption": "Figure 5: Examples of PVChat\u2019s ability with a learned video (e.g., a man named <Sh>and another man named <Ho>). PVChat can recognize and answer questions about the personalized concept in various scenarios, such as medical scenarios (left) and TV series (right).", "description": "Figure 5 presents a comparison of PVChat's performance against two other state-of-the-art video LLMs (InternVideo2 and VideoLLaMA2) on personalized video question answering.  The figure showcases two examples. In the left example, PVChat correctly identifies and describes a person's condition from a single video, while the other models fail. The right example demonstrates PVChat's ability to answer questions about two individuals in a scene, achieving success where the other models fail to even detect the presence of both individuals. This highlights PVChat's superior ability to learn personalized characteristics from only a single reference video.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17069/x4.png", "caption": "Figure 6: The hierarchical structure of our prompt library, which is carefully divided into four levels, such as gender, age, and scenarios, and provides different descriptions according to the specific subject.", "description": "This figure illustrates the hierarchical structure of the prompt library used in the PVChat model. The library is organized into four levels: gender, age, scenario, and specific descriptions.  Different prompts are generated based on the specific combination of these four factors to ensure that the model receives detailed and contextually relevant information about the target subject in each video.  This approach allows for diverse and accurate training data, especially important for personalized video understanding.", "section": "3.2 Data Collection"}, {"figure_path": "https://arxiv.org/html/2503.17069/x5.png", "caption": "Figure 7: The comparison of expert heads activation between MoH [12] and ReMoH in different layers, where HisubscriptH\ud835\udc56\\text{H}_{i}H start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT head. Orange refers to the video without the target individual, while blue represents the video having the character.", "description": "Figure 7 presents a comparison of expert head activation between the standard Mixture-of-Heads (MoH) attention mechanism and the proposed ReLU Routing Mixture-of-Heads (ReMoH) mechanism.  The figure displays the activation of multiple attention heads (H<sub>i</sub>) across different layers of a neural network.  The activation levels are shown for two conditions: videos containing the target character (blue bars) and videos without the target character (orange bars). This visualization helps to demonstrate how the ReMoH mechanism focuses the attention of specific heads on relevant features related to the target character compared to the MoH approach.", "section": "3.3 ReLU Routing Mixture-of-Heads Attention"}, {"figure_path": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example_top.png", "caption": "Figure 1: Example of PVChat.", "description": "This figure demonstrates PVChat's ability to answer personalized questions about individuals in videos using only one example video of that person. The left side shows a single-person evaluation where the model correctly identifies the activity of person Nz. The right side shows a multi-person evaluation where the model identifies the activity of individuals Nz and Ab.  The results show that PVChat outperforms other ViLLMs in personalized video understanding.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example1.png", "caption": "Figure 2: Example of PVChat.", "description": "This figure shows an example of PVChat's ability to perform personalized video understanding.  It demonstrates the system's capability to answer questions about a specific individual's actions and behavior in a video clip, even when the video contains multiple individuals or complex actions.  The example highlights the model's ability to go beyond general video understanding and provide precise, identity-aware answers.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example2.png", "caption": "Figure 3: Example of PVChat.", "description": "This figure showcases PVChat's ability to answer personalized video questions using only one reference video.  The example shows three different queries related to a single video involving three characters (<Cl>, <Ja>, <Xo>).  PVChat successfully identifies and provides information about all three characters, including their potential gift preferences and an in-depth description of their actions in the scene, demonstrating subject-aware question answering capability. The figure highlights the model's superiority compared to other models that struggle with such personalized video understanding tasks.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example3.png", "caption": "Figure 4: Example of PVChat.", "description": "This figure showcases PVChat's ability to answer complex questions about individuals in videos, even with only one-shot learning. It presents a comparison between PVChat and other state-of-the-art video LLMs on a question-answering task. In the example shown, PVChat accurately identifies and describes the actions of individuals in the video, while the others struggle.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/extracted/6299133/supp_example4.png", "caption": "Figure 5: Example of PVChat.", "description": "This figure showcases PVChat's ability to answer personalized questions about individuals in videos. The example demonstrates correct identification and detailed description of actions and appearances even after only seeing a single reference video of each subject. It highlights the model's ability to handle both single-person and multi-person scenarios.", "section": "3. PVChat"}, {"figure_path": "https://arxiv.org/html/2503.17069/x6.png", "caption": "Figure 6: Prompt for Internvideo and GPT query", "description": "This figure shows the prompts used in the data augmentation pipeline.  First, a video is fed into InternVideo2, a model that extracts textual descriptions of the video content. The output text is then fed to ChatGPT-4, which refines the text and replaces all pronouns with the designated character identifier. The refined caption is used to help generate training data.", "section": "3.2 Data Collection"}]