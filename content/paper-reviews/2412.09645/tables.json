[{"content": "| Benchmark | Analysis | Customized\nQueries | | | | | |\n|---|---|---|---|---|---|---|---| \n| | **Supported** | | | | | | |\n| | **# Required** | | | | | | |\n| | **Open Evaluation** | | | | | | |\n| | **Dynamic** | | | | | | |\n| | **Open** | | | | | | |\n| | Tool-Use | | | | | | |\n| FID / FVD [Unterthiner et\u00a0al. (2018); Heusel et\u00a0al. (2017)] | \u2717 | \u2717 | T2I / T2V | 2,048 | \u2717 (Fixed-Form) | \u2717 | \u2717 |\n| T2I-CompBench [Huang et\u00a0al. (2023)] | \u2717 | \u2717 | T2I | 18,000 | \u2717 (Pre-Defined) | \u2717 | \u2717 |\n| VBench [Huang et\u00a0al. (2024a)] | \u2717 | \u2717 | T2V | 4,730 | \u2717 (Pre-Defined) | \u2717 | \u2717 |\n| **Evaluation Agent (Ours)** | \u2713 | \u2713 | T2I & T2V | 400 | \u2713 (Open-Ended) | \u2713 | \u2713 |", "caption": "Table 1: Comparison of the Evaluation Agent Framework with Traditional T2I and T2V Benchmarks. The Evaluation Agent framework supports customized user queries in natural language and works with both T2I and T2V models. Unlike traditional benchmarks, it dynamically updates the evaluation process using multiple tools, providing comprehensive and explainable results with detailed textual analysis.", "description": "Compares Evaluation Agent with traditional benchmarks (FID/FVD, T2I-CompBench, VBench) across several criteria: customized queries, supported models, required samples, open request support, dynamic evaluation, open tool use.  Evaluation Agent efficiently supports customized queries, both T2I and T2V models with dynamic evaluation using multiple tools, unlike others.", "section": "Introduction"}, {"content": "| Models | Subject | Background | Motion | Dynamic | Aesthetic | Imaging | Object | Class |\n|---|---|---|---|---|---|---|---|---| \n| Consistency | **Subject** | | | | | | | |\n| Consistency | **Background** | | | | | | | |\n| Consistency | **Motion** | | | | | | | |\n| Smoothness | **Dynamic** | | | | | | | |\n| Degree | **Aesthetic** | | | | | | | |\n| Quality | **Imaging** | | | | | | | |\n| Quality | **Object** | | | | | | | |\n| Class |  | | | | | | | |\n| Latte-1 <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et\u00a0al. (2024)</cite> | 50% / 80% | 0% / 30% | 40% / 70% | 30% / 70% | 60% / 100% | 70% / 100% | 40% / 50% | |\n| ModelScope <cite class=\"ltx_cite ltx_centering ltx_citemacro_cite\">Wang et\u00a0al. (2023)</cite> | 80% / 80% | 80% / 90% | 60% / 80% | 60% / 100% | 60% / 100% | 100% / 100% | 0% / 50% | |\n| VideoCrafter-0.9 <cite class=\"ltx_cite ltx_centering ltx_citemacro_cite\">He et\u00a0al. (2022)</cite> | 100% / 100% | 80% / 100% | 70% / 100% | 80% / 100% | 90% / 100% | 20% / 100% | 20% / 60% | |\n| VideoCrafter-2 <cite class=\"ltx_cite ltx_centering ltx_citemacro_cite\">Chen et\u00a0al. (2024a)</cite> | 10% / 100% | 60% / 100% | 30% / 90% | 30% / 80% | 80% / 100% | 50% / 100% | 70% / 100% | |", "caption": "Table 2: Evaluation Results Comparison with VBench\u00a0Huang et\u00a0al. (2024a). We evaluated 15 specific ability dimensions in VBench using our Evaluation Agent and compared its results against VBench in terms of conclusion accuracy. The numerical results show the percentages of the Evaluation Agent\u2019s correct predictions falling either within the exact range (left) or within an error margin of one range (right) across ten trials.", "description": "Compares the Evaluation Agent's performance against VBench across 15 ability dimensions, measuring the percentage of correct predictions within the exact specified range and within a one-range error margin.  The Evaluation Agent uses fewer samples and less time than traditional benchmarks, while still achieving comparable accuracy.", "section": "4.1.2 Results Analysis"}, {"content": "| Multiple Objects | Human | Spatial | Scene | Temporal | Overall | | | \n|---|---|---|---|---|---|---|---| \n| Objects | **Human** |  |  |  |  |  |  | \n| Action | **Color** | **Spatial** |  |  |  |  |  | \n| Relationship | **Scene** | **Appearance** |  |  |  |  |  | \n| Style | **Temporal** |  |  |  |  |  |  | \n| Style | **Overall** |  |  |  |  |  |  | \n| Consistency |  |  |  |  |  |  |  | \n| --- | --- | --- | --- | --- | --- | --- | --- | \n| 40% / 100% | 10% / 10% | 30% / 70% | 10% / 80% | 20% / 40% | 70% / 90% | 40% / 100% | 70% / 100% | \n| 50% / 100% | 10% / 40% | 0% / 20% | 10% / 30% | 20% / 100% | 90% / 100% | 50% / 90% | 20% / 100% | \n| 80% / 100% | 10% / 30% | 10% / 40% | 20% / 100% | 30% / 100% | 60% / 100% | 80% / 100% | 0% / 80% | \n| 20% / 60% | 10% / 90% | 90% / 100% | 0% / 70% | 0% / 10% | 80% / 100% | 80% / 100% | 60% / 100% |", "caption": "Table 3: Evaluation Results Comparison with T2I-CompBench\u00a0Huang et\u00a0al. (2023). We evaluated four ability dimensions in T2I-CompBench using our Evaluation Agent and compared its results with those of T2I-CompBench in terms of conclusion accuracy. The numerical results show the percentages of the Evaluation Agent\u2019s correct predictions falling either within the exact range (left) or within an error margin of one range (right) across ten trials.", "description": "This table compares the Evaluation Agent's performance against the T2I-CompBench benchmark across four ability dimensions: Color Binding, Shape Binding, Texture Binding, and Non-Spatial Relationships.  It presents the percentage of times the Evaluation Agent's assessment agreed exactly with T2I-CompBench's conclusion (left number) and the percentage of times it was within one level of agreement (right number).  These percentages are averaged over ten trials for each model and dimension, showcasing the Evaluation Agent's ability to replicate established benchmark results with high fidelity.", "section": "4.1 Experiments on Existing Benchmarks"}, {"content": "| Models | Color | Shape | Texture | Non-Spatial |\n|---|---|---|---|---| \n| SD1.4 Rombach et\u00a0al. (2022) | 50% / 100% | 100% / 100% | 0% / 100% | 50% / 100% |\n| SD2.1 Rombach et\u00a0al. (2022) | 100% / 100% | 60% / 100% | 80% / 100% | 60% / 100% |\n| SDXL Podell et\u00a0al. (2023) | 100% / 100% | 20% / 100% | 80% / 100% | 60% / 100% |\n| SD3.0 Esser et\u00a0al. (2024) | 20% / 90% | 0% / 90% | 0% / 70% | 80% / 90% |", "caption": "Table 4: Time Cost Comparison across Models for VBench Dimensions. This table compares the evaluation time of four different models using the original VBench pipelines versus the Evaluation Agent. The Evaluation Agent significantly reduces the overall evaluation time.", "description": "This table compares the time and the number of samples required for evaluating four different video generation models (Latte-1, ModelScope, VideoCrafter-0.9, VideoCrafter-2) across fifteen dimensions defined in VBench.  It presents results for both the original VBench evaluation pipeline and the proposed Evaluation Agent framework. The results demonstrate a significant reduction in evaluation time (over 90%) when using the Evaluation Agent while maintaining comparable performance.", "section": "4.1 Experiments on Existing Benchmarks"}, {"content": "| Models | VBench (Total Cost) \u2193 | VBench (Avg. Cost per Dimension) \u2193 | Evaluation Agent (Ours) \u2193 |\n|---|---|---|---| \n| Latte-1 Ma et\u00a0al. (2024) | 2557 min, 4355 samples | 170 min, 290 samples | 15 min, 25 samples |\n| ModelScope Wang et\u00a0al. (2023) | 1160 min, 4355 samples | 77 min, 290 samples | 6 min, 23 samples |\n| VideoCrafter-0.9 He et\u00a0al. (2022) | 1459 min, 4355 samples | 97 min, 290 samples | 9 min, 24 samples |\n| VideoCrafter-2 Chen et\u00a0al. (2024a) | 4261 min, 4355 samples | 284 min, 290 samples | 24 min, 23 samples |", "caption": "Table 5: Time Cost Comparison across Models for T2I-CompBench Dimensions. This table compares the evaluation costs for assessing four models across T2I-CompBench dimensions using both the original T2I-CompBench pipelines and our Evaluation Agent. The Evaluation Agent achieves a substantial reduction in evaluation time compared to the traditional pipelines.", "description": "This table compares the time and number of samples required to evaluate four different Stable Diffusion models on four compositional generation dimensions using the traditional T2I-CompBench evaluation method and the proposed Evaluation Agent. The results demonstrate that the Evaluation Agent significantly reduces the evaluation time while using substantially fewer samples.", "section": "4.1 Experiments on Existing Benchmarks"}, {"content": "| Models | T2I-Comp (Total Cost) \u2193 | T2I-Comp (Avg. Cost per Dimension) \u2193 | Evaluation Agent (Ours) \u2193 |\n|---|---|---|---|\n| SD1.4 Rombach et\u00a0al. (2022) | 563 min, 12000 samples | 141 min, 3000 samples | 5 min, 26 samples |\n| SD2.1 Rombach et\u00a0al. (2022) | 782 min, 12000 samples | 196 min, 3000 samples | 6 min, 26 samples |\n| SDXL Podell et\u00a0al. (2023) | 1543 min, 12000 samples | 386 min, 3000 samples | 8 min, 26 samples |\n| SD3.0 Esser et\u00a0al. (2024) | 1410 min, 12000 samples | 353 min, 3000 samples | 7 min, 25 samples |", "caption": "Table 6: Validation on VBench Percentage Dimensions. The numerical results show the percentages of the Evaluation Agent\u2019s correct predictions falling either within the exact range (left) or within an error margin of one range (right) across ten trials.", "description": "This table presents a comparison of the Evaluation Agent's performance against VBench across various percentage-based dimensions, including Human Action, Scene, Color, and Object Class.  The results are presented as percentages, reflecting the accuracy of the Evaluation Agent's predictions over ten trials.  For each dimension, two percentage values are provided: the percentage of predictions falling within the *exact* range defined by VBench, and the percentage of predictions falling within one range *above or below* the exact range. This allows for an assessment of both the precision and general accuracy of the Evaluation Agent.", "section": "4.1.2 Results Analysis"}, {"content": "| Models | Human | Scene | Color | Object Class | \n|---|---|---|---|---| \n| --- | --- | --- | --- | --- |\n| Latte-1 (default) [Ma et\u00a0al. (2024)](https://arxiv.org/html/2412.09645v2#bib.bib21) | 10% / 10% | 20% / 40% | 30% / 70% | 40% / 50% |\n| Latte-1 (30 prompts) [Ma et\u00a0al. (2024)](https://arxiv.org/html/2412.09645v2#bib.bib21) | 10% / 60% | 30% / 50% | 30% / 70% | 40% / 80% |\n| ModelScope (default) [Wang et\u00a0al. (2023)](https://arxiv.org/html/2412.09645v2#bib.bib33) | 10% / 40% | 20% / 100% | 0% / 20% | 0% / 50% |\n| ModelScope (30 prompts) [Wang et\u00a0al. (2023)](https://arxiv.org/html/2412.09645v2#bib.bib33) | 30% / 50% | 30% / 100% | 10% / 30% | 10% / 60% |", "caption": "Table 7: Evaluation Results Comparison with VBench\u00a0Huang et\u00a0al. (2024a) using Claude as Base Model. We adhere to the same experimental settings and parameters as in the main experiments, but we replace the planning and reasoning agents\u2019 backbones with claude-3-5-sonnet-20241022 as the base model.", "description": "This table compares the evaluation results of four different text-to-video generation models on 15 dimensions of the VBench benchmark, using the Claude language model as the backbone for the evaluation agent.  The original experimental settings and parameters were maintained, but the GPT backbone used in the main experiments was replaced with Claude.  The results are presented as percentages, reflecting the proportion of evaluations where the agent's prediction fell within the exact specified range or within one range of the correct value across ten trials. This provides insight into the accuracy and consistency of the Claude-based evaluation agent compared to the original GPT-based agent when assessing video generation models using VBench.", "section": "4.1 Experiments on Existing Benchmarks"}, {"content": "| Models | Subject | Background | Motion | Dynamic | Aesthetic | Imaging | Object | Class |\n|---|---|---|---|---|---|---|---|---| \n| Consistency |  |  |  |  |  |  |  |  |\n| Consistency | **Background** |  |  |  |  |  |  |  |\n| Consistency | **Motion** |  |  |  |  |  |  |  |\n| Smoothness | **Dynamic** |  |  |  |  |  |  |  |\n| Degree | **Aesthetic** |  |  |  |  |  |  |  |\n| Quality | **Imaging** |  |  |  |  |  |  |  |\n| Quality | **Object** |  |  |  |  |  |  |  |\n| Class |  |  |  |  |  |  |  |  |\n| Latte-1 <cite class=\"ltx_cite ltx_citemacro_cite\">Ma et\u00a0al. (2024)</cite> | 0% / 10% | 0% / 10% | 0% / 30% | 0% / 40% | 100% / 100% | 90% / 100% | 0% / 30% |  |\n| ModelScope <cite class=\"ltx_cite ltx_centering ltx_citemacro_cite\">Wang et\u00a0al. (2023)</cite> | 0% / 10% | 30% / 40% | 10% / 80% | 30% / 100% | 40% / 100% | 60% / 100% | 20% / 50% |  |\n| VideoCrafter-0.9 <cite class=\"ltx_cite ltx_centering ltx_citemacro_cite\">He et\u00a0al. (2022)</cite> | 40% / 100% | 30% / 80% | 40% / 90% | 90% / 100% | 90% / 100% | 20% / 100% | 10% / 40% |  |\n| VideoCrafter-2 <cite class=\"ltx_cite ltx_centering ltx_citemacro_cite\">Chen et\u00a0al. (2024a)</cite> | 50% / 100% | 0% / 100% | 0% / 10% | 60% / 100% | 100% / 100% | 80% / 100% | 60% / 90% |  |", "caption": "Table 8: Evaluation Results Comparison with T2I-CompBench\u00a0Huang et\u00a0al. (2023) using Claude as Base Model. We follow the same experimental setting and the parameters in the main experiments but changing the planning and reasoning agent\u2019s backbones with claude-3-5-sonnet-20241022 as the base model.", "description": "This table presents a comparison of evaluation results between the Evaluation Agent framework and the T2I-CompBench, a comprehensive benchmark for text-to-image generation models.  The Evaluation Agent leverages large language models (LLMs) to guide its evaluation process. In this specific experiment, the LLM backbone for the Evaluation Agent was replaced with Claude-3-5-sonnet-20241022.  The table focuses on four key dimensions of image generation: Color Binding, Shape Binding, Texture Binding, and Non-Spatial Relationships. For each model and dimension, the table reports the accuracy of the Evaluation Agent's assessment compared to the ground truth provided by T2I-CompBench. The accuracy is represented as two percentages. The first percentage signifies how often the Evaluation Agent's prediction falls within the *exact* range defined by the benchmark. The second percentage indicates how often the prediction falls within a margin of *one range* of the benchmark's classification. This two-tiered accuracy reporting provides a more nuanced understanding of the Evaluation Agent's performance. Results are averaged over ten trials.", "section": "4 Experiments\n4.1 Experiments on Existing Benchmarks\n4.1.2 Results Analysis"}, {"content": "| Multiple Objects | Human | Spatial | Scene | Temporal | Overall | Consistency |           |\n|---|---|---|---|---|---|---|---| \n| Objects | **Human** |  |  |  |  |  |  |\n| Action | **Color** | **Spatial** |  |  |  |  |  |\n| Relationship | **Scene** | **Appearance** |  |  |  |  |  |\n| Style | **Temporal** |  |  |  |  |  |  |\n| Style | **Overall** |  |  |  |  |  |  |\n| Consistency |  |  |  |  |  |  |  |\n| 10% / 60% | 60% / 70% | 10% / 60% | 30% / 80% | 0% / 40% | 30% / 100% | 80% / 100% | 80% / 100% |\n| 90% / 100% | 40% / 90% | 10% / 20% | 50% / 80% | 40% / 100% | 70% / 100% | 90% / 100% | 40% / 100% |\n| 0% / 40% | 20% / 40% | 10% / 40% | 40% / 100% | 10% / 80% | 100% / 100% | 90% / 100% | 60% / 100% |\n| 50% / 100% | 50% / 80% | 60% / 90% | 50% / 100% | 0% / 50% | 10% / 100% | 80% / 100% | 80% / 100% |", "caption": "Table 9: Time Cost Comparison across Models for VBench\u00a0Huang et\u00a0al. (2024a) Dimensions using Claude as Base Model. This table compares the evaluation time of four different models using the original VBench pipelines versus the Evaluation Agent. The Evaluation Agent significantly reduces the overall evaluation time.", "description": "This table compares the time it takes to evaluate four text-to-video generation models using both the original VBench evaluation suite and the new Evaluation Agent framework. The results show that the Evaluation Agent significantly reduces the evaluation time.", "section": "4.1 Experiments on Existing Benchmarks"}, {"content": "| Models | Color | Shape | Texture | Non-Spatial |\n|---|---|---|---|---| \n| SD1.4 Rombach et\u00a0al. (2022) | 80% / 100% | 70% / 100% | 80% / 100% | 70% / 100% |\n| SD2.1 Rombach et\u00a0al. (2022) | 80% / 100% | 30% / 100% | 60% / 100% | 70% / 100% |\n| SDXL Podell et\u00a0al. (2023) | 90% / 100% | 60% / 100% | 70% / 100% | 30% / 100% |\n| SD3.0 Esser et\u00a0al. (2024) | 10% / 100% | 20% / 100% | 30% / 100% | 20% / 100% |", "caption": "Table 10: Time Cost Comparison across Models for T2I-CompBench\u00a0Huang et\u00a0al. (2023) Dimensions using Claude as Base Model. This table compares the evaluation costs for assessing four models across T2I-CompBench dimensions using both the original T2I-CompBench pipelines and our Evaluation Agent. The Evaluation Agent achieves a substantial reduction in evaluation time compared to the traditional pipelines.", "description": "This table presents a comparison of evaluation time and required number of samples between the original T2I-CompBench evaluation process and the proposed Evaluation Agent, using Claude as the base model, when applied to four different Stable Diffusion models (SD1.4, SD2.1, SDXL, SD3.0).  The table demonstrates that the Evaluation Agent significantly reduces evaluation time while requiring substantially fewer samples across all assessed models and T2I-CompBench dimensions (Color Binding, Shape Binding, Texture Binding, and Non-Spatial Relationships).", "section": "4.1 Experiments on Existing Benchmarks"}]