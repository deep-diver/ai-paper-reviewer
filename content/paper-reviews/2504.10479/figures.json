[{"figure_path": "https://arxiv.org/html/2504.10479/x1.png", "caption": "Figure 1: Multimodal performance of the InternVL series and other advanced MLLMs.\nThe InternVL series has consistently exhibited progressive enhancements in multimodal capabilities. The newly released InternVL3 significantly outperforms existing open-source MLLMs. Moreover, even in comparison with state-of-the-art closed-source commercial models, InternVL3 continues to demonstrate highly competitive performance.", "description": "This figure compares the performance of InternVL3 with other state-of-the-art multimodal large language models (MLLMs), both open-source and closed-source.  It showcases the consistent improvement in multimodal capabilities across the InternVL series (InternVL2.5 and InternVL3), highlighting InternVL3's superior performance compared to open-source alternatives.  Furthermore, the figure demonstrates that InternVL3's performance is highly competitive, even against leading commercial models like ChatGPT-40, Claude 3.5 Sonnet, and Gemini 2.5 Pro.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.10479/x2.png", "caption": "Figure 2: \nPerformance of various MLLMs on the OpenCompass multimodal academic leaderboard.\nThe enhanced InternVL series\u2014InternVL3\u2014demonstrates outstanding multimodal capabilities, significantly outperforming both the Qwen2.5-VL series and closed-source models such as Step-1o, GLM-4v-Plus, and GPT-4o. Remarkably, InternVL3-78B also remains highly competitive with the state-of-the-art Gemini-2.5-Pro.", "description": "Figure 2 presents a comprehensive comparison of various multimodal large language models (MLLMs) on the OpenCompass academic leaderboard, which assesses multimodal capabilities.  InternVL3 significantly outperforms other open-source models (including the Qwen2.5-VL series) and achieves highly competitive results against several leading closed-source commercial MLLMs (Step-10, GLM-4v-Plus, GPT-40, and Gemini-2.5-Pro). The chart visually demonstrates InternVL3's superior performance across different model sizes, particularly InternVL3-78B, highlighting its strong multimodal capabilities.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.10479/x3.png", "caption": "Figure 3: Performance comparison on multimodal benchmarks under different training strategies. Native multimodal pre-training endows MLLMs with strong multimodal capabilities, even without further post-training.", "description": "This figure compares the performance of models trained with different strategies on several multimodal benchmarks.  Specifically, it contrasts the performance of models trained with and without native multimodal pre-training.  The results demonstrate that native multimodal pre-training significantly improves performance on these benchmarks, even before any additional post-training such as supervised fine-tuning. This suggests that jointly learning multimodal and linguistic skills during the initial training phase offers a significant advantage over traditional approaches of first training a text-only model and then adapting it to multimodal data.", "section": "2.3 Post-Training"}]