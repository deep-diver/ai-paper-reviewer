{"references": [{"fullname_first_author": "S. Zhang", "paper_title": "Opt: Open pre-trained transformer language models", "publication_date": "2022-05-01", "reason": "This paper introduces OPT, a large language model that is frequently referenced and compared to in the field, making it highly relevant to the current research."}, {"fullname_first_author": "H. Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "Llama 2 is a significant advancement in open-source large language models, and its architecture and performance are directly relevant to the methods proposed in the paper."}, {"fullname_first_author": "D. Lepikhin", "paper_title": "Gshard: Scaling giant models with conditional computation and automatic sharding", "publication_date": "2020-06-16", "reason": "Gshard is a foundational paper in the field of efficient large model training, introducing techniques that are directly relevant to the Mixture-of-Experts (MoE) approach explored in the current paper."}, {"fullname_first_author": "N. Du", "paper_title": "Glam: Efficient scaling of language models with mixture-of-experts", "publication_date": "2022-00-00", "reason": "GLAM is a key prior work on Mixture-of-Experts (MoE) models for language, whose efficiency is directly contrasted with the approach proposed in the current paper."}, {"fullname_first_author": "W. Fedus", "paper_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "publication_date": "2022-00-00", "reason": "Switch Transformers propose an alternative MoE-style approach that is compared and contrasted against the current method."}]}