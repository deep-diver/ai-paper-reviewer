[{"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/flux_pad.jpeg", "caption": "Figure 1: Images generated with FLUX from different segments of the input prompt.\nDescription of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only the prompt tokens and clean padding tokens, (3) An image generated using only the prompt-contextual pads encoded with the prompt, while the prompt tokens were replaced with clean pad tokens.", "description": "This figure displays images generated by the FLUX model using different parts of the input prompt. The leftmost column shows an image generated with the complete prompt, including both prompt tokens and padding tokens. The middle column shows an image generated using only the prompt tokens and 'clean' padding tokens (padding tokens without contextual information).  The rightmost column illustrates an image produced using only the padding tokens that contain contextual information from the original prompt, with the prompt tokens replaced by 'clean' padding tokens.  This demonstrates how different parts of the input, including the usually overlooked padding tokens, influence the final image generation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/pads_llama_sd3.jpeg", "caption": "Figure 2: ITE: Interpreting information within pad tokens in the text encoder. We first encode the full prompt and an clean pads separately. Next, we keep the tokens we want to interpret and replace all other tokens with clean pad tokens. We then generate an image conditioned on this mixed representation. In the example shown here, we interpret the pad tokens in LLaMA-UNet, revealing semantic information embedded within the pad tokens.", "description": "This figure illustrates the Intervention in Text Encoder (ITE) method used to analyze the impact of padding tokens on image generation.  The process begins by encoding both a full prompt (including padding tokens) and a prompt consisting only of clean padding tokens.  Then, the method selectively replaces parts of the full prompt's encoding with the clean padding tokens' encoding.  This allows researchers to isolate the contribution of specific tokens (prompt tokens vs. padding tokens) to the final image generated. The example shown focuses on the LLaMA-UNet model, demonstrating that padding tokens in this specific model carry semantic information, affecting the final image generation.", "section": "2 Analysis of Padding in Text Encoding"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/main_results.png", "caption": "Figure 3: Images generated from different segments of the input prompt using\u00a0ITE. Description of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only the prompt tokens and clean padding tokens, (3) An image generated using only the prompt-contextual pads encoded with the prompt, while the prompt tokens were replaced with clean pad tokens.", "description": "This figure demonstrates the results of an experiment using the Intervention in Text Encoder (ITE) method. Three images are shown for each of the three columns. The leftmost column shows an image generated using the full prompt, including both prompt tokens and padding tokens. The middle column shows an image generated using only the prompt tokens and 'clean' padding tokens (padding tokens without contextual information from the prompt). The rightmost column shows an image generated using only the prompt-contextual padding tokens (padding tokens with contextual information from the prompt), with the prompt tokens replaced by 'clean' padding tokens. By comparing these images, the impact of padding tokens on the image generation process can be analyzed.", "section": "2 Analysis of Padding in Text Encoding"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/lora_scaled.jpeg", "caption": "Figure 4: \nAverage CLIP score over 5,000 images generated from the different representations: full prompt, only prompt, prompt-contextual pads and clean pads. LDM and LLaMA-UNet are the only models achieving high CLIP scores for images generated from padding tokens, indicating their use during text encoding. See Table 4 in the Appendix for standard deviations.", "description": "This figure displays the average CLIP scores achieved across 5000 images generated using different text representations.  The representations tested are: the complete prompt (full prompt), only the prompt tokens (only prompt), the prompt-contextual padding tokens (prompt-contextual pads), and clean padding tokens (clean pads).  The results show that LDM and LLaMA-UNet are the only models that yield high CLIP scores when generating images from the prompt-contextual pads, indicating these models use the padding tokens during text encoding.  Standard deviations for these scores are available in Table 4 of the appendix.", "section": "2.3 Results"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/ca_strength.jpeg", "caption": "Figure 5: Images generated from Lavi-bridge with LoRa loaded with scaling factor \u03b1\ud835\udefc\\alphaitalic_\u03b1 (y-axis). We analyze pad token segments: the first column shows the full image, and the next columns show three consecutive 20% of the pads. As \u03b1\ud835\udefc\\alphaitalic_\u03b1 decreases, fewer pad tokens are used.", "description": "This figure displays the results of an experiment using the Lavi-Bridge model.  The model uses a LoRA (Low-Rank Adaptation) technique with a scaling factor (\u03b1) applied.  The experiment shows how the use of padding tokens changes as this factor decreases. The figure presents multiple images generated from the same prompt. Each row represents a different value of \u03b1.  The first column of each row shows the image generated using all tokens, including padding tokens, at a specific \u03b1 value. The following three columns show images generated using only a portion of the padding tokens. Each subsequent column excludes a further 20% of the padding tokens, demonstrating how reducing the \u03b1 value results in a reduction in the number of padding tokens used by the model during image generation.  The results are designed to show how the model incorporates padding tokens based on the tuning of the LoRA scaling factor.", "section": "Analysis of Padding in the Diffusion Process"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/token_attention_maps.jpeg", "caption": "Figure 6:  Attention histogram for Stable Diffusion XL and FLUX* for each token reveals that while both models exclude semantic information from padding tokens, FLUX utilizes these tokens, whereas Stable Diffusion does not. *In FLUX, we have removed the long middle part with low attention in order to improve visualization.", "description": "This figure displays attention histograms for Stable Diffusion XL and FLUX, visualizing how much each token in the input sequence contributes to the attention mechanism during image generation.  The histograms show that neither model incorporates semantic information from padding tokens into the main attention process. However, FLUX shows some attention on padding tokens, unlike Stable Diffusion XL, which largely ignores them. The middle section of the FLUX histogram has been removed for improved visualization clarity.", "section": "3 Analysis of Padding in the Diffusion Process"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/diffusion_causal.jpeg", "caption": "Figure 7: Attention maps for FLUX diffusion show strong alignment between prompt tokens and semantically relevant image tokens. These maps also reveal high attention for padding tokens with the main objects in the image.", "description": "The figure displays attention maps generated during the diffusion process of the FLUX model.  The maps visualize the relationships between text tokens (including prompt and padding tokens) and image patches.  Strong attention is observed between the prompt tokens and semantically meaningful parts of the generated image. Notably, high attention is also detected between padding tokens and the main objects of the image, suggesting an unexpected role for these usually ignored tokens in the generation process.", "section": "3 Analysis of Padding in the Diffusion Process"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/sid_example.jpeg", "caption": "Figure 8: IDP: Interpreting information within pad tokens in the diffusion model. We perform a diffusion of two prompts simultaneously: the full prompt and an clean pads. During the diffusion, we keep the tokens we want to interpret (here: the prompt-contextual padding tokens) and replace all other tokens with clean pad tokens. We perform this intervention before each attention block in the diffusion model, through all diffusion steps. We then generate an image conditioned on this mixed representation. In the example shown here, we interpret the pad tokens in FLUX, revealing semantic information embedded within the pad tokens during diffusion.", "description": "This figure illustrates the Intervention in the Diffusion Process (IDP) method used to analyze the role of padding tokens in the diffusion process of text-to-image (T2I) models.  Two prompts are processed simultaneously: one with the full prompt (including padding tokens) and one with only clean padding tokens. During the diffusion process, at each attention block, specific tokens (prompt or padding tokens) are replaced with clean padding tokens, creating a modified representation. This method helps to isolate and evaluate the impact of those specific tokens on the generated image by comparing the results to images generated with the full and only clean padding prompts. The example shown focuses on the FLUX model, highlighting how padding tokens can retain information throughout the diffusion process and affect the final image generation, demonstrating that they are not simply ignored as in language models. ", "section": "3 Analysis of Padding in the Diffusion Process"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/causal_diffusion_examples.jpeg", "caption": "Figure 9: \nImages generated with FLUX from different prompt segments show distinct alignments: prompt tokens produce semantically accurate images, while the visual nuance like \u2019cozy\u2019 emerges only from the prompt-contextual pad tokens.", "description": "This figure shows images generated by the FLUX model using different parts of the input prompt. The first column uses the full prompt (prompt tokens and padding tokens). The second column uses only the prompt tokens, replacing the padding tokens with \"clean\" padding tokens (padding tokens that don't contain information from the prompt).  The third column uses only the prompt-contextual padding tokens, replacing the prompt tokens with \"clean\" padding tokens. The results demonstrate that while the prompt tokens generate images that are semantically correct, subtle visual details, like the \"cozy\" atmosphere in the example image, are only present when the prompt-contextual padding tokens are included in the generation process. This highlights how padding tokens can contribute to the overall image quality and style, despite their seemingly insignificant nature.", "section": "2 Analysis of Padding in Text Encoding"}, {"figure_path": "https://arxiv.org/html/2501.06751/extracted/6123973/figures/max_len.jpeg", "caption": "Figure 10: Additional examples of images generated from different segments of the input prompt using\u00a0IDP. Description of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only the prompt tokens and clean padding tokens that were not encoded with the prompt, (3) An image generated using only the padding tokens encoded with the prompt, while the prompt tokens were replaced with clean pad tokens. See Figure\u00a08 for further technical details.", "description": "This figure displays the results of causal mediation analysis applied to the diffusion process of text-to-image (T2I) models.  Three different image generation scenarios are shown for multiple prompts, each using a different combination of prompt tokens and padding tokens. The first column shows images generated using the complete prompt, including both prompt and padding tokens. The second column shows images where the actual prompt tokens have been replaced by 'clean' padding tokens (padding tokens without contextual information from the prompt). The third column shows images generated using only the prompt-contextualized padding tokens, with the prompt tokens replaced by the clean ones.  This design allows for the isolation of the impact of padding tokens on the image generation process. The differences in the generated images across these three conditions illustrate how the model uses or ignores information embedded within the padding tokens during image generation.", "section": "3 Analysis of Padding in the Diffusion Process"}]