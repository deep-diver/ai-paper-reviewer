[{"figure_path": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/FLAV.png", "caption": "Figure 1: An overview of our RFLAV model architecture.", "description": "This figure presents a detailed architectural overview of the RFLAV model, illustrating the processing pathways for both audio and video streams.  The model incorporates a rolling flow matching mechanism. It begins with separate encoding of audio and video data, which avoids early fusion, allowing for efficient intra-modality interactions (like self-attention) before cross-modality processing.  The figure shows the flow of information through various modules, including VAE encoder/decoder, Mel encoder, HiFi-GAN, and RFLAV blocks.  This architecture is explicitly designed to generate videos of arbitrary length, unlike models restricted by fixed-size encoders.  It emphasizes the three proposed alternatives for cross-modality interaction within the RFLAV block.", "section": "3.1 Architecture"}, {"figure_path": "https://arxiv.org/html/2503.08307/x1.png", "caption": "Figure 2: Temporal alignment between video frames and mel spectrogram segments. Each video frame corresponds to a fixed-size section (F/T) of the mel spectrogram, allowing for a 1:1 mapping.", "description": "This figure illustrates how the model aligns audio and video data.  The model processes video data frame by frame and audio data as a mel spectrogram. Importantly, it maintains a one-to-one correspondence between a single video frame and a fixed-length segment of the mel spectrogram, ensuring precise synchronization between the two modalities. This one-to-one mapping is crucial for generating long, coherent AV sequences with consistent audio-visual alignment.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08307/x2.png", "caption": "(a)", "description": "This figure shows three different designs for cross-modality interaction in the RFLAV model.  (a) depicts a cross-modal interaction using self-attention. Audio embeddings are reshaped and concatenated with video embeddings before being processed by causally masked self-attention. The output is then split back into audio and video embeddings and added to their respective branches. (b) shows a lightweight cross-modality interaction mechanism using temporal average modulation. Temporal averages of both video and audio features are computed and used for cross-modal fusion. (c) combines elements of (b) and enhances it by incorporating timestep embeddings and optional class conditioning embeddings.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08307/x3.png", "caption": "(b)", "description": "This figure shows one of the three cross-modality interaction module designs explored in the paper.  It illustrates a lightweight approach to combining audio and video information. Temporal averages of video and audio features are computed, then combined, before influencing the feed-forward layers in each modality branch. This design avoids the computationally expensive self-attention mechanisms used in other designs, resulting in a more efficient model.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08307/x4.png", "caption": "(c)", "description": "This figure shows the architecture of the lightweight cross-modality interaction module used in the RFLAV model.  It contrasts with other more complex approaches involving self-attention mechanisms.  The diagram details the flow of audio and video data through the module, highlighting the temporal averaging process and its use in modulating both audio and video features before they reach the feed-forward layer. This lightweight design avoids attention mechanisms, resulting in a more computationally efficient model.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/rolling.png", "caption": "Figure 3: a) Cross-modal interaction via self-attention, where \n\n\u2296symmetric-difference\\ominus\u2296\n and \u2296symmetric-difference\\ominus\u2296 mean concatenation and split. b) Lightweight cross-modality interaction mechanism with temporal average modulation. c) Our final proposed RFLAV block, an enhanced lightweight mechanism incorporating timestep embedding t\ud835\udc61titalic_t and optional class conditioning embedding c\ud835\udc50citalic_c.", "description": "Figure 3 details three different cross-modality interaction modules explored in the RFLAV model. (a) shows a self-attention mechanism that processes concatenated audio and video embeddings before splitting them again. This method is computationally expensive. (b) is a lightweight module that uses temporal average modulation, combining aggregated audio and video features for cross-modality interaction, offering improved efficiency. (c) builds upon (b), incorporating timestep embeddings (t) and optional class conditioning embeddings (c) for enhanced performance and flexibility. This last version is the one adopted in the final RFLAV model architecture.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/prerolling.png", "caption": "(a)", "description": "This figure shows three different designs for a cross-modality interaction module within the RFLAV architecture.  Each design explores different methods for combining audio and video features to improve multimodal synchronization and coherence.  (a) illustrates a self-attention-based approach, where audio and video embeddings are concatenated and processed together. (b) shows a simpler design using temporal averages of audio and video features for modulation. (c) enhances (b) by including timestep embedding for better control over the diffusion process.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08307/extracted/6270742/long_video_metrics_2.png", "caption": "(b)", "description": "This figure shows one of the three proposed cross-modality interaction modules within the RFLAV model.  It depicts a lightweight approach to combining audio and video features.  Instead of using computationally expensive self-attention mechanisms, it utilizes temporal averaging to create a compact representation of both audio and video data, allowing for efficient fusion before the next processing stages.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/long-videos.png", "caption": "Figure 4: a) Rolling phase: at each step, a new clean frame is produced (highlighted in red) and subsequently removed from the window. Then, a new noisy frame, (highlighted in blue), is appended to the end of the window. b) Pre-rolling phase: the frames are gradually denoised starting from a full noise configuration. The pre-rolling phase goes on for N\ud835\udc41Nitalic_N steps, until the window is ready for the rolling phase.", "description": "Figure 4 illustrates the rolling diffusion process used in the RFLAV model for video generation.  (a) shows the 'rolling phase', where a sliding window processes video frames. At each step, the model denoises the oldest frame in the window (red), and then removes it. A new, fully noisy frame (blue) is then added to the end of the window to keep the window size constant.  This process continues, enabling the generation of arbitrarily long video sequences. (b) shows the 'pre-rolling phase', which initializes the process. Here, the window starts with all frames completely noisy, gradually denoising until it is ready to transition into the rolling phase.", "section": "3.3 Rolling Flow Matching"}, {"figure_path": "https://arxiv.org/html/2503.08307/extracted/6270742/imgs/comparison.png", "caption": "Figure 5: AV metrics and feature drift calculated on long (i.e., 240 frames) generated videos using a sliding window of 16 frames.", "description": "This figure visualizes the impact of video length on the quality of generated audio-visual data.  It shows how the Fr\u00e9chet Video Distance (FVD), Kernel Video Distance (KVD), and Fr\u00e9chet Audio Distance (FAD) metrics, along with a measure of feature drift, change across a long video (240 frames) generated by the model. The metrics are calculated using a sliding window of 16 frames, providing insights into how well the model maintains consistent quality and alignment over longer sequences. A noticeable jump in the metrics is observed initially, followed by stabilization, indicating a transition phase in the generation process before achieving consistent quality.", "section": "Results on Long Video Generation"}]