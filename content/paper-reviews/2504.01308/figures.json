[{"figure_path": "https://arxiv.org/html/2504.01308/x1.png", "caption": "Figure 1: Visualization of Vision-Language Model\u2019s outputs under different noise conditions. The upper part shows the original image with green text indicating correct responses generated without noise, while the lower part adds slight Gaussian noise, with red text highlighting errors introduced under noisy conditions. Please zoom in for better visualization.", "description": "This figure demonstrates how the addition of Gaussian noise affects the outputs of a Vision-Language Model (VLM).  The top half shows an original image and the VLM's correct response to a query about that image. The bottom half shows the same image with added Gaussian noise. The VLM's response to the noisy image is incorrect, highlighted in red, illustrating the model's vulnerability to noise.  The difference in responses highlights a key finding of the paper: that the lack of noise-augmented training in existing VLMs creates a critical security gap.  This security gap makes these models susceptible even to simple perturbations like Gaussian noise.", "section": "Vulnerability of VLMs to Gaussian Noise Perturbations"}, {"figure_path": "https://arxiv.org/html/2504.01308/x2.png", "caption": "(a) Performance evaluation on MM-Vet benchmark.", "description": "This figure shows the performance comparison of different vision-language models on the MM-Vet benchmark. The MM-Vet benchmark is a comprehensive evaluation suite that assesses various vision-language capabilities. The figure likely displays performance scores (e.g., accuracy, F1 score) achieved by each model across different tasks within the benchmark, potentially broken down by image conditions (e.g., clean images versus images with added Gaussian noise). By comparing model performance under different conditions, the figure helps to demonstrate the impact of Gaussian noise and evaluate the robustness of the vision-language models.", "section": "Vulnerability of VLMs to Gaussian Noise Perturbations"}, {"figure_path": "https://arxiv.org/html/2504.01308/x3.png", "caption": "(b) Attack success rate on RealToxicityPrompts benchmark.", "description": "This figure shows the attack success rate, which measures the percentage of responses classified as toxic, on the RealToxicityPrompts benchmark.  The benchmark contains prompts designed to elicit toxic responses, and the attack success rate indicates the model's vulnerability to generating toxic outputs when presented with a noisy image prompt. Lower attack success rate indicates better safety alignment of the model.", "section": "Vulnerability of VLMs to Gaussian Noise Perturbations"}, {"figure_path": "https://arxiv.org/html/2504.01308/x4.png", "caption": "Figure 2: Comparison of various models\u2019 performance and robustness: (a) helpfulness on the MM-Vet benchmark with clean and noisy image prompts, and (b) attack success rates on the RealToxicityPrompts benchmark using clean and noisy image prompts.", "description": "Figure 2 presents a comparative analysis of three vision-language models (VLMs): MiniGPT-4, LLaVA-v1.5, and InternVL2, evaluating their performance and robustness under both clean and noisy image inputs.  Subfigure (a) shows helpfulness scores measured using the MM-Vet benchmark, illustrating how model performance degrades in the presence of Gaussian noise.  Subfigure (b) displays attack success rates from the RealToxicityPrompts benchmark, highlighting the increased vulnerability of the models to adversarial attacks when images are corrupted with Gaussian noise. This figure demonstrates that while these models are effective under normal conditions, they exhibit vulnerabilities and reduced robustness when presented with noisy or corrupted image data.", "section": "Vulnerability of VLMs to Gaussian Noise Perturbations"}, {"figure_path": "https://arxiv.org/html/2504.01308/x5.png", "caption": "(a) Example of general instruction data.", "description": "This figure shows an example from the General Instruction Data subset of the Robust-VLGuard dataset.  It displays a question, an image relevant to the question, and a detailed, comprehensive answer generated by GPT-4V. The purpose of this data is to ensure the fine-tuned VLMs maintain helpfulness across a wide range of tasks.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x6.png", "caption": "(b) Example of image-text aligned safety data.", "description": "This figure shows an example from the Robust-VLGuard dataset, specifically demonstrating image-text aligned safety data.  It displays an image of a person kneading bread dough, paired with a question about the next step in the process before dividing the dough.  The provided safe response offers detailed instructions, indicating a helpful and safe model output.  This is in contrast to unsafe examples where images might be paired with prompts that elicit harmful or inappropriate responses.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x7.png", "caption": "(c) Example of image-text misaligned safety data.", "description": "This figure shows an example from the Robust-VLGuard dataset's Image-Text Misaligned Safety Data subset.  It highlights a scenario where the image content is deliberately unrelated to the safety-related text prompt.  This is a novel aspect of the dataset, designed to test the VLM's robustness against attacks where irrelevant visuals are paired with sensitive text prompts. The goal is to evaluate whether the model can correctly identify and handle unsafe text prompts, even in the presence of potentially distracting or misleading images.", "section": "3.1. Robust-VLGuard Dataset"}, {"figure_path": "https://arxiv.org/html/2504.01308/x8.png", "caption": "Figure 3: Overview of the Robust-VLGuard dataset. (a) General Instruction Data: Leveraging GPT-4V to generate comprehensive, detailed responses rather than brief replies; (b) Image-Text Aligned Safety Data: The image content directly corresponds to the safety-related text prompts; (c) Image-Text Misaligned Safety Data: The image content that is deliberately unrelated to the safety-related text prompts. Red text indicates content with potential risks, while green text denotes content without risks.", "description": "Figure 3 illustrates the Robust-VLGuard dataset, designed to enhance the robustness of Vision-Language Models (VLMs) against safety-related issues and noise.  It's composed of three parts: (a) General Instruction Data:  This section contains diverse, general instructions with detailed responses generated by GPT-4V, to ensure the VLM maintains helpfulness after fine-tuning. (b) Image-Text Aligned Safety Data: This section includes instructions where the image and text are aligned and relate to safety-related content. (c) Image-Text Misaligned Safety Data: This section introduces a novel aspect, where images are intentionally misaligned with the safety-related text prompts, to better cover scenarios encountered during attacks or real-world usage.  Red text within the examples highlights potentially risky or unsafe content, while green indicates safe content.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x9.png", "caption": "(a) Performance on the MM-Vet benchmark across different instruction ratios.", "description": "This figure shows the results of an ablation study on the impact of varying the ratio of general instructions to safety instructions in the Robust-VLGuard dataset on the model's performance.  The x-axis represents different ratios of general to safety instructions (e.g., 4:1, 4:2, 4:3, 4:4, meaning four general instructions for every one safety instruction, etc.), while the y-axis shows the performance score. Two sets of bars are displayed: one for the model's performance with clean images and another for the model's performance with noisy images.  The figure helps to understand how the balance between general and safety-focused training data impacts both the model's helpfulness (measured by performance score) and its robustness to noisy inputs.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x10.png", "caption": "(b) Attack success rate on the RealToxicityPrompts benchmark across different instruction ratios.", "description": "This figure shows the attack success rate on the RealToxicityPrompts benchmark. The attack success rate is the percentage of responses that are considered toxic. The x-axis shows different ratios of general instructions to safety instructions in the training data. The y-axis shows the attack success rate. The figure shows that as the ratio of safety instructions increases, the attack success rate decreases. This indicates that adding more safety instructions to the training data improves the model's ability to resist toxic attacks.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x11.png", "caption": "Figure 4: Effect of varying instruction ratios on VLM\u2019s robustness of helpfulness and safety alignment.", "description": "This figure analyzes the impact of different ratios of general instructions to safety-focused instructions within the Robust-VLGuard dataset on the performance and robustness of Vision-Language Models (VLMs).  The left graph shows how varying these ratios affects the models' helpfulness scores, as measured by the MM-Vet benchmark.  The right graph displays the impact on the attack success rate against safety-related instructions, using the RealToxicityPrompts benchmark. This helps to determine the optimal balance between maintaining the helpfulness of the model and enhancing its safety against adversarial attacks.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x12.png", "caption": "(a) Performance on the MM-Vet benchmark across different training epochs.", "description": "This figure shows the performance of different vision-language models on the MM-Vet benchmark as the number of training epochs varies.  The MM-Vet benchmark assesses several capabilities of these models.  The plot likely displays the model's performance scores across various tasks within the benchmark, enabling a comparison of model robustness and helpfulness under different training durations.  Higher scores generally indicate better performance.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x13.png", "caption": "(b) Attack success rate on the RealToxicityPrompts benchmark across different training epochs.", "description": "This figure shows how the success rate of attacks changes as the number of training epochs increases. The x-axis represents the number of training epochs, and the y-axis shows the attack success rate, indicating the percentage of times the model generated unsafe responses.  The figure helps assess the model's robustness to attacks over different training durations. A lower attack success rate suggests improved model safety.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x14.png", "caption": "Figure 5: Effect of varying training epochs on VLM\u2019s robustness of helpfulness and safety alignment.", "description": "This figure displays the results of an ablation study evaluating the effect of varying the number of training epochs on the performance and safety of Vision-Language Models (VLMs).  Two subplots are shown: (a) shows helpfulness as measured by the MM-Vet benchmark, and (b) shows safety, measured by the attack success rate using the RealToxicityPrompts benchmark.  The x-axis in both plots represents the number of training epochs (1, 2, 3), and the y-axis represents performance score for helpfulness and attack success rate for safety.  The results show the trade-off between helpfulness and safety with increasing training epochs, indicating the need for careful tuning to balance these aspects.", "section": "3. Noise-Augmented Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2504.01308/x15.png", "caption": "Figure 6: Residual image Gaussianity analysis. We apply DiffPure (t\u2217=50superscript\ud835\udc6150t^{*}=50italic_t start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT = 50) to adversarial image Ia\u2062d\u2062vsubscript\ud835\udc3c\ud835\udc4e\ud835\udc51\ud835\udc63I_{adv}italic_I start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT to obtain diffused image I\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51subscript\ud835\udc3c\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51I_{\\mathit{diffused}}italic_I start_POSTSUBSCRIPT italic_diffused end_POSTSUBSCRIPT. Then we calculate residual images ra\u2062d\u2062vsubscript\ud835\udc5f\ud835\udc4e\ud835\udc51\ud835\udc63r_{adv}italic_r start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT and r\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51subscript\ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51r_{\\mathit{diffused}}italic_r start_POSTSUBSCRIPT italic_diffused end_POSTSUBSCRIPT and evaluate their distribution by the histogram and Q-Q plot.", "description": "This figure displays a Gaussianity analysis of residual images.  The experiment begins with an adversarial image (Iadv), which has been created through adversarial attacks.  DiffPure, a diffusion model, is applied to this adversarial image with a timestep of 50 (t*=50). This process generates a diffused image (Idiffused). The difference between the original clean image and the adversarial image is calculated as the residual image (radv), and similarly, the difference between the clean image and the diffused image is calculated as rdiffused.  The distributions of radv and rdiffused are then analyzed using histograms and Q-Q plots to assess how close these residual images are to a Gaussian distribution. This visualization helps to understand the effectiveness of DiffPure in transforming adversarial noise into Gaussian-like noise.", "section": "4.1. Preprocessing Methods in distribution shifting"}, {"figure_path": "https://arxiv.org/html/2504.01308/x16.png", "caption": "Figure 7:  Gaussianity metrics of r\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51subscript\ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51r_{\\mathit{diffused}}italic_r start_POSTSUBSCRIPT italic_diffused end_POSTSUBSCRIPT under different pixel constraints \u03f5italic-\u03f5\\epsilonitalic_\u03f5 of adversarial image Ia\u2062d\u2062vsubscript\ud835\udc3c\ud835\udc4e\ud835\udc51\ud835\udc63I_{adv}italic_I start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT and timestep t\u2217superscript\ud835\udc61t^{*}italic_t start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT in DiffPure. Please zoom in to see details.", "description": "This figure displays the results of an analysis evaluating the Gaussianity of residual images after applying the DiffPure method to adversarial images.  Specifically, it examines how the kurtosis and Q-Q deviation of the residuals change based on two key factors: the pixel constraint (epsilon) applied during the generation of the adversarial image and the timestep (t*) used in the DiffPure process. Different lines in the plot represent different epsilon values and each point on those lines shows the metrics calculated for a particular timestep. The figure helps to visualize how effective DiffPure is at transforming the distribution of adversarial noise into a Gaussian-like distribution, which is a key aspect of its defense mechanism against optimization-based attacks.", "section": "4.1. Preprocessing Methods in distribution shifting"}, {"figure_path": "https://arxiv.org/html/2504.01308/x17.png", "caption": "Figure 8: The overall framework of DiffPure-VLM.", "description": "The DiffPure-VLM framework starts with an adversarial image and a harmful text prompt.  The adversarial image undergoes a forward and reverse diffusion process using a diffusion model (like DDPM), which transforms the adversarial noise into Gaussian-like noise, thereby purifying the image. The resulting image, now containing only Gaussian noise, is fed into a Gaussian-noise-tolerant vision-language model (VLM) which has been fine-tuned with noise-augmented safety data. This robust VLM then produces a safe response to the harmful prompt.", "section": "4. Generalize to Optimization-based Visual Perturbation Attack"}, {"figure_path": "https://arxiv.org/html/2504.01308/x18.png", "caption": "Figure 9: Ia\u2062d\u2062vsubscript\ud835\udc3c\ud835\udc4e\ud835\udc51\ud835\udc63I_{adv}italic_I start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT, I\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51subscript\ud835\udc3c\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51I_{\\mathit{diffused}}italic_I start_POSTSUBSCRIPT italic_diffused end_POSTSUBSCRIPT and statistics of ra\u2062d\u2062vsubscript\ud835\udc5f\ud835\udc4e\ud835\udc51\ud835\udc63r_{adv}italic_r start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT, r\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51subscript\ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51r_{\\mathit{diffused}}italic_r start_POSTSUBSCRIPT italic_diffused end_POSTSUBSCRIPT under different t\u2217superscript\ud835\udc61t^{*}italic_t start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT in DiffPure (constraint \u03f5=16/255italic-\u03f516255\\epsilon=16/255italic_\u03f5 = 16 / 255). Metrics are shown in \u2018Kurtosis / Q-Q Deviation / Mean / Standard Deviation\u2019. Please zoom in to see details.", "description": "Figure 9 visualizes the effects of applying DiffPure, a diffusion model-based image preprocessing technique, to adversarial images.  It shows the original adversarial image (I\u2090dv), the image after DiffPure processing (Idiffused), and statistical analyses of the residual images (r\u2090dv and rdiffused).  These residuals represent the difference between the original clean image and the adversarial/DiffPure-processed images, respectively. The analysis includes histograms and Q-Q plots for each color channel (red, green, blue) to assess the distribution of pixel values in the residuals.  Different values of t*, a parameter controlling the number of diffusion steps in DiffPure, are shown, highlighting how the distribution shifts with varying levels of processing. The goal is to show how DiffPure transforms adversarial noise into more Gaussian-like noise.", "section": "4.1. Preprocessing Methods in distribution shifting"}, {"figure_path": "https://arxiv.org/html/2504.01308/x19.png", "caption": "Figure 10: Ia\u2062d\u2062vsubscript\ud835\udc3c\ud835\udc4e\ud835\udc51\ud835\udc63I_{adv}italic_I start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT, I\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51subscript\ud835\udc3c\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51I_{\\mathit{diffused}}italic_I start_POSTSUBSCRIPT italic_diffused end_POSTSUBSCRIPT and statistics of ra\u2062d\u2062vsubscript\ud835\udc5f\ud835\udc4e\ud835\udc51\ud835\udc63r_{adv}italic_r start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT, r\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51subscript\ud835\udc5f\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc52\ud835\udc51r_{\\mathit{diffused}}italic_r start_POSTSUBSCRIPT italic_diffused end_POSTSUBSCRIPT under different t\u2217superscript\ud835\udc61t^{*}italic_t start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT in DiffPure (constraint \u03f5=32/255italic-\u03f532255\\epsilon=32/255italic_\u03f5 = 32 / 255). Metrics are shown in \u2018Kurtosis / Q-Q Deviation / Mean / Standard Deviation\u2019. Please zoom in to see details.", "description": "Figure 10 visualizes the effects of applying the DiffPure algorithm with varying timesteps (t*) on adversarial images.  The top row shows the original adversarial image (Iadv) and the images produced by DiffPure (Idiffused) for different t* values. The bottom rows display the statistical properties (Kurtosis, Q-Q Deviation, Mean, Standard Deviation) of the residual images (radv and rdiffused) which represent the difference between the original clean image and the adversarial/DiffPure processed images. These statistics help assess how well DiffPure transforms the adversarial noise into a Gaussian-like distribution.  The figure uses an adversarial image created with a pixel constraint of \u03f5=32/255. This analysis helps to evaluate the effectiveness of DiffPure in mitigating adversarial noise in images before feeding them into vision-language models.", "section": "4.1. Preprocessing Methods in distribution shifting"}]