[{"figure_path": "https://arxiv.org/html/2503.07860/extracted/6268239/figs/pull-fig-5.png", "caption": "Figure 1: The Video Action Differencing task and benchmark (VidDiffBench). Given a pair of videos and an action, the task is to generate a list of differences as natural language descriptions.\nOur VidDiffBench consists of annotated differences across diverse domains, where the differences are relevant to human skill learning.\nThe first row emphasizes the first key challenge: localization of sub-actions\nbetween segments of the video for comparison. The second row highlights the second key challenge: fine-grained image understanding of actions in order to perform comparison.", "description": "Figure 1 illustrates the Video Action Differencing (VidDiff) task and its accompanying benchmark dataset, VidDiffBench.  The task involves identifying subtle differences between two videos depicting the same action. VidDiffBench contains video pairs from diverse domains (fitness, sports, music, surgery, diving) with human-annotated differences, relevant to skill learning. The top row of the figure exemplifies the challenge of localizing specific sub-actions within the videos for accurate comparison. The bottom row highlights the challenge of fine-grained visual understanding required to discern subtle differences in the execution of actions.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2503.07860/x1.png", "caption": "Figure 2: VidDiff Method. One input is an action description (e.g. \u201cweighted squat\u201d). The Difference Proposer generates potential differences using a large language model (LLM). The Frame Localizer assigns frames where these differences are observable. Finally, the Action Differencer checks each difference using a vision-language model, determining whether it applies more to video A or video B, or neither.", "description": "The VidDiff method is a three-stage process for identifying subtle differences between two videos showing the same action.  First, a large language model (LLM) proposes potential differences between the videos (e.g., \"faster ascent\", \"wider stance\").  Next, a frame localization module identifies the specific frames in each video where these differences are visible. Finally, a vision-language model compares the identified frames to determine whether each proposed difference applies more strongly to video A, video B, or neither.", "section": "5 VIDDIFF METHOD"}, {"figure_path": "https://arxiv.org/html/2503.07860/x2.png", "caption": "Figure 3: Examples of \u2018success cases\u2019 (left) \u2013 differences where GPT-4o has high accuracy \u2013 and failure cases (right). Success cases typically involve coarse differences, easy localization, or simple actions, while failure cases often involve fine differences, precise localization or complex actions.", "description": "This figure showcases examples of successful and unsuccessful video action difference predictions made by the GPT-40 model.  The 'success cases' (left) illustrate situations where GPT-40 achieves high accuracy. These cases generally involve readily apparent differences between the videos, easy identification of the specific frames where the differences occur, and actions that are relatively straightforward or simple in nature.  In contrast, the 'failure cases' (right) represent instances where GPT-40's performance is poor.  These typically involve subtle differences requiring keen visual discernment, precise temporal localization to pinpoint the exact frames containing the difference, or actions that are complex and nuanced.  The differences between success and failure highlight the challenges of identifying and localizing nuanced action differences.", "section": "6 Results"}, {"figure_path": "https://arxiv.org/html/2503.07860/x3.png", "caption": "Figure 4: Sample frame localizations: prediction vs ground truth.", "description": "This figure showcases examples of frame localization within the VidDiff method. It visually compares the ground truth frame selections with the model's predictions for various action differences. Each row represents a different difference, highlighting the model's ability to pinpoint the relevant frames where visual differences are observable.  The differences shown range in complexity and granularity, demonstrating the challenges in precise temporal localization within video action differencing.", "section": "VidDiff Method"}]