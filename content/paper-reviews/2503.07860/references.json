{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-15", "reason": "This paper is important because it describes GPT-4, one of the state-of-the-art large multimodal models used as a baseline in the video action differencing task and within the VidDiff method."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper is important because it describes CLIP, a contrastive language-image model used for frame localization in the VidDiff method."}, {"fullname_first_author": "Kristen Grauman", "paper_title": "Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives", "publication_date": "2023-11-29", "reason": "This paper is important as the Ego-Exo4D dataset, featuring expert commentary on skilled actions, is used in the creation of VidDiffBench."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond", "publication_date": "2023-08-24", "reason": "This paper is important because it describes Qwen2-VL, a leading open-source model used as a baseline in the video action differencing task."}, {"fullname_first_author": "Zhongang Cai", "paper_title": "Humman: Multi-modal 4d human dataset for versatile sensing and modeling", "publication_date": "2022-01-01", "reason": "This paper is important as the HuMMan dataset is used for the Fitness video category within VidDiffBench."}]}