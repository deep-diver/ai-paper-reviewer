[{"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/word_acc_comparison.png", "caption": "Figure 1:  Breaking the Limits: Long-Text Image Generation Remains Elusive for Existing Models.\nState-of-the-art text rendering models, such as Text Diffusion 2\u00a0[4] and AnyText\u00a0[42], perform well on short text but struggle with longer passages.\nLarge diffusion models like Stable Diffusion 3.5 Large\u00a0[10] can handle longer text but exhibit lower accuracy. The text recognition on generated images was conducted using Qwen2-VL\u00a0[46] model.\nFor this evaluation, we sampled 140 examples from the interleaved Obelics\u00a0[18] dataset with truncation.", "description": "The figure illustrates the limitations of existing text-to-image models in handling long text.  While models like Text Diffusion 2 and AnyText perform well with short text inputs, their accuracy significantly decreases when processing longer passages. Even large diffusion models such as Stable Diffusion 3.5 Large struggle to maintain high accuracy with long text.  This experiment used the Qwen2-VL model for text recognition on images generated from 140 examples (with truncation) of the interleaved Obelics dataset.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/TextBinarizer.png", "caption": "Figure 2: \nTextBinarizer implementation details.\nThis approach allows for direct quantization.", "description": "Figure 2 illustrates the architecture of TextBinarizer, a novel text-focused binary tokenizer.  It shows how high-dimensional word embeddings are converted into binary tokens through a quantization process, enabling direct quantization without the need for a codebook. The figure depicts the encoder and decoder components, highlighting the use of convolutional neural networks (CNNs) and a quantization discriminator to optimize the process for enhanced text detail preservation.", "section": "3.2. TextBinarizer: A Text-Centric Tokenization"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/main_ppl.png", "caption": "Figure 3: The main pipeline of LongTextAR.\nOur trained text-focused tokenizer converts the long-text image into discrete token IDs.\nA corresponding long-text prompt is generated, and the model is then tasked with predicting the image token IDs based on this long text prompt.", "description": "The figure illustrates the architecture of LongTextAR, a model designed for generating images from long text.  The process begins with a text-focused tokenizer converting the long text into discrete tokens. Simultaneously, a corresponding long-text prompt is generated.  The core model then uses this prompt to predict the image's token IDs, which are subsequently decoded to produce the final image.", "section": "4. LongTextAR"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/tokenizer_comparison.png", "caption": "Figure 4: \nTokenizer reconstruction comparison on data with long-text.\nComparing with well-trained VQ tokenizer from Chameleon\u00a0[38], our text-focus tokenizer leads to better reconstruction result on detail generation for letters.", "description": "This figure displays a comparison of text reconstruction quality between a traditional Vector Quantization (VQ) tokenizer and the novel TextBinarizer tokenizer introduced in the paper.  The comparison uses a dataset containing long text passages.  The results visually demonstrate that TextBinarizer, designed with a focus on detailed text features, achieves superior reconstruction of fine details such as individual letters compared to the VQ tokenizer.  The improved quality with TextBinarizer is particularly evident in the more complex, long-text data.", "section": "3.1 Tokenization Challenges in Text Rendering"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/controable_variable.png", "caption": "Figure 5: Controllable experiment, we modify the text font type, text color and text rotation degree, also the alignment way.", "description": "This figure demonstrates the controllability of the LongTextAR model in generating text images.  Multiple examples are shown, each with different modifications to the text's font type, color, rotation degree, and alignment. This showcases the model's ability to precisely follow user-specified formatting instructions, resulting in high-fidelity text rendering and diverse visual styles.", "section": "5.3. Controllable Long Text Rendering"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/all_model_comparison.png", "caption": "Figure 6: \nText-conditioned long-text image generation comparison.\nThe Stable Diffusion3.5 Large\u00a0[10] and GPT-4o\u00a0[30]+Dall-E3\u00a0[3] using the prompt Generate a white-background text image and the text is: [Text Prompt].\nThe text is clear and large.\nFor TextDiffuser 2\u00a0[4] we use the prompt A text image and input other text as tags.\nWe use the Qwen2-VL\u00a0[46] to recognize words from generated images and compute the accuracy according to the ground-truth text [Text Prompt].\nThe image generation capabilities of GPT4o\u00a0[30], released at the end of March 2025, have shown a huge gap over all other models, both open-source and closed-source.", "description": "This figure compares the long-text image generation capabilities of several models: LongTextAR, Stable Diffusion 3.5 Large, GPT-40+DALL-E 3, TextDiffuser 2, and AnyText.  The models were prompted to generate images containing a specific long text passage.  Stable Diffusion 3.5 Large and GPT-40+DALL-E 3 used a prompt specifying a white background and the target text. TextDiffuser 2 used a different prompt and included the text as tags.  The accuracy of text recognition in the generated images was assessed using Qwen2-VL. The results reveal that GPT-40, released in March 2025, significantly outperforms all other models, both open-source and closed-source.", "section": "5.4. Comparison with Existing Models"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/natural_images.png", "caption": "Figure 7: \nNatural image text rendering examples.", "description": "This figure showcases examples of LongTextAR's ability to render text within natural images.  Unlike synthetically generated images with precisely placed text, these examples demonstrate the model's capacity to integrate text naturally into real-world scenes, handling variations in text style and image context.", "section": "5.5 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/ppt_application.png", "caption": "Figure 8: LongTextAR\u00a0 generates interleaved PowerPoint data from long-text prompts.\nThe model accurately produces layouts and renders both text and images effectively.\nBounding boxes from the text input are shown as a reference.", "description": "Figure 8 showcases LongTextAR's ability to generate PowerPoint slides from long text prompts.  The model not only accurately renders the text and images provided but also effectively creates the layout of the slide, demonstrating proficiency in arranging both visual and textual elements in a coherent manner. Bounding boxes around the text within the input highlight the precise placement and alignment of elements in the generated slides.", "section": "4. LongTextAR"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/supplementary/lumina_mgpt_same_case.png", "caption": "Figure 9: \nBaseline comparison.\nLumina-mGPT\u00a0[21], an enhanced generation model built upon Chameleon\u00a0[38], demonstrates limited capability in effectively following complex textual instructions.", "description": "Figure 9 shows a comparison of the capabilities of Lumina-mGPT, a model based on Chameleon, in handling complex textual instructions for image generation.  The figure demonstrates that Lumina-mGPT, while an enhanced model, has limitations in effectively following and accurately representing intricate or lengthy textual instructions during the image generation process.  The results highlight that simply enhancing a model is not sufficient to overcome the challenge of accurately interpreting complex instructions and converting them into high-fidelity images.", "section": "7. Comparison with Baseline Models Lacking Text-Focused Training"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/powerpoint_application_comparison.png", "caption": "Figure 10: The interleaved powerpoint generation comparison.", "description": "This figure compares the performance of different models, including Stable Diffusion 3.5 Large, GPT-4.0 + DALL-E 3, and the authors' LongTextAR model, on the task of generating interleaved PowerPoint slides from text prompts.  The comparison highlights the ability of each model to accurately render text and images within the specified layout and formatting requirements. It demonstrates the superior performance of LongTextAR in terms of both accuracy and adherence to the prompt's instructions.", "section": "8. Comparison with Existing Models on PowerPoint Data Generation"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/interleaved_examples.png", "caption": "Figure 11: More interleaved data examples.", "description": "This figure showcases additional examples of the model's ability to generate PowerPoint-like slides containing both text and images.  The slides demonstrate the model's capacity to handle various layouts and styles, incorporating both text-only and interleaved content effectively.  It highlights the model's ability to seamlessly integrate different modalities and generate visually appealing and informationally complete slides.", "section": "8. Comparison with Existing Models on PowerPoint Data Generation"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/controable_experiment.png", "caption": "Figure 12: \nExperiment on Controllable Variables:\nStable Diffusion 3.5 Large\u00a0[10] demonstrates poor instruction-following ability, ignoring all specified variants and occasionally generating irrelevant images.\nGPT-4-O\u00a0[30], while showing noticeably better instruction-following capability, produces outputs with low accuracy and fails to capture the specified controllable variables.", "description": "This figure compares the performance of Stable Diffusion 3.5 Large and GPT-4-O in generating images based on prompts with controllable variables (font color, type, rotation).  Stable Diffusion 3.5 Large largely ignores the specified instructions, frequently producing unrelated images. GPT-4-O shows improvement in following instructions but still lacks accuracy and fails to incorporate all specified variables into the generated images.", "section": "9. Controllable Experiments"}, {"figure_path": "https://arxiv.org/html/2503.20198/extracted/6310534/figures/src/more_powerpoing_examples.png", "caption": "Figure 13: \nMore powerpoint generation examples.", "description": "This figure displays additional examples of PowerPoint slides generated by the LongTextAR model.  The examples showcase the model's ability to handle diverse layouts and styles, including those with varying text lengths and combinations of text and images.  This demonstrates the model's flexibility in generating complex, multi-modal documents.", "section": "10. Additional PowerPoint Generation Examples"}]