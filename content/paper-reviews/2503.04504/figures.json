[{"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure1.png", "caption": "Figure 1: Comparison of traditional video Anomaly Detection (VAD) and customizable video anomaly detection (C-VAD). Traditional VAD models struggle with generalization, making them hard to apply in diverse environments, while C-VAD can handle various video environments.", "description": "This figure compares traditional Video Anomaly Detection (VAD) and the proposed Customizable Video Anomaly Detection (C-VAD).  Traditional VAD methods typically train on a specific environment and learn patterns of 'normal' behavior. When encountering a new environment or scenarios with different normal activities, these models often fail due to poor generalization. This is illustrated in the left side of the figure, showing a traditional VAD model trained in a campus setting incorrectly flagging a car in a road environment as anomalous.  In contrast, C-VAD uses user-defined textual descriptions to specify what constitutes an anomaly. This approach removes the need to train on specific normal patterns, offering better generalization across diverse settings, as shown on the right. The AnyAnomaly model is introduced as an example of a C-VAD approach.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure2_2.jpg", "caption": "Figure 2: The architecture of AnyAnomaly", "description": "AnyAnomaly processes a video segment by first using a key frame selection module to choose representative frames.  These frames are then used to generate position context (PC) highlighting important locations and temporal context (TC) showing scene changes over time.  PC and TC, along with the representative frame, are fed into a large vision language model (LVLM) along with a user-defined text prompt describing the anomalous event. The LVLM outputs an anomaly score indicating the likelihood of the described event being present in the segment.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure3a.png", "caption": "(a) Key frames Selection Module (KSM)", "description": "The Key Frames Selection Module (KSM) is a crucial component in AnyAnomaly's segment-level approach to video anomaly detection. It efficiently selects four keyframes from a video segment to represent the entire segment for processing.  This selection process utilizes the CLIP model to determine the frames most relevant to the user-defined abnormal event. By selecting representative keyframes, KSM significantly reduces computational cost while maintaining effective anomaly detection.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure3b.png", "caption": "(b) WinCLIP-based Attention (WA)", "description": "The WinCLIP-based Attention (WA) module enhances the object analysis capability of the Large Vision Language Model (LVLM) by emphasizing regions related to the user-provided text in the key frame.  It uses a multi-scale approach, generating embeddings from small, medium, and large-scale windows of the key frame. These embeddings are compared to the text embedding, generating a similarity map that highlights important regions. This similarity map, combined with the key frame, creates the position context (PC) input for the LVLM.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure3c.png", "caption": "(c) Grid Image Generation (GIG)", "description": "This module generates temporal context (TC) for context-aware VQA.  It takes the key frames selected by the Key Frame Selection Module as input. These frames are divided into multiple windows, and windows at the same position are combined into a 2x2 grid to create grid images at different scales (small, medium, and large).  These grid images represent the temporal evolution of the scene across these key frames. Finally, the grid image with the highest similarity to the user-provided text is selected as the temporal context to be fed to the large vision language model.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure4.png", "caption": "Figure 3: Architecture of the proposed modules. KSM is essential for the segment-level approach, and WA and GIG are crucial for context generation.", "description": "This figure details the architecture of the AnyAnomaly model's core modules.  The Key Frame Selection Module (KSM) is highlighted as fundamental to the model's segment-level processing, which improves efficiency by analyzing groups of frames instead of individual frames.  The figure then illustrates the WinCLIP-based Attention (WA) and Grid Image Generation (GIG) modules. WA refines object analysis by focusing attention on relevant image regions based on textual input. GIG incorporates temporal context by generating grid-based representations from sequences of frames, enhancing the model's understanding of actions and events over time.  The combination of KSM, WA, and GIG enables context-aware visual question answering (VQA), allowing AnyAnomaly to accurately detect anomalies based on user-defined descriptions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure5.png", "caption": "Figure 4: Proposed prompt for VQA", "description": "This figure shows the detailed prompt engineering used for visual question answering (VQA) in the AnyAnomaly model.  The prompt is broken down into three main sections: Task, Consideration, and Output. The \"Task\" section clearly defines the objective of the VQA task, which is to evaluate the presence of a user-specified textual description (e.g., 'bicycle') within a given image on a scale of 0 to 1. The \"Consideration\" section provides additional instructions to guide the model towards more accurate results. It emphasizes that the presence of the target object, regardless of its visual prominence or central position within the image, should be considered. Finally, the \"Output\" section specifies the desired format of the model's response, requiring a numerical score (rounded to one decimal place) along with a concise textual explanation justifying the score. For temporal context, an additional 'Context' section was added to describe the temporal sequence shown in the images.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure6a.jpg", "caption": "Figure 5: Comparison between the VAD and C-VAD datasets", "description": "Figure 5 illustrates the key difference in data organization between traditional Video Anomaly Detection (VAD) datasets and the proposed Customizable Video Anomaly Detection (C-VAD) datasets.  Traditional VAD datasets contain videos with a mix of normal and abnormal events, without explicitly labeling or categorizing the types of anomalies.  In contrast, C-VAD datasets organize videos into categories based on specific abnormal events (e.g., bicycle, car). Each category contains positive examples (videos showcasing the event) and negative examples (videos without the event). This structured approach allows for more precise evaluation of anomaly detection performance for specific anomaly types and is crucial for the zero-shot learning paradigm.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure6b.jpg", "caption": "Figure 6: Anomaly score comparison and context visualization", "description": "This figure visualizes the performance of the proposed AnyAnomaly model on two example video segments.  The top row shows a segment containing a bicycle, while the bottom row shows a segment with a person jumping. For each segment, it displays the anomaly scores over time (ground truth and prediction) and shows the importance of position context and temporal context in improving the accuracy of anomaly detection.  The visualizations demonstrate how including contextual information enhances AnyAnomaly's ability to correctly identify and classify anomalous events compared to using only the keyframe.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure10.png", "caption": "Figure S1: Prompt details. The content written in the simple version is not utilized when applying reasoning.", "description": "This figure details the design of prompts used for visual question answering (VQA) in the AnyAnomaly model. It shows three prompt variations.  The first, a \"simple version,\" just requests an anomaly score. The second adds a \"reasoning\" component, requiring a brief explanation along with the score. The third prompt includes a \"consideration\" section emphasizing that an object's presence should be given a high score even if it isn't the main focus of the image.  A final version also adds a \"context\" element to handle temporal data within a video segment, instructing the model to treat the input as a sequence of frames.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure8a.png", "caption": "Figure S2: Example of complementarity between position and temporal context. The first example highlights the importance of position context and the second example emphasizes the importance of temporal context.", "description": "This figure demonstrates how position and temporal context contribute to the accuracy of video anomaly detection.  The top example shows a situation where a bicycle is partially visible in a frame. Using only the key frame yields a low anomaly score. However, incorporating position context (PC), which uses attention to highlight the relevant area of the key frame, significantly improves the score, accurately identifying the bicycle. The bottom example shows a person jumping. Again, the key frame alone doesn't fully capture the action. However, using temporal context (TC), which incorporates information across a sequence of frames to better understand the action, provides a much higher and more accurate anomaly score.  This illustrates the complementarity of position and temporal context for effective anomaly detection. ", "section": "C.1. Context Complementarity"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure8b.png", "caption": "(a) jumping-falling-pickup", "description": "This figure visualizes the anomaly detection scores for a video segment containing three types of anomalous events: jumping, falling, and a person picking something up. The plot displays the anomaly scores over time, with the ground truth labels indicating the presence and type of anomaly. The high scores in the corresponding regions indicate successful anomaly detection.", "section": "4.4 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure7a.png", "caption": "(b) bicycle-running", "description": "The figure shows the anomaly detection scores over time for a video segment containing both bicycle and running events. The top graph displays the anomaly scores generated by the model, while the bottom shows the ground truth labels for each frame indicating whether a bicycle or running activity is present.  The visualization helps illustrate the model's ability to detect different types of anomalous events within a single video sequence and highlights the temporal aspects of the detection process.", "section": "4.4. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure7b.png", "caption": "(c) bicycle-stroller", "description": "The figure displays the results of anomaly detection for a video segment containing both bicycles and strollers.  The top graph shows the anomaly score over time, with higher scores indicating a higher probability of an anomaly. The bottom row shows key frames from the video segment, illustrating instances where both bicycles and strollers are present, demonstrating the model's ability to distinguish between user-defined anomalies ('bicycle' and 'stroller').", "section": "4.4 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure7c.png", "caption": "Figure S3: Anomaly detection in diverse scenarios. Various abnormal events can emerge over time.", "description": "Figure S3 showcases the AnyAnomaly model's ability to detect various types of anomalies in video sequences.  Each row displays results for a different anomaly type, visually demonstrating how the model's anomaly scores change over time, highlighting the temporal aspect of the detection. The scores are generated using the proposed AnyAnomaly model, showing its effectiveness in detecting diverse anomalies.", "section": "C.2. Anomaly Detection in Diverse scenarios"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure9a.png", "caption": "(a) Anomaly event: jaywalking", "description": "The figure shows the results of video anomaly detection applied to a scene where the anomaly event is 'jaywalking'.  It visually demonstrates AnyAnomaly's performance by displaying the anomaly score over time (as a graph), the key frame, the position context (PC), and the temporal context (TC).  The position context highlights relevant image regions using a heatmap that is produced by a WinCLIP-based attention module. The temporal context depicts the temporal progression of the jaywalking event, by using a series of key frames in a grid. The text below the visualization provides a qualitative interpretation of the results, including explanations of why specific scores are assigned. This figure helps illustrate how AnyAnomaly, using context-aware Visual Question Answering (VQA), achieves a more accurate and contextual understanding of abnormal events compared to methods that rely only on key frames.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure9b.png", "caption": "(b) Anomaly event: driving outside lane", "description": "The figure visualizes the results of the AnyAnomaly model on a video segment containing the anomaly of a car driving outside its designated lane.  The top part shows the anomaly scores over time, generated by the model. The bottom part displays key frames from the video segment along with the position and temporal context information used by the model for its decision. The text under the image describes the model's reasoning process, indicating a high anomaly score (0.9) due to the observation of the car driving outside its designated lane in multiple frames.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure9c.png", "caption": "(c) Anomaly event: people and car accident", "description": "This figure visualizes the results of AnyAnomaly on a video segment depicting a car accident involving people. The top graph displays the anomaly scores generated by the model, highlighting the specific frames where an accident is detected. Below the graph, key frames, position context, and temporal context are shown.  The key frames provide visual snapshots of the accident, while the position and temporal contexts provide additional information which improves the model's ability to detect the abnormal event. The text below summarizes the model's output for the video segment.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04504/extracted/6257000/figure9d.png", "caption": "(d) Anomaly event: walking drunk", "description": "This figure visualizes the results of AnyAnomaly on a video depicting the anomaly event of a person walking while appearing intoxicated.  The top graph shows the anomaly score over time, highlighting the model's detection of the anomalous activity. Below the graph, key frames from the video are displayed, along with visualizations produced by the position context (PC) and temporal context (TC) modules of AnyAnomaly. PC focuses on identifying specific objects of interest within a single frame, while TC analyzes the temporal evolution of events across multiple frames. The text below the image provides a description and reasoning behind AnyAnomaly's score.", "section": "4. Experiments"}]