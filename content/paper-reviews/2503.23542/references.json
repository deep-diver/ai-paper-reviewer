{"references": [{"fullname_first_author": "A. Radford", "paper_title": "Robust speech recognition via large-scale weak supervision", "publication_date": "2022-00-00", "reason": "This paper is the foundation of the Whisper model, which the current study seeks to improve upon."}, {"fullname_first_author": "A. Graves", "paper_title": "Speech recognition with deep recurrent neural networks", "publication_date": "2013-00-00", "reason": "This paper is one of the groundbreaking works using recurrent neural networks for speech recognition and helped push the limits of what could be achieved."}, {"fullname_first_author": "W. Chan", "paper_title": "Listen, attend and spell", "publication_date": "2015-08-00", "reason": "This paper is another landmark work introducing attention mechanisms in ASR models, pushing the limits of what these systems could achieve."}, {"fullname_first_author": "Q. Zhang", "paper_title": "Transformer transducer: A streamable speech recognition model with transformer encoders and RNN-T loss", "publication_date": "2020-02-00", "reason": "This paper is a critical study on transformer-based ASR models, addressing challenges in various applications."}, {"fullname_first_author": "A. Hannun", "paper_title": "Deep Speech: Scaling up end-to-end speech recognition", "publication_date": "2014-00-00", "reason": "This is one of the foundational papers on end-to-end speech recognition, which the paper extends and builds upon."}]}