[{"figure_path": "https://arxiv.org/html/2503.23542/x5.png", "caption": "Figure 1: Distribution of dataset hours across different training phases.", "description": "This figure shows the distribution of audio data (measured in hours) used for training the Whisper ASR model across different phases: pre-training, fine-tuning, and evaluation.  It visually represents the relative amount of data used in each phase for four different languages: Basque (eu), Galician (gl), Catalan (ca), and Spanish (es). The pre-training data consists of a massive multilingual corpus, while the fine-tuning and evaluation sets are smaller, language-specific datasets. The visual representation helps to understand the data resources available for each language and the relative emphasis placed on each training phase.", "section": "4 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2503.23542/x6.png", "caption": "Figure 2: Effective robustness of RER by model size.", "description": "This figure shows the effective robustness of relative error reduction (RER) across different Whisper model sizes.  Effective robustness, in this context, measures how consistently a model's performance scales from in-distribution (ID) to out-of-distribution (OOD) scenarios.  The graph likely displays the ERER values for each model size (Tiny, Base, Small, Medium, Large, Large-V2, Large-V3), calculated as the difference between OOD and ID RER values.  Higher ERER indicates more consistent performance across ID and OOD conditions, signifying greater robustness.  The plot probably shows that while fine-tuning (FT) alone might offer high performance on ID data, its robustness decreases with larger model sizes. Integrating either n-gram language models (FT+LM) or large language models (FT+LLM) appears to improve robustness, with LLMs potentially providing more consistent performance gains across model sizes.", "section": "5.1.4 Comparison of Method Robustness"}, {"figure_path": "https://arxiv.org/html/2503.23542/x7.png", "caption": "Figure 3: Effective robustness of RER by language.", "description": "This figure shows the effective robustness of relative error reduction (RER) for different languages.  Effective robustness, in this context, measures how consistently a model's performance scales from in-distribution (ID) to out-of-distribution (OOD) scenarios.  The chart compares the performance of models fine-tuned (FT), fine-tuned with n-gram language models (FT+LM), and fine-tuned with large language models (FT+LLM). The y-axis represents the ERER score, reflecting the difference between OOD and ID relative error reduction, where a higher score indicates better robustness. Each bar shows the average ERER and standard deviation for each method within each language. This visual representation helps to assess the relative robustness of the different language model integration approaches across various languages and highlights any language-specific differences in the effectiveness of these methods.", "section": "5.1.4 Comparison of Method Robustness"}, {"figure_path": "https://arxiv.org/html/2503.23542/x8.png", "caption": "Figure 4: The averaged RER across different model sizes to study the impact of various evaluation parameters on the WER. Negative values indicate performance decreases when changing from our selected baseline.", "description": "Figure 4 analyzes the impact of various evaluation parameters on Word Error Rate (WER) across different Whisper model sizes.  It presents the average Relative Error Reduction (RER) for each parameter change, compared to a baseline configuration (beam size 5, diacritics removed, timestamps excluded, language specified, temperature 0.0). Negative RER values signify performance degradation compared to the baseline. The figure helps determine which parameters significantly influence WER and model size.", "section": "4.5 Ablation Study of Evaluation Parameters"}, {"figure_path": "https://arxiv.org/html/2503.23542/x9.png", "caption": "Figure 5: LM optimization trials with better scores being more opaque.", "description": "This figure visualizes the results of hyperparameter optimization for integrating n-gram language models into the Whisper ASR system. Each point represents a trial during the optimization process, with its position on the graph determined by the chosen values for the alpha (\u03b1) and beta (\u03b2) parameters. Alpha scales the contribution of the language model's score, while beta scales the sentence length's effect.  The opacity of each point corresponds to the model's performance (WER), with more opaque points indicating better performance (lower WER).  The plot shows how different languages require different hyperparameter settings for optimal performance. Lower-resource languages exhibit greater variability in effective parameter values, indicated by a wider spread of points in the plot, while higher-resource languages show a more concentrated cluster of optimal parameter values.", "section": "A.3.1 Optimization of Parameter Value Ranges in Language Model Integration"}, {"figure_path": "https://arxiv.org/html/2503.23542/x10.png", "caption": "Figure 6: LLM optimization trials with better scores being more opaque.", "description": "This figure visualizes the results of hyperparameter optimization for integrating Large Language Models (LLMs) with the Whisper speech recognition system. Each point represents a trial during the optimization process, with the x-axis showing the weighting parameter 'alpha' and the y-axis showing the weighting parameter 'beta'.  The opacity of each point corresponds to its performance; more opaque points indicate lower Word Error Rate (WER), signifying better performance.  The different colored points represent different languages (Basque, Galician, Catalan, Spanish). The visualization helps to understand how the optimal weighting parameters for LLM integration vary across different languages.", "section": "A.3.1 Optimization of Parameter Value Ranges in Language Model Integration"}]