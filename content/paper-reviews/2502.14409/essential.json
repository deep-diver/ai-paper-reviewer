{"importance": "This paper introduces **a novel method for LLMs to generate query-focused summaries with unstructured evidence attribution**. It addresses the problem of positional bias in LLMs, enhancing the reliability & transparency of generated summaries, & paving the way for future research on mitigating bias in long-context summarization.", "summary": "LLMs struggle with positional bias and lack transparency when summarizing long contexts. This paper introduces SUnsET dataset and fine-tuning methods to improve unstructured evidence citation and summary quality.", "takeaways": ["LLMs struggle to cite unstructured evidence and are affected by positional bias.", "Fine-tuning on SUnsET data improves evidence citation accuracy, coverage, and summary quality.", "Shuffling context order can mitigate positional bias during fine-tuning."], "tldr": "Large language models (LLMs) can generate summaries, but struggle citing evidence due to positional biases, affecting transparency and reliability. Previous work focuses on evidence citation with fixed granularity. This paper tackles **long-context query focused summarization with unstructured evidence citation**, where models extract text spans. Existing systems struggle citing evidence and are affected by the \"lost-in-the-middle\" problem.\n\nTo address this, the authors introduce **SUnsET, a dataset for fine-tuning LLMs to cite unstructured evidence**. Experiments across models and datasets show LLMs adapted with SUnsET generate more relevant evidence, extract evidence from diverse context locations, and generate better summaries. The study explores position-aware and position-agnostic training, showing shuffled training mitigates positional bias.", "affiliation": "University of Copenhagen", "categories": {"main_category": "Natural Language Processing", "sub_category": "Text Summarization"}, "podcast_path": "2502.14409/podcast.wav"}