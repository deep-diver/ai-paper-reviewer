[{"figure_path": "https://arxiv.org/html/2503.15879/x1.png", "caption": "Figure 1: An overview of Typed-RAG. Non-factoid questions are classified by the type classifier and processed based on their type. Prompts for the multi-aspect decomposer and answer aggregator handle the unique requirements of each type. Details of the prompt can be found in Appendix A.4.", "description": "This figure illustrates the Typed-RAG architecture, a framework for answering non-factoid questions.  The process begins with a non-factoid question being inputted into a type classifier.  Based on the question's type (e.g., debate, experience, comparison), it then follows a tailored pathway.  Multi-aspect questions are decomposed into single-aspect sub-queries, which are then processed through retrieval and generation modules. Finally, an answer aggregator synthesizes these individual responses into a comprehensive, well-rounded answer that addresses the nuanced aspects of the initial non-factoid question.  The specific prompts used in the multi-aspect decomposition and answer aggregation steps are detailed in Appendix A.4.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.15879/x2.png", "caption": "Figure 2: Mean Percentile Rank (MPR) performance comparison of LLM, RAG, and Typed-RAG on six different non-factoid question categories from the Wiki-NFQA dataset. Results are reported using different model configurations (Llama-3.2-3B and Mistral-7B) and scorer LLMs (Mistral-7B and GPT-4o mini). The y-axis represents the MPR score (%), with higher values indicating better performance.", "description": "Figure 2 presents a comparison of the performance of three different question answering methods (LLM, RAG, and Typed-RAG) across six categories of non-factoid questions.  The performance metric used is Mean Percentile Rank (MPR), expressed as a percentage.  Higher MPR scores indicate better performance.  The figure showcases results obtained using two different base language models (Llama-3.2-3B and Mistral-7B) and two different scorer LLMs (Mistral-7B and GPT-40 mini), demonstrating the impact of these model choices on the overall results. Each bar in the figure represents the average MPR for a particular question answering method within a specific non-factoid question category.", "section": "5 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.15879/x3.png", "caption": "Figure 3: Prompt template proposed by Yang et\u00a0al. (2024) to generate the highest standard reference answer using LLM\u2019s internal knowledge.", "description": "This figure presents the prompt template employed by Yang et al. (2024) to generate a high-quality reference answer using the internal knowledge of a Large Language Model (LLM).  The template instructs the LLM to rewrite a provided non-factoid question and its corresponding ground truth answer, leveraging its internal knowledge base to produce a superior, more comprehensive and polished version of the answer. This ensures the reference answers used for evaluating the model's performance are of high quality and consistency, thereby enhancing the robustness and reliability of the evaluation process.", "section": "A.1 Reference List Construction"}]