[{"content": "| Method | Single-Frame SC | Single-Frame AD | Single-Frame TD | Single-Frame HD | Single-Frame Avg. | Multi-Frame SC | Multi-Frame AD | Multi-Frame TD | Multi-Frame HD | Multi-Frame Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|\n| **Generalist Models** |  |  |  |  |  |  |  |  |  |  |\n| GPT-4o [28] | 3.34 | 2.96 | 3.01 | 2.50 | 2.95 | 4.15 | 3.31 | 3.11 | 2.43 | 3.25 |\n| GPT-4o-mini [28] | 3.56 | 2.85 | 2.87 | 2.38 | 2.92 | 3.89 | 3.18 | 2.62 | 2.50 | 3.05 |\n| InternVL2-26B [8] | 3.55 | 2.99 | 2.57 | 2.25 | 2.84 | 4.08 | 3.35 | 3.08 | 2.28 | 3.20 |\n| Qwen2-VL-7B [43] | 2.97 | 2.24 | 2.03 | 2.31 | 2.39 | 3.30 | 2.54 | 2.22 | 2.12 | 2.55 |\n| **Specialist Models** |  |  |  |  |  |  |  |  |  |  |\n| **Image-level models** |  |  |  |  |  |  |  |  |  |  |\n| Osprey-7B [46] | 3.19 | 2.16 | 1.54 | 2.45 | 2.34 | 3.30 | 2.66 | 2.10 | 1.58 | 2.41 |\n| Ferret-7B [44] | 3.08 | 2.01 | 1.54 | 2.14 | 2.19 | 3.20 | 2.38 | 1.97 | 1.38 | 2.23 |\n| **Video-level models** |  |  |  |  |  |  |  |  |  |  |\n| Elysium-7B [41] | 2.35 | 0.30 | 0.02 | 3.59 | 1.57 |  |  |  |  |  |\n| Artemis-7B [32] |  |  |  |  |  | 3.42 | 1.34 | 1.39 | 2.90 | 2.26 |\n| **VideoRefer-7B** | 4.41 | 3.27 | 3.03 | 2.97 | 3.42 | 4.44 | 3.27 | 3.10 | 3.04 | 3.46 |", "caption": "Table 1: Performance comparisons on VideoRefer-BenchDD{}^{\\text{D}}start_FLOATSUPERSCRIPT D end_FLOATSUPERSCRIPT. The best results are bold and the second-best results are underlined. \u201c\u2013\u201d means that the model does not support the certain input form. Grey entries denote cases where the original method cannot accomplish the task; for these tests, masks of the targets were overlaid on the original video (the same below).", "description": "This table presents a comparison of different models' performance on the VideoRefer-BenchD benchmark, focusing on four evaluation metrics (Subject Correspondence (SC), Appearance Description (AD), Temporal Description (TD), and Hallucination Detection (HD)).  It compares both generalist and specialist models (image-level and video-level) in both single-frame and multi-frame settings.  The best and second-best results for each model are highlighted, and '-' indicates models that don't support a specific input type.  Grey entries show results where model input was modified to include manually overlaid target masks, allowing for comparison where the original method couldn't process the data.", "section": "3.3 VideoRefer-Bench"}, {"content": "| Method | Basic Questions | Sequential Questions | Relationship Questions | Reasoning Questions | Future Predictions | Average |\n|---|---|---|---|---|---|---|\n| **Generalist Models** |  |  |  |  |  |  |\n| GPT-4o [28] | 62.3 | 74.5 | 66.0 | 88.0 | 73.7 | 71.3 |\n| GPT-4o-mini [28] | 57.6 | 67.1 | 56.5 | 85.9 | 75.4 | 65.8 |\n| InternVL2-26B [8] | 58.5 | 63.5 | 53.4 | 88.0 | 78.9 | 65.0 |\n| Qwen2-VL-7B [43] | 62.0 | 69.6 | 54.9 | 87.3 | 74.6 | 66.0 |\n| **Specialist Models** |  |  |  |  |  |  |\n| Osprey-7B [46] | 45.9 | 47.1 | 30.0 | 48.6 | 23.7 | 39.9 |\n| Ferret-7B [44] | 35.2 | 44.7 | 41.9 | 70.4 | 74.6 | 48.8 |\n| **VideoRefer-7B** | 75.4 | 68.6 | 59.3 | 89.4 | 78.1 | 71.9 |", "caption": "Table 2: Performance comparisons on VideoRefer-BenchQQ{}^{\\text{Q}}start_FLOATSUPERSCRIPT Q end_FLOATSUPERSCRIPT. Note:\nVideo-level specialist models, including Elysium\u00a0[41] and Artemis\u00a0[32], do not have the ability to handle multi-choice questions on VideoRefer-BenchQQ{}^{\\text{Q}}start_FLOATSUPERSCRIPT Q end_FLOATSUPERSCRIPT.", "description": "This table presents a comparison of different models' performance on the VideoRefer-BenchQ benchmark, specifically focusing on the accuracy of answering multiple-choice questions related to videos.  It shows the average performance across five question types: Basic Questions, Sequential Questions, Relationship Questions, Reasoning Questions, and Future Predictions.  Note that certain video-specialized models (Elysium and Artemis) were not included in the comparison because they lack the capability to handle multiple-choice questions.", "section": "3.3 VideoRefer-Bench"}, {"content": "| Method | BLEU@4 | METEOR | ROUGE_L | CIDER | SPICE |\n|---|---|---|---|---|---| \n| Merlin [45] | 3.3 | 11.3 | 26.0 | 10.5 | 20.1 |\n| Artemis [32] | 15.5 | 18.0 | 40.8 | 53.2 | 25.4 |\n| **VideoRefer** | **16.5** | **18.7** | **42.4** | **68.6** | **28.3** |", "caption": "Table 3: Exprimental results on video-based referring metrics on the HC-STVG\u00a0[38] test set.", "description": "This table presents a comparison of the performance of different models on video-based referring tasks, specifically using the HC-STVG [38] test set.  The metrics used to evaluate the models' performance are BLEU@4, METEOR, ROUGE-L, CIDEr, and SPICE, all commonly used in evaluating the quality of generated descriptions for videos. The table allows for a direct comparison of how well different models generate descriptions that align with the visual content of the videos, highlighting the relative strengths and weaknesses of each model on various aspects of video captioning.", "section": "4.2.1 Video Referring Tasks"}, {"content": "| Method | Perception-Test | MVBench | VideoMME |\n|---|---|---|---|\n| VideoLLaMA2 [9] | 51.4 | 54.6 | 47.9/50.3 |\n| VideoLLaMA2.1 [9] | 54.9 | 57.3 | 54.9/56.4 |\n| Artemis [32] | 47.1 | 34.1 | 28.8/35.3 |\n| **VideoRefer** | **56.3** | **59.6** | **55.9/57.6** |", "caption": "Table 4: Exprimental results on general video understanding tasks.", "description": "This table presents the performance comparison of different models on general video understanding tasks.  It shows the accuracy scores achieved by various models on three established benchmarks: Perception-Test, MVBench, and VideoMME. The results highlight the relative strengths and weaknesses of each model in terms of its overall video comprehension capabilities.", "section": "4.2 Main Results"}, {"content": "| Mode | VideoRefer-Bench<sup>D</sup> |  |  | VideoRefer-Bench<sup>Q</sup> |  |  |  |\n|---|---|---|---|---|---|---|---|\n| **Mode** | TD | HD | Avg. | SQ | RQ | Avg. |  |\n| **Single-frame** | 3.03 | 2.97 | 3.42 | 68.3 | 59.1 | 71.9 |  |\n| **Multi-frame** | 3.10 | 3.04 | 3.46 | 70.6 | 60.5 | 72.1 |  |", "caption": "Table 5: Results using different modes during the inference. Here, SQ and RQ are Sequential Questions and Relationship Questions.", "description": "This table presents a comparison of the model's performance using single-frame and multi-frame modes during inference.  The metrics evaluated include average performance across all questions, along with a breakdown of scores for Sequential Questions (SQ) and Relationship Questions (RQ). This allows for an analysis of how the model's ability to understand temporal relationships and object interactions changes depending on the input type (single frame vs. multiple frames).", "section": "3.3 VideoRefer-Bench"}, {"content": "| Method | Bench<sup>D</sup> | Bench<sup>Q</sup> | MVBench |\n|---|---|---|---| \n| **w/o Regional data** | \u2013 | \u2013 | 57.9 |\n| + Short description | 2.43 | 68.3 | 58.0 |\n| + QA | 2.45 | 71.7 | 58.4 |\n| + Detailed description | 3.42 | 71.9 | 59.6 |", "caption": "Table 6: Ablation results on various data types in VideoRefer-700K dataset. Bench denotes VideoRefer-Bench for simplicity.", "description": "This table presents the ablation study results, showing how different types of data in the VideoRefer-700K dataset affect the performance of the VideoRefer model on the VideoRefer-Bench benchmark.  Specifically, it compares the model's performance when trained only on short captions, short captions plus question-answer pairs (QA), and short captions plus QA plus detailed descriptions. The results demonstrate the incremental improvements in performance as more comprehensive data is included in the training process.", "section": "3.3 VideoRefer-Bench"}, {"content": "| Union | VideoRefer-Bench<sup>D</sup> | VideoRefer-Bench<sup>Q</sup> |\n|---|---|---|\n| u |  |  |\n| TD | HD | SQ | RQ |\n| 32 | 3.17 | 3.01 | 68.7 | 58.1 |\n| 16 | **3.20** | 2.99 | 69.3 | 58.5 |\n| 8 | 3.18 | 3.02 | 69.6 | 57.8 |\n| 4 | 3.10 | **3.04** | **70.6** | 60.5 |\n| 1 | 3.08 | 2.98 | 68.9 | **60.9** |", "caption": "Table 7: Temporal and sequential performance comparisons for various union u\ud835\udc62uitalic_u in the TTM module under multi-frame mode.", "description": "This table presents a comparison of temporal and sequential performance metrics achieved by the VideoRefer model using different numbers of unions (u) in the Temporal Token Merge (TTM) module, under a multi-frame setting.  The results demonstrate how varying the number of unions affects the model's ability to capture temporal relationships and generate accurate descriptions.  Specifically, it shows the scores for Temporal Description (TD), Hallucination Detection (HD), Sequential Questions (SQ), and Relationship Questions (RQ) tasks for several values of u.", "section": "3.3 VideoRefer-Bench"}, {"content": "| | Manually True | Manually False |\n|---|---|---|\n| **Reviewer True** | 88 (TP) | 12 (FP) |\n| **Reviewer False** | 36 (FN) | 64 (TN) |", "caption": "Table 8: Confusion matrix of the randomly sampled 100 items in the Reviewer evaluation.", "description": "This table presents the results of a manual evaluation assessing the performance of the Reviewer component within a multi-agent data engine.  The Reviewer's task is to verify the accuracy and relevance of object-level captions and masks generated by other components in the pipeline. The confusion matrix shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) based on a random sample of 100 items (50 where the Reviewer flagged the data as correct and 50 where the Reviewer flagged the data as incorrect). This allows for the calculation of precision, recall, F1-score and accuracy metrics to evaluate the Reviewer's effectiveness.", "section": "C.1 Human Evaluation on Reviewer"}]