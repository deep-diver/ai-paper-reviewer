{"references": [{"fullname_first_author": "Zesen Cheng", "paper_title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms", "publication_date": "2024-06-07", "reason": "This paper is foundational to the current work, providing the VideoLLaMA2 model upon which the VideoRefer model is built and significantly advancing spatial-temporal video understanding."}, {"fullname_first_author": "Tsai-Shien Chen", "paper_title": "Panda-70M: Captioning 70M videos with multiple cross-modality teachers", "publication_date": "2024-00-00", "reason": "This paper provides the Panda-70M dataset, which is a crucial component of the VideoRefer-700K dataset and allows the current work to train and evaluate the VideoRefer model on high-quality object-level video instruction data."}, {"fullname_first_author": "Zhe Chen", "paper_title": "How far are we to gpt-4v? Closing the gap to commercial multimodal models with open-source suites", "publication_date": "2024-04-16", "reason": "This paper introduces InternVL2-26B, a significant model utilized in the VideoRefer suite's multi-agent data engine, crucial for generating high-quality object-level video instruction data."}, {"fullname_first_author": "Henghui Ding", "paper_title": "Mevis: A large-scale benchmark for video segmentation with motion expressions", "publication_date": "2023-00-00", "reason": "This paper introduces the MeViS dataset, which serves as a crucial component of the VideoRefer-700K dataset, enhancing the quality and diversity of the object-level video instruction data."}, {"fullname_first_author": "Jordi Pont-Tuset", "paper_title": "The 2017 davis challenge on video object segmentation", "publication_date": "2017-04-00", "reason": "This paper introduces the DAVIS-2017 dataset, which is part of the VideoRefer-Bench dataset, enabling comprehensive evaluation of the VideoRefer model's performance across various video referring tasks."}]}