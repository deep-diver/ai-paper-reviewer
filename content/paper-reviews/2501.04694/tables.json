[{"content": "| Model | Base Model | HumanEval Base | HumanEval Plus | MBPP Base | MBPP Plus | Average |\n|---|---|---|---|---|---|---|\n| GPT-4-Turbo (April 2024) | - | 90.2 | 86.6 | 85.7 | 73.3 | 84.0 |\n| GPT-4 (May 2023) | - | 88.4 | 79.3 | - | - | - |\n| GPT-3.5-Turbo (Nov 2023) | - | 76.8 | 70.7 | 82.5 | 69.7 | 75.0 |\n| claude-3-opus (Mar 2024) | - | 82.9 | 77.4 | 89.4 | 73.3 | 80.8 |\n| claude-3-sonnet (Mar 2024) | - | 70.7 | 64.0 | 83.6 | 69.3 | 71.9 |\n| claude-3-haiku (Mar 2024) | - | 76.8 | 68.9 | 80.2 | 68.8 | 73.7 |\n| Qwen2.5-Coder-32B-Instruct | - | 92.1 | 87.2 | 90.5 | 77.0 | 86.7 |\n| DeepSeek-Coder-V2-Instruct | - | 85.4 | 82.3 | 89.4 | 75.1 | 83.1 |\n| OpenCoder-8B-Instruct | - | 81.7 | 77.4 | 82.0 | 71.4 | 78.1 |\n| DeepSeek-Coder-33B-instruct | - | 81.1 | 75.0 | 80.4 | 70.1 | 76.7 |\n| Codestral-22B-v0.1 | - | 79.9 | 73.8 | 72.5 | 61.9 | 72.0 |\n|  ~ 7B Scale |  |  |  |  |  |  |\n| DSCoder-6.7B-Base | - | 47.6 | 39.6 | 72.0 | 58.7 | 54.5 |\n| DeepSeekCoder-6.7b-Instruct | https://arxiv.org/html/2501.04694/figure/deepseek.png | 74.4 | 71.3 | 74.9 | 65.6 | 71.6 |\n| Magicoder-S-DS | https://arxiv.org/html/2501.04694/figure/deepseek.png | 76.8 | 71.3 | 79.4 | 69.0 | 74.1 |\n| WaveCoder-Ultra-6.7B | https://arxiv.org/html/2501.04694/figure/deepseek.png | 75.0 | 69.5 | 74.9 | 63.5 | 70.7 |\n| OpenCodeInterpreter-DS-6.7B | https://arxiv.org/html/2501.04694/figure/deepseek.png | 77.4 | 72.0 | 76.5 | 66.4 | 73.1 |\n| **EpiCoder-DS-6.7B** | https://arxiv.org/html/2501.04694/figure/deepseek.png | **80.5** | **76.8** | **81.5** | 68.3 | **76.8** |\n| Qwen2.5-Coder-7B-Base | https://arxiv.org/html/2501.04694/figure/qwen2.png | 61.6 | 53.0 | 76.9 | 62.9 | 63.6 |\n| Qwen2.5-Coder-7B-Instruct | https://arxiv.org/html/2501.04694/figure/qwen2.png | 88.4 | **84.1** | 83.5 | **71.7** | **81.9** |\n| **EpiCoder-Qwen-7B** | https://arxiv.org/html/2501.04694/figure/qwen2.png | **89.0** | 82.3 | **84.1** | 71.4 | 81.7 |", "caption": "Table 1: Pass@1 (%) results of different LLMs on HumanEval (+) and MBPP (+) computed with greedy decoding. We report the results uniformly from the EvalPlus Leaderboard 444https://evalplus.github.io/leaderboard.html.", "description": "This table presents the pass@1 scores, a metric representing the percentage of correctly solved problems, achieved by various Large Language Models (LLMs) on two widely used code generation benchmarks: HumanEval and MBPP.  The results are obtained using greedy decoding, a method of generating text. The '+' symbol indicates an enhanced version of the benchmarks, likely including additional test cases or improved evaluation processes. The data is compiled from the EvalPlus Leaderboard, ensuring consistency and comparability across different LLMs.", "section": "3.2 Function-level Generation"}, {"content": "| Model | Base | BigCodeBench-Full Complete | BigCodeBench-Full Instruct | BigCodeBench-Hard Complete | BigCodeBench-Hard Instruct | Avg |\n|---|---|---|---|---|---|---|\n| **Closed-source Model** |\n| GPT-4o (May 2024) | - | 61.1 | 51.1 | 29.1 | 25.0 | 41.6 |\n| DeepSeek-V2-Chat (June 2024) | - | 59.4 | 48.9 | 32.4 | 25.0 | 41.4 |\n| Claude-3.5-Sonnet (June 2024) | - | 58.6 | 46.8 | 33.1 | 25.7 | 41.1 |\n| **7B+ Scale** |\n| Qwen2.5-Coder-32B-Instruct | - | 58.0 | 49.0 | 33.8 | 27.7 | 42.1 |\n| DeepSeek-Coder-V2-Instruct | - | 59.7 | 48.2 | 29.7 | 24.3 | 40.5 |\n| Llama-3.3-70B-Instruct | - | 57.5 | 46.9 | 28.4 | 28.4 | 40.3 |\n| Codestral-22B-v0.1 | - | 52.5 | 41.8 | 24.3 | 16.9 | 33.9 |\n| DeepSeek-Coder-33B-Instruct | - | 51.1 | 42.0 | 20.9 | 17.6 | 32.9 |\n| OpenCoder-8B-Instruct | - | 50.9 | 43.2 | 18.9 | 18.2 | 32.8 |\n| **\u223c 7B Scale** |\n| ![](https://arxiv.org/html/2501.04694/figure/deepseek.png) DSCoder-6.7B-Base | - | 41.8 | - | 13.5 | - | - |\n| DeepSeekCoder-6.7b-Instruct | ![](https://arxiv.org/html/2501.04694/figure/deepseek.png) | 43.8 | 35.5 | 15.5 | 10.1 | 26.2 |\n| Magicoder-S-DS | ![](https://arxiv.org/html/2501.04694/figure/deepseek.png) | 47.6 | 36.2 | 12.8 | 13.5 | 27.5 |\n| WaveCoder-Ultra-6.7B | ![](https://arxiv.org/html/2501.04694/figure/deepseek.png) | 43.7 | 33.9 | 16.9 | 12.8 | 26.8 |\n| OpenCodeInterpreter-DS-6.7B | ![](https://arxiv.org/html/2501.04694/figure/deepseek.png) | 44.6 | 37.1 | 16.9 | 13.5 | 28.0 |\n| **EpiCoder-DS-6.7B** | ![](https://arxiv.org/html/2501.04694/figure/deepseek.png) | **50.6** | **37.9** | **19.6** | 12.8 | **30.2** |\n| ![](https://arxiv.org/html/2501.04694/figure/qwen2.png) Qwen2.5-Coder-7B-Base | - | 45.8 | - | 16.2 | - | - |\n| Qwen2.5-Coder-7B-Instruct | ![](https://arxiv.org/html/2501.04694/figure/qwen2.png) | 48.8 | 40.4 | 20.3 | 20.9 | 32.6 |\n| **EpiCoder-Qwen-7B** | ![](https://arxiv.org/html/2501.04694/figure/qwen2.png) | **51.9** | **43.8** | **27.7** | **22.3** | **36.4** |", "caption": "Table 2: Pass@1 (%) results of different LLMs on BigCodeBench computed with greedy decoding. We conducted the evaluation on the Full and Hard subsets of this benchmark, including the Complete and Instruct tasks. Except for the results underlined, which are sourced from their respective papers, all other results are obtained from the BigCodeBench-Leaderboard666https://huggingface.co/spaces/bigcode/bigcodebench-leaderboard.", "description": "This table presents the pass@1 scores achieved by various Large Language Models (LLMs) on the BigCodeBench benchmark. BigCodeBench is a comprehensive benchmark designed to evaluate code generation capabilities across various programming tasks and domains.  The evaluation was performed using greedy decoding and focused on both the 'Full' and 'Hard' subsets of the benchmark, which include 'Complete' and 'Instruct' tasks. The table highlights the performance differences between various LLMs, showcasing the relative strengths and weaknesses of each model. Scores not directly obtained from the BigCodeBench leaderboard (underlined in the table) were taken from the respective LLMs' original papers, ensuring consistency in evaluation methodology.", "section": "3.2 Function-level Generation"}, {"content": "| Model | Difficult | Creative | Subtle | Combine | Tool Use | Avg |\n|---|---|---|---|---|---|---|\n| **Closed-source Model** |  |  |  |  |  |  |\n| GPT-4-Turbo | 50.0 | 61.0 | 82.0 | 45.0 | 69.0 | 61.4 |\n| GPT-4 | 52.0 | 66.0 | 76.0 | 53.0 | 68.0 | 63.0 |\n| Claude-3 | 50.0 | 53.0 | 81.0 | 42.0 | 69.0 | 59.0 |\n| ChatGPT | 33.0 | 42.0 | 70.0 | 33.0 | 64.0 | 48.4 |\n| Claude-3-haiku | 40.0 | 47.0 | 65.0 | 17.0 | 56.0 | 45.0 |\n| **7B+ Scale** |  |  |  |  |  |  |\n| DeepSeekCoder-33b-Instruct | 47.0 | 47.0 | 67.0 | 31.0 | 66.0 | 51.6 |\n| WizardCoder-33b-1.1 | 48.0 | 48.0 | 66.0 | 20.0 | 64.0 | 49.2 |\n| CodeLlama-70b-Instruct | 31.0 | 41.0 | 65.0 | 18.0 | 65.0 | 44.0 |\n| OpenCoder-8B-Instruct | 45.0 | 50.0 | 73.0 | 28.0 | 50.0 | 49.2 |\n| \u223c 7B Scale |  |  |  |  |  |  |\n| DeepSeek-Coder-6.7B-base | 21.0 | 24.0 | 47.0 | 5.0 | 55.0 | 30.4 |\n| DeepSeekCoder-6.7b-Instruct | 40.0 | 37.0 | 61.0 | 18.0 | 51.0 | 41.4 |\n| Magicoder-S-DS-6.7B | 40.0 | 34.0 | 67.0 | 21.0 | 61.0 | 44.6 |\n| WaveCoder-Ultra-6.7B | 38.0 | 42.0 | 71.0 | 24.0 | 35.0 | 42.0 |\n| OpenCodeInterpreter-DS-6.7B | 43.0 | 37.0 | 65.0 | 25.0 | 51.0 | 44.2 |\n| **EpiCoder-DS-6.7B** | 40.0 | 45.0 | 70.0 | 30.0 | 65.0 | 50.0 |\n| Qwen2.5-Coder-7B-Base | 35.0 | 20.0 | 55.0 | 27.0 | 41.0 | 35.6 |\n| Qwen2.5-Coder-7B-Instruct | 48.0 | 49.0 | 77.0 | 37.0 | 65.0 | 55.2 |\n| **EpiCoder-Qwen-7B** | 53.0 | 48.0 | 78.0 | 47.0 | 68.0 | 58.8 |", "caption": "Table 3: Pass@1 (%) results of different LLMs on EvoEval computed with greedy decoding.", "description": "This table presents the pass@1 scores achieved by various Large Language Models (LLMs) on the EvoEval benchmark. EvoEval is a challenging code generation benchmark that tests a model's ability to generalize across different coding tasks (difficult, creative, subtle, combined, and tool-use).  The results illustrate the relative performance of each LLM in handling the complexity and diversity inherent in these tasks, showcasing strengths and weaknesses in code generation capabilities.", "section": "3.2 Function-level Generation"}, {"content": "| Model | BP | AP | SE | DP | MA | DW | ML | SC | DB | MM | OS | Others | Overall |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Close-Sourced API Model |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| OpenAI o1-preview | 55.56 | 78.61 | 64.29 | 76.80 | 79.14 | 18.75 | 51.28 | 61.76 | 40.00 | 47.37 | 100.00 | 74.47 | 66.47 |\n| OpenAI o1-mini | 72.22 | 75.62 | 50.00 | 76.00 | 80.58 | 28.75 | 56.41 | 56.62 | 40.00 | 57.89 | 100.00 | 72.34 | 66.23 |\n| Claude-35-Sonnet | 50.00 | 75.62 | 71.43 | 76.00 | 76.26 | 13.75 | 51.28 | 61.76 | 50.00 | 63.16 | 100.00 | 78.72 | 65.52 |\n| GPT 4o-0806 | 72.22 | 72.14 | 53.57 | 78.40 | 76.98 | 21.25 | 66.67 | 55.15 | 40.00 | 68.42 | 100.00 | 72.34 | 65.05 |\n| Doubao-Coder-Preview | 55.56 | 69.65 | 50.00 | 77.60 | 75.54 | 27.50 | 51.28 | 60.29 | 20.00 | 63.16 | 50.00 | 55.32 | 62.91 |\n| DeepSeek-v2.5 | 55.56 | 68.16 | 50.00 | 76.00 | 76.26 | 20.00 | 48.72 | 56.62 | 40.00 | 63.16 | 50.00 | 65.96 | 61.85 |\n| Qwen-Max | 50.00 | 70.15 | 39.29 | 77.60 | 72.66 | 13.75 | 56.41 | 57.35 | 30.00 | 47.37 | 50.00 | 63.83 | 60.78 |\n| GLM-4-Plus | 55.56 | 65.67 | 39.29 | 76.80 | 74.82 | 13.75 | 58.97 | 50.00 | 40.00 | 52.63 | 100.00 | 53.19 | 58.77 |\n| 20B+ Instruction Tuned Coder |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| DeepSeekCoder-v2-Instruct | 55.56 | 68.66 | 35.71 | 81.60 | 79.14 | 16.25 | 48.72 | 53.68 | 40.00 | 52.63 | 50.00 | 57.45 | 61.26 |\n| Qwen2.5-Coder-32B-Instruct | 50.00 | 70.15 | 50.00 | 77.60 | 66.19 | 17.50 | 61.54 | 43.38 | 30.00 | 47.37 | 100.00 | 61.70 | 58.41 |\n| DeepSeekCoder-33B-Instruct | 50.00 | 59.70 | 21.43 | 71.20 | 48.92 | 18.75 | 48.72 | 40.44 | 30.00 | 42.11 | 50.00 | 44.68 | 49.05 |\n| CodeLlama-34B-Instruct | 5.56 | 22.89 | 14.29 | 40.00 | 17.27 | 16.25 | 15.38 | 18.38 | 30.00 | 26.32 | 0.00 | 23.40 | 22.27 |\n| 13B+ Instruction Tuned Coder |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen2.5-Coder-14B-Instruct | 55.56 | 62.69 | 32.14 | 76.00 | 70.50 | 18.75 | 53.85 | 38.97 | 30.00 | 57.89 | 100.00 | 55.32 | 55.57 |\n| DeepSeekCoder-v2-Lite-Instruct | 50.00 | 64.68 | 32.14 | 64.00 | 56.12 | 26.25 | 43.59 | 33.82 | 60.00 | 21.05 | 50.00 | 53.19 | 50.47 |\n| StarCoder2-15B-Instruct-v0.1 | 61.11 | 44.28 | 32.14 | 63.20 | 36.69 | 31.25 | 53.85 | 28.68 | 60.00 | 36.84 | 50.00 | 53.19 | 43.01 |\n| CodeLlama-13B-Instruct | 11.11 | 22.39 | 25.00 | 24.00 | 20.86 | 30.00 | 20.51 | 13.97 | 40.00 | 10.53 | 50.00 | 23.40 | 21.56 |\n| 6B+ Instruction Tuned Coder |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Qwen2.5-Coder-7B-Instruct | 33.33 | 58.21 | 39.29 | 66.40 | 48.92 | 18.75 | 38.46 | 32.35 | 40.00 | 47.37 | 50.00 | 59.57 | 47.51 |\n| Yi-Coder-9B-Chat | 61.11 | 50.25 | 32.14 | 66.40 | 46.76 | 26.25 | 43.59 | 36.76 | 50.00 | 36.84 | 50.00 | 48.94 | 46.56 |\n| DeepSeek-Coder-7B-Instruct-v1.5 | 50.00 | 51.74 | 25.00 | 64.80 | 37.41 | 25.00 | 30.77 | 34.56 | 20.00 | 52.63 | 50.00 | 48.94 | 43.60 |\n| OpenCoder-8B-Instruct | 44.44 | 53.73 | 28.57 | 57.60 | 35.97 | 26.25 | 28.21 | 28.68 | 0.00 | 47.37 | 0.00 | 44.68 | 41.11 |\n| DeepSeek-Coder-6.7B-Instruct | 61.11 | 49.75 | 28.57 | 65.60 | 38.13 | 18.75 | 38.46 | 22.79 | 30.00 | 31.58 | 50.00 | 42.55 | 40.88 |\n| CodeQwen1.5-7B-Chat | 38.89 | 45.77 | 50.00 | 58.40 | 31.65 | 15.00 | 33.33 | 22.79 | 20.00 | 31.58 | 0.00 | 42.55 | 37.20 |\n| CodeLlama-7B-Instruct | 27.78 | 23.88 | 25.00 | 28.00 | 20.86 | 23.75 | 10.26 | 11.76 | 50.00 | 10.53 | 0.00 | 21.28 | 21.33 |\n| **EpiCoder-DS-6.7B** | **61.11** | **47.26** | **25.00** | **61.60** | **41.01** | **40.00** | **41.03** | **27.21** | **50.00** | **36.84** | **50.00** | **42.55** | **43.25** |\n| **EpiCoder-Qwen-7B** | **44.44** | **61.19** | **17.86** | **72.80** | **61.15** | **28.75** | **51.28** | **27.94** | **20.00** | **47.37** | **50.00** | **40.43** | **50.24** |", "caption": "Table 4: Model performance across domains of Python in the English Subset of FullStackBench.", "description": "This table presents a comprehensive evaluation of various large language models (LLMs) on the FullStackBench benchmark, specifically focusing on their performance in different domains of Python programming within the English subset of the benchmark.  It assesses the models' capabilities across a diverse range of tasks and programming styles, providing a detailed breakdown of their performance in various sub-domains.  The results offer insights into the strengths and weaknesses of each model, highlighting their proficiency in handling diverse programming challenges.", "section": "3.2 Function-level Generation"}, {"content": "| Dataset | Unique Operators | Unique Operands | Total Operators | Total Operands |\n|---|---|---|---|---|\n| Code Alpaca [Chaudhary (2023)] | 4.83 | 8.22 | 10.66 | 15.89 |\n| Evol CodeAlpaca [Luo et al. (2023)] | 7.94 | 18.97 | 29.91 | 46.70 |\n| CodeFeedBack [Zheng et al. (2024b)] | 8.11 | 20.42 | 30.98 | 50.05 |\n| OSS Instruct [Wei et al. (2024b)] | 7.44 | 20.99 | 28.05 | 47.55 |\n| Ours (func-level) | 10.66 | 44.32 | 56.98 | 100.36 |\n| Ours (file-level) | 11.64 | 72.87 | 100.24 | 179.98 |", "caption": "Table 5: Comparison of Halstead complexity between ours and existing codebase.", "description": "This table presents a comparison of Halstead complexity metrics between the synthetic code data generated by the proposed method and several existing code datasets.  The Halstead complexity metrics used are: unique operators (n1), unique operands (n2), total operators (N1), and total operands (N2).  The table highlights the differences in code complexity between the newly generated dataset and other datasets, showing that the proposed method generates code with significantly higher Halstead complexity, indicating a greater level of complexity and potentially a higher level of difficulty in the generated code.", "section": "4.1 Complexity Evaluation"}, {"content": "| Dataset | Mean | Median | Std |\n|---|---|---|---| \n| Code Alpaca | 0.18 | 0.00 | **0.52** |\n| Evol CodeAlpaca | 0.82 | 0.00 | **1.63** |\n| CodeFeedBack | 0.97 | 0.00 | 2.09 |\n| OSS Instruct | 1.50 | 1.00 | 2.19 |\n| Ours (func-level) | **4.95** | **4.00** | 3.77 |\n| Ours (file-level) | **5.41** | **4.00** | 3.85 |", "caption": "Table 6: Comparison of Strictness complexity (left) and Cyclomatic complexity (right).", "description": "This table presents a comparison of code complexity metrics, specifically Strictness and Cyclomatic complexity, across several datasets.  Strictness Complexity measures how strictly the code adheres to a single execution path, while Cyclomatic Complexity assesses the control flow complexity, indicating the number of linearly independent paths through the code. By comparing these metrics across different datasets (Code Alpaca, Evol CodeAlpaca, CodeFeedBack, OSS Instruct, and the authors' own function-level and file-level datasets), the table allows for an evaluation of the relative complexity of the code generated by each method.  The use of median and standard deviation provide a robust statistical analysis of the complexity scores.", "section": "4.1 Complexity Evaluation"}, {"content": "| Dataset | Mean | Median | Std |\n|---|---|---|---| \n| Code Alpaca | 2.10 | 1.00 | 1.66 |\n| Evol CodeAlpaca | 3.76 | 3.00 | 3.48 |\n| CodeFeedBack | 3.96 | 3.00 | 3.33 |\n| OSS Instruct | 3.45 | 3.00 | 2.98 |\n| Ours (func-level) | 5.14 | 5.00 | 3.01 |\n| Ours (file-level) | 14.93 | 14.00 | 6.73 |", "caption": "Table 7: Comparison of code complexity across four dimensions using GPT-4o.", "description": "This table presents a quantitative comparison of code complexity across four key dimensions: Error Handling, Modularity, Dependency, and Data Structure.  The complexity of code samples from different datasets is evaluated using GPT-40, a large language model, which assigns a score to each sample based on predefined standards for each dimension.  Higher scores indicate greater complexity.", "section": "4.1 Complexity Evaluation"}, {"content": "| Dataset | Error Handling | Modularity | Dependency | Data Structure | Avg. |\n|---|---|---|---|---|---| \n| Code Alpaca | 2.04 | 2.10 | 2.09 | 2.38 | 2.15 |\n| Evol CodeAlpaca | 2.53 | 3.32 | 2.66 | 3.58 | 3.02 |\n| CodeFeedBack | 2.71 | 3.47 | 2.23 | 3.75 | 3.04 |\n| OSS Instruct | 2.74 | 3.79 | 2.78 | 3.92 | 3.31 |\n| Ours (func-level) | 4.11 | 4.71 | 3.83 | 4.90 | 4.39 |\n| Ours (file-level) | 4.23 | 5.94 | 4.62 | 5.41 | 5.05 |", "caption": "Table 8: Distribution of unique features.", "description": "This table presents a quantitative analysis of the diversity of features extracted from different code datasets using a large language model (LLM).  It breaks down the number of unique features found across various categories, such as workflow, implementation style, functionality, resource usage, and data processing, offering insights into the richness and variety of the code samples represented in each dataset.  This analysis is crucial for evaluating the quality and representativeness of the training data used to train large language models (LLMs) for code generation.", "section": "4.2 Diversity Evaluation"}, {"content": "| Datasets | Workflow | Implementation | Style | Functionality | Resource | Usage | Computation | Operation | Security | User | Interaction | Data | Processing | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Alpaca | 994 | 6 | 393 | 7 | 282 | 8 | 82 | 221 |  | 11 | 54 | 1 | 43 | 2.48 |\n| CodeFeedback | 2079 | 6 | 535 | 18 | 689 | 48 | 143 | 895 |  | 39 | 229 | 10 | 121 | 5.45 |\n| Evol-Alpaca | 2163 | 11 | 591 | 21 | 783 | 60 | 134 | 1401 |  | 55 | 212 | 15 | 226 | 6.38 |\n| OSS-Instruct | 2254 | 5 | 669 | 39 | 413 | 49 | 192 | 903 |  | 102 | 211 | 62 | 238 | 5.54 |\n| Ours (func-level) | 2422 | 6 | 657 | 37 | 819 | 156 | 363 | 2533 |  | 203 | 357 | 96 | 305 | 8.53 |\n| Ours (file-level) | 2475 | 11 | 812 | 43 | 536 | 103 | 800 | 2196 |  | 387 | 311 | 218 | 447 | 8.95 |", "caption": "Table 9: Comparison of Test Functions and Test Cases before and after augmentation for 930 data samples.", "description": "This table presents a quantitative comparison of the number of test functions and test cases in the XFileDep benchmark dataset before and after data augmentation.  It shows the total counts, averages per sample, and maximum counts found within individual files. The augmentation process significantly increased both the number of test functions and test cases, improving the overall coverage and robustness of the benchmark.", "section": "B.1 Cross-File Dependency Benchmark"}, {"content": "| Implementation | Style |\n|---|---|", "caption": "Table 10: The index of the data samples presented in the case study.", "description": "This table shows the indices of specific data samples used in a case study on data leakage analysis.  The case study examines the similarity between samples from the HumanEval benchmark dataset and the evol-codealpaca-v1 dataset.  The table presents four similarity scores (99%, 95%, 90%, and 85%) and lists the corresponding indices from both datasets for each score, illustrating the degree of similarity between the benchmark and training data at various levels.", "section": "C.1 Leakage Threshold Setting"}, {"content": "| Resource | Usage |\n|---|---|", "caption": "Table 11: Derived Halstead metrics. These metrics are derived from unique operators (n1subscript\ud835\udc5b1n_{1}italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT), unique operands (n2subscript\ud835\udc5b2n_{2}italic_n start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT), total operators (N1subscript\ud835\udc411N_{1}italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT), and total operands (N2subscript\ud835\udc412N_{2}italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT).", "description": "This table presents a detailed comparison of Halstead complexity metrics across different code datasets.  Halstead metrics quantify software complexity based on counts of unique and total operators and operands. The table shows the values for program length, vocabulary, volume, and difficulty for each dataset. This allows for a quantitative comparison of the complexity of different codebases, highlighting the relative complexity of the datasets.", "section": "4.1.1 Evaluation from the Software Engineering Perspective"}, {"content": "| Computation | Operation |\n|---|---|", "caption": "Table 12: Comparison of different control flow and logical operation frequencies.", "description": "This table presents a quantitative comparison of the frequencies of various control flow and logical operations across different code datasets.  It shows the counts of 'if', 'while', 'for', 'and', 'or', 'except', 'return', 'break', 'continue', and 'bool_op' statements within the code. The datasets compared include Code Alpaca, Evol Code Alpaca, CodeFeedBack, OSS Instruct, and the authors' own function-level and file-level datasets. The table helps to illustrate the differences in code complexity and style across different datasets, highlighting aspects like the use of loops, exception handling, and boolean logic.", "section": "4.1.1 Evaluation from the Software Engineering Perspective"}, {"content": "| User | Interaction |\n|---|---|", "caption": "Table 13: Detailed metrics of code strictness complexity", "description": "This table presents a detailed breakdown of code strictness complexity metrics across different datasets.  It goes beyond a simple count and examines various aspects related to code quality and rigor, such as exception handling, documentation (docstrings), input validation, type hinting, and assertion usage. This granular analysis allows for a more nuanced comparison of code quality across datasets, offering insights into the adherence to best practices and coding standards.  The values likely represent frequencies or percentages of these features in the code samples from each dataset.", "section": "4.1.1 Evaluation from the Software Engineering Perspective"}, {"content": "| Data | Processing |\n|---|---|", "caption": "Table 14: Distribution of total features across 1k samples.", "description": "This table presents a detailed breakdown of the feature diversity observed in 1,000 samples from various code datasets.  It compares the distribution of features across different categories, such as workflow, implementation, resource usage, and data processing, offering a quantitative assessment of the richness and variety of code characteristics represented within each dataset.  The datasets included are Alpaca, CodeFeedback, Evol-Alpaca, OSS-Instruct, and two versions of data generated by the authors' method (function-level and file-level).  This comparison highlights the relative complexity and diversity of code features within each data source, providing valuable insight for assessing the suitability of different datasets for training code generation models.", "section": "4.3 Diversity Evaluation"}]