[{"figure_path": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/teaser_fig.png", "caption": "Figure 1: Benchmark performance of \nEpiCoder-Qwen-7B\n(fine-tuned on Qwen2.5-Coder-7B-Base) and its counterparts. XFileDep is file-level code generation benchmark, all others are function-level.", "description": "Figure 1 presents a comparison of the performance of EpiCoder-Qwen-7B (a model fine-tuned using the Qwen2.5-Coder-7B-Base model) against several other code generation models across various benchmarks.  The benchmarks assess code generation capabilities at both the function level (where the model generates code for single functions) and the file level (where the model generates multiple files and handles dependencies between them).  XFileDep is a benchmark specifically designed for file-level code generation, while the other benchmarks evaluate function-level code generation performance. The figure displays the accuracy of each model on each benchmark, allowing for a direct comparison of their relative strengths and weaknesses in various code generation tasks.", "section": "3 Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.04694/x1.png", "caption": "Figure 2: Overview of our feature tree-based code generation framework, which consists of three steps: (a) Feature Tree Extraction, where we first extract the feature set to construct the tree structure demonstration and then extract the feature trees; (b) Feature Tree Evolution, where the feature tree is iteratively expanded in depth and breadth; and (c) Feature Tree-Based Code Generation, where the evolved feature tree is used to generate diverse code instruction data. A detailed example of feature evolution and code generation is shown in Appendix\u00a0A.", "description": "This figure illustrates the three main steps of the EpiCoder code generation framework: 1) Feature Tree Extraction: a feature set is extracted from raw code data to construct a tree structure demonstration, which then guides the extraction of feature trees that represent semantic relationships between code elements. 2) Feature Tree Evolution: the feature tree is iteratively expanded in both depth and breadth to enhance the diversity and quantity of extracted features. 3) Feature Tree-Based Code Generation: subtrees are sampled from the evolved feature tree to generate diverse code instruction data with varying complexity.  Appendix A provides a detailed example of feature evolution and code generation.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2501.04694/x2.png", "caption": "Figure 3: An example of file-level code generation (including test code file). Different files contain different functional modules, with dependencies existing across files.", "description": "This figure showcases an example of file-level code generation, demonstrating the framework's ability to produce more complex and realistic code.  The example includes multiple files, each responsible for a distinct functional module (e.g., scraper, parser, storage, search, optimizer), and illustrates how these modules interact with each other. The dependencies between these modules highlight the ability of the framework to handle intricate multi-file projects, a capability that surpasses the limitations of simpler, single-file code generation methods.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/xfiledep_barchart.png", "caption": "Figure 4: Pass@1 (%) results of different LLMs on XFileDep computed with greedy decoding.", "description": "This figure displays the performance of various Large Language Models (LLMs) on the XFileDep benchmark, a specialized evaluation metric designed to assess the ability of LLMs to generate code that handles cross-file dependencies.  The XFileDep benchmark goes beyond simpler function-level evaluations by testing the models' understanding of the interrelationships between multiple files within a project.  The chart visually compares the Pass@1 scores (the percentage of times the LLM correctly generated the needed code on the first attempt) for each model, illustrating their relative strengths in handling complex, multi-file code generation tasks using greedy decoding.", "section": "3.3 File-level Generation"}, {"figure_path": "https://arxiv.org/html/2501.04694/x3.png", "caption": "Figure 5: An example of our repo-level code generation. The left part shows the original LLaMA-Factory repository structure, the middle part presents the structure of LLMTune, which we generated based on the extracted feature tree, and the right part illustrates an example file from the generated repository.", "description": "Figure 5 showcases the EpiCoder model's capability for repository-level code generation.  The figure is a three-panel comparison. The left panel displays the original file structure of the LLaMA-Factory repository. The middle panel shows the structure of the LLMTune repository, which was generated by EpiCoder using its feature tree-based approach. The right panel provides a sample code file from this newly generated LLMTune repository to illustrate the synthesized code's characteristics. This demonstrates EpiCoder's ability to generate code that mimics real-world repository structures and complexity.", "section": "3.4 Potential Repo-level Generation"}, {"figure_path": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/data_leakage_dis.png", "caption": "Figure 6: The distribution of cosine similarity scores between different various datasets and the benchmark datasets HumanEval, MBPP, and BigCodeBench.", "description": "This figure displays the cosine similarity scores between the embeddings of various code datasets (including the authors' own datasets and other existing datasets) and three popular code generation benchmarks: HumanEval, MBPP, and BigCodeBench.  The distribution of these scores helps to visualize the degree of similarity between the training data and the benchmark datasets, providing insights into the potential for data leakage or overfitting.  A high degree of similarity between a training dataset and a benchmark suggests potential overfitting.", "section": "4.3 Data Leakage"}, {"figure_path": "https://arxiv.org/html/2501.04694/x4.png", "caption": "Figure 7: The scaling law of code instruction data. The results obtained from randomly sampled subsets of 380k data points across the HumanEval, MBPP, and BigCodeBench benchmarks.", "description": "This figure presents the scaling law observed in code instruction data.  It demonstrates how model performance, measured by Pass@1 accuracy on three widely-used benchmarks (HumanEval, MBPP, and BigCodeBench), improves as the size of the training dataset increases.  Data points were randomly sampled from a total of 380,000 data points to illustrate the relationship between dataset size and model accuracy. The graph shows that performance continues to improve even at larger dataset sizes, suggesting the data's diversity prevents overfitting.", "section": "4.4 Scaling of Code Instruction Data"}, {"figure_path": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/evol_process.png", "caption": "Figure 8: An example of feature evolution.", "description": "This figure illustrates the process of feature tree evolution in the EpiCoder code generation framework. Starting with an initial set of 5000 features, the tree is iteratively expanded both in depth (adding more specific sub-features to existing features) and breadth (adding new sibling features at the same level). After 9000 steps of evolution, the number of features increases significantly to 140,000.  The figure visually represents this growth using a tree-like structure, showing how the initial features branch out and evolve into a much larger and more diverse set of features suitable for generating diverse and complex code instructions.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/XFileDep_sankey.png", "caption": "Figure 9: The Sankey diagram for the creation of the XFileDep benchmark, with numbers indicating the quantity of data samples.", "description": "This Sankey diagram illustrates the process of constructing the XFileDep benchmark dataset. It starts with 35,000 initial cross-file data samples. After filtering for samples with at least 5 files and sufficient complexity, 2,934 samples remain.  Further filtering based on runtime and test requirements results in 2,231 samples. Test case augmentation expands this to 611 samples that pass the tests.  Finally, after iterative test refinement and unsafe filtering steps, 930 samples form the final XFileDep dataset.", "section": "B Appendix of Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.04694/extracted/6119347/figure/XFileDep_folder_stats.png", "caption": "Figure 10: the distribution of file quantities and the average file length for each data sample.", "description": "This figure shows two histograms. The left histogram displays the distribution of the number of files in each data sample used for the XFileDep benchmark after filtering, showing the prevalence of samples with varying numbers of files. The right histogram illustrates the distribution of the average file length (in characters) within each sample, providing insight into the size and complexity of the code files.", "section": "B.1 Cross-File Dependency Benchmark"}, {"figure_path": "https://arxiv.org/html/2501.04694/x5.png", "caption": "Figure 11: Cases from the HumanEval benchmark dataset (left) and the evol-codealpaca-v1 dataset (right) with varying similarity. The embeddings are computed based on the \"output\" portions of the training dataset and the \"prompt + canonical_solution\" of the HumanEval benchmark data.", "description": "This figure displays pairs of code snippets, one from the HumanEval benchmark dataset and the other from the evol-codealpaca-v1 dataset.  These pairs are selected based on their cosine similarity scores, calculated using embeddings generated from the 'output' sections of the training dataset and the 'prompt + canonical_solution' of the HumanEval dataset. The figure visually represents how similar the code generated by the evol-codealpaca-v1 model is to the canonical solutions in the HumanEval dataset, indicating potential data leakage issues. The varying similarity scores highlight the degrees of overlap between the datasets.", "section": "Data Leakage"}]