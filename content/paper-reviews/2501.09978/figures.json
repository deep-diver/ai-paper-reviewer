[{"figure_path": "https://arxiv.org/html/2501.09978/x2.png", "caption": "Figure 1: \nWe introduce GaussianAvatar-Editor, a method for text-driven editing of animatable Gaussian head avatars with fully controllable expression, pose, and viewpoint. We show qualitative results of our GaussianAvatar-Editor at the inference time above. Our edited avatars can achieve photorealistic editing results with strong spatial and temporal consistency.", "description": "This figure showcases the capabilities of GaussianAvatar-Editor, a novel method for modifying animatable 3D head models using text prompts. The top row displays a variety of editing tasks achieved through text-based instructions, demonstrating the system's ability to control expression, pose, and viewpoint.  The bottom row shows several more examples of avatar editing. The results highlight the system's ability to generate photorealistic and consistent edits across time and space, even for complex changes.  The figure provides compelling visual evidence of the system's capabilities.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09978/x3.png", "caption": "Figure 2: The overview of our method. We follow a render-edit-aggregate optimization pipeline as in Instruct-NeRF2NeRF\u00a0[9]. We introduce a Weighted Alpha Blending Equation (WABE) to overcome the motion occlusion problem and our novel loss functions to enhance the spatial-temporal consistency. Our edited avatars can generate high-quality and consistent 4D renderings and can be controlled by other actors.", "description": "This figure illustrates the GaussianAvatar-Editor pipeline.  It begins with an animatable Gaussian avatar, which is then rendered. This rendering is then passed to a 2D diffusion editor, allowing for text-based modifications. The core innovation is the Weighted Alpha Blending Equation (WABE), which addresses motion occlusion issues by weighting the contribution of visible and invisible Gaussian components.  Novel loss functions are applied to maintain spatial-temporal consistency throughout the animation process, ensuring high-quality and consistent results. The final output is an edited avatar capable of high-quality and consistent 4D rendering, controllable by other actors.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.09978/x4.png", "caption": "Figure 3: \nIllustration of the Weighted alpha blending equation (WABE), which is adjusted to suppress non-visible parts while enhancing visible parts. Lower left: results when WABE is disabled. Lower right: when WABE is enabled, motion-occluded regions like teeth and tongue can be successfully optimized.", "description": "This figure demonstrates the Weighted Alpha Blending Equation (WABE).  The left side shows the result of editing an animated 3D head model without WABE. Notice how the editing artifacts affect the occluded areas (teeth, tongue). The right side shows the improved result with WABE enabled. The algorithm successfully prioritizes visible areas during editing, leading to a more natural and accurate result, even in areas that were previously occluded.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.09978/x5.png", "caption": "Figure 4: Our results on novel view synthesis. We show our edited results using the text prompt \u201cTurn her into the Tolkien Elf\u201d.", "description": "This figure showcases the capabilities of the GaussianAvatar-Editor in generating novel views of edited avatars. Using the text prompt \"Turn her into the Tolkien Elf,\" the model transforms an original avatar into a version resembling a Tolkien elf. The results demonstrate the model's ability to produce photorealistic and consistent results across different viewpoints, highlighting its capacity for text-driven editing of animatable Gaussian head avatars.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09978/x6.png", "caption": "Figure 5: Comparison on novel view synthesis. Our method produces more high-quality and multi-view consistent results than baselines.", "description": "This figure compares novel view synthesis results from different methods, including the proposed approach and several baselines.  It demonstrates that the proposed method generates higher-quality images and maintains better consistency across multiple viewpoints compared to the other methods. Each column shows the rendered views from the same method, with the rows showing different viewpoints.  The visual differences highlight the improved quality and consistency achieved by the proposed approach.", "section": "4.3 4D Consistent Editing"}, {"figure_path": "https://arxiv.org/html/2501.09978/x7.png", "caption": "Figure 6: Our results on self-reenactment. Self-reenactment renders held-out unseen head pose and expressions from 16 training camera viewpoints. The bottom part shows the text prompts.", "description": "Figure 6 presents results demonstrating the model's self-reenactment capabilities.  It shows the original avatar and several edited versions generated using different text prompts.  Each row represents an edit based on a different prompt. The model is able to generate new head poses and expressions that were not present in the original training data.  Importantly, the edits are consistent across all 16 training camera viewpoints, indicating strong spatial and temporal consistency.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09978/x8.png", "caption": "Figure 7: Comparison of self-reenactment. Our edited avatar can correctly produce detailed facial features under unseen expressions and head poses from the same subject.", "description": "Figure 7 presents a comparison of self-reenactment results. It showcases the ability of the proposed GaussianAvatar-Editor to generate high-quality and consistent 4D renderings of head avatars, even under unseen expressions and head poses. The figure highlights the superior performance of the proposed method in producing detailed and realistic facial features compared to baseline methods, demonstrating its robustness and effectiveness in handling various scenarios.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09978/x9.png", "caption": "Figure 8: Our results on cross-identity reenactment. Cross-identity reenactment animates the avatar to render images with unseen head poses and expressions from sequences of a different actor. The bottom part shows the text prompts.", "description": "This figure demonstrates the capabilities of GaussianAvatar-Editor in cross-identity reenactment.  It shows how the system can animate a head avatar with unseen head poses and facial expressions from video sequences of a different person than the original avatar.  The top portion displays the results generated by the model, illustrating the ability to transfer the style and characteristics to a new identity.  The bottom portion provides examples of the text prompts used to drive the editing process, showcasing the text-to-image capability of the framework.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09978/x10.png", "caption": "Figure 9: Comparison of Cross-identity reenactment. Different edited avatars are controlled by the same source actor. Our method can render high-quality results with novel expressions, while baseline methods suffer from artifacts.", "description": "Figure 9 presents a comparison of cross-identity reenactment results, where different methods are used to edit avatars controlled by the same source actor. The goal is to generate novel facial expressions.  The figure showcases that the proposed method produces high-quality results with realistic and detailed facial features, unlike baseline methods which exhibit noticeable artifacts and distortions in the generated expressions.", "section": "5.2 Head Avatar Editing and Animation"}]