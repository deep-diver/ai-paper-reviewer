[{"content": "| Method | Apple |  | Block |  | Paper-windmill |  | Space-out |  |\n|---|---|---|---|---|---|---|---|---|\n| SC-GS [20] | 14.96 / 0.692 / 0.508 | 173.3 | 13.98 / 0.548 / 0.483 | 115.7 | 14.87 / **0.221** / 0.432 | 446.3 | **14.79** / 0.511 / **0.440** | 114.2 |\n| Deformable 3DGS [57] | <u>15.61</u> / <u>0.696</u> / **0.367** | 87.71 | <u>14.87</u> / <u>0.559</u> / **0.390** | 118.9 | <u>14.89</u> / 0.213 / **0.341** | 160.2 | 14.59 / 0.510 / <u>0.450</u> | <u>42.01</u> |\n| 4DGS [54] | 15.41 / 0.691 / 0.524 | <u>61.52</u> | 13.89 / 0.550 / 0.539 | <u>63.52</u> | 14.44 / 0.201 / 0.445 | 123.9 | 14.29 / <u>0.515</u> / 0.473 | 52.02 |\n| MoDec-GS (Ours) | **16.48** / **0.699** / <u>0.402</u> | **23.78** | **15.57** / **0.590** / <u>0.478</u> | **13.65** | **14.92** / <u>0.220</u> / <u>0.377</u> | **17.08** | <u>14.65</u> / **0.522** / 0.467 | **18.24** |", "caption": "Table 1: Quantitative results comparison on iPhone datasets [16]. Red and blue denote the best and second best performances, respectively. Each block element of 5-performance denotes (mPSNR(dB)\u2191\u2191\\uparrow\u2191 / mSSIM\u2191\u2191\\uparrow\u2191 / mLPIPS\u2193\u2193\\downarrow\u2193 \u2009 Storage(MB)\u2193\u2193\\downarrow\u2193).", "description": "Table 1 presents a quantitative comparison of different novel view synthesis (NVS) methods on the iPhone dataset [16].  The table evaluates the performance of each method across four metrics: mean Peak Signal-to-Noise Ratio (mPSNR) in decibels (dB), mean Structural Similarity Index (mSSIM), mean Learned Perceptual Image Patch Similarity (mLPIPS), and model storage size in Megabytes (MB). Higher mPSNR and mSSIM values indicate better visual quality, while a lower mLPIPS value signifies higher perceptual similarity to the ground truth. A smaller storage size is also desirable.  The best and second-best performing methods for each metric are highlighted in red and blue, respectively.  Each row represents a different NVS method, and each column shows the performance of that method on a specific video sequence in the iPhone dataset.", "section": "5. Experiments"}, {"content": "| Method | Spin |  | Teddy |  | Wheel |  | Average |  |\n|---|---|---|---|---|---|---|---|---|\n| SC-GS [20] | 14.32 / 0.407 / 0.445 | 219.1 | 12.51 / 0.516 / 0.562 | 318.7 | 11.90 / 0.354 / 0.484 | 239.2 | 13.90 / 0.464 / 0.479 | 232.4 |\n| Deformable 3DGS [57] | 13.10 / 0.392 / 0.490 | 133.9 | 11.20 / 0.508 / 0.573 | 117.1 | 11.79 / 0.345 / 0.394 | 106.1 | 13.72 / 0.461 / 0.430 | 109.4 |\n| 4DGS [54] | 14.89 / 0.413 / 0.441 | 71.80 | 12.31 / 0.509 / 0.605 | 80.44 | 10.83 / 0.339 / 0.538 | 96.50 | 13.72 / 0.460 / 0.509 | 78.54 |\n| MoDec-GS (Ours) | 15.53 / 0.433 / 0.366 | 26.84 | 12.56 / 0.521 / 0.598 | 12.28 | 12.44 / 0.374 / 0.413 | 16.68 | 14.60 / 0.480 / 0.443 | 18.37 |", "caption": "Table 2: Quantitative results comparison on (a) HyperNeRF [45] and (b) Nvidia monocular [58] dataset.", "description": "Table 2 presents a quantitative comparison of the proposed MoDec-GS model against other state-of-the-art methods for dynamic novel view synthesis.  It shows the performance on two datasets: (a) HyperNeRF [45], which is a complex dataset known for challenging dynamic scenes, and (b) the Nvidia monocular dataset [58], another widely-used benchmark for evaluating monocular video reconstruction. The metrics used for comparison include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and the model's storage size in Megabytes (MB).  Higher PSNR and SSIM values indicate better visual quality, while a lower LPIPS score signifies that the generated images are perceptually closer to the ground truth. A smaller storage size is preferred as it indicates computational efficiency.", "section": "5. Experiments"}, {"content": "| Methods | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | Storage \u2193 |\n|---|---|---|---|---|\n| SC-GS [CVPR\u201924] [20] | 26.95 | 0.815 | **0.213** | 226.0 |\n| Deformable 3DGS [CVPR\u201924] [57] | 25.96 | 0.766 | 0.294 | 87.13 |\n| 4DGS [CVPR\u201924] [54] | **27.44** | 0.797 | 0.302 | **72.65** |\n| Ours | **27.78** | **0.827** | **0.219** | **40.82** |", "caption": "Table 3: Ablation studies of the proposed methods. Yellow-green backgrounds highlight cases where the applying of the proposed method resulted in a noticeable reduction in storage.", "description": "This table presents the results of ablation studies conducted on the MoDec-GS model. It systematically evaluates the contribution of each component of the model to its overall performance, specifically focusing on storage size reduction.  Different model variants are compared, each omitting or modifying a specific component, such as anchor deformation, temporal interval adjustment, or the two-stage deformation process. The table shows the impact of these modifications on key metrics, highlighting the effectiveness of the proposed methods in achieving compact model sizes without sacrificing visual quality. Yellow-green shading indicates where the proposed method resulted in significantly reduced storage.", "section": "4. Proposed Method"}, {"content": "| Methods | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | Storage \u2193 |\n|---|---|---|---|---|\n| (b) Nvidia |\n| 4DGS [CVPR\u201924] [54] | 25.82 | 0.844 | 0.219 | 67.44 |\n| Ours | **26.65** | **0.876** | **0.171** | **39.64** |", "caption": "Table 4: Performance comparison with a NeRF-extension framework, including training and rendering speed. Averaged over 536\u00d7\\times\u00d7960 HyperNeRF\u2019s vrig datasets [45]. The performance numbers of [44, 45, 11, 22, 18] are sourced from [54]. The training times and run times reported in [54] were measured on an NVIDIA RTX 3090 GPU, while our framework was tested on an RTX A6000 GPU. Please note that the A6000 GPU has approximately 20 %percent\\%% lower memory bandwidth compared to that of the RTX 3090.", "description": "Table 4 presents a performance comparison between MoDec-GS and several other state-of-the-art novel view synthesis (NVS) methods.  The comparison specifically focuses on methods that extend NeRFs, evaluating metrics such as PSNR, MS-SSIM, training time, rendering speed (FPS), and model size (storage).  The results are averaged across the 'vrig' subset of the HyperNeRF dataset [45], using videos with a resolution of 536x960 pixels.  It's important to note that while the performance numbers for the comparison methods were taken from a previous work [54], those results were obtained using an NVIDIA RTX 3090 GPU, whereas MoDec-GS was evaluated on an RTX A6000 GPU.  Since the RTX A6000 has approximately 20% lower memory bandwidth, this should be kept in mind when interpreting the results.", "section": "5. Experiments"}, {"content": "| Variant | mPSNR \u2191 | mSSIM \u2191 | mLPIPS \u2193 | Storage \u2193 |\n|---|---|---|---|---|\n| (a) 1stage, Gaussian deform. ([54]) | 13.73 | 0.460 | 0.509 | 78.54 |\n| (b) 1stage, anchor deform. | 13.56 | 0.449 | 0.510 | 36.92 |\n| (c) 2stage, all anchor deform. | 13.93 | 0.453 | 0.492 | 55.29 |\n| (d) 2stage, GAD + LGD (**GLMD**) | 14.48 | 0.475 | 0.455 | 49.70 |\n| (e) (d) with smaller hexplane | 14.46 | 0.475 | 0.451 | 22.67 |\n| (f) (e) with  d<sub>G</sub> and d<sub>L</sub> (anchor dynamics) | 14.51 | 0.478 | 0.447 | 22.72 |\n| (g) (f) with **TIA** (our final MoDec-GS) | 14.60 | 0.480 | 0.443 | 18.37 |", "caption": "Table 5: Quantitative results comparison on (a) iPhone [16], (b) HyperNeRF [45], (c) Nvidia [58] datasets. Red and blue denote the best and second best performances, respectively. Each block element of 5-performance denotes (PSNR(dB)\u2191\u2191\\uparrow\u2191 / SSIM\u2191\u2191\\uparrow\u2191 [53] / LPIPS\u2193\u2193\\downarrow\u2193 [59] / tOF\u2193\u2193\\downarrow\u2193 [7] \u2009 Storage(MB)\u2193\u2193\\downarrow\u2193). For iPhone dataset, the masked metrics are used. For Nvidia monocular dataset, the tOF values are not calculated due to the teleporting artifacts present in the test views.", "description": "This table presents a quantitative comparison of different methods for dynamic 3D Gaussian splatting across three datasets: iPhone, HyperNeRF, and Nvidia.  The metrics used for comparison include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), Temporal Optical Flow (tOF), and model storage size (in MB). The best and second-best performing methods for each metric are highlighted in red and blue, respectively. For the iPhone dataset, masked metrics (considering only co-visible pixels) are reported.  Due to the presence of 'teleporting artifacts' in the Nvidia dataset's test views, tOF values are not calculated for this dataset.", "section": "5. Experiments"}]