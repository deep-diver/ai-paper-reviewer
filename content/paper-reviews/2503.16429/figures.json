[{"figure_path": "https://arxiv.org/html/2503.16429/x2.png", "caption": "Figure 1: \nMain properties.\nSonata leads to reliable 3D self-supervised pretraining with the following superior and emerging properties: 1.\u00a0Perception. Sonata advances state-of-the-art results across 3D indoor and outdoor perception tasks; 2.\u00a0Linear probing. With less than 0.2% learnable parameters, Sonata achieves strong and usable linear probing performance which is 3.3\u00d7\\times\u00d7 better than previous SOTA;\n3.\u00a0Decoder-free. Sonata moves beyond the inflexible U-Net structure, offering multi-scale representations that unchain future 3D research from previous architectural constraints.\n4.\u00a0Semantic awareness. Sonata reveals semantic structure in PCA and K-means visualizations. 5.\u00a0Spatial reasoning. Sonata allows spatial correspondence even under strong augmentations as visualized via feature similarity.", "description": "Sonata, a self-supervised learning method for reliable 3D point cloud representations, is presented.  Figure 1 highlights Sonata's key advantages: superior performance on various 3D perception tasks; high efficiency in linear probing, requiring minimal additional parameters for strong performance; a decoder-free architecture that allows for more flexible and scalable models; the ability to reveal semantic structure in its representations; and robust spatial reasoning capabilities, even with significant data augmentation.  The figure visually demonstrates these properties with visualizations and performance metrics.", "section": "Main properties"}, {"figure_path": "https://arxiv.org/html/2503.16429/x3.png", "caption": "Figure 2: Geometric shortcut. We select a point on the sofa arm and compute pairwise similarity with other points. The similarity heatmap reveals that CSC\u00a0[38] collapses to surface normals, and MSC\u00a0[88] overfits to point height. In contrast, our Sonata can extract higher-level concepts, as can be seen by the high similarity between all sofa arms highlighted in red.", "description": "This figure demonstrates the concept of \"geometric shortcuts\" in 3D self-supervised learning.  It visualizes pairwise similarity heatmaps for three different models (CSC, MSC, and Sonata) when computing similarities between a point on a sofa arm and all other points in the scene. CSC's heatmap shows similarities largely aligned with surface normals, indicating that the model focuses on low-level geometric cues. MSC's heatmap displays high similarity with points sharing similar heights, revealing an over-reliance on point height information. In contrast, Sonata's heatmap exhibits strong similarities among all points belonging to the sofa arms, demonstrating that it has learned higher-level semantic concepts and spatial relationships rather than simply relying on surface normals or point heights.", "section": "3. Pilot Study and Design Principle"}, {"figure_path": "https://arxiv.org/html/2503.16429/x4.png", "caption": "Figure 3: The geometric shortcut is unique to 3D. When comparing the information contained in 2D image and 3D point cloud data after removing the input feature (indicated by color), it is evident that in images all information is within the input feature. Whereas point clouds retain geometric information in point positions, which is directly utilized by operators. This characteristic leads to what we term geometric shortcuts in 3D SSL.", "description": "Figure 3 illustrates the core concept of \"geometric shortcut\" in 3D self-supervised learning (SSL).  It compares 2D image data and 3D point cloud data. When the input features (color information in the images, and potentially semantic features in the point clouds) are removed, images lose essentially all their information. However, 3D point clouds still retain geometric information inherent in the point positions themselves. Because 3D point cloud operators directly use this positional data, models can easily exploit these low-level geometric cues, rather than learning higher-level semantic features. This reliance on readily available geometric information is what the authors refer to as the \"geometric shortcut\", hindering the effectiveness of 3D SSL.", "section": "Pilot Study and Design Principle"}, {"figure_path": "https://arxiv.org/html/2503.16429/x5.png", "caption": "Figure 4: What is learned by the hierarchical backbone? We visualize PCA embeddings from different stages of a hierarchical encoder and decoder, trained for semantic segmentation. The encoder captures diverse and dispersed feature patterns, indicating a broad range of information. Notably, as the point cloud becomes coarser, accessible geometric information within point coordinates becomes increasingly global. In contrast, the decoder\u2019s representations are more uniform and structured, suggesting a focus on refining features for task-specific outputs.", "description": "This figure visualizes PCA embeddings from different layers of a hierarchical encoder-decoder network, trained on semantic segmentation task.  The visualization reveals how the encoder learns increasingly global geometric information as the point cloud resolution decreases, while the decoder focuses on generating more structured and uniform representations relevant for specific downstream tasks.  This contrast illustrates the distinct roles of the encoder (feature learning) and decoder (task-specific refinement) in a hierarchical architecture, and the different types of information each component extracts from point cloud data. The encoder captures a broad range of features, initially dispersed but becoming more globally structured as the point cloud coarsens. Conversely, the decoder's outputs are more spatially uniform and task-focused.", "section": "Pilot Study and Design Principle"}, {"figure_path": "https://arxiv.org/html/2503.16429/x6.png", "caption": "Figure 5: Self-distillation framework of Sonata. (1) Local views (bottom left) and global views (right) are generated with dedicated spatial and photometric augmentations, while masked views are created by randomly masking out grid-based patches from the global views (top left). (2) Embeddings from local and masked views are extracted by the student, with global views processed by the teacher (top). (3) Points from local and masked views are matched with corresponding points in the global views based on their original spatial distance, allowing for the distillation of embeddings from global views to local and masked views (bottom).", "description": "Sonata's self-distillation framework uses three types of views: local, global, and masked.  Local views are created with small, dedicated spatial and photometric augmentations. Global views use larger augmentations, providing more context. Masked views are generated by randomly removing grid-based patches from the global views. A student network processes the local and masked views, while a teacher network processes the global views.  The student learns by matching points in the local and masked views to their corresponding points in the global view (based on their original, un-augmented locations), effectively distilling knowledge from the teacher.", "section": "4. Point Self-distillation with Sonata"}, {"figure_path": "https://arxiv.org/html/2503.16429/x7.png", "caption": "Figure 6: The roadmap. We evolve Mask Scene Contrast\u00a0[88] into our Sonata by modernizing self-supervised learning with self-distillation, addressing the geometric shortcut, and scaling up training. Our designs are validated through progressive ablation with linear and decoder probing on ScanNet semantic segmentation\u00a0[23]. Starting with 23k training data (a combination of ScanNet and Structured3D\u00a0[109]) and a 39M PTv3 model\u00a0[89], we ultimately scale up to 140k assets (Tab.\u00a02) and a 108M PTv3 model.", "description": "Figure 6 illustrates the development of the Sonata model from Mask Scene Contrast [88].  It shows a step-by-step improvement process, starting with a smaller dataset and model (23k training samples, 39M parameter PTv3 model), and incrementally adding techniques like self-distillation to address the geometric shortcut problem inherent in 3D point cloud data.  The process culminates in a larger-scale model (140k samples, 108M parameter PTv3 model) that achieves superior performance on ScanNet semantic segmentation [23]. Each step in the roadmap includes linear and decoder probing results to validate the effectiveness of the design choices.  The figure emphasizes the iterative improvements and scaling-up strategies employed in creating the Sonata model.", "section": "4. Point Self-distillation with Sonata"}, {"figure_path": "https://arxiv.org/html/2503.16429/x8.png", "caption": "Table 1: Data source collection.", "description": "This table details the datasets used in the Sonata model training.  It lists the name of each dataset, whether the data is real or simulated, and the number of training, validation, and test samples.  The datasets include indoor and outdoor scenes and are a mix of real and simulated point clouds.  The mixed nature of the data helps train a more robust model. The table also shows the total number of samples used in the study.", "section": "4. Implementation and Evaluation Protocols"}, {"figure_path": "https://arxiv.org/html/2503.16429/x9.png", "caption": "Table 2: Data scale comparison.", "description": "This table compares the scale of datasets used for training different point cloud self-supervised learning methods. It shows that the Sonata method utilizes a significantly larger dataset (139,768 point clouds) compared to previous methods such as PC (1,613), MSC (6,660), and PPT (23,706).  This demonstrates the scale advantage of Sonata's training data.", "section": "4. Point Self-distillation with Sonata"}, {"figure_path": "https://arxiv.org/html/2503.16429/x10.png", "caption": "Figure 7: Zero-shot comparison with DINOv2. We compare the PCA visualizations of DINOv2, Sonata, and their combined feature representation. DINOv2 excels at capturing photometric details, while Sonata better distinguishes spatial information. The combined model demonstrates improved coherence and detail, showcasing the complementary strengths of both models.", "description": "This figure compares the zero-shot performance of DINOv2 and Sonata, two different self-supervised learning models, on point cloud representation.  PCA visualizations are used to highlight the different strengths of each model.  DINOv2 excels at capturing fine-grained photometric details (color and texture information), while Sonata is better at distinguishing spatial relationships between points.  A combined representation that utilizes both DINOv2 and Sonata features is shown, demonstrating improved coherence and detail compared to either model alone, illustrating their complementary nature and the potential benefits of combining their approaches.", "section": "Main Results"}, {"figure_path": "https://arxiv.org/html/2503.16429/x11.png", "caption": "Table 3: Numerical comparison with DINO series.", "description": "Table 3 presents a quantitative comparison of the linear and decoder probing results of Sonata against the DINOv2 and DINOv2.5 image-based self-supervised models. The comparison is performed on the ScanNet and ScanNet200 semantic segmentation datasets and includes metrics such as mean Intersection over Union (mIoU), mean accuracy (mAcc), and overall accuracy (allAcc).  This table helps demonstrate Sonata's effectiveness in learning robust 3D representations.", "section": "5. Main Results"}]