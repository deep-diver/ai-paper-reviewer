[{"content": "| Method | OmniObject3D |  |  |  | GSO |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  | RRE \u2193 | RRA@15\u00b0 \u2191 | RRA@30\u00b0 \u2191 | TE \u2193 | RRE \u2193 | RRA@15\u00b0 \u2191 | RRA@30\u00b0 \u2191 | TE \u2193 |\n|---|---|---|---|---|---|---|---|---|\n| FORGE | 76.822 | 0.081 | 0.257 | 0.430 | 97.814 | 0.022 | 0.083 | 0.898 |\n| MASt3R | 96.670 | 0.052 | 0.112 | 0.524 | 61.820 | 0.244 | 0.445 | 0.353 |\n| FreeSplatter-O | 11.550 | 0.909 | 0.937 | 0.081 | 3.851 | 0.962 | 0.978 | 0.030 |\n|  | ScanNet++ |  |  |  | CO3Dv2 |  |  |  |\n|---|---|---|---|---|---|---|---|---|\n|  | RRE \u2193 | RRA@15\u00b0 \u2191 | RRA@30\u00b0 \u2191 | TE \u2193 | RRE \u2193 | RRA@15\u00b0 \u2191 | RRA@30\u00b0 \u2191 | TE \u2193 |\n|---|---|---|---|---|---|---|---|---|\n| PoseDiffusion | - | - | - | - | 7.950 | 0.803 | 0.868 | 0.328 |\n| MASt3R | 0.724 | 0.988 | 0.993 | 0.104 | 2.918 | 0.975 | 0.989 | 0.112 |\n| FreeSplatter-S | 0.791 | 0.982 | 0.987 | 0.110 | 3.054 | 0.976 | 0.986 | 0.148 |", "caption": "Table 1: Quantitative results on camera pose estimation. We highlight the best metric as red.", "description": "This table presents a quantitative comparison of different methods for camera pose estimation.  The methods are evaluated on two datasets: OmniObject3D and Google Scanned Objects (GSO).  For each method and dataset, the table shows the relative rotation error (RRE), relative rotation accuracy at 15\u00b0 (RRA@15\u00b0), relative rotation accuracy at 30\u00b0 (RRA@30\u00b0), and translation error (TE). Lower values for RRE and TE indicate better performance, while higher values for RRA indicate better accuracy.  The best metric for each column is highlighted in red.", "section": "4.3 CAMERA POSE ESTIMATION"}, {"content": "| Method | OmniObject3D PSNR \u2191 | OmniObject3D SSIM \u2191 | OmniObject3D LPIPS \u2193 | GSO PSNR \u2191 | GSO SSIM \u2191 | GSO LPIPS \u2193 |\n|---|---|---|---|---|---|---|\n| LGM (w/ gt pose) | 24.852 | 0.942 | 0.060 | 24.463 | 0.891 | 0.093 |\n| InstantMesh (w/ gt pose) | 24.077 | 0.945 | 0.062 | 25.421 | 0.891 | 0.095 |\n| Ours (FreeSplatter-O) | 31.929 | 0.973 | 0.027 | 30.443 | 0.945 | 0.055 |\n| Method | ScanNet++ PSNR \u2191 | ScanNet++ SSIM \u2191 | ScanNet++ LPIPS \u2193 | CO3Dv2 PSNR \u2191 | CO3Dv2 SSIM \u2191 | CO3Dv2 LPIPS \u2193 |\n|---|---|---|---|---|---|---|\n| pixelSplat (w/ gt pose) | 24.974 | 0.889 | 0.180 | - | - | - |\n| MVSplat (w/ gt pose) | 22.601 | 0.862 | 0.208 | - | - | - |\n| Splatt3R | 21.013 | 0.830 | 0.209 | 18.074 | 0.740 | 0.197 |\n| Ours (FreeSplatter-S) | 25.807 | 0.887 | 0.140 | 20.405 | 0.781 | 0.162 |", "caption": "Table 2: Quantitative results on sparse-view reconstruction.", "description": "This table presents a quantitative comparison of different methods for sparse-view 3D reconstruction.  It shows the performance of various techniques, including both pose-dependent and pose-free methods, across two datasets: OmniObject3D and Google Scanned Objects (GSO).  Metrics used for comparison include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  Higher PSNR and SSIM values indicate better reconstruction quality, while a lower LPIPS value suggests improved perceptual similarity to ground truth.", "section": "4.2 Sparse-View Reconstruction"}, {"content": "| Method | GSO PSNR \u2191 | GSO SSIM \u2191 | GSO LPIPS \u2193 | ScanNet++ PSNR \u2191 | ScanNet++ SSIM \u2191 | ScanNet++ LPISP \u2193 |\n|---|---|---|---|---|---|---|\n| FreeSplatter-O (w/o \\mathcal{L}_{align}) | 26.684 | 0.898 | 0.092 | 21.330 | 0.832 | 0.201 |\n| FreeSplatter-S (w/o \\mathcal{L}_{align}) | 21.330 | 0.832 | 0.201 | 25.807 | 0.887 | 0.140 |\n| FreeSplatter-O | 30.443 | 0.945 | 0.055 | 25.807 | 0.887 | 0.140 |", "caption": "Table 3: Ablation on the pixel-alignment loss.", "description": "This table presents the ablation study on the impact of the pixel-alignment loss on the model's performance. It compares the performance metrics (PSNR, SSIM, LPIPS) of the FreeSplatter model trained with and without the pixel-alignment loss on two datasets: Google Scanned Objects (GSO) and ScanNet++. The results demonstrate the significant contribution of the pixel-alignment loss to the model's performance, particularly in improving the visual fidelity of the generated novel views.", "section": "4 Experiments"}, {"content": "| Method | GSO PSNR \u2191 | GSO SSIM \u2191 | GSO LPIPS \u2193 | OmniObject3D PSNR \u2191 | OmniObject3D SSIM \u2191 | OmniObject3D LPIPS \u2193 |\n|---|---|---|---|---|---|---|\n| Evaluate renderings at G.T. novel-view poses |\n| PF-LRM | 25.08 | 0.877 | 0.095 | 21.77 | 0.866 | 0.097 |\n| FreeSplatter-O | 23.54 | 0.864 | 0.100 | 22.83 | 0.876 | 0.088 |\n| Evaluate renderings at predicted input poses |\n| PF-LRM | 27.10 | 0.905 | 0.065 | 25.86 | 0.901 | 0.062 |\n| FreeSplatter-O | 25.50 | 0.897 | 0.076 | 26.49 | 0.926 | 0.050 |", "caption": "Table 4: Quantitative comparison with PF-LRM on sparse-view reconstruction.", "description": "This table presents a quantitative comparison of the proposed FreeSplatter model and the PF-LRM method on sparse-view 3D reconstruction tasks.  It compares the performance of both models across two key metrics: PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity Index).  The LPIPS (Learned Perceptual Image Patch Similarity) metric, which assesses the perceptual difference between images, is also included. The comparison is made for two different scenarios: using the ground truth novel view poses for evaluation and using the predicted input poses for evaluation.  This provides a comprehensive comparison of reconstruction quality under varying conditions.", "section": "4.2 Sparse-View Reconstruction"}, {"content": "| Method | GSO RRE \u2193 | GSO RRA@15\u00b0 \u2191 | GSO RRA@30\u00b0 \u2191 | GSO TE \u2193 | OmniObject3D RRE \u2193 | OmniObject3D RRA@15\u00b0 \u2191 | OmniObject3D RRA@30\u00b0 \u2191 | OmniObject3D TE \u2193 |\n|---|---|---|---|---|---|---|---|---|\n| PF-LRM | 3.99 | 0.956 | 0.976 | 0.041 | 8.013 | 0.889 | 0.954 | 0.089 |\n| FreeSplatter-O | 8.96 | 0.909 | 0.936 | 0.090 | 3.446 | 0.982 | 0.996 | 0.039 |", "caption": "Table 5: Quantitative comparison with PF-LRM on pose estimation.", "description": "This table presents a quantitative comparison of FreeSplatter-O and PF-LRM on camera pose estimation performance.  It uses metrics such as Relative Rotation Error (RRE), Relative Rotation Accuracy at 15\u00b0 and 30\u00b0 (RRA@15\u00b0, RRA@30\u00b0), and Translation Error (TE) to compare the accuracy of pose estimation by both methods on the OmniObject3D and GSO datasets.", "section": "4.2 SPARSE-VIEW RECONSTRUCTION"}, {"content": "| Method | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---|\n| pixelSplat (w/ GT poses) | 24.469 | 0.829 | 0.224 |\n| MVSplat (w/ GT poses) | 20.033 | 0.789 | 0.280 |\n| Splatt3R (no pose) | 16.634 | 0.604 | 0.422 |\n| FreeSplatter-S (no pose) | 18.851 | 0.659 | 0.369 |", "caption": "Table 6: Quantitative sparse-view reconstruction results on RealEstate10K.", "description": "This table presents a quantitative comparison of different methods for sparse-view 3D reconstruction on the RealEstate10K dataset.  It evaluates the performance of several approaches, including methods that use ground truth camera poses (pixelSplat and MVSplat) and pose-free methods (Splatt3R and FreeSplatter-S). The metrics used to assess the performance are PSNR, SSIM, and LPIPS, providing a comprehensive evaluation of the reconstruction quality.", "section": "4.2 SPARSE-VIEW RECONSTRUCTION"}, {"content": "| Method | RRE \u2193 | RRA@15\u00b0\u2191 | RRA@30\u00b0\u2191 | TE \u2193 |\n|---|---|---|---|---|\n| PoseDiffsion | 14.387 | 0.732 | 0.780 | 0.466 |\n| RayDiffsion | 12.023 | 0.767 | 0.814 | 0.439 |\n| RoMa | 5.663 | 0.918 | 0.947 | 0.402 |\n| MASt3R | 2.341 | 0.972 | 0.994 | 0.374 |\n| FreeSplatter-S | 3.513 | 0.982 | 0.995 | 0.293 |", "caption": "Table 7: Quantitative pose estimation results on RealEstate10K.", "description": "This table presents a quantitative comparison of camera pose estimation methods on the RealEstate10K dataset.  The metrics used to evaluate the accuracy of pose estimation include Relative Rotation Error (RRE), Relative Rotation Accuracy at 15\u00b0 and 30\u00b0 thresholds (RRA@15\u00b0, RRA@30\u00b0), and Translation Error (TE).  The methods compared include several state-of-the-art techniques, both pose-dependent and pose-free, allowing for a comprehensive assessment of pose estimation accuracy in the context of sparse-view reconstruction.", "section": "4.2 SPARSE-VIEW RECONSTRUCTION"}, {"content": "| Architecture | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---|\n| L=16, P=16 | 25.417 | 0.896 | 0.088 |\n| L=16, P=8 | 28.945 | 0.934 | 0.064 |\n| L=24, P=16 | 28.622 | 0.927 | 0.063 |\n| L=24, P=8 | <span style=\"background-color:#FFB3B3;\">30.443</span> | <span style=\"background-color:#FFB3B3;\">0.945</span> | <span style=\"background-color:#FFB3B3;\">0.055</span> |", "caption": "Table 8: Ablation study on model architectures. The results are evaluated on GSO dataset. L\ud835\udc3fLitalic_L and P\ud835\udc43Pitalic_P denote number of transformer layers and patch size, respectively.", "description": "This table presents the results of an ablation study conducted to analyze the impact of different model architectures on the performance of the FreeSplatter model.  Specifically, it investigates the effects of varying the number of transformer layers (L) and the patch size (P) used in the model.  The study was performed using the Google Scanned Objects (GSO) dataset, and the results shown represent the PSNR, SSIM, and LPIPS metrics achieved by each model configuration. This allows for a comprehensive evaluation of how architectural choices influence the reconstruction quality and provides insights into the optimal balance between model complexity, computational resources and performance.", "section": "A.3 ABLATION STUDIES"}]