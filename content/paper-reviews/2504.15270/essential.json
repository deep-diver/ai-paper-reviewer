{"importance": "This paper presents **a novel LMM architecture, Quicksviewer**, that dynamically adjusts to video content, offering potential breakthroughs in efficient video processing. Its focus on reducing computational demands and enhancing performance with limited data could be transformative.", "summary": "Efficient video understanding via adaptive compression.", "takeaways": ["Quicksviewer dynamically adjusts to video content density.", "Achieves high compression rates with large receptive fields.", "Outperforms baselines with less training data and fewer tokens."], "tldr": "Existing Large Multimodal Models(LMMs) process all video frames uniformly, leading to inefficiencies due to varying temporal information density. This paper aims to address this by dynamically compress videos based on their temporal density, aiming to enhance processing efficiency. The challenge lies in achieving this without losing critical information or requiring extensive computational resources. \n\nThe authors introduce **Quicksviewer, an LMM that partitions videos into nonuniform cubes using Gumbel Softmax and employs unified resampling for efficient video understanding**. Quicksviewer dynamically compress video by 45x and also enalbes large receptive field training. Quicksviewer also scales up the number of input frames which reveals a clear power law of the model capabilities. Quicksviewer performs better than other baseline models.", "affiliation": "Tsinghua University", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2504.15270/podcast.wav"}