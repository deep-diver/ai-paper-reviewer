[{"figure_path": "https://arxiv.org/html/2501.06187/x2.png", "caption": "Figure 1: \nGiven a text prompt as well as reference images for each subject (i.e., man, dog) and background images (i.e., bridge, desert, sea ice, Moon\u2019s surface), Video Alchemist synthesizes natural motions while preserving subject identity and background fidelity.", "description": "Video Alchemist is a video generation model that takes a text prompt and reference images as input.  The prompt describes a scene involving multiple subjects (like a man and a dog) and a background (like a bridge, desert, sea ice, or the moon's surface). The model then generates a short video that depicts the described scene. Importantly, the video generated accurately reflects the identities of the subjects (i.e., a specific man and dog) and maintains the fidelity of the background. The generated video is realistic and features natural movements.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.06187/x3.png", "caption": "Figure 2: Dataset collection pipeline for video personalization.\u00a0\nWe construct our training dataset using video and caption pairs through three steps. First, we identify three categories of entity words from the caption: subject, object, and background. Next, we use these entity words to localize and segment the target subjects and objects in three selected video frames. Finally, we extract a clean background image by removing the subjects and objects from the middle frame.", "description": "This figure illustrates the three-step process of creating a dataset for video personalization.  First, the caption of a video is analyzed to identify key entities: subject (e.g., a person or animal), object (e.g., a coat or car), and background (e.g., a room or landscape). Second, using these identified entities, the system automatically locates and segments the subject and object within three frames selected from the video (beginning, middle, and end). Finally, the system produces a clean background image by removing the subject and object from the middle frame. This resulting dataset of images (subjects, objects, backgrounds) and their corresponding video captions serves as the basis for training a multi-subject, open-set video personalization model.", "section": "3.1. Dataset Collection"}, {"figure_path": "https://arxiv.org/html/2501.06187/x4.png", "caption": "Figure 3: Model architecture.\u00a0\nOur model is a latent DiT\u00a0[50], where we first encode a video into video tokens and denoise them with a deep cascade of DiT blocks in the latent space. Each DiT block includes an additional cross-attention operation with personalization embeddings f=Concat\u2062(f1,\u2026,fn,\u2026,fN)\ud835\udc53Concatsubscript\ud835\udc531\u2026subscript\ud835\udc53\ud835\udc5b\u2026subscript\ud835\udc53\ud835\udc41f=\\textrm{Concat}(f_{1},\\dots,f_{n},\\dots,f_{N})italic_f = Concat ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , \u2026 , italic_f start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ), where fnsubscript\ud835\udc53\ud835\udc5bf_{n}italic_f start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT fuses the embeddings of both the reference image xnsubscript\ud835\udc65\ud835\udc5bx_{n}italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT and the corresponding entity word cnsubscript\ud835\udc50\ud835\udc5bc_{n}italic_c start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. Each square in the figure represents a 1-D token.", "description": "This figure illustrates the architecture of the Video Alchemist model, a latent diffusion transformer (DiT).  It details how a video is first encoded into a sequence of video tokens. These tokens are then denoised through a series of DiT blocks. Crucially, each DiT block incorporates a cross-attention mechanism that integrates information from both text embeddings (describing the overall video scene) and personalization embeddings (specific to individual subjects and objects within the video). These personalization embeddings are created by fusing reference image embeddings and corresponding word embeddings for each subject or object. The output of this process produces a refined sequence of video tokens that is then decoded to reconstruct a video.  Each square in the diagram represents a single 1D token, highlighting the model's token-based processing of the video.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.06187/x5.png", "caption": "Figure 4: Test sample in MSRVTT-Personalization.\u00a0\nWe present a comprehensive video personalization benchmark. Our benchmark supports various modes, including face conditioning, single or multiple subjects conditioning, and foreground and background conditioning.", "description": "Figure 4 showcases an example from the MSRVTT-Personalization benchmark, a newly developed comprehensive evaluation protocol for video personalization.  This benchmark is designed to assess a video generation model's ability to create videos that are personalized across various factors and conditions.  The image shows a scene with a specific subject (a man in a blue cap) and object (a horse), set against a particular background (a forested area). The image displays different conditioning scenarios the benchmark supports, including face conditioning (using a face crop for personalization), single subject conditioning (where the model is only conditioned on a single subject), multi-subject conditioning (the model is conditioned on multiple subjects), and foreground and background conditioning. The versatility of the benchmark allows for thorough testing of a model\u2019s capability to generate varied and realistic video content.", "section": "4.1. MSRVTT-Personalization Benchmark"}, {"figure_path": "https://arxiv.org/html/2501.06187/x6.png", "caption": "Figure 5: Qualitative comparison on MSRVTT-Personalization.\u00a0\nWe use a single reference image to each model for a fair comparison. Compared to existing methods, our results closely match the input text prompt and reference subjects while exhibiting natural motion and pose variations.", "description": "Figure 5 presents a qualitative comparison of video generation results from several different models using the MSRVTT-Personalization benchmark.  Each model was given the same text prompt and a single reference image as input to ensure a fair comparison. The results demonstrate that Video Alchemist produces videos that closely match the provided text and reference image, exhibiting natural-looking movements and pose variations, unlike other models that may struggle with this task. This highlights Video Alchemist's superior ability to faithfully generate personalized videos.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06187/x7.png", "caption": "Figure 6: Qualitative results of the ablation study.\u00a0\nFrom top to bottom, we show that 1) Video Alchemist achieves better subject fidelity using DINOv2\u00a0[47] as the image encoder; 2) it correctly binds the conditional image and entity word with the usage of word tokens; 3) it mitigates the copy-and-paste effect and synthesizes text-aligned videos via the proposed data augmentation. The reference image is synthesized by DALL\u00b7E 3\u00a0[3].", "description": "Figure 6 presents an ablation study comparing three different model variations of Video Alchemist.  The top row shows improved subject fidelity when using the DINOv2 [47] image encoder. The middle row demonstrates the importance of correctly binding conditional images and entity words via word tokens for accurate subject representation. The bottom row showcases how the proposed data augmentation method reduces the 'copy-and-paste' effect, leading to more natural and text-aligned video generation.  The reference image used was created with DALL-E 3 [3].", "section": "3.3 Reducing Model Overfitting"}, {"figure_path": "https://arxiv.org/html/2501.06187/extracted/6124560/figures/sources/word_cloud.png", "caption": "Figure 7: Prompt template for retrieving the entity words.", "description": "This figure shows the prompt template used to instruct a large language model (LLM) to extract entity words from video captions. These entity words are categorized into three types: background, subject, and object. The prompt provides clear definitions for each category to ensure accurate extraction, along with examples to guide the LLM's output format.  The goal is to create a structured representation of the entities present in the video caption, facilitating the training data preparation process for video personalization.", "section": "3.1. Dataset Collection"}, {"figure_path": "https://arxiv.org/html/2501.06187/x8.png", "caption": "Figure 8: Word cloud of the entity words.\u00a0\nWe randomly sample 10k videos from our training dataset and plot the word cloud of the retrieved subject and object entity words.", "description": "This word cloud visualizes the frequency of subject and object words extracted from the captions of 10,000 randomly selected videos in the training dataset.  The size of each word corresponds to its frequency; larger words indicate more frequent appearance in the training data captions. This provides a visual representation of the diversity and prevalence of different subjects and objects present in the training dataset, giving insights into the types of content the model was trained on.", "section": "A. Details of Training Datasets and Augmentations"}, {"figure_path": "https://arxiv.org/html/2501.06187/x9.png", "caption": "Figure 9: Additional results of multi-subject open-set personalization.", "description": "This figure showcases supplementary results from the Video Alchemist model, demonstrating its ability to handle multi-subject, open-set personalization.  Multiple rows show the same video generated with varying numbers of input reference images (P1, P2, P3). The bottom row shows the video generated without any reference images, highlighting how the reference images influence the generation. The scenes involve diverse subjects and backgrounds, illustrating the model's capacity to adapt to various scenarios and creatively combine elements.", "section": "C. More Comparisons on Different Conditional Subjects"}, {"figure_path": "https://arxiv.org/html/2501.06187/x10.png", "caption": "Figure 10: Additional results of multi-subject open-set personalization.", "description": "This figure displays multiple video generation results showcasing the model's ability in multi-subject open-set personalization.  Each row represents a different video with increasing complexity. The leftmost column shows a short caption that describes the generated video, specifying the subjects and scene. The next columns show a sequence of frames from the video, demonstrating the video's content and the success of personalized video generation. The model successfully integrates multiple subjects and background details mentioned in the text prompt while maintaining the fidelity of each entity throughout the video sequence.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06187/x11.png", "caption": "Figure 11: Additional results of multi-subject open-set personalization.", "description": "This figure showcases supplementary results demonstrating the model's capabilities in multi-subject open-set video personalization.  Multiple examples are shown, each with varying numbers of subjects and backgrounds, highlighting the model's ability to generate coherent and realistic videos that accurately reflect the specified conditions. The results indicate the model's capacity to handle diverse subjects and settings while maintaining high visual quality and accuracy in the generated video.", "section": "C. More Comparisons on Different Conditional Subjects"}, {"figure_path": "https://arxiv.org/html/2501.06187/x12.png", "caption": "Figure 12: Additional results of multi-subject open-set personalization.", "description": "This figure displays further examples of the Video Alchemist model's ability to generate videos with multiple subjects and open-set entities.  Each row shows a video generated from a text prompt and several reference images corresponding to distinct subjects (like a person and a rocket) and/or background scenes (like the Moon's surface). The progression within each row demonstrates how adding more reference images influences the resulting video's fidelity and realism. The final column in each row shows a result without any image reference, highlighting the impact of incorporating reference images for improved personalization.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06187/x13.png", "caption": "Figure 13: Same text prompt with different reference images of person.", "description": "This figure demonstrates the robustness of the Video Alchemist model by showcasing its ability to generate diverse videos from the same text prompt and subject but with different reference images of the person. The consistency of the generated videos across various person images highlights the model's ability to focus on the core concept and maintain fidelity, regardless of the person's appearance.", "section": "C.2. Same Text Prompt with Different Reference Images"}, {"figure_path": "https://arxiv.org/html/2501.06187/x14.png", "caption": "Figure 14: Same text prompt with different reference images of dog.", "description": "This figure demonstrates the model's ability to generate diverse video content from the same text prompt by changing only the reference image of the dog.  The results show multiple variations in the generated video due to differences in the dog's breed, color, pose and size, while other aspects of the video, including the person and the background, remain constant. This highlights the model's capacity for open-set personalization and its resilience to variations in input.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06187/x15.png", "caption": "Figure 15: Qualitative comparison on the conditional subject of dog.", "description": "This figure shows a qualitative comparison of different video generation models' ability to generate videos of dogs based on a text prompt and a reference image. It demonstrates the fidelity of the generated videos compared to the ground truth, highlighting the strengths and weaknesses of each model in terms of realism, naturalness, and accuracy in replicating the subject (dog).", "section": "4.2. Comparisons with the State-of-the-Arts"}, {"figure_path": "https://arxiv.org/html/2501.06187/x16.png", "caption": "Figure 16: Qualitative comparison on the conditional subject of cat.", "description": "This figure displays a qualitative comparison of several video generation models' performance when generating videos conditioned on a cat.  The models' outputs are shown alongside the ground truth video for a direct comparison.  The video shows a brown cat on a gray chair licking its left paw in a room with a white wall and brown shelves.  This allows for a visual assessment of each model's ability to accurately represent the specified subject (the cat), its actions, and the overall scene fidelity.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06187/x17.png", "caption": "Figure 17: Qualitative comparison on the conditional subject of car.", "description": "This figure displays a qualitative comparison of different video generation models' ability to personalize a video featuring a car.  It shows the ground truth video alongside outputs from Video Alchemist, DreamVideo, VideoBooth, and ELITE, all conditioned on the same text prompt describing a white sports car driving down a tree-lined road. The comparison highlights the differences in video quality, subject fidelity (how well the generated car resembles the reference image), and the overall realism of the generated sequences.  This allows for a visual assessment of each model's strengths and weaknesses in terms of video personalization and generation quality.", "section": "4.2. Comparisons with the State-of-the-Arts"}]