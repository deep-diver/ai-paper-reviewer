[{"figure_path": "https://arxiv.org/html/2501.06425/x1.png", "caption": "Figure 1: Tensor Product Attention (TPA) in the Tensor ProducT ATTenTion Transformer (T6). Different from multi-head attention, in each layer, firstly the hidden state goes through different linear layers to get the latent factor matrices \ud835\udc00\ud835\udc00\\mathbf{A}bold_A\u2019s and \ud835\udc01\ud835\udc01\\mathbf{B}bold_B\u2019s for query, key, and value. We additionally apply RoPE to \ud835\udc01Qsubscript\ud835\udc01\ud835\udc44\\mathbf{B}_{Q}bold_B start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT and \ud835\udc01Ksubscript\ud835\udc01\ud835\udc3e\\mathbf{B}_{K}bold_B start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT for query and key. Then the multi-head query, key, and value vectors are attained by the tensor product of \ud835\udc00(\u22c5)subscript\ud835\udc00\u22c5\\mathbf{A}_{(\\cdot)}bold_A start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT and \ud835\udc01(\u22c5)subscript\ud835\udc01\u22c5\\mathbf{B}_{(\\cdot)}bold_B start_POSTSUBSCRIPT ( \u22c5 ) end_POSTSUBSCRIPT. Finally, the output of TPA is produced by scaled dot-product attention followed by linear projection of concatenated results of multiple heads.", "description": "This figure illustrates the Tensor Product Attention (TPA) mechanism within the T6 Transformer model. Unlike traditional multi-head attention, TPA leverages tensor decompositions to compactly represent queries, keys, and values.  The process begins with the hidden state passing through separate linear layers to generate latent factor matrices A and B for each of the query, key, and value components.  RoPE (Rotary Position Embedding) is then applied to the B matrices for queries and keys. The multi-head query, key, and value vectors are subsequently computed using the tensor product of the corresponding A and B matrices. Finally, the scaled dot-product attention mechanism is applied to these vectors, followed by a linear projection of the concatenated results from multiple heads, producing the TPA output.", "section": "3 Tensor Product Attention"}, {"figure_path": "https://arxiv.org/html/2501.06425/x2.png", "caption": "(a) Training Loss", "description": "This figure shows training loss curves for different attention mechanisms during the pretraining phase of large language models.  The x-axis represents the number of training tokens (in billions), and the y-axis shows the training loss. Different curves represent different attention mechanisms: MHA (Multi-Head Attention), MQA (Multi-Query Attention), GQA (Grouped-Query Attention), MLA (Multi-head Latent Attention), TPA-KVonly (a variant of Tensor Product Attention), and TPA (Tensor Product Attention).  The plot helps to compare the training efficiency and convergence speed of various attention mechanisms.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2501.06425/x3.png", "caption": "(b) Validation Loss", "description": "This figure shows the validation loss curves for different attention mechanisms during the training process of large-size language models. The x-axis represents the number of training tokens in billions, and the y-axis represents the validation loss. Each curve corresponds to a different attention mechanism: MHA (Multi-Head Attention), MQA (Multi-Query Attention), GQA (Grouped-Query Attention), MLA (Multi-Head Latent Attention), TPA-KVonly (Tensor Product Attention with only key-value factorization), and TPA (Tensor Product Attention). The figure helps compare the performance of various attention mechanisms in terms of their validation loss, providing insights into their efficiency and effectiveness in language modeling.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2501.06425/x4.png", "caption": "Figure 2: Training loss and validation loss of pretraining large-size (773M) models with different attention mechanisms on the FineWeb-Edu-100B dataset.", "description": "This figure displays the training and validation loss curves during the pretraining phase of large language models (LLMs) with 773 million parameters.  Different attention mechanisms are compared: Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), Multi-head Latent Attention (MLA), TPA-KVonly, and TPA. The x-axis represents the number of training tokens (in billions), and the y-axis represents the loss. This visualization helps assess the convergence speed and final performance of each attention mechanism on the FineWeb-Edu-100B dataset.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2501.06425/x5.png", "caption": "(a) Training Loss", "description": "This figure shows the training loss curves for different attention mechanisms during the pre-training phase on the FineWeb-Edu-100B dataset.  The plot illustrates how the training loss decreases over training tokens (in billions) for various methods: Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped Query Attention (GQA), Multi-head Latent Attention (MLA), TPA-KVonly, and TPA.  Comparing the slopes and final loss values provides insights into the training efficiency and overall performance of each attention mechanism.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2501.06425/x6.png", "caption": "(b) Validation Loss", "description": "This figure shows the validation loss curves for large-size (773M parameter) Language Models during pretraining.  The models utilize different attention mechanisms, including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), Multi-head Latent Attention (MLA), and the proposed Tensor Product Attention (TPA). The x-axis represents the training tokens (in billions) and the y-axis represents the validation loss.  The plot allows for comparison of the performance and convergence speed of various attention mechanisms in a large-scale language model training setting.", "section": "2 Background"}, {"figure_path": "https://arxiv.org/html/2501.06425/x7.png", "caption": "Figure 3: The training loss and validation loss of medium-size (353M) models with different attention mechanisms on the FineWeb-Edu 100B dataset.", "description": "This figure displays the training and validation loss curves for medium-sized language models (353M parameters) during training on the FineWeb-Edu 100B dataset.  Different lines represent models using various attention mechanisms: Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped Query Attention (GQA), Multi-head Latent Attention (MLA), TPA-KVonly, and TPA.  The plot shows how these different attention mechanisms affect the model's learning progress, allowing comparison of their performance in terms of convergence speed and final loss.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06425/x8.png", "caption": "(a) Validation Perplexity of Medium Models", "description": "This figure shows the validation perplexity results of medium-sized language models (353M parameters) during the pretraining phase. The x-axis represents the number of training tokens (in billions), while the y-axis represents the validation perplexity. Different lines on the graph correspond to various attention mechanisms used in the models, including MHA, MQA, GQA, MLA, TPA-KVonly, and TPA. The figure illustrates how the validation perplexity changes over time for each attention mechanism, allowing for a comparison of their performance.", "section": "4.1 Language Modeling Tasks"}, {"figure_path": "https://arxiv.org/html/2501.06425/x9.png", "caption": "(b) Validation Perplexity of Large Models", "description": "This figure shows the validation perplexity results for large-size language models (773M parameters) during the pretraining phase using the FineWeb-Edu-100B dataset.  Different attention mechanisms (MHA, MQA, GQA, MLA, TPA-KVonly, TPA) are compared, illustrating the trend of validation perplexity over the training process (in billions of tokens).  Lower perplexity indicates better language modeling performance.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06425/x10.png", "caption": "Figure 4: The validation perplexity of medium-size (353M) models and large-size (773M) models with different attention mechanisms on the FineWeb-Edu 100B dataset.", "description": "This figure displays the validation perplexity curves for medium-size (353M parameters) and large-size (773M parameters) language models during training on the FineWeb-Edu 100B dataset.  Multiple curves are shown, each representing a different attention mechanism: Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped Query Attention (GQA), Multi-head Latent Attention (MLA), TPA-KVonly, and TPA. The x-axis represents the number of training tokens (in billions), and the y-axis represents the validation perplexity.  The plot allows for comparison of the different attention mechanisms' performance in terms of both convergence speed and final perplexity.", "section": "4.1 Language Modeling Tasks"}]