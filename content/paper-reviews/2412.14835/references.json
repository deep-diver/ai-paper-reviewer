{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-18", "reason": "This paper introduces CLIP, a foundational vision-language model used in the AR-MCTS framework for cross-modal retrieval."}, {"fullname_first_author": "Guanting Dong", "paper_title": "Progressive Multimodal Reasoning via Active Retrieval", "publication_date": "2024-12-19", "reason": "This is the main research paper, which introduces the AR-MCTS framework for progressive multimodal reasoning."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-11-28", "reason": "This paper introduces Chain-of-Thought prompting, a technique that improves the reasoning capabilities of LLMs, a concept relevant to the AR-MCTS framework."}, {"fullname_first_author": "Pan Lu", "paper_title": "MathVista: Evaluating mathematical reasoning of foundation models in visual contexts", "publication_date": "2024-05-07", "reason": "This paper introduces the MATHVISTA benchmark, one of the datasets used for evaluating the performance of the AR-MCTS framework."}, {"fullname_first_author": "Runqi Qiao", "paper_title": "WE-MATH: Does your large multimodal model achieve human-like mathematical reasoning?", "publication_date": "2024-07-19", "reason": "This paper introduces the WE-MATH benchmark, another dataset used for evaluating the performance of the AR-MCTS framework."}]}