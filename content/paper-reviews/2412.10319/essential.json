{"importance": "**SCBench provides a standardized platform for evaluating long-context LLMs**, crucial for efficient memory management in increasingly complex language processing tasks. **It spotlights suboptimal performance of some current methods in handling multi-turn requests and shared contexts**, exposing areas for improvement and inspiring new research directions. The benchmark's focus on KV cache optimization **paves the way for creating more efficient and resource-aware LLMs**, enabling wider applications in scenarios like multi-turn dialogue systems.", "summary": "New benchmark for evaluating long-context models finds sub-O(n) methods lacking in real-world use cases.", "takeaways": ["Existing long-context LLM benchmarks overlook the important aspect of KV cache reuse across multiple requests, leading to unrealistic evaluations.", "Sub-O(n) memory methods for long-context LLMs show significant performance degradation in real-world usage scenarios like multi-turn conversations and shared contexts.", "Dynamic sparsity in encoding and O(n) memory usage in decoding achieve robust performance across different tasks and multiple queries with shared contexts"], "tldr": "Long-context language models (LLMs) struggle with memory efficiency, especially when dealing with multiple requests and reusable contexts. Current benchmarks evaluate models on single requests, overlooking the important aspect of Key-Value (KV) cache reuse, common in real-world applications like chatbots. This leads to unrealistic evaluations, as models often reuse previous context to save computation. Efficient long-context solutions are needed, but their evaluation under realistic reusable context scenarios remains a challenge. Current methods focus on single-turn benchmarks which cannot fully reflect real world scenarios with shared context and multiple rounds of conversations or interactions. This leads to inaccurate model evaluations. Specifically, sub-O(n) methods struggle in handling shared context scenarios where O(n) memory is necessary to preserve all essential information for different queries. \nThis paper introduces SCBench, a new benchmark designed to tackle these limitations. SCBench evaluates models on shared-context scenarios and multi-turn interactions, using two common context reuse patterns. It features 12 tasks assessing four key capabilities: string retrieval, semantic retrieval, global information processing, and multi-tasking. SCBench's design offers a more realistic evaluation of long-context models, reflecting real-world application scenarios. Testing eight LLMs and 13 long-context methods, including a novel method called Tri-shape, it reveals that O(n) memory with sub-O(n^2) pre-filling calculation is critical for robust multi-turn performance, while dynamic sparse attention methods show higher efficiency and effectiveness than static sparse methods.", "affiliation": "Microsoft Corporation", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2412.10319/podcast.wav"}