[{"content": "| Methods | Taxonomy | Stage | P-stage Efficient | D-stage Efficient | KV Cache Size | Prefilling Complexity | Decoding Complexity | \n|---|---|---|---|---|---|---|---| \n| Codestral Mamba (team, 2024) | Gated Linear RNN | \u2776 | \u2713 | \u2713 | O(k) | O(kn) | O(km) | \n| Jamba (Lieber et al., 2024) | Gated Linear RNN + Full Attention | \u2776 | \u2713 | \u2713 | O(n) | O(n\u00b2) | O(nm) | \n| LLMLingua-2 (Pan et al., 2024) | Prompt Compression | \u2776 | \u2713 | \u2717 | O(\u03b1n) | O(\u03b1\u00b2n\u00b2) | O(\u03b1nm) | \n| A-shape (Xiao et al., 2024b) | Sparse Attention | \u2776 | \u2713 | \u2717 | O(n) | O(kn) | O(nm) | \n| Tri-shape | Sparse Attention | \u2776 | \u2713 | \u2717 | O(n) | O(kn) | O(nm) | \n| MInference (Jiang et al., 2024) | Sparse Attention | \u2776 | \u2713 | \u2717 | O(n) | O(kn) | O(nm) | \n| StreamingLLM (Xiao et al., 2024b) | KV Cache Dropping | \u2777 | \u2717 | \u2713 | O(k) | O(n\u00b2) | O(km) | \n| SnapKV (Li et al., 2024c) | KV Cache Dropping | \u2777 | \u2717 | \u2713 | O(k) | O(n\u00b2) | O(km) | \n| PyramidKV (Cai et al., 2024) | KV Cache Dropping | \u2777 | \u2717 | \u2713 | O(k) | O(n\u00b2) | O(km) | \n| KIVI (Liu et al., 2024e) | KV Cache Quantitation | \u2777 | \u2717 | \u2713 | O(n) | O(n\u00b2) | O(nm) | \n| CacheBlend (Yao et al., 2024a) | KV Cache Retrieval | \u2778 | \u2713 | \u2717 | O(n) | O(n\u00b2) | O(nm) | \n| Quest (Tang et al., 2024) | KV Cache Loading | \u2779 | \u2717 | \u2713 | O(n) | O(n\u00b2) | O(km) | \n| RetrievalAttention (Liu et al., 2024b) | KV Cache Loading | \u2779 | \u2717 | \u2713 | O(n) | O(n\u00b2) | O(km) |", "caption": "Table 1: We evaluated long-context methods on SCBench, where n\ud835\udc5bnitalic_n represents the token size of the input prompt and m\ud835\udc5amitalic_m represents the generation token size, with n\u226bmmuch-greater-than\ud835\udc5b\ud835\udc5an\\gg mitalic_n \u226b italic_m.", "description": "This table provides a taxonomy of long-context methods evaluated on SCBench, categorizing them by the stage of optimization (KV Cache Generation, Compression, Retrieval, Loading), the efficiency of their operations (pre-filling and decoding stages), the resulting KV cache size (O(n), O(k)), and the computational complexity of pre-filling and decoding. The table uses 'n' to represent the input prompt token size and 'm' for the generation token size, with 'n' significantly larger than 'm'.  A checkmark indicates the method employs an efficient operation at the corresponding stage (P-stage: pre-filling, D-stage: decoding).", "section": "2 A KV CACHE-CENTRIC PERSPECTIVE ON LONG-CONTEXT METHODS"}, {"content": "| Task | Description | Capability | Avg. Input Length | Avg. Output Length | #Sessions / #Turns |\n|---|---|---|---|---|---| \n| Retr.KV | Key-value retrieval from many key-value pairs | String Retrieval | 125K | 943 | 100/500 |\n| Retr.Prefix-Suffix | Find string with specific prefix and suffix in a dict | String Retrieval | 112K | 914 | 100/500 |\n| Retr.MultiHop | Tracking variables assignment in a long input | String Retrieval | 124K | 410 | 90/450 |\n| Code.RepoQA | Functions retrieval from a GitHub repo | Semantic Retrieval | 65K | 6,058 | 88/440 |\n| En.QA | English Question Answering | Semantic Retrieval | 198K | 272 | 69/351 |\n| Zh.QA | Chinese Question Answering | Semantic Retrieval | 1.5M | 322 | 35/189 |\n| En.MultiChoice | English Multi-Choice Questions | Semantic Retrieval | 188K | 215 | 58/299 |\n| Math.Find | Math computation tasks within long sequence arrays | Global Information | 120K | 172 | 100/240 |\n| ICL.ManyShot | Hundreds-shot in-context learning | Global Information | 22K | 975 | 54/270 |\n| En.Sum | Summarize a doc given multiple docs as input | Global Information | 104K | 1,170 | 79/350 |\n| Mix.Sum+NIAH | Multi-tasking of En.Sum and Needle in A Haystack | Multi-tasking | 105K | 3,441 | 70/560 |\n| Mix.RepoQA+KV | Multi-tasking of RepoQA and KV retrieval | Multi-tasking | 68K | 5,318 | 88/704 |\n| **Total** | - | - | **227K** | **1,684** | **931/4,853** |", "caption": "Table 2: Overview of SCBench tasks.", "description": "SCBench tasks are categorized by capability (String Retrieval, Semantic Retrieval, Global Information, Multi-tasking) and include metrics, average input/output lengths, and the number of sessions/turns.", "section": "3.1 LONG-CONTEXT TASK DETAILS"}, {"content": "| Task | Source | Configuration | Example |\n|---|---|---|---| \n| Retr.KV | Lost in the Middle (Liu et\u00a0al., 2024d) | num kv pairs = 2500<br>len of key & value = 36<br>metric = Accuracy | Input: {`<key #1>`: `<value #1>`, \u2026, `<key #100>`: `<value #100>`} <br>Turn 1: The value of the `<key #1>` is? Answer 1: \u2026`<value #1>`\u2026<br>Turn 2: The value of the `&lt;key #20&gt;` is? Answer 2: \u2026`&lt;value #20&gt;`\u2026<br>Turn 3: The value of the `&lt;key #40&gt;` is? Answer 3: \u2026`&lt;value #40&gt;`\u2026 |\n| Retr.Prefix-Suffix | Ours | size of dict = 6000<br>len of string = [65, 123)<br>metric = Accuracy | Input: Dictionary = [`<str #1>`, `<str #2>`, \u2026, `<str #100>`]<br>Turn 1: Prefix: `<px #1>`; Suffix: `<sx #1>`. The word with both prefix and suffix from the dict is? Answer: `<str>`<br>Turn 2: Prefix: `<px #2>`; Suffix: `<sx #2>`. Answer: `<str>` |\n| Retr.MultiHop | RULER (Hsieh et\u00a0al., 2024) | num chains = 2<br>num hops = 2<br>metric = Accuracy | Input: VAR `<X1>` = `12345` \u2026\u2026 VAR `<Y1>` = `54321` \u2026..`<noise>`<br>VAR `<X2>` = X1 \u2026\u2026 VAR Y2 = Y1 \u2026\u2026`<noise>`<br>VAR `<X3>` = X2 \u2026\u2026 VAR Y3 = Y2 \u2026\u2026`<noise>`<br>Turn 1: Variables that are assigned to `12345`? Answer 1: `<X1 X2 X3>`<br>Turn 2: Variables that are assigned to 54321? Answer 1: Y1 Y2 Y3 |\n| Code.RepoQA | RepoQA (Liu et\u00a0al., 2024c) | func description from GPT-4<br>metric = Pass@1 | Input: `<func 1>` + `<func 2>` + \u2026 + `<func 100>`<br>Turn 1: `<description of func 1>`. Answer 1: `<func 1>`<br>Turn 2: `<description of func 20>`. Answer 2: `<func 20>` |\n| En.QA<br>Zh.QA | InfiniteBench (Zhang et\u00a0al., 2024a) | ground_truth from human<br>metric = Accuracy | Input: Read the book below and answer a question. `<context>`<br>Turn 1: `<question>` Be very concise. Answer 1: \u2026`<ans>`\u2026<br>Turn 2: `<question>` Be very concise. Answer 2: \u2026`<ans>`\u2026 |\n| En.MultiChoice | InfiniteBench (Zhang et\u00a0al., 2024a) | ground_truth from human<br>metric = Accuracy | Input: Read the book and answer the question. `<context>`<br>Turn 1: `<question>` + `<Option A,B,C,D>`. Answer 1: \u2026`<ans>`\u2026<br>Turn 2: `<question>` + `<Option A,B,C,D>`. Answer 2: \u2026`<ans>`\u2026 |\n| Math.Find | Ours | len_array=30000<br>num_digits=3<br>metric = Accuracy | Input: `<a large array of number>`<br>Turn 1: The `<max number>` in the array is? Answer 1: \u2026`<max number>`\u2026<br>Turn 2: The `<max number>` in the array is? Answer 2: \u2026`<max number>`\u2026 |\n| ICL.ManyShot | ManyShotICL (Srivastava et\u00a0al., 2023) | num_examples = ~150<br>Tasks = date, salient, tracking7<br>metric = Accuracy | Input: ICL Demo. 1 + Demo. 2 + \u2026.. + Demo. 1000<br>Turn 1: `<question>`. Answer 1: \u2026`<ans>`\u2026<br>Turn 2: `<question>`. Answer 2: \u2026`<ans>`\u2026 |\n| En.Sum | Ours | Concatenated arXiv papers<br>ground_truth from GPT-4<br>num document = ~8<br>metric = ROUGE | Input: `<Doc 1>` + Doc 2 + Doc 3 + \u2026 + Doc 10.<br>Turn 1: Please summarize `<Doc 1>`. Answer 1: \u2026 `<summary of Doc 1>`\u2026<br>Turn 2: Please summarize Doc 3. Answer 2: \u2026 `<summary of Doc 3>`\u2026<br>Turn 3: Please summarize Doc 5. Answer 2: \u2026 `<summary of Doc 5>`\u2026 |\n| Mix.Sum+NIAH | Ours | num needle = 5<br>num document = ~8<br>metric = ROUGE + Acc | Input: `<Doc 1>` + `<Passkeys>` + Doc 2 + \u2026 + `<Passkeys>` + Doc 10.<br>Turn 1: Please summarize `<Doc 1>`. Answer 1: \u2026`<summary>` of Doc 1\u2026<br>Turn 2: What is the needle? Answer 2: ..`<needle>`\u2026 |\n| Mix.RepoQA+KV | Ours | num KV pairs = ~100<br>metric = Pass@1 + Acc | Input: `<func 1>` + KV pairs + `<func 2>` + \u2026 + KV pairs + `<func 100>`<br>Turn 1: `<description of func 1>`. Answer 1: `<func 1>`<br>Turn 2: The value of the `<key #1>` is? Answer 2: \u2026`<value #1>`.. |", "caption": "Table 3: Task examples and configurations in SCBench.\nWe use different colors to highlight the questions, answers, and distractors in our examples.", "description": "This table provides examples and configurations for the tasks included in SCBench. It showcases the diversity of tasks, including string retrieval, semantic retrieval, global information processing, and multi-tasking, across various domains like code, retrieval, question answering, summarization, in-context learning, multi-hop tracing, and multi-tasking.  The table illustrates the input format, expected output, evaluation metrics, and specific configurations (e.g., number of key-value pairs, dictionary size, number of chains/hops, etc.) for each task. Color-coding is used to distinguish between questions, correct answers, and distractor information within the examples.", "section": "3.1 LONG-CONTEXT TASK DETAILS"}, {"content": "| Retr.KV |", "caption": "Table 4: Average performance of various long-context methods across different base models in two shared context modes on SCBench. For additional results on base models such as Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K, see Table\u00a010 in \u00a7D.\nHere, \u03c4\ud835\udf0f\\tauitalic_\u03c4 denotes the target compression rate.", "description": "This table presents the average performance of various long-context methods, evaluated across different base large language models (LLMs).  The evaluation uses SCBench, a new benchmark designed to assess performance in scenarios involving shared context and multiple rounds or requests.  These scenarios are common in real-world applications, where the same context (e.g., a conversation history) is reused across multiple interactions. The table is divided based on two \"shared context modes\": Multi-turn Mode, where the context is cached within a single session (like a continuous conversation), and Multi-request Mode, where the context is cached across multiple sessions (like different users interacting with the same information source).  The metrics reported are average performance scores across various tasks within SCBench, covering string retrieval, semantic retrieval, global information processing, and multi-tasking capabilities of LLMs. The table also includes a compression rate (\u03c4), indicating the level of context compression applied by certain methods.", "section": "4 EXPERIMENTS & RESULTS"}, {"content": "| Lost in the Middle |\n|---| \n| (Liu et\u00a0al., 2024d) |", "caption": "Table 5: Results of query-awareness long-context methods. w/ (first) and w/o (later) query.", "description": "This table presents a comparison of the performance of three query-aware long-context methods on a subset of SCBench tasks. The methods include SnapKV (KV cache dropping), Tri-shape (sparse attention), and MInference (dynamic sparse attention). Results are shown for Llama-3.1-8B with and without the query provided during inference, to assess the impact of query awareness on performance in KV cache reuse scenarios.  The metrics used are Retr.String (string retrieval), Retr.Semantic (semantic retrieval), Global (Global Information), and Multi-task, representing different capabilities of long-context models.  The underlined values represent the scores of methods when the query is NOT provided during the later rounds of testing.", "section": "5. Analysis"}, {"content": "| num kv pairs = 2500 |\n|---|---| \n| len of key & value = 36 | \n| metric = Accuracy |", "caption": "Table 6: Comparison of Long-Context Benchmarks.", "description": "This table compares existing long-context benchmarks, including LongBench, InfiniteBench, RULER, LongCTXBench, HELMET, Michelangelo, and SCBench (the benchmark introduced in this paper). The comparison focuses on several key aspects: the types of long-context capabilities assessed by each benchmark (precise retrieval, semantic retrieval, global information processing, and multi-tasking), the types of requests considered (single question, multi-turn, and multi-request), and whether the benchmark's implementation involves reusing the key-value cache, a crucial aspect for efficient handling of long contexts.", "section": "RELATED WORKS"}, {"content": "| Retr.Prefix-Suffix | \n|---|", "caption": "Table 7: Comparing the summarization capability of efficient long-context methods on prior benchmarks and our SCBench.", "description": "This table presents a comparison of the summarization capabilities of various efficient long-context methods, using both prior benchmarks (InfiniteBench and LongBench) and the newly introduced SCBench.  Results are presented for various turns in multi-request scenarios in SCBench, as well as overall performance on InfiniteBench and LongBench.  This comparison aims to highlight the unique insights offered by SCBench, especially in evaluating performance under multi-request settings which are not typically covered by existing benchmarks. The table demonstrates how SCBench can reveal the strengths and weaknesses of different long-context methods in handling summarization tasks, particularly their ability to maintain performance across multiple requests within a shared context.", "section": "4 EXPERIMENTS & RESULTS"}, {"content": "| Ours |", "caption": "Table 8: Comparing the retrieval capability of efficient long-context methods on prior benchmarks and our SCBench.", "description": "This table compares the retrieval performance of various efficient long-context methods on existing benchmarks (InfiniteBench and LongBench) and the newly proposed SCBench. It focuses on string retrieval tasks and includes results for different request modes (multi-request, turn 1-5). The goal is to highlight how these methods perform in retrieving information from long sequences, particularly when the context is reused across multiple queries.", "section": "4 Experiments & Results"}, {"content": "| size of dict = 6000 |\n|---|---|\n| len of string = [65, 123) |\n| metric = Accuracy |", "caption": "Table 9: Configurations of long-context methods in SCBench.", "description": "This table provides a comprehensive overview of the configurations used for various long-context methods evaluated in SCBench. It details specific settings for each method, including parameters for State Space Models (SSMs), Mamba-Attention hybrid architectures, sparse attention mechanisms (A-shape, Tri-shape, MInference), KV cache compression techniques (StreamingLLM, PyramidKV, SnapKV), KV cache quantization (KIVI), KV cache retrieval (CacheBlend), prompt compression (LLMLingua-2), and KV cache loading (Quest, RetrievalAttention).  These configurations provide insights into the architectural choices and hyperparameter settings that enable efficient long-context modeling within the benchmark.", "section": "C.2 ADDITIONAL IMPLEMENTATION DETAILS"}, {"content": "| Retr.MultiHop |\n|---|", "caption": "Table 10: The average results of various long-context methods on Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K with two shared context modes on SCBench.", "description": "This table presents the average results of various long-context methods evaluated on three large language models (LLMs): Llama-3.1-70B, Qwen2.5-32B, and Llama-3-8B-262K.  The evaluation is conducted using SCBench, a new benchmark designed to assess long-context LLM performance. Results are shown for two shared context modes: multi-turn and multi-request. The table breaks down the performance for four key long-context capabilities: string retrieval, semantic retrieval, processing global information, and multi-tasking.  An average score across all tasks is also provided. This comparison allows for analysis of how different long-context methods and varying model sizes affect performance in different context scenarios.", "section": "4 Experiments & Results"}, {"content": "| RULER |\n| --- |\n| [Hsieh et\u00a0al., 2024](https://arxiv.org/html/2412.10319v1#bib.bib34) |", "caption": "Table 11: The results breakdown of SCBench for all sub-tasks in multi-turn mode.", "description": "This table presents a detailed breakdown of the performance results of various long-context methods on each individual sub-task within SCBench, specifically focusing on the multi-turn mode. This mode evaluates how well these methods maintain performance when the context is carried over across multiple conversational turns. The table provides insights into the strengths and weaknesses of different methods on a granular level, offering a more nuanced understanding of their capabilities in handling various tasks like retrieval, question answering, summarization, and multi-tasking within an ongoing dialogue or multi-turn scenario.", "section": "4 Experiments & Results"}, {"content": "| num chains = 2 |\n|---| \n| num hops = 2 |\n| metric = Accuracy |", "caption": "Table 12: The results breakdown of SCBench for all sub-tasks in multi-requests mode.", "description": "This table presents a detailed breakdown of the performance of various long-context language models on the SCBench benchmark, specifically focusing on the *multi-request* mode.  It covers numerous sub-tasks within four key categories: String Retrieval (Retr.KV, Retr.PS, Retr.MH), Semantic Retrieval (RepoQA, En.QA, Zh.QA, En.MC), Global Information (ICL, En.Sum, Math.Find), and Multi-tasking (Mix.Sum+NIAH, Mix.RepoQA+KV).  The results are presented as scores for each model on each sub-task, allowing for a granular comparison of performance and an analysis of strengths and weaknesses across different task types and models within a multi-request context where a single context is shared among multiple requests.", "section": "Experiments & Results"}, {"content": "| Code.RepoQA | \n|---|", "caption": "Table 13: Results when disabling golden answer as context. The later number indicate the gap compared to golden-answer-as-context.", "description": "This table compares the performance of a long-context language model (Llama-3.1-8B) and several efficient long-context methods (A-shape, Tri-shape, StreamingLLM, MInference) across multiple turns of a conversation when model-generated answers from previous turns are used as context for subsequent turns, unlike the main experiments where ground-truth answers were used. The table highlights the performance difference (positive or negative) compared to using ground-truth answers, offering insights into the impact of error propagation and how different models handle context generated by themselves.", "section": "E ERROR PROPAGATION USING GENERATION AS CONTEXT."}, {"content": "| RepoQA |\n| -------- |\n| [Liu et\u00a0al., 2024c](https://arxiv.org/html/2412.10319v1#bib.bib52) |", "caption": "Table 14: Case Study of En.Sum. We use blue to indicate mising informaiton, and orange to mark potential hallucination.", "description": "This table presents a case study of the En.Sum (English Summarization) task, comparing the performance of various large language models (LLMs) and long-context approaches.  It includes a ground truth summary and the responses generated by different models and methods, such as Jamba-1.5-Mini, Llama variants, Qwen2.5 variants, as well as Llama models with added A-Shape, Tri-Shape, MInference, and StreamingLLM methods. The table uses blue color to highlight missing information and orange to indicate potential hallucinations or inaccuracies in the generated summaries.", "section": "Case Study"}, {"content": "| func description from GPT-4 |\n|-----------------------------|\n| metric = Pass@1 |", "caption": "Table 15: Case Study of Retr.Prefix-Suffix. Orange is used to mark the difference of model response compared to the ground truth.", "description": "This table presents a case study of the Retr.Prefix-Suffix task, which evaluates the ability of long-context LLMs and efficient long-context methods to retrieve strings with specific prefixes and suffixes within a large dictionary.  It highlights the difference in model responses compared to the ground truth, using orange color to mark these discrepancies. By analyzing these differences, the study aims to showcase the impact of model architecture and efficiency techniques on string retrieval accuracy, especially in scenarios where exact matching of both prefix and suffix is required.", "section": "Case Study"}, {"content": "| En.QA |\n| --- |\n| Zh.QA |", "caption": "Table 16: Case Study of Mix.RepoQA + KV. Orange indicate the potential model hallucination.", "description": "This table presents a case study comparing the responses of Llama-3.1-70B and Llama-3.1-70B with MInference on the Mix.RepoQA + KV multi-tasking benchmark.  The task involves retrieving a key-value pair and reproducing a Python function. The table highlights differences in function reproduction and minor hallucinations (marked in orange) in the model outputs compared to the ground truth.", "section": "F. CASE STUDY"}, {"content": "| InfiniteBench |\n|---| \n| [Zhang et\u00a0al., 2024a](https://arxiv.org/html/2412.10319/main.pdf) |", "caption": "Table 17: Case Study of Retr.KV to compare A-shape and Tri-shape.", "description": "This table presents a case study comparing the performance of A-shape and Tri-shape sparse attention methods on the Retr.KV (key-value retrieval) task. It showcases example responses from Llama-3.1-70B with and without these sparse attention methods, highlighting Tri-shape's ability to maintain instruction-following capabilities, unlike A-shape, which disrupts task structure and generates incomplete outputs.", "section": "Case Study"}]