[{"figure_path": "https://arxiv.org/html/2412.10319/x1.png", "caption": "Figure 1: \nKV Cache lifecycle.\nPrior benchmarks focus on single-request, while real-world applications reuse KV cache across requests. We propose SCBench and categorize long-context methods into KV Cache Generation, Compression, Retrieval, and Loading from a KV-cache-centric perspective.", "description": "This figure illustrates the lifecycle of a key-value (KV) cache in long-context large language models (LLMs). Traditional benchmarks focus on single requests, ignoring the reuse of KV caches across requests common in real-world applications. The proposed benchmark, SCBench, addresses this gap by considering the full KV cache lifecycle, categorizing long-context methods into generation, compression, retrieval, and loading stages. The diagram contrasts single-request and multi-request LLM interactions with their corresponding KV cache processes.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.10319/x2.png", "caption": "(a) Two Shared Context Modes", "description": "The figure shows two shared context modes used for evaluating long-context language models: Multi-turn Mode and Hinted KV Cache Multi-request Mode. In Multi-turn Mode (1), the KV cache generated from previous turns within the *same session* is reused for subsequent turns.  Each turn involves a query (Q) and answer (A) pair, and the LLM stores information from previous turns (Q1, A1, Q2, A2\u2026) in the KV cache. This mode simulates a conversational setting.  The Hinted KV Cache Multi-request Mode (2) allows for KV cache reuse across multiple requests, potentially even across different users or sessions. A 'hinted' KV cache from a previous request is provided as input, alongside the new request. This mode emulates scenarios like code repository access where multiple users interact with a shared context. Both modes assess an LLM's ability to leverage and access information stored in the KV cache to efficiently respond to subsequent queries or requests within and across contexts.", "section": "Long-context Shared Context Modes Details"}, {"figure_path": "https://arxiv.org/html/2412.10319/x3.png", "caption": "(b) Overview of SCBench", "description": "SCBench assesses four key long-context abilities (string retrieval, semantic retrieval, global information, and multi-tasks) across 12 tasks with two shared context modes (multi-turns, multi-requests). Each test example includes a shared context and multiple follow-up queries.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.10319/x4.png", "caption": "Figure 2: Long-context tasks often involve contexts sharing, e.g., multi-turn dialogues, multi-step reasoning, and repository-level tasks. (a) Illustration of two common shared-context patterns. (b) Overview of tasks and scenarios covered by our benchmark, encompassing four categories of long-context abilities and two shared-context modes.", "description": "Figure 2 visualizes the concept of shared contexts in long-context tasks, which is central to the SCBench evaluation.  Subfigure (a) illustrates two common shared-context patterns: 1) A multi-turn dialogue where context is carried within a single session.  2) A multi-request scenario where context is shared across multiple sessions, even potentially with different users (like a shared code repository). Subfigure (b) offers an overview of SCBench, categorized by long-context capabilities (String Retrieval, Semantic Retrieval, Global Information, and Multi-tasking) and shared context modes (Multi-turn and Multi-request).  This overview shows how the benchmark covers a broad range of long-context scenarios and abilities, all focused on evaluating the effectiveness of KV cache mechanisms.", "section": "BENCHMARK BUILDING"}, {"figure_path": "https://arxiv.org/html/2412.10319/x5.png", "caption": "(a) Performance Across Different Requests", "description": "This figure, located in the \"Experiments & Results\" section, illustrates how various long-context methods perform when handling multiple requests involving a shared context. Specifically, it shows the accuracy of these methods on a set of tasks designed to test their ability to retrieve and utilize information from a lengthy input.  The x-axis represents the number of requests made, while the y-axis represents the accuracy achieved. Different lines represent different long-context methods, categorized by their memory usage during decoding (either O(n) or sub-O(n)). The key takeaway is that methods with O(n) memory usage show improving performance as the number of requests increases, whereas methods with sub-O(n) memory only perform well on the initial request but struggle with subsequent ones. This highlights the importance of memory capacity in handling multiple requests that rely on a shared context.", "section": "4 EXPERIMENTS & RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.10319/x6.png", "caption": "(b) Performance in Different Abilities", "description": "This radar chart displays the performance of different long-context methods across various task categories, including Retrieval L1, Retrieval L2, Global Information, and Multi-tasks, in both multi-turns and multi-requests settings. Each vertex of the radar chart represents a task category, and the distance from the center indicates the average performance of a given method on that task. The shaded area enclosed by the connected points represents the overall performance profile of a long-context method. Different colors are used to distinguish between different methods, allowing for easy comparison of their strengths and weaknesses across different task categories.  Figure 3b shows that long context models show a performance drop on Retrieval L1 task, where methods with O(n) memory cost outperform other methods by a large margin. While methods with sub-O(n) memory cost achieve close performance on other task categories, and even slightly outperform O(n) method on Multi-task.", "section": "4 Experiments & Results"}, {"figure_path": "https://arxiv.org/html/2412.10319/x7.png", "caption": "Figure 3: Overview of performance results for SCBench. (a) Performance trends of various long-context methods across multiple requests. Methods with O\u2062(n)\ud835\udc42\ud835\udc5bO(n)italic_O ( italic_n ) memory cost in decoding\nshow improving performance as requests increase. In contrast, methods with sub-O\u2062(n)\ud835\udc42\ud835\udc5bO(n)italic_O ( italic_n ) KV cache in decoding, like KV cache dropping methods,\nperform well only in the first request.\n(b) Specific performance of different long-context methods across various long-context capability tasks. All evaluated long-context methods exhibit some loss in Retrieval capability while largely maintaining Global Information processing capability.", "description": "This figure provides a general overview of how different long-context methods performed in SCBench across different tasks and scenarios.\n\nFigure 3(a) showcases the performance trends of various long-context methods as the number of requests increases in SCBench. The x-axis represents the number of requests, while the y-axis represents the accuracy. The methods are categorized based on their KV cache memory costs during decoding: O(n) (linear) and sub-O(n) (sublinear). The plot shows that methods with linear memory costs generally improve or maintain their performance as requests increase, while methods with sublinear memory costs often perform well only in the first request but degrade as requests increase.\n\nFigure 3(b) displays the specific performance of each long-context method on different long-context capability tasks. The x-axis represents the four capabilities: Retrieval L1, Retrieval L2, Global Information, and Multi-tasks. The y-axis represents the accuracy. Each capability is further divided into multi-turn and multi-request scenarios. The radar chart shows that almost all long-context methods exhibit some loss in retrieval capability while largely maintaining global information processing capability.", "section": "4 EXPERIMENTS & RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.10319/x8.png", "caption": "Figure 4: Performance of various long-context methods at different compression rates on SCBench using Llama-3.1-8B\u00a0(Dubey et\u00a0al., 2024).", "description": "This figure evaluates the performance of various long-context methods on SCBench with varying compression rates using the Llama-3.1-8B model as a base. Compression rate refers to the ratio between the size of the compressed KV cache and the original one. The x-axis of the plot is the compression rate, and the y-axis is the average accuracy across all SCBench tasks under the multi-request setting.  Each line in the graph corresponds to one specific long-context method with varying compression rates, illustrating how performance changes with memory reduction. Lower compression rates signify higher memory savings but potentially larger performance drops, highlighting the trade-off between efficiency and effectiveness. The observation is that most methods can maintain reasonable performance when the compression rates are above 1/4, as the context can still be captured even with a certain level of compression or sparsity.  However, as the compression rate grows larger, the model will lack important information or connectivity for proper generation, which explains the substantial performance degradation for all the approaches at a 1/8 compression rate or lower.", "section": "4 EXPERIMENTS & RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.10319/x9.png", "caption": "Figure 5: The sparse attention methods framework.", "description": "This figure visually compares two sparse attention patterns: A-shape and Tri-shape. The triangular shape within the attention matrix represents the areas where attention is focused. The sink, local, and last window query regions are highlighted. Tri-shape includes an additional last window query region compared to A-shape.", "section": "2 A KV CACHE-CENTRIC PERSPECTIVE ON LONG-CONTEXT METHODS"}, {"figure_path": "https://arxiv.org/html/2412.10319/x10.png", "caption": "(a) String Retrieval", "description": "This figure, located in **Section 4 (Experiments & Results)**, illustrates the performance of various long-context methods, specifically for string retrieval tasks, over multiple turns.  It shows how the accuracy of different methods changes as the conversation progresses (from Turn 1 to Turn 5). The comparison includes methods like Full Attention, A-Shape, Tri-Shape, MInference, StreamingLLM, SnapKV, LLMLingua-2, and Quest, all evaluated against a baseline LLM. The x-axis represents the turn number, while the y-axis indicates the task accuracy. This visualization helps to understand how well different approaches maintain performance in string retrieval as more context is added to the conversation.", "section": "4. Experiments & Results"}, {"figure_path": "https://arxiv.org/html/2412.10319/x11.png", "caption": "(b) Semantic Retrieval", "description": "Figure 3 (b) presents the performance of various long-context methods on semantic retrieval tasks within SCBench.  The figure shows how each method performs across four key sub-categories of semantic retrieval, providing a visual comparison of their effectiveness in this specific capability area.  It allows for the evaluation of how well different optimizations for handling long sequences perform when tasked with understanding meaning and context, rather than just matching strings.", "section": "4. Experiments & Results"}, {"figure_path": "https://arxiv.org/html/2412.10319/x12.png", "caption": "(c) Global Information", "description": "This figure, belonging to the \"Experiments & Results\" section, presents the performance of various long-context methods on Global Information tasks and turns within SCBench. These tasks assess the models' capacity to process and aggregate information from the entire context, encompassing areas like summarization, statistical tasks, and in-context learning. The downward trend across turns for several methods indicates potential challenges in maintaining performance with increased context length or repeated queries within the same context. This visualization allows for comparisons between sparse and dense methods, prompt compression techniques, and hybrid models in managing global information effectively.", "section": "4 Experiments & Results"}, {"figure_path": "https://arxiv.org/html/2412.10319/x13.png", "caption": "Figure 6: Performance of different long-context methods across various tasks and turns. The results for multi-tasking tasks are shown in Fig.\u00a010, and the results are averaged across all tested base LLMs.", "description": "This figure, located in Section 4 of the paper, presents a performance comparison of various long-context methods (Full Attention, A-shape, Tri-shape, MInference, StreamingLLM, SnapKV, LLMLingua-2, Quest) across different tasks and conversation turns in the SCBench. The tasks are categorized into three main groups: String Retrieval, Semantic Retrieval, and Global Information. The figure shows the accuracy trends of each method across five conversation turns (Turn 1 to Turn 5). The results for multi-tasking tests are presented in Figure 10. The results are averaged across all the base LLMs tested in the benchmark.", "section": "4 EXPERIMENTS & RESULTS"}]