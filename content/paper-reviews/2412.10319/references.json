{"references": [{"fullname_first_author": "Yucheng Li", "paper_title": "SCBENCH: A KV CACHE-CENTRIC ANALYSIS OF LONG-CONTEXT METHODS", "publication_date": "2024-12-13", "reason": "This paper introduces SCBench, a novel benchmark designed to evaluate long-context language models by focusing on the key-value cache lifecycle and shared context scenarios, thus addressing the gap of existing benchmarks in evaluating real-world long-context applications."}, {"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention-2: Faster attention with better parallelism and work partitioning", "publication_date": "2024-00-00", "reason": "This paper introduces FlashAttention-2, an optimized attention mechanism with improved parallelism and work partitioning, which significantly accelerates long-context LLM inference by reducing computational overhead."}, {"fullname_first_author": "Huiqiang Jiang", "paper_title": "MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention", "publication_date": "2024-00-00", "reason": "This paper introduces MInference, a state-of-the-art dynamic sparse attention approach that reduces the computational complexity of the pre-filling stage in long-context LLMs while improving accuracy."}, {"fullname_first_author": "Tri Dao", "paper_title": "Transformers are SSMs: Generalized and efficient algorithms through structured state space duality", "publication_date": "2024-00-00", "reason": "This paper demonstrates the duality between Transformers and State Space Models (SSMs), providing a new perspective on understanding and optimizing Transformers for long sequences and potentially offering a pathway to more efficient long-context models."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The Llama 3 Herd of Models", "publication_date": "2024-07-00", "reason": "This paper introduces the Llama 3 models, which represents the state-of-the-art open-source long-context LLMs, and are used as the backbone models for evaluating different long-context methods in the proposed SCBench."}]}