[{"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/input-settings.png", "caption": "Figure 1. VLMs are able to process a multimodal presentation in various unimodal and multimodal representations.", "description": "This figure illustrates the various ways Vision-Language Models (VLMs) can handle input data from multimodal presentations.  It shows that a VLM can accept input in several forms:  a raw multimodal presentation record (containing audio and video),  a transcript of the presentation's speech, individual slides extracted from the presentation, an unstructured combination of slides and a transcript, and finally, a structured multimodal representation where slides and transcript segments are interleaved according to their timing in the presentation.  Each of these different input formats may affect the cost and performance of the summarization process.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/venn_coverage.png", "caption": "Figure 2. Average token count in speech transcript and OCR, and their overlap", "description": "This figure shows a Venn diagram illustrating the quantitative relationship between the number of tokens in the speech transcript, the number of tokens extracted from the slides via OCR, and the overlap between these two sources.  The numerical values represent the average token counts obtained from the TIB dataset after preprocessing. The overlap region indicates tokens that are present in both the transcript and the OCR data, highlighting the extent to which the speech and the slides convey redundant information. This is useful for understanding data redundancy and potential areas of improved efficiency in multimodal summarization.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/slides_vs_video.png", "caption": "Figure 3. Unimodal performance at different token budgets", "description": "Figure 3 illustrates the performance of a single-modality input (video, slides, or transcript) on a summarization task using the Qwen2-VL model.  The x-axis represents the visual token budget per frame, reflecting the amount of visual information provided to the model. The y-axis displays the ROUGE-1 score, a common metric for evaluating the quality of generated summaries.  The graph compares the performance of different input types across various visual token budget levels, revealing the relative effectiveness of each modality for summarization at different computational costs.", "section": "5 Fine-grained Analysis"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/register_tokens.png", "caption": "Figure 4. The addition of structure improves the Rouge score compared to the transcript alone", "description": "This figure demonstrates the impact of adding structural information to the model's input on the Rouge score, a metric that measures the quality of generated summaries.  The experiment compares the performance of summarization using only the transcript versus using the transcript combined with slides, with the slides and transcript tokens interleaved according to their temporal order in the presentation. The results show that the addition of structured multimodal input (interleaved slides and transcript) significantly improves the Rouge score compared to using only the transcript.  The improvement showcases that incorporating the temporal relationship between visual and textual information enhances summarization performance.", "section": "5.1 Impact of the structure and modalities"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/visual_tokens.png", "caption": "Figure 5. Rouge score with different visual token budgets", "description": "This figure displays the relationship between Rouge scores (a common metric for evaluating the quality of text summarization) and varying visual token budgets for different input representations to a vision-language model (VLM).  The different input representations include using only the transcript, only slides, a combination of both, and structured/unstructured versions of the combined modalities. The graph shows how the effectiveness of the model changes as more visual information (represented by the visual token budget) is provided. It demonstrates the trade-off between the quantity of visual data and the improvement in summarization quality, considering various input types and the impact of structured text and image combinations.", "section": "5.2 Model and visual token budget scaling"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/model_scaling.png", "caption": "Figure 6. Rouge score with varying visual token budgets for different model sizes", "description": "This figure illustrates how the performance of different sized language models (2B and 7B parameters) on the summarization task varies based on the amount of visual information provided (visual token budget).  It examines the Rouge-1 score, a common metric for evaluating the quality of generated summaries, across various visual token budgets and for different model sizes. The graph visually represents the impact of increasing visual token budget on summarization performance for each model size, highlighting the interplay between model capacity and visual input.", "section": "5.2 Model and visual token budget scaling"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/average_length.png", "caption": "Figure 7. \nSlides-only and structured multimodal representations constitute input-length-optimal representations for different input lengths. Bigger icons depict higher visual token budget.", "description": "Figure 7 illustrates the trade-off between the input length and the ROUGE-1 score, a metric for evaluating the quality of generated summaries.  The x-axis shows the average input token length, encompassing various combinations of unimodal (slides only) and multimodal (slides and transcripts) inputs.  The y-axis represents the ROUGE-1 score. Different marker shapes and sizes denote specific input configurations; larger markers indicate higher visual token budgets (more visual information used). The figure highlights the Pareto frontier of input-length-optimal representations.  This means that within certain ranges of input length, some representations offer the best ROUGE-1 scores relative to the input token length.  For shorter input lengths, using slides alone is the most efficient.  For longer input lengths, using the structured multimodal representation (interleaved slides and transcripts) is optimal.", "section": "5.3 Cost-effective input settings"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_coverage.png", "caption": "(a) Unimodal or multimodal input", "description": "This figure shows the extractive coverage (the extent of vocabulary overlap between the generated summary and the source text) and extractive density (average length of extractive fragments from the source in the summary) for different input settings.  Part (a) shows the results for unimodal (single modality, such as just transcript or just images) and multimodal (combining modalities, such as transcript and images) inputs.  The bars represent the average extractive coverage and density for each input type and modality, making it easy to compare how well summaries generated from different input combinations retain information from each of the source modalities.", "section": "5.4 Extractiveness and relevance of the extractive fragments with respect to input modalities"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_coverage_struct.png", "caption": "(b) Structured or unstructured input", "description": "This figure compares the extractive coverage and density of summaries generated using structured versus unstructured multimodal inputs.  'Structured' refers to a representation where visual (slide) and textual (transcript) data are interleaved, reflecting their temporal alignment in the original presentation. 'Unstructured' signifies a concatenation of all slides followed by the entire transcript, lacking explicit structural information.  The comparison highlights how the organization of multimodal data affects the model's ability to extract and utilize information relevant to the original sources (speech transcript and slide OCR text).", "section": "5 Fine-grained Analysis"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_density.png", "caption": "Figure 8. Extractive coverage of the predicted summaries with reference to input modality", "description": "This figure shows the extractive coverage of the generated summaries, comparing how much of the vocabulary from the different input modalities (slides' OCR text, transcript, and the combination of both) is present in the generated summaries.  It's a measure of how much of the original information sources are directly reflected in the summaries. The figure helps to understand if the summarization model is accurately incorporating the information from the different input modalities.", "section": "5.4 Extractiveness and relevance of the extractive fragments with respect to input modalities"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_extractive_density_strcut.png", "caption": "(a) Unimodal or multimodal input", "description": "This figure shows the extractive coverage of the generated summaries, comparing unimodal (text only, slides only, images only) and multimodal inputs.  Extractive coverage measures the overlap between the vocabulary of the generated summary and the original source text (transcript, OCR from slides, or both). The chart visualizes the average coverage for each input modality, revealing how effectively different models use the various modalities to generate summaries that reflect the source material.", "section": "5.4 Extractiveness and relevance of the extractive fragments with respect to input modalities"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_relevance_score.png", "caption": "(b) Structured or unstructured input", "description": "This figure shows the extractive coverage and density of the generated summaries when using structured (interleaved slides and transcript) versus unstructured (slides followed by transcript) input.  It compares the vocabulary overlap between the generated summary and either the text extracted from slides (OCR), the speech transcript, or both (Overall). The analysis helps understand how the organization of multimodal input affects the summary's extractiveness, revealing whether a structured approach leads to summaries that are more closely aligned with the individual modalities.", "section": "5 Fine-grained Analysis"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/vlmsumm_relevance_score_struct.png", "caption": "Figure 9. Extractive density of the predicted summaries with reference to input modality", "description": "Figure 9 shows the average length of contiguous text fragments extracted from the source (speech transcript or OCR text from slides) that appear in the generated summaries.  It compares this 'extractive density' across different input modalities (unimodal or multimodal) and for structured versus unstructured input representations. Higher values indicate that longer stretches of the original source text are preserved in the summaries.", "section": "5.4 Extractiveness and relevance of the extractive fragments with respect to input modalities"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/relevance_extractiveness.png", "caption": "(a) Unimodal or multimodal input", "description": "This figure shows the extractive coverage of the generated summaries in unimodal and multimodal settings. Extractive coverage measures the overlap between the vocabulary of the generated summary and the vocabulary of the input source (speech transcript, image OCR, or both).  Panel (a) presents results for various unimodal and multimodal inputs (text only, images only, text and images). The results are shown separately for the transcript, OCR and combined (overall) input modalities.  The figure demonstrates how the inclusion of additional modalities affects the extractive coverage of the summary, revealing whether generated summaries tend to extract more information from a specific modality or a combination of modalities.", "section": "5 Fine-grained Analysis"}, {"figure_path": "https://arxiv.org/html/2504.10049/extracted/6358140/figures/relevance_extractiveness-dens.png", "caption": "(b) Structured or unstructured input", "description": "This figure presents a comparison of extractive coverage and density for different input methods (structured vs. unstructured) in the context of multimodal summarization.  Extractive coverage represents the proportion of vocabulary from the original source (speech transcripts and OCR-extracted text from slides) that is included in the generated summary. Extractive density refers to the average length of the extractive fragments from the source text present in the summary.  The figure shows that a structured input (interleaving slides and transcript) leads to higher extractive coverage and density compared to an unstructured input (all slides followed by the transcript).", "section": "5 Fine-grained Analysis"}]