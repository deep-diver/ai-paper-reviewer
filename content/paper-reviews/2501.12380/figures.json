[{"figure_path": "https://arxiv.org/html/2501.12380/x5.png", "caption": "Figure 1: \nOverview of the \\gradientRGBMMVU53,93,20310,10,80 benchmark. \\gradientRGBMMVU53,93,20310,10,80 includes 3,000 expert-annotated examples, covering 27 subjects across four core disciplines. It is specifically designed to assess multimodal foundation models in expert-level, knowledge-intensive video understanding and reasoning tasks.", "description": "MMVU is a benchmark dataset for evaluating multimodal foundation models' ability to understand and reason with videos at an expert level.  It contains 3000 expert-annotated question-answer pairs across 27 subjects within four core disciplines: Science, Healthcare, Humanities & Social Sciences, and Engineering. The questions are designed to challenge models' knowledge and reasoning capabilities by requiring them to analyze specialized videos and apply domain-specific expertise.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.12380/x6.png", "caption": "Figure 2: An overview of the \\gradientRGBMMVU53,93,20310,10,80 benchmark construction pipeline.", "description": "The figure illustrates the three-stage pipeline for creating the MMVU benchmark dataset.  Stage 1 (Preliminary Setup) involves selecting subjects through a user study and recruiting and training expert annotators.  Stage 2 (Textbook-Guided QA Annotation) details the process of collecting videos with Creative Commons licenses, creating question-answer pairs, and annotating detailed solutions and relevant domain knowledge.  The final stage (Quality Control) describes the measures used to ensure data quality, including expert validation and compensation for annotator time spent.", "section": "3 MMVU BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2501.12380/x7.png", "caption": "Figure 3: \nA dataset example from \\gradientRGBMMVU53,93,20310,10,80 with the discipline of chemistry. Each example in \\gradientRGBMMVU53,93,20310,10,80 includes expert annotation of relevant domain knowledge and step-by-step reasoning rational.", "description": "Figure 3 shows an example from the MMVU dataset, specifically focusing on a chemistry question.  The figure highlights the comprehensive nature of the dataset by showcasing the question, multiple-choice options, relevant textbook information, and a detailed, step-by-step expert-annotated reasoning process. The inclusion of textbook references and rationales demonstrates the level of detail and expert-level knowledge-intensive reasoning that MMVU aims to evaluate in multimodal foundation models.", "section": "3 MMVU BENCHMARK"}, {"figure_path": "https://arxiv.org/html/2501.12380/x8.png", "caption": "Figure 4: Comparison of model performance between CoT and direct answering on the validation set. The full results are provided in \u00a7C.1.", "description": "Figure 4 presents a bar chart comparing the performance of various multimodal foundation models on the MMVU validation set using both Chain-of-Thought (CoT) prompting and direct answering.  For each model, two bars are displayed: one representing accuracy when using CoT and another showing accuracy without CoT. This visualization allows for a direct comparison of how much the use of CoT improves model performance for each model.  The models are ordered by their overall performance on the validation set. More detailed results are available in section C.1 of the paper.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12380/x9.png", "caption": "Figure 5: \nIllustrations of visual perception error and misuse or lack domain knowledge in reasoning.", "description": "Figure 5 presents two examples highlighting common errors made by multimodal foundation models when processing video data for complex reasoning tasks.  The left panel illustrates a 'visual perception error' where the model incorrectly interprets the traversal order of a binary tree in a video, demonstrating a failure to accurately perceive visual information. The right panel showcases a 'misuse or lack of domain knowledge in reasoning' error. Here, the model incorrectly associates the presence of bats (shown in a video about a virus) with poor sanitation, leading to a false conclusion about the type of virus.  These examples demonstrate the challenges models face in correctly integrating visual and domain-specific knowledge for accurate answers.", "section": "4.3 Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2501.12380/extracted/6146392/figures/interface/interface1.png", "caption": "Figure 6: \nAnnotation Interface - Step 1: Video Collection.\nIn this step, annotators are required to input the YouTube video URL and select the desired question type. The backend system of the interface will automatically verify whether the provided YouTube video is under a Creative Commons license using the YouTube Data API v3. If the video does not meet this requirement, as shown in the figure, a warning message will be displayed, and the submission will be blocked. Once a valid example is submitted, the annotation interface will proceed to Step 2, which is illustrated in the following two figures.", "description": "This figure shows the first step in the MMVU benchmark's annotation process.  Annotators must provide a YouTube video URL and select a question type (multiple choice or open-ended). The system then automatically checks if the video has a Creative Commons license using the YouTube Data API v3. If the license is invalid, an error message appears, and submission is blocked.  Successful submission proceeds to step 2 of the annotation process.", "section": "3.2 Textbook-Guided QA Example Annotation"}, {"figure_path": "https://arxiv.org/html/2501.12380/extracted/6146392/figures/interface/interface2.png", "caption": "Figure 7: \nAnnotation Interface - Step 2: Multiple-choice Question Annotation.", "description": "This figure shows a screenshot of the annotation interface used in the MMVU benchmark creation process.  Specifically, it depicts Step 2 of the annotation process, focusing on the creation of multiple-choice questions.  The interface allows annotators to input a video's start and end times, the question text, multiple-choice options, the correct answer, relevant domain knowledge (with links to Wikipedia pages), and the reasoning process behind the correct answer.  The annotator can shuffle the options and add or remove Wikipedia links as needed.  This detailed interface ensures the quality and consistency of the expert-level annotations in the MMVU benchmark.", "section": "3.2 Textbook-Guided QA Example Annotation"}, {"figure_path": "https://arxiv.org/html/2501.12380/extracted/6146392/figures/interface/interface3.png", "caption": "Figure 8: \nAnnotation Interface - Step 2: Open-ended Question Annotation.", "description": "This figure shows a screenshot of the annotation interface used in the MMVU benchmark creation process.  Specifically, it depicts Step 2 of the annotation process, where annotators are creating and annotating open-ended questions. The interface displays a video player showing a segment of a video, fields to enter start and end times of the relevant video segment, spaces to add question text, enter the open-ended answer, specify the relevant textbook and chapter, enter related domain knowledge (linking to Wikipedia pages), and detail the reasoning process used to arrive at the answer. The interface also allows annotators to add or remove Wikipedia links supporting the domain knowledge.", "section": "3.2 Textbook-Guided QA Example Annotation"}, {"figure_path": "https://arxiv.org/html/2501.12380/extracted/6146392/figures/interface/interface4.png", "caption": "Figure 9: \nValidation Interface.\nHuman validators are required to thoroughly review each annotation feature to ensure alignment with benchmark construction criteria and annotation guidelines.\nIf revisions are not feasible, detailed feedback must be provided to the original annotator, who will then revise and resubmit the annotation for a second review.\nAdditionally, validators may discard examples deemed to be of low quality and unlikely to meet the desired criteria through revision.", "description": "This figure shows the interface used for validating annotations in the MMVU benchmark. Human validators carefully check each annotation, ensuring consistency with benchmark criteria and guidelines. If corrections are needed, detailed feedback is given to the annotator, who then revises and resubmits their work.  Low-quality annotations that cannot be improved are discarded.", "section": "Data Quality Control"}, {"figure_path": "https://arxiv.org/html/2501.12380/x10.png", "caption": "Figure 10: CoT reasoning prompt, adopted from MMMU-Pro\u00a0Yue et\u00a0al. (2024b), for answering multiple-choice question.", "description": "This figure shows the Chain-of-Thought (CoT) prompt used in the MMVU benchmark for answering multiple-choice questions.  The prompt guides the model to answer the question step-by-step, explaining its reasoning process clearly before providing the final answer. This approach encourages more detailed and transparent reasoning from the model, making it easier to analyze the model's thought process and identify potential weaknesses.  The prompt is adapted from the MMMU-Pro benchmark, indicating a lineage and methodological connection to prior work in evaluating multi-modal models. The use of CoT in this context is a significant aspect of how MMVU aims to assess expert-level reasoning.", "section": "Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2501.12380/x11.png", "caption": "Figure 11: CoT reasoning prompt for answering open-ended question.", "description": "This figure shows the Chain-of-Thought (CoT) prompt used in the MMVU benchmark for open-ended questions.  The prompt instructs the model to answer the question step-by-step, explaining its reasoning process clearly before providing the final answer.  The format for the final answer is specified to ensure consistency. The prompt includes placeholders for the question and the processed video input, highlighting the multimodal nature of the task.", "section": "Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2501.12380/x12.png", "caption": "Figure 12: Direct Answer prompt, adopted from MMMU-Pro\u00a0Yue et\u00a0al. (2024b), for answering multiple-choice question.", "description": "This figure shows the prompt used in the MMVU benchmark for multiple-choice questions when the model is instructed to directly answer without providing any reasoning steps.  It's a more straightforward approach than the chain-of-thought prompting. The prompt includes the question, the multiple-choice options (A-E), and the visual information from the video. The model is instructed to simply output the letter corresponding to the correct answer, without providing any intermediate reasoning steps.", "section": "Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2501.12380/x13.png", "caption": "Figure 13: Direct Answer prompt for answering open-ended question.", "description": "This figure shows the prompt used in the MMVU benchmark for evaluating the models' ability to directly answer open-ended questions without generating intermediate reasoning steps.  The prompt instructs the model to directly output the final answer using only the provided question and video information, without any intermediate reasoning or step-by-step explanation.", "section": "Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2501.12380/x14.png", "caption": "Figure 14: Evaluation prompt used for assessing the accuracy of multi-choice QA.", "description": "This figure shows the evaluation prompt used to assess the accuracy of the model's responses to multiple-choice questions.  The prompt instructs the evaluator (likely GPT-4) to extract the model's answer, then compare it to the ground truth, and finally output a JSON object indicating whether the extracted answer is correct. This process ensures a standardized and objective evaluation of the model's performance on multiple choice questions.", "section": "Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2501.12380/x15.png", "caption": "Figure 15: Evaluation prompt used for assessing the accuracy of open-ended QA.", "description": "This figure shows the evaluation prompt used to assess the accuracy of open-ended questions in the MMVU benchmark.  The prompt instructs the evaluator (in this case, GPT-4) to extract the final answer from the model's response and compare it to the ground truth answer.  It emphasizes that a correct answer doesn't need to be verbatim but should reflect the same technique or concept as the ground truth. The prompt also specifies the expected output format: a JSON object containing the extracted answer (as a string) and a boolean value indicating whether the answer is correct.", "section": "Experiment Setup"}, {"figure_path": "https://arxiv.org/html/2501.12380/x16.png", "caption": "Figure 16: Comparison of model performance between CoT reasoning and direct answering on the validation set.", "description": "This bar chart compares the performance of various multimodal foundation models on the MMVU validation set, using both Chain-of-Thought (CoT) reasoning and direct answering approaches.  For each model, two bars represent its accuracy scores, one for CoT and one for direct answering. This allows for a visual comparison of how each model's performance changes when using CoT prompting versus directly generating an answer. The chart helps to illustrate the effectiveness of CoT prompting in improving model performance on the MMVU benchmark.  Models are ranked by their CoT accuracy score in descending order.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12380/x17.png", "caption": "Figure 17: An error case of Thermodynamics.", "description": "This figure shows an example from the MMVU benchmark where the model incorrectly identifies a thermodynamic process.  The model is shown the animation of an adiabatic compression.  The correct answer is adiabatic compression (B), because the gas is thermally isolated and returns to its original state through compression. However, the model incorrectly identifies the process as adiabatic expansion (D). The model's reasoning is based on a misinterpretation of the graph showing pressure versus volume, and a failure to account for the thermal isolation of the system. This illustrates the challenges of accurately assessing visual information and applying domain knowledge in video understanding.", "section": "C.2 Error Case Analysis: Visual Perception Error"}, {"figure_path": "https://arxiv.org/html/2501.12380/x18.png", "caption": "Figure 18: An error case of Electromagnetism.", "description": "The figure shows a model's incorrect interpretation of a video depicting a change in a circuit's resistance. The model hallucinates the presence of water and misinterprets the change in resistance as a change in deformation, demonstrating a visual perception error.", "section": "C.2 Error Case Analysis: Visual Perception Error"}, {"figure_path": "https://arxiv.org/html/2501.12380/x19.png", "caption": "Figure 19: An error case of Art.", "description": "This figure shows an example where the model incorrectly identifies the cinematic technique used in a video.  The video shows a dolly zoom, a technique that creates a visual distortion effect by simultaneously adjusting the focal length of the lens while the camera is moving. However, the model incorrectly identifies the technique as panning, where the camera simply moves horizontally. This highlights a failure in the model's ability to accurately perceive and interpret visual motion in video.", "section": "C.2 Error Case Analysis: Visual Perception Error"}, {"figure_path": "https://arxiv.org/html/2501.12380/x20.png", "caption": "Figure 20: An error case of Computer Science.", "description": "The figure shows an example where the model incorrectly identifies the algorithm shown in a video. The video depicts a selection sort algorithm, where the algorithm repeatedly finds the minimum element from the unsorted part and puts it at the beginning. However, the model mistakenly identifies the array indices as the values themselves and therefore incorrectly identifies the algorithm as a selection sort. This highlights the model's difficulty in accurately interpreting visual information and applying domain-specific knowledge in algorithm recognition.", "section": "C.3 Error Case Analysis: Misuse or Lack Domain Knowledge in Visual Perception"}, {"figure_path": "https://arxiv.org/html/2501.12380/x21.png", "caption": "Figure 21: An error case of Electrical Engineering.", "description": "The figure shows an example where the model incorrectly identifies a resistor as an inductor in a circuit diagram. This misidentification leads to an incorrect conclusion about the type of filter implemented in the circuit.  The model's reasoning process is detailed, demonstrating its reliance on visual information and domain knowledge, but also highlighting a gap in understanding basic electrical components.", "section": "C.3 Error Case Analysis: Misuse or Lack Domain Knowledge in Visual Perception"}, {"figure_path": "https://arxiv.org/html/2501.12380/x22.png", "caption": "Figure 22: An error case of Pharmacy.", "description": "The figure showcases a qualitative analysis case study focusing on a model's error in the Pharmacy discipline within the MMVU benchmark.  The model misinterprets the visual depiction of an embryo transfer procedure. Instead of correctly identifying the procedure as embryo transfer, the model hallucinates and describes the process as fetal development. This misidentification stems from the model's inaccurate understanding of the visual elements presented in the video and a misuse of domain-specific knowledge.", "section": "Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2501.12380/x23.png", "caption": "Figure 23: An error case of Computer Science.", "description": "The figure shows a model's error in identifying a sorting algorithm from a video.  The video depicts a selection sort, where elements are repeatedly selected and placed in their correct sorted position. The model, however, misidentifies the algorithm as a bubble sort, demonstrating a failure to accurately perceive and reason over the visual steps of the sorting process and a misuse of domain-specific knowledge about visual representations of algorithms.", "section": "C.3 ERROR CASE ANALYSIS: MISUSE OR LACK DOMAIN KNOWLEDGE IN VISUAL PERCEPTION"}]