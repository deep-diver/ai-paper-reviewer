{"importance": "This paper is important because it addresses the critical need for efficient large multimodal models (LMMs), a key challenge in the field of AI.  By achieving significant improvements in efficiency and speed, while maintaining comparable performance, LLaVA-Mini opens new avenues for real-time multimodal applications and provides valuable insights into the design of efficient LMMs.  The work is particularly relevant given the computational costs associated with existing LMMs.  It demonstrates a novel approach to compressing visual information that deserves attention from other researchers developing LMMs.", "summary": "LLaVA-Mini achieves comparable performance to state-of-the-art LMMs using only one vision token, drastically reducing computational cost and latency.", "takeaways": ["LLaVA-Mini uses only one vision token while maintaining high performance.", "Modality pre-fusion significantly improves efficiency by fusing visual information into text tokens before the LLM.", "LLaVA-Mini achieves significant reductions in FLOPs (77%) and inference latency (2.9x faster) compared to LLaVA-v1.5."], "tldr": "Large multimodal models (LMMs) are powerful but computationally expensive, especially when handling images and videos. Existing efficient LMMs focus on model downsizing, neglecting the crucial issue of excessive vision tokens. This often leads to performance degradation.  This limits real-time applications. \nLLaVA-Mini tackles this by analyzing how LMMs process visual information, discovering that vision tokens are crucial in early layers. It introduces modality pre-fusion to fuse visual information into text tokens beforehand, allowing extreme compression to just one vision token.  This leads to a 77% reduction in FLOPs and 2.9x faster inference speed, while matching state-of-the-art performance on various benchmarks. LLaVA-Mini handles high-resolution images and long videos efficiently, paving the way for low-latency multimodal interactions.", "affiliation": "Key Laboratory of Intelligent Information Processing", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2501.03895/podcast.wav"}