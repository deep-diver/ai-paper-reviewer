{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond", "publication_date": "2023-08-12", "reason": "This paper introduces Qwen-VL, a strong baseline for vision-language models that is frequently compared against in the paper."}, {"fullname_first_author": "Daniel Bolya", "paper_title": "Token merging: Your vit but faster", "publication_date": "2023-10-09", "reason": "This paper proposes token merging, a method to improve the efficiency of vision transformers that is discussed and compared to the proposed method in the paper."}, {"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "publication_date": "2023-03-30", "reason": "This paper introduces Vicuna, a large language model used as the backbone for LLaVA-Mini, and its performance is frequently compared to in the paper."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-00-00", "reason": "This paper introduces Vision Transformers (ViT), the vision encoder used in LLaVA-Mini and other models, which is a key component of many of the models discussed in the paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "LLaVA: A Large Language and Vision Assistant", "publication_date": "2023-00-00", "reason": "This paper introduces LLaVA, the foundation model upon which LLaVA-Mini is built, and its performance and efficiency is frequently used as a comparison point in this paper."}]}