[{"figure_path": "https://arxiv.org/html/2501.03895/x1.png", "caption": "Figure 1: LLaVA-Mini achieves comparable performance to LLaVA-v1.5 using only 1 vision token instead of 576, yielding efficient computation, lower latency, and reduced VRAM usage.", "description": "LLaVA-Mini, a new large multimodal model, achieves performance on par with the existing LLaVA-v1.5 model.  The key improvement is its efficiency: LLaVA-Mini uses only one vision token to represent an image, compared to 576 tokens in LLaVA-v1.5. This significant reduction in tokens leads to substantially lower computational costs, faster inference times (lower latency), and reduced memory usage (VRAM).  The graph visually depicts this efficiency gain by comparing the performance of LLaVA-Mini against LLaVA-v1.5 across various benchmarks, showing comparable accuracy with drastically reduced resource requirements.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.03895/x2.png", "caption": "(a) LLaVA-v1.5-Vicuna-7B", "description": "Figure 2(a) displays the layer-wise attention weights assigned to different token types (instruction, vision, and response) in the LLaVA-v1.5-Vicuna-7B model.  The attention weight from one token type to another is shown, revealing how the model's focus shifts between these token types as the layers progress.  The visualization shows that vision tokens receive significant attention in earlier layers, where visual information is integrated into text tokens. However, the attention given to vision tokens decreases as the layers deepen.", "section": "3 How Does LLaVA Understand Vision Tokens?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x3.png", "caption": "(b) LLaVA-v1.5-Vicuna-13B", "description": "Figure 3(b) presents a graph illustrating the attention entropy across different layers in the LLaMA-13B language model within the LLaVA-v1.5 architecture.  Attention entropy, a measure of attention distribution, is plotted for three token types: instructions, vision tokens, and responses. The graph shows how the attention weight given to vision tokens decreases significantly as the model processes information through deeper layers, while attention to instruction tokens increases. This visualization helps to understand the role of vision tokens at different stages of language model processing within a multimodal architecture.", "section": "How Does LLaVA Understand Vision Tokens?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x4.png", "caption": "(c) LLaVA-v1.6-Mistral-7B", "description": "Figure 3(c) presents a graph illustrating the attention entropy assigned to different token types (instruction, vision, and response) across various layers within the LLaMA-v1.6-Mistral-7B large language model.  Attention entropy measures the distribution of attention weights, providing insights into which token type is most crucial at different processing stages.  The graph allows for a layer-wise comparison of the importance of visual information relative to textual instructions and responses, highlighting the dynamic shift in attention weights during the processing of multimodal information.", "section": "3 How DOES LLAVA UNDERSTAND VISION TOKENS?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x5.png", "caption": "(d) LLaVA-NeXT-Vicuna-7B", "description": "Figure 3(d) presents the attention entropy assigned to different token types across various layers within the LLaVA-NeXT-Vicuna-7B model.  Attention entropy measures the uncertainty or randomness in the attention distribution. High entropy indicates that attention is spread more evenly across different tokens, while low entropy indicates that attention is concentrated on a smaller subset of tokens. The plot shows how the entropy of attention weights for instruction, vision, and response tokens changes across different layers of the LLM. This visualization helps understand how the model processes and integrates visual and textual information at different stages of its reasoning process.", "section": "3 How DOES LLAVA UNDERSTAND VISION TOKENS?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x6.png", "caption": "Figure 2: Layer-wise variation of attention weights assigned to different types of tokens (including instruction, vision, and response) in LMMs. \u201cA\u2192\u2192\\rightarrow\u2192B\u201d means the attention weights from A to B.", "description": "This figure visualizes how the attention mechanism in Large Multimodal Models (LMMs) distributes attention across different token types (instruction, vision, and response) at various layers of the model.  The graphs show that vision tokens receive considerably more attention in the early layers of the LMM, indicating their crucial role in the initial stages of visual information processing.  As the model progresses through deeper layers, the attention shifts towards instruction tokens and then response tokens, reflecting the flow of information processing within the LMM.  Different line styles and colors represent different LMM configurations.", "section": "3 How Does LLaVA Understand Vision Tokens?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x7.png", "caption": "(a) LLaVA-v1.5-Vicuna-7B", "description": "Figure 3(a) presents a detailed layer-wise analysis of attention weights in the LLaVA-v1.5-Vicuna-7B multimodal model.  It illustrates the distribution of attention across different token types (instruction, vision, and response) at various layers of the LLM. This visualization helps understand how the model's reliance on visual information changes as processing progresses through the layers. The figure shows that vision tokens are heavily attended to in the early layers where visual information is initially fused with textual information. However, their importance decreases significantly in later layers, indicating that the crucial role of vision tokens is primarily in the initial integration process.", "section": "3 How DOES LLAVA UNDERSTAND VISION TOKENS?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x8.png", "caption": "(b) LLaVA-v1.5-Vicuna-13B", "description": "Figure 3(b) shows the attention entropy assigned to different types of tokens (instruction, vision, and response) across various layers in the LLaVA-v1.5-Vicuna-13B model.  Attention entropy measures the distribution of attention weights across different tokens within each layer. A higher entropy indicates more even distribution of attention across tokens, while a lower entropy means attention is concentrated on a few tokens. This figure helps to visualize how the importance of vision tokens changes across layers of the LLM, showing that vision tokens are more important in earlier layers and less crucial in later layers, implying that the fusion of vision and text information happens predominantly in the initial layers.", "section": "3 How Does LLaVA Understand Vision Tokens?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x9.png", "caption": "(c) LLaVA-v1.6-Mistral-7B", "description": "Figure 3(c) presents a detailed visualization of attention entropy across different layers of the LLaVA-v1.6-Mistral-7B model.  Attention entropy, a measure of attention distribution across different tokens, illustrates the importance of various tokens (instructions, vision tokens, and responses) at different processing stages. The figure allows for insights into the role of vision tokens in the LLM's understanding process across various layers.  It indicates the relative weighting and focus on the different token types in different layers of the model.", "section": "3 How Does LLaVA Understand Vision Tokens?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x10.png", "caption": "(d) LLaVA-NeXT-Vicuna-7B", "description": "This figure shows the layer-wise attention weight distribution across different token types (instruction, vision, and response) in the LLaVA-NeXT-Vicuna-7B model.  The attention weights illustrate how the model's focus on visual information changes across different layers of the LLM.  It visualizes the relative importance of vision tokens at different processing stages during the understanding of image and text inputs.", "section": "3 How DOES LLAVA UNDERSTAND VISION TOKENS?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x11.png", "caption": "Figure 3: Attention entropy assigned to different types of tokens across different layers in LMMs.", "description": "This figure visualizes the entropy of attention weights assigned to different token types (instruction, vision, and response) across various layers of four different Large Multimodal Models (LMMs).  Attention entropy measures the distribution of attention across tokens within each layer.  A high entropy indicates that attention is distributed relatively evenly across all tokens of that type, while a low entropy indicates that attention is focused on only a few tokens. The graphs show that the attention entropy for vision tokens is much higher in the earlier layers, indicating vision tokens are more evenly attended to in early layers. As the layer number increases, the attention weight shifts to instruction tokens. This suggests vision tokens play a crucial role in fusing visual information with text in the early layers of the LLM.", "section": "How Does LLaVA Understand Vision Tokens?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x12.png", "caption": "Figure 4: Attention visualization at different layers in LLaVA-v1.5 (color bar: logarithmic scale).", "description": "This figure visualizes the attention weights assigned to different vision tokens across various layers of the LLaVA-v1.5 model.  The color intensity in each heatmap represents the attention weight (logarithmic scale), showing how much attention each layer gives to individual vision tokens.  It demonstrates that in the early layers, almost all vision tokens receive a high level of attention, whereas in later layers, attention focuses on fewer, specific tokens. This visualization helps to explain why removing vision tokens has a more significant impact in the initial layers.", "section": "How Does LLaVA Understand Vision Tokens?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x13.png", "caption": "Figure 5: Performance of LLaVA-v1.5 when removing all vision tokens in various layers of LMM.", "description": "This figure displays the performance of the LLaVA-v1.5 model on the GQA and MMBench datasets when vision tokens are removed from different layers of the large language model (LLM).  It demonstrates how the importance of vision tokens varies across different layers of the LLM. Removing tokens from early layers results in a significant drop in performance, indicating that vision tokens are more critical in those initial layers for fusing visual information into textual representations.", "section": "3 How DOES LLAVA UNDERSTAND VISION TOKENS?"}, {"figure_path": "https://arxiv.org/html/2501.03895/x14.png", "caption": "Figure 6: Architecture of LLaVA-Mini. Left: LLaVA-Mini represents each image with one vision token. Right: Detailed view of the proposed query-based compression and modality pre-fusion.", "description": "LLaVA-Mini is an efficient large multimodal model that uses only one vision token to represent each image.  The figure's left side shows a high-level overview of this architecture.  The right side provides a detailed breakdown of the proposed query-based compression module and the modality pre-fusion module. The compression module reduces the number of vision tokens, while the pre-fusion module integrates visual information into text tokens before they reach the LLM backbone, which significantly enhances the model's efficiency without sacrificing performance.", "section": "4 LLaVA-MINI"}, {"figure_path": "https://arxiv.org/html/2501.03895/x15.png", "caption": "Table 4: Results on MLVU (accuracy) of long video understanding. Evaluation includes Topic Reasoning (TR), Anomaly Recognition (AR), Needle QA (NQA), Ego Reasoning (ER), Plot QA (PQA), Action Order (AO), and Action Count (AC).", "description": "This table presents the performance of different models on the MLVU benchmark for long video understanding.  MLVU assesses various aspects of video comprehension, including the ability to reason about topics (TR), identify anomalies (AR), answer questions requiring detailed information (Needle QA, NQA), understand first-person perspectives (Ego Reasoning, ER), comprehend plot details (Plot QA, PQA), determine the order of events (Action Order, AO), and count actions (Action Count, AC).  The results show the accuracy achieved by each model on these distinct subtasks.  Higher scores indicate better performance in understanding the nuances of long videos.", "section": "5.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.03895/x16.png", "caption": "Table 5: Results on EgoSchema (accuracy), a long-form video benchmark (\u223csimilar-to\\sim\u223c 3 minutes) for first-person view temporal reasoning.", "description": "Table 5 presents the performance evaluation results of various models on the EgoSchema benchmark. EgoSchema is a challenging dataset designed for evaluating long-form video understanding, specifically focusing on temporal reasoning within first-person perspective videos (approximately 3 minutes long).  The table shows the accuracy of different models in performing this task, highlighting the relative strengths and weaknesses of each model in understanding and reasoning about the temporal aspects of first-person videos.  The performance is measured as accuracy, indicating the proportion of correctly answered questions.", "section": "5. Experimental Setting"}, {"figure_path": "https://arxiv.org/html/2501.03895/x17.png", "caption": "Figure 7: FLOPs and latency of LLaVA-Mini.", "description": "This figure shows the computational efficiency and speed improvements of LLaVA-Mini compared to LLaVA-v1.5.  It displays a significant reduction in FLOPs (floating-point operations) and a substantial decrease in inference latency (response time) for LLaVA-Mini, highlighting its efficiency in processing images.", "section": "5.3 Efficiency"}, {"figure_path": "https://arxiv.org/html/2501.03895/x18.png", "caption": "Figure 8: FLOPs and latency of LLaVA-Mini-HD.", "description": "This figure shows the computational efficiency of LLaVA-Mini-HD, a variant of LLaVA-Mini designed for high-resolution images.  It presents a comparison of FLOPs (floating-point operations) and inference latency (the time it takes to get a result) between LLaVA-Mini-HD and LLaVA-v1.5, a previous model.  The x-axis likely represents different numbers of vision tokens, while the y-axis shows both FLOPs and latency, demonstrating how LLaVA-Mini-HD achieves significantly fewer FLOPs and lower latency, especially as the number of vision tokens increases. This highlights the efficiency gains achieved through the model's unique design.", "section": "5.3 Efficiency"}, {"figure_path": "https://arxiv.org/html/2501.03895/x19.png", "caption": "Figure 9: VRAM usage (3-hour video) of LLaVA-Mini.", "description": "This figure compares the GPU memory usage of LLaVA-Mini against several other video LLMs when processing a 3-hour long video.  It demonstrates LLaVA-Mini's significant memory efficiency. While other models struggle with memory constraints, even on high-end GPUs with 40GB of VRAM, LLaVA-Mini efficiently processes the video on a 24GB GPU thanks to its minimal vision token usage, showcasing its suitability for real-time applications with long videos.", "section": "5.3 Efficiency"}, {"figure_path": "https://arxiv.org/html/2501.03895/x20.png", "caption": "Table 6: Performance of LLaVA-Mini with different numbers of modality pre-fusion layers Nf\u2062u\u2062s\u2062i\u2062o\u2062nsubscript\ud835\udc41\ud835\udc53\ud835\udc62\ud835\udc60\ud835\udc56\ud835\udc5c\ud835\udc5bN_{fusion}italic_N start_POSTSUBSCRIPT italic_f italic_u italic_s italic_i italic_o italic_n end_POSTSUBSCRIPT.", "description": "This table presents the results of experiments conducted to evaluate the impact of varying the number of modality pre-fusion layers in the LLaVA-Mini model.  The modality pre-fusion module is a key component of LLaVA-Mini, designed to integrate visual and textual information effectively before feeding it into the main language model.  The table shows how changes in the number of pre-fusion layers affect model performance across different metrics (e.g., VQAv2, GQA, and MMB). This allows researchers to determine the optimal number of layers to balance performance and computational cost.", "section": "5.3 Efficiency"}, {"figure_path": "https://arxiv.org/html/2501.03895/x21.png", "caption": "Table 7: Performance of LLaVA-Mini with various vision tokens.", "description": "This table presents a comparison of LLaVA-Mini's performance using different numbers of vision tokens. It shows how varying the number of vision tokens affects the model's accuracy on three benchmark tasks (VQAV2, GQA, and MMB).  This allows for an analysis of the trade-off between efficiency (fewer tokens) and performance.", "section": "5.3 Efficiency"}]