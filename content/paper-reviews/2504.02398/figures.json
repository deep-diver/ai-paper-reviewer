[{"figure_path": "https://arxiv.org/html/2504.02398/x1.png", "caption": "Figure 1: Analysing metric scaling of interleaved SLMs, considering the best model per-compute. We compare scaling trends to textless SLMs (Cuervo & Marxer, 2024). \\gmFont maybe still a touch small?", "description": "This figure displays a scaling analysis of interleaved speech language models (SLMs) by comparing the best performing model for each compute budget.  The graphs illustrate how various metrics (such as cross-entropy loss and semantic speech evaluation scores) change as the compute budget (measured in FLOPs) increases. Different colored lines represent different model families or training approaches.  The key finding is that interleaved SLMs scale more efficiently with compute than textless SLMs (as reported by Cuervo & Marxer (2024)), which is a significant improvement.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02398/x2.png", "caption": "Figure 2: Comparing SLMs trained with the same recipe for 20k steps, from different TextLM initialisations. Models are sorted by parameter count from large to small.", "description": "This figure compares the performance of several Speech Language Models (SLMs) trained for 20,000 steps using the same training recipe, but initialized from different pre-trained Text Language Models (TextLMs).  Each SLM's performance is evaluated using three metrics: Multi-speaker Spoken StoryCloze (MS-sSC), Multi-speaker Topic StoryCloze (MS-tSC), and validation cross-entropy. The models are ordered on the x-axis by the number of parameters (model size), from largest to smallest.  The y-axis shows the performance score for each metric.", "section": "4.2 Model Family Impact"}, {"figure_path": "https://arxiv.org/html/2504.02398/x3.png", "caption": "Figure 3: Comparing SLMs based on Qwen2.5-0.5B with interleaving, without interleaving and without TWIST initialisation (denoted GSLM). This helps analyse the impact of these choices on performance and thus on scaling analysis. See Appendix 6 for other metrics.", "description": "This figure compares the performance of three different types of speech language models (SLMs) based on the Qwen2.5-0.5B architecture. The three models are: (1) an interleaved SLM trained with TWIST initialization; (2) an interleaved SLM trained without TWIST initialization; and (3) a non-interleaved SLM (denoted GSLM).  The comparison is made across a range of training steps, showing the evolution of performance metrics (multi-speaker tSC and validation speech cross-entropy) over time. The purpose of this comparison is to analyze the impact of both interleaving and TWIST initialization on the scaling behavior of SLMs.  Additional performance metrics are available in Appendix 6 of the paper.", "section": "4 Building the Scaffolding for Scaling"}, {"figure_path": "https://arxiv.org/html/2504.02398/x4.png", "caption": "Figure 4: Comparing the loss on speech only of interleaved SLMs of different model sizes trained for specific compute budgets.", "description": "This figure displays the results of an experiment comparing the performance of interleaved speech language models (SLMs) with varying model sizes and compute budgets.  The performance metric used is the validation loss on speech-only data.  The graph shows how the speech-only validation loss changes depending on both the model size (number of parameters) and the compute budget (in FLOPs). This analysis helps determine the optimal balance between model size and compute resources for achieving the lowest speech-only validation loss in interleaved SLMs.", "section": "5 Scaling Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02398/x5.png", "caption": "Figure 5: Comparing SLMs based on OPT125M with interleaving, without interleaving and without TWIST initialisation. Comparing to the Figure 3, we can see that OPT125 benefits less from interleaving and TWIST initialisation compared to Qwen2.5-0.5B.", "description": "Figure 5 presents a comparison of the performance of three different language models trained using the OPT125M base model: one with both interleaving and TWIST initialization, one with only interleaving, and one trained without either technique. The results show that the benefits of interleaving and TWIST initialization, which were significant for the Qwen2.5-0.5B model (shown in Figure 3), are less pronounced for the OPT125M model.  Specifically, the performance differences between the three models are smaller with OPT125M.  The figure shows the multi-speaker tSC metric (semantic understanding) and validation speech cross-entropy plotted against training steps for each of the three models.", "section": "4 Building the Scaffolding for Scaling"}, {"figure_path": "https://arxiv.org/html/2504.02398/x6.png", "caption": "Figure 6: Comparing SLMs based on Qwen2.5-0.5B with interleaving, without interleaving and without TWIST initialisation. This compliments Figure 3, yet results are a bit more noisy, perhaps because they are nearer to random.", "description": "Figure 6 presents a comparison of three different training approaches for speech language models (SLMs) using the Qwen2.5-0.5B architecture. The three methods compared are: interleaved training with TWIST initialization, interleaved training without TWIST initialization, and speech-only training with TWIST initialization.  The figure displays the results for three different metrics: Multi-Speaker sSC, Multi-Speaker tSC, and validation speech cross-entropy, plotted against the number of training steps. The results show the impact of interleaving and TWIST initialization on SLM performance.  The results using the Multi-Speaker sSC and tSC metrics show more variance than Figure 3, which might be attributed to their closer proximity to randomness in the experimental setup.", "section": "4 Building the Scaffolding for Scaling"}, {"figure_path": "https://arxiv.org/html/2504.02398/x7.png", "caption": "Figure 7: Analysing the scaling properties of interleaved SLMs regarding multi-speaker sSC. This compliments Figure 4, yet results are a bit more noisy, perhaps because they are nearer to random.", "description": "Figure 7 displays the scaling behavior of interleaved speech language models (SLMs) using the multi-speaker Spoken StoryCloze (sSC) metric.  It complements Figure 4, which showed similar scaling analysis using a different metric (multi-speaker tSC).  The figure shows how well the models perform on the sSC metric as model size and compute budget are increased.  The results exhibit more noise than those in Figure 4, possibly because the sSC metric's values are closer together (less variance) compared to the tSC metric, making the trends less clear and more susceptible to random fluctuations.", "section": "5 Scaling Analysis"}]