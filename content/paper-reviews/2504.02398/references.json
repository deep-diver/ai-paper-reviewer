{"references": [{"fullname_first_author": "Hoffmann, Jordan", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-01", "reason": "This paper proposes approaches for scaling analysis of language models, which is used as a basis for the scaling analysis in the main paper."}, {"fullname_first_author": "Kaplan, Jared", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-01", "reason": "This paper describes how performance scales with increased compute resources, which is important for scaling analysis."}, {"fullname_first_author": "Radford, Alec", "paper_title": "Robust speech recognition via large-scale weak supervision", "publication_date": "2022-12-04", "reason": "This paper provides a method for robust speech recognition via large-scale weak supervision, and time aligned transcriptions for the datasets are estimated using Whisper v3-large-turbo based on this paper."}, {"fullname_first_author": "Cuervo, Santiago", "paper_title": "Scaling properties of speech language models", "publication_date": "2024-04-01", "reason": "This paper proposes the first scaling analysis for textless-SLMs and is an important comparison point for the interleaved SLMs studied in the main paper."}, {"fullname_first_author": "Hsu, Wei-Ning", "paper_title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units", "publication_date": "2021-01-01", "reason": "This paper proposes a self-supervised speech representation learning method, and the main paper focuses on HuBERT as it is the most widely used speech encoder for SLMs."}]}