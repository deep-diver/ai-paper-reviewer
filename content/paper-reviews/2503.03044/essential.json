{"importance": "This study addresses the critical gap between the accuracy and practical usability of word-level QE in real-world post-editing scenarios. By using a diverse group of **professional translators** and **realistic tasks**, it offers insights into factors influencing the effectiveness of QE and provides direction for future QE development and implementation.", "summary": "QE4PE: Word-level QE's impact on MT post-editing with 42 pro-editors across English-Italian/Dutch is investigated. Usability&accuracy challenges in professional workflows are underlined.", "takeaways": ["Domain, language, and editor speed are critical factors for effective highlights.", "The usability of QE highlights may not be improved even with increased accuracy.", "There are performance differences between human and automated QE highlights."], "tldr": "While word-level quality estimation (QE) has the potential to aid human post-editing of machine translation (MT), its practical impact on post-editing workflows and the factors influencing its effectiveness are not well-understood. Current QE evaluation practices often assume that increased accuracy directly translates to improved usability, neglecting the real-world challenges of integrating these methods into professional translation processes. Thus, QE's true contribution remains unclear.\n\nThe study QE4PE aims to bridge this gap by conducting a large-scale evaluation with 42 professional translators in English-Italian/Dutch. It compares four error-span highlight modalities, incl. supervised/uncertainty-based word-level QE, using behavioral logs, human annotation, and questionnaires. The results underscore the importance of domain, language, and editor speed, and reveal modest differences between human and automated highlights, revealing the gap in accuracy and usability.", "affiliation": "CLCG, University of Groningen", "categories": {"main_category": "Natural Language Processing", "sub_category": "Machine Translation"}, "podcast_path": "2503.03044/podcast.wav"}