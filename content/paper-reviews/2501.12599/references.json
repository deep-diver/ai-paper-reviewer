{"references": [{"fullname_first_author": "Ouyang, Long", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper is foundational to the field of RLHF, a core technique used in training Kimi k1.5, and its methodology is directly referenced in the paper's RL training strategy."}, {"fullname_first_author": "Kaplan, Jared", "paper_title": "Scaling Laws for Neural Language Models", "publication_date": "2020-01-01", "reason": "This paper introduces scaling laws, a critical concept underlying the scaling of LLMs and the justification for the authors' exploration of RL as a new scaling axis."}, {"fullname_first_author": "Hoffmann, Jordan", "paper_title": "Training Compute-Optimal Large Language Models", "publication_date": "2022-03-01", "reason": "This paper provides insights into compute-optimal training, which is relevant to the resource-efficient approach adopted for training Kimi k1.5."}, {"fullname_first_author": "Wei, Jason", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-12-01", "reason": "This paper introduces Chain-of-Thought prompting, a key technique used in Kimi k1.5 for improved reasoning capabilities, and its core concepts are fundamentally integrated into the model's training process."}, {"fullname_first_author": "Silver, David", "paper_title": "Mastering the game of go without human knowledge", "publication_date": "2017-01-01", "reason": "This paper highlights the success of reinforcement learning in mastering complex games, providing a strong foundation for applying RL to LLM training, as explored in Kimi k1.5."}]}