[{"figure_path": "https://arxiv.org/html/2412.13795/x1.png", "caption": "Figure 1: (a) Post-LN layer; (b) Pre-LN layer; (c) Mix-LN layer.", "description": "This figure visually represents the architecture of three different layer normalization techniques within the context of transformer layers: Post-LN, Pre-LN and Mix-LN. Post-LN applies layer normalization after the residual connection adding the output of the attention or FFN block to the input of the block, Pre-LN applies it before the residual connection and Mix-LN combines Post-LN and Pre-LN within the same model. Mix-LN applies Post-LN to the earlier layers and Pre-LN to the deeper layers, in order to ensure more uniform gradients across layers. In the figure, 'x' represents the input to the layer, 'Layer Norm' is the layer normalization operation, and both 'Attention' and 'FFN' represent multi-head attention and feed-forward network blocks, respectively.", "section": "HYPOTHESIS EVALUATION"}, {"figure_path": "https://arxiv.org/html/2412.13795/x2.png", "caption": "Figure 2: Results of open-weight large-scale LLMs. Angular Distance (a, b): Each column represents the angular distance from the initial layer \u2113\u2113\\ellroman_\u2113 (x-axis) and its subsequent nt\u2062hsuperscript\ud835\udc5b\ud835\udc61\u210en^{th}italic_n start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer (y-axis). The distance is scaled to the range [0, 1], where yellow indicates smaller distances and purple indicates larger distances. Performance Drop (c, d): (c): SQuAD v1.1 performance drop of removing each layer from BERT-large; (d): MMLU accuracy drop of removing each layer from LLaMa2-7B.", "description": "This figure presents an analysis of two large language models (LLMs), BERT-large and LLaMa2-7B, using two metrics: Angular Distance and Performance Drop.  Angular Distance measures the similarity between the activations of a layer and a subsequent layer.  A smaller distance (yellow) implies greater similarity and potentially redundancy. Performance drop measures the decrease in model performance when a specific layer is removed. A larger drop indicates the layer is more important. Subfigures (a) and (c) show these metrics for BERT-large (a Post-LN model), while (b) and (d) show the same for LLaMa2-7B (a Pre-LN model). The results demonstrate the differing behavior of layer effectiveness between these normalization methods.", "section": "2.3 Evaluation Results"}, {"figure_path": "https://arxiv.org/html/2412.13795/x3.png", "caption": "Figure 3: Results of in-house small-scale LLaMa-130M. Angular Distance (a, b): Each column represents the angular distance from the initial layer \u2113\u2113\\ellroman_\u2113 (x-axis) and its subsequent nt\u2062hsuperscript\ud835\udc5b\ud835\udc61\u210en^{th}italic_n start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT layer (y-axis). The distance is scaled to the range [0, 1], where yellow indicates smaller distances and purple indicates larger distances. Performance Drop (c, d): ARC-e performance drop of removing each single layer from LLaMa-130M. Gradient Norm (e): Gradient norm of each layer in LLaMa-130M.", "description": "This figure presents an analysis of in-house trained small-scale LLaMa-130M models, comparing the effectiveness of different normalization methods (Pre-LN, Post-LN, and Mix-LN).  Subfigures (a) and (b) visualize the angular distance between layer representations, demonstrating how layer similarity changes with depth in different normalization schemes.  Yellow indicates high similarity, purple indicates low similarity.  Subfigures (c) and (d) show the performance drop (ARC-e score) resulting from removing individual layers, indicating each layer's importance. Subfigure (e) displays the gradient norm of each layer for both Post-LN and Pre-LN at the beginning of training, illustrating the impact of normalization on gradient flow.", "section": "2 HYPOTHESIS EVALUATION"}, {"figure_path": "https://arxiv.org/html/2412.13795/x4.png", "caption": "Figure 4: Training curve (eval perplexity) of Mix-LN and Pre-LN with LLaMa-7B.", "description": "This figure presents the training curves of LLaMa-7B models using two different layer normalization methods: Mix-LN (proposed method) and the standard Pre-LN. The y-axis represents the evaluation perplexity (lower is better), and the x-axis represents the number of training steps. The plot visually demonstrates that Mix-LN consistently achieves lower perplexity than Pre-LN throughout the training process, suggesting its effectiveness in improving the model's performance.", "section": "4.2 SCALING UP TO 7B MODEL"}, {"figure_path": "https://arxiv.org/html/2412.13795/x5.png", "caption": "Figure 5: Angular distance from initial layer \u2113\u2113\\ellroman_\u2113 (x-axis) with block size n\ud835\udc5bnitalic_n (y-axis) of LLaMA-130M.", "description": "This figure visualizes the angular distance between the hidden states of different layers in the LLaMA-130M model, comparing Pre-LN, Post-LN, and Mix-LN normalization methods.  Each cell (\u2113,n)(\u2113,n)(\"ell\", \"n\") in a heatmap represents the angular distance between the input to layer \u2113\u2113\"ell\" and the input to layer \u2113+n\u2113+n\"ell\" + \"n\".  A smaller distance (yellow) indicates higher similarity between layer representations, while a larger distance (purple) suggests greater dissimilarity.  Mix-LN tends to produce larger angular distances (more dissimilarity), particularly in the deeper layers, than the other methods, while Post-LN shows high similarity (smaller angular distances) mostly in the earlier layers. This observation illustrates how Mix-LN creates more diversity of learned representations across layers.", "section": "5 ANALYSIS AND MORE EVALUATIONS"}, {"figure_path": "https://arxiv.org/html/2412.13795/x6.png", "caption": "(a) Layer gradient norm of LLaMA-250M with various normalization techniques.", "description": "This figure, located in Section 5 (Analysis and More Evaluations), visualizes the gradient norms of layers within the LLaMA-250M model. It compares the gradient norms of each layer under various layer normalization techniques including Pre-LN, Post-LN and the proposed Mix-LN. The figure is intended to show the impact of different normalization techniques on gradient flow and stability of training. ", "section": "5 ANALYSIS AND MORE EVALUATIONS"}, {"figure_path": "https://arxiv.org/html/2412.13795/x7.png", "caption": "(b) Performance drop comparison of LLaMA-130M across layers for Pre-LN, Post-LN, and Mix-LN.", "description": "This figure compares the performance drop (reduction in ARC-e accuracy after supervised fine-tuning) when individual layers are pruned from a 130M parameter LLaMA model.  Three normalization methods are compared: Pre-LN, Post-LN, and the proposed Mix-LN.  The x-axis represents the index of the layer being pruned, and the y-axis represents the change in ARC-e score. A larger drop indicates the layer is more important for performance.  Mix-LN shows greater performance drops for deeper layers than Pre-LN, suggesting that deeper layers are better utilized and contribute more meaningfully when using Mix-LN.", "section": "5 ANALYSIS AND MORE EVALUATIONS"}]