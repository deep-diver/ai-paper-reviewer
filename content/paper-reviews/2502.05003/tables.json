[{"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1\"><math alttext=\"P\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.1.1.1.m1.1\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mi id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">P</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><ci id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">\ud835\udc43</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">P</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.1.1.1.m1.1d\">italic_P</annotation></semantics></math></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.2\">1</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.3\">2</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.4\">3</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.5\">4</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.6\">8</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.7\">16</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_tt\" id=\"S4.T1.2.2.1\"><math alttext=\"\\text{eff}(P)\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.2.2.1.m1.1\"><semantics id=\"S4.T1.2.2.1.m1.1a\"><mrow id=\"S4.T1.2.2.1.m1.1.2\" xref=\"S4.T1.2.2.1.m1.1.2.cmml\"><mtext id=\"S4.T1.2.2.1.m1.1.2.2\" xref=\"S4.T1.2.2.1.m1.1.2.2a.cmml\">eff</mtext><mo id=\"S4.T1.2.2.1.m1.1.2.1\" xref=\"S4.T1.2.2.1.m1.1.2.1.cmml\">\u2062</mo><mrow id=\"S4.T1.2.2.1.m1.1.2.3.2\" xref=\"S4.T1.2.2.1.m1.1.2.cmml\"><mo id=\"S4.T1.2.2.1.m1.1.2.3.2.1\" stretchy=\"false\" xref=\"S4.T1.2.2.1.m1.1.2.cmml\">(</mo><mi id=\"S4.T1.2.2.1.m1.1.1\" xref=\"S4.T1.2.2.1.m1.1.1.cmml\">P</mi><mo id=\"S4.T1.2.2.1.m1.1.2.3.2.2\" stretchy=\"false\" xref=\"S4.T1.2.2.1.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.1.m1.1b\"><apply id=\"S4.T1.2.2.1.m1.1.2.cmml\" xref=\"S4.T1.2.2.1.m1.1.2\"><times id=\"S4.T1.2.2.1.m1.1.2.1.cmml\" xref=\"S4.T1.2.2.1.m1.1.2.1\"></times><ci id=\"S4.T1.2.2.1.m1.1.2.2a.cmml\" xref=\"S4.T1.2.2.1.m1.1.2.2\"><mtext id=\"S4.T1.2.2.1.m1.1.2.2.cmml\" xref=\"S4.T1.2.2.1.m1.1.2.2\">eff</mtext></ci><ci id=\"S4.T1.2.2.1.m1.1.1.cmml\" xref=\"S4.T1.2.2.1.m1.1.1\">\ud835\udc43</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.1.m1.1c\">\\text{eff}(P)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.2.2.1.m1.1d\">eff ( italic_P )</annotation></semantics></math></th>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\" id=\"S4.T1.2.2.2\">0.02</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\" id=\"S4.T1.2.2.3\">0.16</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\" id=\"S4.T1.2.2.4\">0.43</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\" id=\"S4.T1.2.2.5\">0.70</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\" id=\"S4.T1.2.2.6\">1.02</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_tt\" id=\"S4.T1.2.2.7\">1.00</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Fitted scaling-law \u201ceffective parameter\u201d multipliers.", "description": "This table presents the fitted scaling law's \"effective parameter\" multipliers for different numerical precisions (P). The effective parameter count is a metric that takes into account the impact of precision on model size and performance, offering a way to compare models of different architectures, sizes, and precisions under the same performance constraints. It shows how many parameters of a full-precision model are effectively being used when training with lower precisions, providing insights into the trade-offs between precision, compute, and model size.", "section": "4.3. Scaling Laws"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A1.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A1.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"A1.T2.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A1.T2.1.1.1.2\">HellaSWAG Accuracy (%) \u2191</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A1.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A1.T2.1.2.1.1\">BF16 (800M, 80B tokens)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A1.T2.1.2.1.2\">39.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A1.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"A1.T2.1.3.2.1\">QuEST 4-bit (800M, 80B tokens)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A1.T2.1.3.2.2\">39.22</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: Zero-shot evaluation on HellaSWAG comparing QuEST 4-bit to its BF16 counterpart. The results are nearly identical, confirming that training with QuEST is lossless.", "description": "This table presents the results of a zero-shot evaluation on the HellaSWAG benchmark, a test of commonsense reasoning.  It compares the accuracy of an 800-million parameter model trained using QuEST with 4-bit precision against a model trained with standard BF16 precision.  Both models were trained on 80 billion tokens.  The nearly identical accuracy scores demonstrate that QuEST's quantization-aware training preserves model performance, even with significantly reduced computational cost and precision.", "section": "4. Experimental Validation"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T3.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A2.T3.1.1.1.1\">Model size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.1.1.2\">30M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.1.1.3\">50M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.1.1.4\">100M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.1.1.5\">200M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.1.1.6\">430M</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.1.1.7\">800M</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A2.T3.1.2.2.1\">Num. Blocks</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.2.2.2\">6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.2.2.3\">7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.2.2.4\">8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.2.2.5\">10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.2.2.6\">13</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.2.2.7\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T3.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A2.T3.1.3.3.1\">Hidden Size</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.3.3.2\">640</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.3.3.3\">768</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.3.3.4\">1024</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.3.3.5\">1280</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.3.3.6\">1664</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.3.3.7\">2048</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T3.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A2.T3.1.4.4.1\">Num. Attn. Heads</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.4.4.2\">5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.4.4.3\">6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.4.4.4\">8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.4.4.5\">10</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.4.4.6\">13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.4.4.7\">16</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T3.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A2.T3.1.5.5.1\">Learning Rate</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.5.5.2\">0.0012</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.5.5.3\">0.0012</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.5.5.4\">0.0006</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.5.5.5\">0.0003</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.5.5.6\">0.00015</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.5.5.7\">0.000075</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T3.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A2.T3.1.6.6.1\">Num. Tokens</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.6.6.2\">3B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.6.6.3\">5B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.6.6.4\">10B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.6.6.5\">20B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.6.6.6\">43B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T3.1.6.6.7\">80B</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: Hyper-parameters used for each model size.", "description": "This table lists the hyperparameters used to train Llama-family language models of different sizes.  It shows the number of blocks, the hidden size, the number of attention heads, the learning rate, and the total number of tokens used for training for each model size (30M, 50M, 100M, 200M, 430M, and 800M parameters). These parameters were used for experiments in the paper to ensure consistent training across different model sizes.", "section": "4.1 Implementation Details"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"A2.T4.1.1.1.1\">Weight Decay</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T4.1.1.1.2\">2-bit PPL \u2193</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T4.1.1.1.3\">3-bit PPL \u2193</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T4.1.1.1.4\">4-bit PPL \u2193</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T4.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"A2.T4.1.2.1.1\">0.001</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.2.1.2\">37.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.2.1.3\">31.10</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.2.1.4\">27.93</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A2.T4.1.3.2.1\">0.01</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.3.2.2\">36.91</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.3.2.3\">30.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.3.2.4\">27.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\" id=\"A2.T4.1.4.3.1\">0.1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.4.3.2.1\">36.54</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.4.3.3.1\">30.26</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.4.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.4.3.4.1\">27.51</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"A2.T4.1.5.4.1\">1.0</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.1.5.4.2\">38.12</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.1.5.4.3\">31.16</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.1.5.4.4\">28.67</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 4: Weight decay hyperparameter search results for LSQ across different bitwidths of 30M model. The best-performing setting is highlighted in bold.", "description": "This table presents the results of a hyperparameter search for the LSQ (Learned Step Size Quantization) method, focusing on the weight decay parameter.  The search was conducted for a 30-million parameter language model across three different bitwidths (2-bit, 3-bit, and 4-bit).  For each bitwidth, the table shows the validation perplexity (PPL) achieved with various weight decay values. The lowest PPL for each bitwidth, indicating the best-performing hyperparameter setting, is highlighted in bold.", "section": "B.3. Hyper-parameter Search for Baseline Methods"}]