[{"figure_path": "https://arxiv.org/html/2504.10337/x1.png", "caption": "Figure 1: Scaling of Heimdall. Left: the verification accuracy scales with the response length during RL training. With more reasoning tokens, Heimdall gives more accurate judgment on the solutions on AIME2024. Middle: the verification accuracy scales with repeated sampling and Majority Voting. By sampling multiple verification trajectories and voting, the accuracy can be further improved. Right: with Heimdall scoring the solutions on AIME2025, the problem solving accuracy scales with the number of solutions. We verify 16161616 times on each solution and select the most likely correct one with Pessimistic Verification(\u00d716absent16\\times 16\u00d7 16). When inter-playing with various solver models, Heimdall gives significant improvements over pure solver-based Majority Voting(MV).", "description": "Figure 1 demonstrates Heimdall's scalability in verification accuracy.  The left panel shows that longer reasoning sequences during reinforcement learning (RL) training lead to more accurate assessments of solution correctness on the AIME2024 dataset. The middle panel illustrates that repeated sampling and majority voting further enhance accuracy. The right panel showcases how Heimdall, when integrated with a solver model using Pessimistic Verification, improves problem-solving accuracy on the AIME2025 dataset by selecting the most likely correct solution from multiple candidates (each verified 16 times). The improvement is particularly noteworthy when compared to using Majority Voting alone.", "section": "4.2 Scaling of verification"}, {"figure_path": "https://arxiv.org/html/2504.10337/x2.png", "caption": "Figure 2: Accuracy and response length during RL training. PPO w/o data filtering is the RL training with all problems in the dataset. Left: the accuracy on AIME2024 with the training steps. Right: the response length on the training dataset with the training steps.", "description": "Figure 2 illustrates the impact of reinforcement learning (RL) training on Heimdall's performance.  The left panel shows the accuracy of Heimdall on the AIME2024 dataset across various training steps. It demonstrates how accuracy improves with additional training.  The right panel displays the length of the model's responses (in tokens) during the same RL training process. This shows the relationship between model accuracy and the complexity/length of its reasoning process. A comparison is made between training with all problems in the dataset (PPO w/o data filtering) and a filtered dataset (PPO).", "section": "4.2 Scaling of verification"}, {"figure_path": "https://arxiv.org/html/2504.10337/x3.png", "caption": "Figure 3: The inference-time scaling of verification ability on problem solutions in AIME2024 and AIME2025. Top-left: We show the accuracy of Heimdall when we sample multiple verification responses and make the judgment by majority voting. Top-right: We show the decreasing false-negative rate(FNR) and false-positive rate(FPR) as we scale up verification responses with majority voting. Bottom-left: We calculate the average score of verification responses and draw the AUC along each number of responses. Bottom-right: We collect the verification failure cases on every math problem and draw the relation between the difficulty of the problem and the number of verification failures, which reveals that the verification difficulty may not necessarily correlate with the difficulty of the original problem.", "description": "Figure 3 demonstrates how the accuracy and reliability of Heimdall's verification improve with increased computational resources. The top-left subplot shows the accuracy of Heimdall using majority voting on multiple verification responses for AIME2024 and AIME2025 problems.  The top-right subplot displays the decreasing false-negative and false-positive rates as the number of verification responses increases. The bottom-left subplot shows the Area Under the Curve (AUC) for the average verification scores as the number of responses increases. Finally, the bottom-right subplot examines the relationship between problem difficulty and the number of verification failures, revealing that problem difficulty does not always directly correlate with verification difficulty.", "section": "4.2 Scaling of verification"}, {"figure_path": "https://arxiv.org/html/2504.10337/x4.png", "caption": "Figure 4: The inference-time scaling of problem solving with Heimdall.\nThe two figures show the accuracy on AIME datasets as the number of solutions scales up.\nLeft: the problem solving accuracy on AIME2025 dataset scales with the number of solutions. The colored shaded area represents the area covered by the accuracy curves of a selection algorithm as the number of verifications increases from 1 to 64. Right: the contour map of the accuracy of Pessimistic Verification as the number of solutions (x-axis) and the number of verifications (y-axis) increase. The red curve indicates the optimal configurations within various overall compute budgets.", "description": "Figure 4 illustrates how the accuracy of problem-solving using Heimdall changes as the number of solutions and verifications increases. The left panel shows the accuracy on the AIME2025 dataset as the number of solutions increases. The shaded regions represent the range of accuracies obtained by different selection algorithms (with varying numbers of verifications from 1 to 64). The right panel provides a contour map depicting the accuracy of the Pessimistic Verification method based on the number of solutions and verifications. The red line highlights the optimal configurations for various computational budgets.", "section": "4.3 Scaling of problem solving with verification"}, {"figure_path": "https://arxiv.org/html/2504.10337/x5.png", "caption": "Figure 5: The distribution of verification scores on the problems of a synthetic dataset. The x-axis is the sum of scores across 8888 verifications and the y-axis is the number of problems corresponding to each sum.", "description": "This figure shows the distribution of verification scores obtained from evaluating a synthetic dataset of math problems using the Heimdall model.  The x-axis represents the sum of verification scores across 8 verifications for each problem.  The y-axis indicates the number of problems that received each particular sum of scores. This visualization helps understand the model's performance across the dataset and provides insights into the quality and error rate of the synthetic problems.", "section": "Verification on automatic knowledge discovery"}]