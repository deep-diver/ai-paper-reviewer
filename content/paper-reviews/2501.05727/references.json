{"references": [{"fullname_first_author": "Zhengyang Tang", "paper_title": "Enabling Scalable Oversight via Self-Evolving Critic", "publication_date": "2025-01-10", "reason": "This is the main research paper where the proposed SCRIT framework and its findings are presented."}, {"fullname_first_author": "L. Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-09-25", "reason": "This paper is foundational to the field as it introduces RLHF, a crucial technique for aligning LLMs with human preferences, which SCRIT builds upon."}, {"fullname_first_author": "K. Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-26", "reason": "This paper introduces GSM8K, a benchmark dataset used in the evaluation, demonstrating the importance of mathematical reasoning and verification which SCRIT focuses on."}, {"fullname_first_author": "N. McAleese", "paper_title": "LLM critics help catch LLM bugs", "publication_date": "2024-07-01", "reason": "This paper introduces the concept of using LLMs as critics, directly inspiring the core idea behind SCRIT's self-evolving critic approach."}, {"fullname_first_author": "C. Zheng", "paper_title": "ProcessBench: Identifying process errors in mathematical reasoning", "publication_date": "2024-12-01", "reason": "This paper introduces ProcessBench, a key benchmark dataset for evaluating critique capabilities, used in SCRIT's evaluation and analysis."}]}