[{"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Adaptation_Scheme_v2.drawio.png", "caption": "Figure 1: Our method adapts Stable Diffusion\u00a0[89] to generate high-fidelity textured meshes in 1.2 seconds.", "description": "This figure showcases the high-fidelity textured 3D mesh models generated by the proposed method, Progressive Rendering Distillation (PRD).  The models are created from text prompts in just 1.2 seconds, demonstrating the speed and quality of the method. The examples show diverse and complex scenes, indicating the model's ability to handle challenging inputs and generate realistic outputs.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Parameter-Efficient-Triplane-Adaptor_v3.drawio.png", "caption": "Figure 2: Comparison between (a) traditional SD adaptation and (b) our proposed progressive rendering distillation (PRD) for native 3D generation. Traditional approach requires ground-truth 3D representations \u03b8\ud835\udf03\\thetaitalic_\u03b8 and their latents \ud835\udc9b0subscript\ud835\udc9b0\\boldsymbol{z}_{0}bold_italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT for each 3D sample to generate \ud835\udc9b0subscript\ud835\udc9b0\\boldsymbol{z}_{0}bold_italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Our proposed PRD scheme progressively denoises latents \ud835\udc9btsubscript\ud835\udc9b\ud835\udc61\\boldsymbol{z}_{t}bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT initialized from random noise into \ud835\udc9b0subscript\ud835\udc9b0\\boldsymbol{z}_{0}bold_italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, which are decoded to \u03b8\ud835\udf03\\thetaitalic_\u03b8, using multi-view diffusion models as teachers for distillation, eliminating the need for 3D data during adaptation and overcoming data scarcity.", "description": "Figure 2 illustrates the core difference between traditional Stable Diffusion (SD) adaptation for 3D generation and the proposed Progressive Rendering Distillation (PRD) method.  Traditional methods require ground truth 3D data (\u03b8) and their corresponding latent codes (z\u2080) to train the model, using these to generate new latent codes (z\u2080).  In contrast, PRD eliminates the need for ground truth 3D data. Instead, it progressively denoises latent codes (z\u209c) starting from random noise, iteratively refining them towards z\u2080, which are then decoded into 3D representations (\u03b8).  Multi-view diffusion models guide this denoising process through distillation, transferring knowledge without explicit 3D supervision. This data-free approach addresses the scarcity of high-quality 3D training data.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Comparision_v4.drawio.png", "caption": "Figure 3: Illustration of TriplaneTurbo: an SD-adapted native 3D generator using our PRD scheme. Our model generates six feature planes comprising geometry Triplane \u03b8geosubscript\ud835\udf03geo\\theta_{\\mathrm{geo}}italic_\u03b8 start_POSTSUBSCRIPT roman_geo end_POSTSUBSCRIPT and texture Triplane \u03b8texsubscript\ud835\udf03tex\\theta_{\\mathrm{tex}}italic_\u03b8 start_POSTSUBSCRIPT roman_tex end_POSTSUBSCRIPT in 4 steps. We introduce Parameter Efficient Triplane Adaptation (PETA), which requires only 2.5%percent2.52.5\\%2.5 % additional parameters for adaptation. The parameter arrangement is illustrated in the figure.", "description": "Figure 3 illustrates TriplaneTurbo, a novel 3D mesh generator built by adapting Stable Diffusion (SD).  Unlike training a 3D generator from scratch, TriplaneTurbo leverages the pre-trained capabilities of SD, significantly reducing training time and data requirements. The core of the method is Progressive Rendering Distillation (PRD), which uses multi-view teacher models to guide the adaptation without relying on 3D ground truth data.  The architecture generates six feature planes in four steps: two Triplanes (one for geometry and one for texture) and four accompanying features.  The Parameter-Efficient Triplane Adaptation (PETA) method only adds 2.5% additional trainable parameters to the pre-trained SD model, keeping the adaptation computationally efficient.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Showcase_supp_v1-1.drawio_lq.jpg", "caption": "Figure 4: Qualitative comparison of text-to-mesh generation results by competing methods. Please refer to Sec.\u00a04.2 for details.", "description": "This figure provides a qualitative comparison of text-to-mesh generation results from several competing methods.  It showcases various 3D model outputs generated from the same set of text prompts, allowing for a visual comparison of the quality, detail, and accuracy of each method in rendering diverse objects described by text.  Section 4.2 of the paper provides detailed quantitative analysis that complements the qualitative overview presented in this figure.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Ablation_v2-1-1.drawio.png", "caption": "Figure 5: More results of our model trained with expanded corpus.", "description": "This figure showcases additional results generated by the model after training with an expanded dataset.  The images depict diverse 3D models of varying complexity and artistic styles, demonstrating the model's improved ability to generate high-fidelity and text-consistent 3D meshes from creative and complex prompts.  The variety of subjects, from fantasy characters to everyday objects, highlights the model's capacity for generalization.", "section": "4.2. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Ablation_v2-1-2.drawio.png", "caption": "Figure 6: Visualizations on the ablation studies of PRD algorithm.", "description": "This figure presents a qualitative comparison of the results obtained using the Progressive Rendering Distillation (PRD) algorithm with different numbers of steps (K). The results demonstrate that increasing the number of steps (K) in PRD leads to improved 3D model generation, specifically the quality of both geometry and textures. The models generated with more steps show more refined details and better overall quality.", "section": "3.2 Progressive Rendering Distillation"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Showcase_supp_v1-2.drawio_lq.jpg", "caption": "Figure 7: Visualizations on the ablation study of PETA.", "description": "This figure visualizes the ablation study on the effectiveness of the Parameter-Efficient Triplane Adaptation (PETA) module.  It compares the results of using a fully-trained model, a model with basic LoRA adaptation, and the proposed PETA model. The comparison highlights PETA's superior performance in generating high-quality, text-consistent 3D meshes.  Visual examples are provided to illustrate the differences in the quality of the generated outputs from each method.", "section": "3.3 Parameter-Efficient Triplane Adaptation"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Comparison_LATTE3D_v1.png", "caption": "Figure 8: More results of our model trained with expanded corpus.", "description": "This figure showcases additional results generated by the model after training with an expanded dataset.  The images demonstrate the model's ability to generate diverse and high-fidelity 3D meshes from a wide range of complex and creative text prompts.  The examples highlight the model's improved ability to handle challenging prompts and produce consistent, detailed outputs.", "section": "4.2. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Comparison_PI3D_v1.drawio.png", "caption": "Figure 9: Qualitative comparison with LATTE3D\u00a0[134].", "description": "This figure compares the results of the proposed Progressive Rendering Distillation (PRD) method with those of the LATTE3D method [134].  It showcases side-by-side visual results for two example text prompts: \"A blue tulip\" and \"A pile of dice on a green tabletop.\" The comparison highlights the differences in generated 3D model quality and realism between the two approaches, demonstrating PRD's ability to produce higher-fidelity results.", "section": "4.2. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Comparison_HexaGen3D_v1.drawio.png", "caption": "Figure 10: Qualitative comparison with PI3D\u00a0[67].", "description": "This figure presents a qualitative comparison of 3D model generation results between the proposed method and PI3D [67].  It showcases several examples of text prompts and their corresponding generated 3D models, highlighting the differences in visual quality, geometric accuracy, and text consistency between the two approaches. This visual comparison allows for a direct assessment of the advantages of the proposed method over PI3D in terms of generating high-fidelity and text-consistent 3D outputs.", "section": "4.2. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Ablation_v2-3.drawio.png", "caption": "Figure 11: Qualitative comparison with HexaGen3D\u00a0[78].", "description": "Figure 11 presents a qualitative comparison of 3D model generation results between the proposed method and HexaGen3D [78].  It showcases example text prompts and their corresponding generated outputs from both methods, allowing for a visual assessment of the differences in terms of mesh quality, texture detail, and overall fidelity. The figure highlights the advantages of the proposed approach in generating higher-quality 3D models.", "section": "4.2. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Ablation_v2-4.drawio.png", "caption": "Figure 12: Visualizations for the ablation study on jointly using SD, MV and RD as multi-view teachers.", "description": "This ablation study visualizes the impact of using different combinations of Stable Diffusion (SD), Multi-view Dream (MV), and RichDreamer (RD) as multi-view teachers during the training process.  Each row shows the results for a different configuration, with 'w/o SD' indicating SD was excluded, 'w/o MV' indicating MV was excluded, 'w/o RD' indicating RD was excluded, and 'w/ All (Proposed)' indicating all three models were used. The images show generated 3D models for the same prompts across different conditions to highlight how the exclusion of a teacher model affects the quality and consistency of the output. Notice the significant drop in quality when SD or MV are removed.", "section": "4.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.21694/extracted/6315163/figures/Ablation_v2-5.drawio.png", "caption": "Figure 13: Ablation study on dual rendering. The cross mark means the model fails to generate mesh due to training instability.", "description": "This ablation study investigates the impact of using both volume rendering and mesh rasterization for supervising the training of the 3D generator. The results show that using only volume rendering leads to training instability and failure to generate valid meshes. Conversely, only using mesh rasterization results in low-quality mesh geometry and texture. The study demonstrates that combining both methods is crucial for achieving stable training and high-quality 3D generation.", "section": "4.3. Ablation Study"}]