[{"figure_path": "https://arxiv.org/html/2412.10316/x2.png", "caption": "Figure 1: BrushEdit\u00a0can achieve all-in-one inpainting for arbitrary mask shapes without requiring separate model training for each mask type. This flexibility in handling arbitrary shapes also enhances user-driven editing, as user-provided masks often combine segmentation-based structural details with random mask noise. By supporting arbitrary mask shapes, BrushEdit\u00a0avoids the artifacts introduced by the random-mask version of BrushNet-Ran and the edge inconsistencies caused by the segmentation-mask version BrushNet-Seg\u2019s strong reliance on boundary shapes.", "description": "BrushEdit allows users to input their own masks, which often combine the detailed structure of segmentation masks with the noise of random masks. Unlike previous methods that require separate models for different mask types (e.g., BrushNet-Ran for random masks and BrushNet-Seg for segmentation masks), BrushEdit uses a single, unified model, resulting in more natural and coherent edits.  This figure shows four examples of user-provided masks and how BrushEdit uses them to generate realistic and artifact-free inpainting results.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.10316/x3.png", "caption": "Figure 2: Model overview. Our model outputs an inpainted image given the mask and masked image input. Firstly, we downsample the mask to accommodate the size of the latent, and input the masked image to the VAE encoder to align the distribution of latent space. Then, noisy latent, masked image latent, and downsampled mask are concatenated as the input of BrushEdit. The feature extracted from BrushEdit\u00a0is added to pretrained UNet layer by layer after a zero convolution block[33]. After denoising, the generated image and masked image are blended with a blurred mask.", "description": "BrushEdit's model architecture takes a mask and a masked image as input and outputs an inpainted image.  It involves downsampling the mask, encoding the masked image with VAE, concatenating noisy latent, masked image latent, and downsampled mask as BrushEdit input, adding extracted BrushEdit features to pre-trained UNet layers (using a zero convolution block), denoising, and blending the generated and masked images with a blurred mask.", "section": "IV. METHOD"}, {"figure_path": "https://arxiv.org/html/2412.10316/x4.png", "caption": "Figure 3: Benchmark overview. I and II separately show natural and artificial images, masks, and caption of BrushBench. (a) to (d) show images of humans, animals, indoor scenarios, and outdoor scenarios. Each group of images shows the original image, inside-inpainting mask, and outside-inpainting mask, with an image caption on the top. III show image, mask, and caption from EditBench\u00a0[32], with (e) for generated images and (f) for natural images. The images are randomly selected from both benchmarks.", "description": "Figure 3 provides a visual overview of the benchmarks used for evaluating the BrushEdit model.  The top two rows (I and II) showcase examples from BrushBench, featuring both natural and artificially generated images alongside their corresponding masks and captions. These examples cover various scenarios, including humans, animals, indoor, and outdoor settings.  Each image in these rows is accompanied by two masks: one for \"inside\" inpainting, where the inner part of the image is masked, and one for \"outside\" inpainting, where the outer area is masked. The bottom row (III) displays samples from EditBench, another benchmark used for testing inpainting performance.  It includes both generated images and natural images with their respective masks and captions. All images shown are randomly selected from their respective benchmarks.", "section": "V. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.10316/x5.png", "caption": "Figure 4: Comparison of previous editing methods and BrushEdit\u00a0on natural and synthetic images, covering image editing operations such as removing objects\u00a0(I), adding objects\u00a0(II), modifying attributes\u00a0(III), and swapping objects\u00a0(IV).", "description": "This figure visually compares existing image editing methods (Prompt-to-Prompt, MasaCtrl, pix2pix-zero, and Plug-and-Play with DDIM and PnP inversion) with BrushEdit. It demonstrates various editing operations like removing, adding, modifying, and swapping objects on both natural and synthetic images.  Specifically, row I shows removing a flower from a dog's mouth and a laptop from a table. Row II presents adding a collar to a dog and flowers to a woman's hair. Row III depicts changing a jacket to a blouse and a wreath to a crown. Lastly, row IV showcases replacing dumplings with sushi and a car with a motorcycle. BrushEdit exhibits better coherence between edited and unedited regions, closer adherence to editing instructions, smoother transitions at mask boundaries, and higher overall consistency.", "section": "IV. METHOD"}, {"figure_path": "https://arxiv.org/html/2412.10316/x6.png", "caption": "Figure 5: Performance comparisons of BrushEdit\u00a0and previous image inpainting methods across various inpainting tasks: (I) Random Mask Inpainting (II) Segmentation Mask Inpainting. Each group of results contains 7777 inpainting methods: (b) Blended Latent Diffusion (BLD)\u00a0[27], (c) Stable Diffusion Inpainting (SDI)\u00a0[5], (d) HD-Painter (HDP)\u00a0[30], (e) PowerPaint (PP)\u00a0[29], (f) ControlNet-Inpainting (CNI)\u00a0[33], (g) Our Previous BrushNet and (h) Ours.", "description": "Figure 5 compares the output of BrushEdit and other image inpainting models on various inpainting tasks and masks, including random and segmentation masks. Each example includes the original masked image and outputs from multiple methods: BLD, SDI, HDP, PP, CNI, BrushNet, and BrushEdit. The figure showcases BrushEdit's ability to produce more coherent and contextually appropriate inpainted regions compared to other state-of-the-art inpainting models across various scenes, objects, and masking conditions.", "section": "V. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.10316/x7.png", "caption": "Figure 6: Integrating BrushEdit\u00a0to community fine-tuned diffusion models. We use five popular community diffusion models fine-tuned from stable diffusion v1.5: DreamShaper (DS)\u00a0[99], epiCRealism (ER)\u00a0[100], Henmix_Real (HR)\u00a0[101], MeinaMix (MM)\u00a0[102], and Realistic Vision (RV)\u00a0[103]. MM is specifically designed for anime images.", "description": "Figure 6 demonstrates the integration of BrushEdit with five popular community-fine-tuned Stable Diffusion 1.5 models: DreamShaper, epiCRealism, Henmix_Real, MeinaMix (specifically designed for anime), and Realistic Vision.  The figure showcases how BrushEdit, through its flexible design, allows these diverse models to perform image editing tasks while preserving background details and adhering to the textual instructions.", "section": "V. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.10316/x8.png", "caption": "Figure 7: Flexible control scale of BrushEdit. (a) shows the given masked image, (b)-(h) show adding control scale w\ud835\udc64witalic_w from 1.01.01.01.0 to 0.20.20.20.2. Results show a gradually diminishing controllable ability from precise to rough control.", "description": "Figure 7 demonstrates how BrushEdit's control scale parameter, *w*, allows adjustment of the balance between preserving the original content of the unmasked regions and applying the edits specified in the text prompt. Subfigures (b) through (h) show the output images as *w* decreases from 1.0 to 0.2.  A larger *w* value prioritizes preserving the original image, leading to more subtle edits. Conversely, a smaller *w* value emphasizes the edits, potentially at the cost of altering more of the original content. This demonstrates that *w* acts as a slider for how strongly the edits are applied, giving users fine-grained control over the editing process.", "section": "V. Experiments"}]