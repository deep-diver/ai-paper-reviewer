[{"content": "| Method | Temporal | Subject | Background | Motion | Dynamic | Imaging | FID-FVD \u2193 | FVD \u2193 | CD-FVD \u2193 | \n|---|---|---|---|---|---|---|---|---|---| \n| **Stable Diffusion1.5** |  |  |  |  |  |  |  |  |  | \n| MagicPose [Chang et al., 2023](https://arxiv.org/html/2412.09349/bib.bib5) | 96.65 | 95.12 | 94.55 | 98.29 | 22.70 | 63.87 | 15.53 | 1015.04 | 693.24 |\n| Moore [MooreThreads, 2024](https://arxiv.org/html/2412.09349/bib.bib19) | 96.86 | 95.18 | 95.37 | 98.01 | 25.51 | 69.14 | 11.58 | 924.40 | 687.88 |\n| MusePose [Tong et al., 2024](https://arxiv.org/html/2412.09349/bib.bib28) | 97.02 | 95.27 | 95.16 | 98.45 | 27.31 | 71.56 | 11.48 | 866.36 | 626.59 |\n| MusePose+Ours | **97.63** | **95.70** | **95.64** | **98.51** | **31.34** | **71.89** | **11.26** | **764.00** | **622.64** |\n| **Stable Video Diffusion** |  |  |  |  |  |  |  |  |  | \n| ControlNeXt [Peng et al., 2024](https://arxiv.org/html/2412.09349/bib.bib21) | 97.55 | 94.58 | 95.60 | 98.75 | 27.58 | 70.40 | 10.49 | 496.87 | 624.51 |\n| MimicMotion [Zhang et al., 2024](https://arxiv.org/html/2412.09349/bib.bib43) | 97.56 | 94.95 | 95.36 | 98.67 | 28.42 | 68.42 | 10.50 | 598.41 | 621.90 |\n| MimicMotion+Ours | **97.73** | **95.72** | **95.90** | **98.89** | **29.51** | **71.33** | **10.24** | **466.93** | **603.27** |", "caption": "Table 1: Quantitative comparisons on Tiktok dataset.", "description": "This table presents a quantitative comparison of different human image animation methods on the TikTok dataset.  The methods are evaluated across several key metrics, including temporal flickering, subject consistency, background consistency, motion smoothness, dynamic range, and image quality.  Lower values for FID, FVD, and CD-FVD indicate better video quality, while higher values for VBench suggest better overall video quality according to human perception.", "section": "5 EXPERIMENTS"}, {"content": "| Method | Temporal | Subject | Background | Motion | Dynamic | Imaging | Aesthetic |\n|---|---|---|---|---|---|---|---| \n| **Method** | **Flickering** | **Consistency** | **Consistency** | **Smoothness** | **Degree** | **Quality** | **Quality** |\n| *Stable Diffusion1.5* |  |  |  |  |  |  |  |\n| MagicPose (Chang et al., 2023) | 92.65 | 93.71 | 98.51 | 25.67 | 63.78 | 93.65 | 46.16 |\n| Moore (MooreThreads, 2024) | 92.83 | 92.42 | 98.12 | 27.43 | 65.32 | 94.61 | 47.23 |\n| MusePose (Tong et al., 2024) | 93.12 | 93.97 | 98.58 | 28.72 | 65.26 | 96.41 | 49.34 |\n| MusePose+Ours | **93.43** | **94.22** | **98.76** | **29.61** | **65.48** | **96.63** | **49.39** |\n| *Stable Video Diffusion* |  |  |  |  |  |  |  |\n| ControlNeXt (Peng et al., 2024) | 93.25 | 94.27 | 98.70 | 28.42 | 64.36 | 97.42 | 49.10 |\n| MimicMotion (Zhang et al., 2024) | 93.32 | 94.12 | 98.50 | 29.81 | 64.51 | 97.45 | 49.03 |\n| MimicMotion+Ours | **93.59** | **94.35** | **98.75** | **30.02** | **65.56** | **97.80** | **49.93** |", "caption": "Table 2: Quantitative comparisons on unseen dataset.", "description": "This table presents a quantitative comparison of different human image animation methods on an unseen dataset.  It evaluates the performance of each method across several key metrics, including temporal flickering, subject consistency, background consistency, motion smoothness, dynamic degree, imaging quality, and aesthetic quality.  The unseen dataset helps assess the generalizability of the models beyond the training data.", "section": "5 EXPERIMENTS"}, {"content": "| Method | Temporal | Subject | Background | Motion | Dynamic | Imaging | FID-FVD | FVD |\n|---|---|---|---|---|---|---|---|---|\n| w/o Motion | 97.66 | 95.04 | 95.31 | 98.75 | 29.42 | 69.53 | 10.31 | 478.91 |\n| w/o Point | 97.47 | 95.57 | 95.43 | 98.42 | 29.14 | 70.14 | 10.28 | 498.74 |\n| Full Model | 97.73 | 95.72 | 95.90 | 98.89 | 29.51 | 71.33 | 10.24 | 466.93 |", "caption": "Table 3: Ablation study on different control guidance. \u201cw/o Motion\u201d denotes the model configuration that disregards motion filed guidance. \u201cw/o Point\u201d indicates the variant model that removes the keypoint correspondence.", "description": "This ablation study analyzes the impact of different control signals on the performance of the DisPose model for human image animation.  The table compares the full DisPose model against two variants: one without motion field guidance ('w/o Motion') and one without keypoint correspondence ('w/o Point').  The results demonstrate the contribution of each component to overall performance metrics, including temporal flickering, subject and background consistency, motion smoothness, and image quality.", "section": "5 EXPERIMENTS"}, {"content": "| Methods | FID-FVD\u2193 | FVD\u2193 |\n|---|---|---|\n| Exp1 | 10.43 | 514.83 |\n| Exp2 | 10.94 | 551.32 |\n| Full Model | 10.24 | 466.93 |", "caption": "Table 6: Performance comparisons for image-level metrics.", "description": "This table presents a quantitative comparison of different methods for human image animation, focusing on image-level metrics.  It shows the performance of various models, including baselines and those incorporating the proposed DisPose method, across metrics such as SSIM, PSNR, LPIPS, and L1.  Lower values for PSNR, LPIPS, and L1 indicate better performance, while higher SSIM values are desirable. This allows for assessment of the image quality generated by each method.", "section": "5 Experiments"}, {"content": "| Methods | subject consistency \u2191 | background consistency \u2191 |\n|---|---|---|\n| Full Model w/o CMP | 93.94 | 97.83 |\n| Full Model | 94.35 | 98.75 |", "caption": "Table 7: Performance comparisons for image-level metrics.", "description": "This table presents a quantitative comparison of different methods for human image animation, focusing on image-level metrics.  It shows a comparison of the trainable parameters (in MB) and inference time (seconds per frame) for several methods, including MusePose, MimicMotion, and versions of these methods that incorporate the proposed DisPose approach.  This allows for a comparison of computational cost and efficiency among the different animation techniques.", "section": "5 Experiments"}]