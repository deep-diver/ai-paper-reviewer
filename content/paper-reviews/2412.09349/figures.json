[{"figure_path": "https://arxiv.org/html/2412.09349/x1.png", "caption": "Figure 1: Our method demonstrates its ability to produce diverse animations and preserve consistency of appearance.", "description": "This figure showcases the results of the DISPOSE method on a set of input images, demonstrating its ability to generate a wide variety of animations while maintaining the visual consistency of the subject's appearance.  The animations are diverse in terms of pose and movement, highlighting the method's effectiveness in controlling the animation process. The consistency shows that the generated animations maintain the original appearance, avoiding significant changes or distortions. Each animation maintains a consistent look and feel, even with varied movements. ", "section": "ABSTRACT"}, {"figure_path": "https://arxiv.org/html/2412.09349/x2.png", "caption": "Figure 2: The overview of proposed DisPose.", "description": "This figure provides a detailed overview of the DisPose architecture. It illustrates the input elements: a reference image and driving video. The process is broken down into stages: (a) user input (reference image and driving video), (b) existing animation model, (c) motion field estimation which involves both sparse and dense motion field calculation from the driving video and reference image, (d) point feature extraction from reference and target images, and (e) the final hybrid ControlNet which combines all information to generate the final video. The figure visually summarizes how DisPose processes inputs, generating a dense motion field and keypoint correspondence for controllable animation.", "section": "4 DISPOSE"}, {"figure_path": "https://arxiv.org/html/2412.09349/x3.png", "caption": "Figure 3: Qualitative comparisons between our method and the state-of-the-art models on the TikTok dataset.", "description": "Figure 3 presents a qualitative comparison of video generation results from different methods on the TikTok dataset.  The figure shows generated videos alongside a reference image and ground truth video.  This allows for a visual assessment of each method's ability to accurately animate the reference image based on the driving video, considering factors such as motion fidelity, temporal consistency, and overall visual quality.  The comparison includes our proposed method (MusePose+Ours and MimicMotion+Ours) against state-of-the-art methods such as Stable Diffusion, MagicPose, Moore, MusePose, ControlNeXt, and MimicMotion. By visually inspecting these generated videos, one can qualitatively evaluate the effectiveness and visual appeal of various approaches to controllable human image animation.", "section": "5.2 QUANTITATIVE COMPARISON"}, {"figure_path": "https://arxiv.org/html/2412.09349/x4.png", "caption": "Figure 4: Qualitative comparison of our approach with the dense control-based method.", "description": "Figure 4 presents a qualitative comparison of the proposed DisPose method against a dense control-based method (Champ).  The figure showcases animation results from both methods using the same reference and driving videos. The visual comparison highlights the differences in generated video quality and consistency between the approaches, emphasizing how DisPose can maintain better appearance consistency and fidelity to the reference image even in scenarios where significant body shape differences exist between the reference and driving images. This illustrates DisPose's effectiveness in generating high-quality human image animation without the need for dense guidance.", "section": "5.2 Quantitative Comparison"}, {"figure_path": "https://arxiv.org/html/2412.09349/x5.png", "caption": "Figure 5: Comparison of our reference-based dense motion field and existing dense conditions.", "description": "Figure 5 presents a qualitative comparison between DisPose's reference-based dense motion field and existing dense methods (like Champ) for human image animation.  It visually demonstrates the generalization capability of DisPose by showing generated videos from various methods using the same reference and driving video. The goal is to highlight how DisPose handles differences in body shape between reference and driving video better than methods that rely on strict dense constraints.", "section": "5.3 QUALITATIVE RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.09349/x6.png", "caption": "Figure 6: The demonstration of cross ID animation from the proposed method.", "description": "This figure demonstrates the capability of the proposed DisPose method to perform cross-identity animation.  It shows several examples where the motion from a driving video is applied to a reference image of a different person.  The results highlight the model's ability to maintain the appearance of the reference person while accurately replicating the motion from the driving video, showcasing its versatility and effectiveness in controlling both appearance and motion independently.", "section": "5.2 Quantitative Comparison"}, {"figure_path": "https://arxiv.org/html/2412.09349/x7.png", "caption": "Figure 7: Qualitative analysis of semantic correspondence. Given a red source point in an image (far left), we use its diffusion feature to retrieve the corresponding point in the image on the right.", "description": "This figure demonstrates the concept of semantic correspondence in the DisPose model.  A red point is selected in a source image (far left). The DisPose model then uses the diffusion features extracted from that point to locate and highlight the corresponding point in another image (far right). This showcases the model's ability to maintain consistent appearance across different images by mapping keypoints based on their semantic meaning rather than just their spatial location.", "section": "Qualitative analysis of semantic correspondence"}, {"figure_path": "https://arxiv.org/html/2412.09349/x8.png", "caption": "Figure 8: Qualitative results of our method for multi-view video generation.", "description": "This figure showcases the results of applying the proposed DisPose method to generate multi-view videos.  It demonstrates the model's ability to produce consistent and high-quality animations from a single reference image, even when generating multiple viewpoints of the same action. This highlights one of the key capabilities of DisPose: its generalization and effectiveness in various situations, moving beyond the limitations of traditional single-view human image animation.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.09349/x9.png", "caption": "Figure 9: More Qualitative Comparisons.", "description": "Figure 9 presents additional qualitative results comparing the proposed DisPose method against other state-of-the-art human image animation methods.  The figure showcases multiple examples of generated videos, highlighting the improved quality and consistency achieved by DisPose in terms of temporal coherence, motion smoothness, and overall visual fidelity.  It visually demonstrates DisPose's ability to generate diverse and realistic animations while maintaining consistency in appearance and accurately reflecting the pose and motion from the driving video.", "section": "5.2 Quantitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2412.09349/x10.png", "caption": "Figure 10: More Qualitative Comparisons.", "description": "Figure 10 presents additional qualitative results, showcasing the performance of the proposed DisPose method on various human image animation scenarios.  It visually demonstrates the model's ability to generate high-quality and consistent video animations across diverse body shapes, poses, and backgrounds.  The figure complements the quantitative results presented earlier in the paper by providing concrete visual examples of the method's capabilities.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.09349/extracted/6065057/image/abs.png", "caption": "Figure 11: Qualitative results about motion field guidance and keypoints correspondence.", "description": "This figure provides a qualitative comparison of the model's performance with and without different components of the proposed method.  Specifically, it shows the impact of motion field guidance and keypoint correspondence on the generated video quality.  By comparing results with and without these components, the figure demonstrates their individual contributions to improving the overall quality and consistency of the generated animations.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.09349/extracted/6065057/image/abs_pipeline.png", "caption": "Table 4: The impact of hybrid ControlNet.", "description": "This table presents a quantitative analysis of the impact of using a hybrid ControlNet architecture on the performance of the DisPose model for human image animation.  It compares results across various metrics, such as FID-FVD and FVD, which measure the quality of generated videos,  likely including measures of temporal consistency and visual fidelity. By showing results with and without the hybrid ControlNet, the table highlights its contribution to improving the overall quality and consistency of the generated animations.", "section": "5. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.09349/extracted/6065057/image/flow1.png", "caption": "Table 5: The impact of CMP.", "description": "Table 5 presents the results of an ablation study assessing the impact of Conditional Motion Propagation (CMP) on the overall performance of the proposed DisPose method.  It shows how CMP affects the model's ability to generate videos with improved consistency and quality.", "section": "5. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.09349/extracted/6065057/image/flow2.png", "caption": "Figure 12: Different hybrid ControlNet architectures.", "description": "This figure illustrates two variations of the hybrid ControlNet architecture used in the DisPose model.  The hybrid ControlNet integrates motion field guidance and keypoint correspondence to enhance the quality of human image animation. The left architecture (a) shows the motion field information fed directly into the denoising U-Net, while the right architecture (b) depicts the use of a separate hybrid ControlNet module to process this information before feeding it to the denoising U-Net.  The difference highlights alternative approaches for integrating additional control signals into the diffusion process.", "section": "4.3 PLUG-AND-PLAY HYBRID CONTROLNET"}, {"figure_path": "https://arxiv.org/html/2412.09349/extracted/6065057/image/flow_noise.png", "caption": "Figure 13: Body matched motion field visualization.", "description": "This figure visualizes the motion fields generated by the proposed method during different stages. (a) shows the frame-by-frame optical flow extracted from the driving video, highlighting the overall movement. (b) illustrates the motion field computed during training without sample refinement, revealing a less refined and possibly noisy representation. (c) presents the motion field generated with sample refinement during training, showing a smoother and more accurate motion representation. Finally, (d) shows the motion field used during inference, demonstrating the final refined motion guidance used to animate the target image.", "section": "4.1 Motion Field Guidance"}]