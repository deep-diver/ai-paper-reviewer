[{"figure_path": "https://arxiv.org/html/2503.12271/x1.png", "caption": "Figure 1: \\ours\u00a0iteratively refines image generation by using a vision-language model (VLM) to critique generations and a Diffusion Transformer (DiT) to self-improve using past generations and feedback. Specifically, at each generation step N, feedback from previous iterations (N-3, N-2, N-1, \u2026) are incorporated to progressively improve future generations. Unlike traditional best-of-N sampling, \\ours\u00a0actively corrects errors in object count, position, and attributes, enabling more precise generations with fewer samples.", "description": "Reflect-DiT refines image generation iteratively.  A vision-language model (VLM) critiques each generated image, providing feedback.  A Diffusion Transformer (DiT) uses this feedback, along with previous generations, to improve subsequent generations.  This contrasts with standard best-of-N sampling, which passively selects from multiple independent generations. Reflect-DiT actively corrects errors (object count, position, attributes), leading to higher quality images with fewer samples.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12271/x2.png", "caption": "Figure 2: Architecture of \\ours. Given a prompt, past images and feedback, we first encode the images into a set of vision embeddings [V1,V2,\u2026]subscript\ud835\udc491subscript\ud835\udc492\u2026[V_{1},V_{2},\\dots][ italic_V start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , \u2026 ] using a vision encoder, and encode text feedback to a set of text embeddings [E1,E2\u2062\u2026]subscript\ud835\udc381subscript\ud835\udc382\u2026[E_{1},E_{2}...][ italic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT \u2026 ]. We then concatenate these embeddings into a single sequence M\ud835\udc40Mitalic_M, and pass it through the Context Transformer to obtain M\u2032superscript\ud835\udc40\u2032M^{\\prime}italic_M start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT. The extra context M\u2032superscript\ud835\udc40\u2032M^{\\prime}italic_M start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is concatenated directly after the standard prompt embeddings and passed into the cross-attention layers of the Diffusion Transformer (DiT).", "description": "Figure 2 illustrates the architecture of Reflect-DiT, a model that refines image generation iteratively using past generations and feedback.  The process begins with a prompt.  Past images are encoded into vision embeddings (V1, V2, etc.) using a vision encoder, while textual feedback is encoded into text embeddings (E1, E2, etc.). These embeddings are concatenated into a sequence (M) and passed through a Context Transformer to generate a refined context (M'). This refined context (M') is added to the standard prompt embeddings and fed into the cross-attention layers of a Diffusion Transformer (DiT). The DiT then produces a refined image, and the cycle repeats until the desired image quality is achieved.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.12271/x3.png", "caption": "Figure 3: Side-by-side qualitative comparison of \\ours\u00a0and best-of-N sampling. \\ours\u00a0leverages feedback to iteratively refine image generations, resulting in more accurate and visually coherent outputs. In the first example, \\ours\u00a0progressively adjusts object positions to better satisfy the prompt \u201ca cup left of an umbrella,\u201d achieving significantly better image-text alignment than best-of-N sampling. The second example demonstrates how \\ours\u00a0corrects multiple counting constraints (\u201cfive monarch butterflies\u201d and \u201ca single dandelion\u201d) over successive iterations, gradually converging to the correct solution. Lastly, in the rightmost example, \\ours\u00a0uses in-context feedback to refine object shapes, producing a more precise and intentional design compared to best-of-N.", "description": "This figure presents a qualitative comparison between Reflect-DiT and the traditional best-of-N sampling method for text-to-image generation.  It showcases three examples where Reflect-DiT's iterative refinement process, guided by feedback, leads to more accurate and visually coherent results than best-of-N. The first example illustrates Reflect-DiT's ability to correct object positioning, the second demonstrates its capacity to resolve multiple counting constraints, and the third highlights its potential for refining object shapes.  These examples demonstrate how Reflect-DiT actively addresses specific issues, converging towards more precise and aligned image generations compared to the passive selection approach of best-of-N.", "section": "4. Qualitative Examples"}, {"figure_path": "https://arxiv.org/html/2503.12271/x4.png", "caption": "Figure 4: Comparison of \\ours\u00a0with other finetuning methods. We find that \\ours\u00a0 is able to consistently outperform finetuning methods, like supervised finetuning (SFT) and Diffusion-DPO (DPO). Using only 4 samples, \\ours\u00a0can outperform related finetuning baselines using best-of-20 sampling.", "description": "This figure compares Reflect-DiT's performance against other fine-tuning methods like Supervised Fine-Tuning (SFT) and Diffusion-DPO (DPO) on the GenEval benchmark.  The x-axis represents the number of samples used for image generation, and the y-axis shows the GenEval score.  The graph demonstrates that Reflect-DiT consistently achieves higher GenEval scores than SFT and DPO, even when using significantly fewer samples. Notably, with only 4 samples, Reflect-DiT surpasses the performance of SFT and DPO using 20 samples each.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12271/x5.png", "caption": "Figure 5: Human evaluation win-rate (%) on PartiPrompts dataset. We perform a user study to evaluate the effectiveness of \\ours\u00a0in broadly improving text-to-image generation. Results show that human evaluators consistently prefer generations from \\ours\u00a0over best-of-N sampling.", "description": "This figure displays the results of a human evaluation study comparing Reflect-DiT's image generation capabilities to a traditional best-of-N sampling method.  The study used the PartiPrompts dataset, which focuses on more complex image generation tasks than the GenEval benchmark used in other parts of the paper. The win-rate represents the percentage of times human evaluators preferred images generated by Reflect-DiT over the best-of-N approach.  The results demonstrate a clear preference for Reflect-DiT, suggesting it produces images of higher quality and better alignment with the prompts.", "section": "4.6 Human Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.12271/x6.png", "caption": "Figure 6: Illustration of the iterative refinement process in \\ours. \\ours\u00a0starts with an initial image generated from the prompt and progressively refines it based on textual feedback until the final output meets the desired criteria, demonstrating the effectiveness of our reflection-based approach. In the first sequence, \\ours\u00a0handles a complex scene by gradually repositioning multiple objects\u2014\u201cwoman,\u201d \u201ctree,\u201d \u201ccat,\u201d and \u201cdog\u201d\u2014to achieve correct spatial alignment. Additionally, it recognizes subtle object misclassifications, such as changing the second \u201cdog\u201d to a \u201ccat\u201d based on feedback. The second example demonstrates a counting problem, where the model iteratively adjusts the number of detached seeds until it converges to the correct count. The final example presents a particularly challenging scenario: the prompt requires the \u201cdog\u201d to be positioned to the \u201cright of a tie\u201d, an unusual object to appear independently. Initially, the model misinterprets the instruction, generating a dog wearing a tie. However, through multiple refinement steps, \\ours\u00a0learns to separate the objects and ultimately produces the correct spatial arrangement.", "description": "Figure 6 showcases Reflect-DiT's iterative refinement process through three examples.  The first demonstrates correction of object positions and classifications within a complex scene (woman, tree, cat, dog). The second example shows iterative adjustment of object counts (seeds on a dandelion head). The third example highlights the model's ability to overcome initial misinterpretations of unusual spatial relationships (dog positioned relative to a tie). Each example demonstrates Reflect-DiT's ability to refine generations based on feedback, ultimately achieving alignment with the prompt's specifications.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.12271/x7.png", "caption": "Figure 7: Failure cases of \\ours. Failure cases of \\ours. While \\ours\u00a0demonstrates strong refinement capabilities, the generated feedback can occasionally introduce errors between iterations. In the first example, the model fails to recognize that the specific lighting conditions signify a \u201csunset\u201d, leading to an incorrect adjustment. Similarly, in the second example, the model struggles to distinguish the color of the \u201cdining table\u201d because the purple hue from the \u201cdog\u201d reflects off the table, creating ambiguity. These cases highlight subjectivity in the VLM evaluation, where the model\u2019s interpretation may still be reasonable. However, the final two examples illustrate more typical failure cases. In both images, objects (\u201cboat\u201d and \u201cbutterfly\u201d) are completely overlooked by the feedback model. This issue likely arises because the objects are too small or unusually shaped, which makes them difficult to detect, resulting in incorrect evaluations.", "description": "Reflect-DiT, while generally effective, occasionally produces errors due to limitations in the feedback from its Vision-Language Model (VLM).  The first two examples show cases where the VLM misinterprets subtle details (lighting signifying sunset, color reflection ambiguity). The final two showcase more common failures where small or unusually shaped objects are overlooked, leading to inaccurate feedback and refinement.", "section": "A. Additional Results and Discussions"}, {"figure_path": "https://arxiv.org/html/2503.12271/x9.png", "caption": "Figure 8: User interface for human annotators.", "description": "This figure shows the user interface used in a human evaluation study to compare the quality of image generations from Reflect-DiT and a best-of-N baseline.  Users were presented with a prompt and two images, one from each method, and asked to select the image that best matched the prompt.  Additional space was provided for comments in cases of ambiguity or bugs.", "section": "4.6. Human Evaluation"}]