[{"figure_path": "https://arxiv.org/html/2502.04728/x1.png", "caption": "Figure 1: An overview of the proposed method. Our test-time compute scaling approach consists of two main steps:\n(1) Best-of-N Sampling for PDDL Initialization (see Section\u00a03.2): We start by running a parallel sampling process to generate multiple chain-of-thought responses that are composed of the formalized PDDL-based world model representation \ud835\udc03isubscript\ud835\udc03\ud835\udc56\\mathbf{D}_{i}bold_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the natural language thought \ud835\udc13isubscript\ud835\udc13\ud835\udc56\\mathbf{T}_{i}bold_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\n(2) Closed-loop Iteration with iVML (see Section\u00a03.3): We use Instance Verbalized Machine Learning (iVML) to iteratively improve the solutions.\nThe iVML incorporates: (1) An optimizer LLM foptsubscript\ud835\udc53optf_{\\mathrm{opt}}italic_f start_POSTSUBSCRIPT roman_opt end_POSTSUBSCRIPT that evaluates the solutions from the previous iteration, and (2) A learner LLM flearnersubscript\ud835\udc53learnerf_{\\mathrm{learner}}italic_f start_POSTSUBSCRIPT roman_learner end_POSTSUBSCRIPT that learns from the feedback and updates the PDDL-based world model \ud835\udc03isubscript\ud835\udc03\ud835\udc56\\mathbf{D}_{i}bold_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.\nThe most optimal PDDL-based world model would be sent to the systematic search engine for planning.", "description": "This figure illustrates the two-stage test-time scaling approach for PDDL domain generation.  The first stage, Best-of-N sampling, uses parallel LLM calls to generate multiple chain-of-thought responses, each consisting of a PDDL world model (D<sub>i</sub>) and a natural language explanation (T<sub>i</sub>).  The top K responses are selected. The second stage, closed-loop iteration with Instance Verbalized Machine Learning (iVML), refines the initial PDDL model. iVML uses two LLMs: an optimizer (f<sub>opt</sub>) that evaluates the current PDDL model and provides feedback, and a learner (f<sub>learner</sub>) that uses this feedback to update the PDDL model (D<sub>i</sub>). This iterative process continues until the best PDDL model is found, which is then used in a systematic search algorithm for planning.", "section": "3 The Proposed Test-time Compute Scaling Approach"}, {"figure_path": "https://arxiv.org/html/2502.04728/x2.png", "caption": "Figure 2: OpenAI-o1 plans for Termes: o1 frequently exhibits hallucination during the planning process. Specifically, in steps three and four, the LLM violates predefined rules when selecting and leveraging actions. Additionally, step four hallucinates the achievement of the goal, leading to incorrect or unrealistic outcomes. Even when using o1 itself to evaluate the hallucinated plan, it incorrectly identifies the plan as valid.", "description": "Figure 2 illustrates the shortcomings of directly using Large Language Models (LLMs) for complex planning tasks.  It shows a specific example using the Termes planning domain, where the OpenAI-o1 model produces a plan that contains several critical errors.  These errors include rule violations (the model takes actions that break predefined rules in steps three and four), and a hallucinated goal achievement (incorrectly stating that the goal has been reached in step four).  The hallucination leads to incorrect outcomes, and even when the OpenAI-o1 model is used to evaluate its own plan, it incorrectly identifies it as valid, highlighting the unreliability of relying on LLMs alone for such complex tasks.", "section": "2 Related Work"}, {"figure_path": "https://arxiv.org/html/2502.04728/x3.png", "caption": "Figure 3: Left: The performance trend of iVML with increasing training epochs. Right: The performance trend of BoN with increasing sampling numbers.", "description": "Figure 3 presents two graphs illustrating the convergence behavior of the proposed test-time scaling approach. The left graph shows how the accuracy of the Instance Verbalized Machine Learning (iVML) method improves as the number of training epochs increases.  The right graph displays the accuracy of the Best-of-N (BoN) sampling method as the sampling budget (number of candidates) grows. Both graphs demonstrate the effectiveness of the methods in PDDL domain synthesis tasks.  The plots show the number of accurately generated PDDL domains for both NL2Domain and Prob2Domain tasks. This figure highlights the balance between exploration and exploitation achieved by combining BoN and iVML.", "section": "Experiments and Results"}, {"figure_path": "https://arxiv.org/html/2502.04728/x4.png", "caption": "Figure 4: The performance of iVML on NL2Domain tasks across different initialization settings.", "description": "Figure 4 illustrates the performance comparison of the Instance Verbalized Machine Learning (iVML) algorithm under different initialization strategies for the NL2Domain task (Natural Language to PDDL Domain translation).  It shows the number of accurately generated PDDL domains across various iterations (epochs) of iVML.  Different lines represent different initialization methods (e.g., BoN with different sampling budgets, or single-pass initialization) for different LLMs (QwenCoder, dscoder, llama). This visualization helps understand how different initialization strategies affect the convergence and accuracy of the iVML algorithm in generating high-quality PDDL domains from natural language descriptions. The x-axis represents the number of iVML iterations, and the y-axis represents the number of correctly generated PDDL domains.", "section": "4.3 Convergence Comparison between BoN and iVML"}, {"figure_path": "https://arxiv.org/html/2502.04728/x5.png", "caption": "Figure 5: The performance of iVML on Prob2Domain tasks across different initialization settings.", "description": "Figure 5 presents a detailed analysis of the Instance Verbalized Machine Learning (iVML) algorithm's performance on Prob2Domain tasks.  It showcases how different initialization strategies impact iVML's ability to successfully generate PDDL domains from problem descriptions. The figure illustrates the convergence behavior of iVML across multiple runs, each employing a unique initialization method. This allows for a comparative assessment of the effectiveness and efficiency of various initialization strategies in achieving successful PDDL domain synthesis within the context of the Prob2Domain task.", "section": "4.3 Convergence Comparison between BoN and iVML"}, {"figure_path": "https://arxiv.org/html/2502.04728/extracted/6185517/content/imgs/generated.png", "caption": "Figure 6: The planning graph for Termes", "description": "This figure shows a planning graph for the Termes task, which visualizes the sequence of states and actions involved in solving the planning problem. Each node represents a state, and each edge represents an action that transitions between states. The graph illustrates the explicit state transition during the planning process for the Termes task.", "section": "3.1 PDDL-based World Model Representation"}]