[{"figure_path": "https://arxiv.org/html/2501.07730/x1.png", "caption": "Figure 1: \nText-to-Image (T2I) Generation Results by MaskGen.\nMaskGen, powered by the proposed compact text-aware 1D tokenizer TA-TiTok, is an efficient masked generative model that achieves state-of-the-art performance on multiple T2I benchmarks using only open data. The open-data, open-weight MaskGen models are designed to promote broader access and democratize T2I masked generative models.", "description": "This figure showcases the results of text-to-image generation using the MaskGen model.  MaskGen is a novel model that uses a compact, text-aware 1D tokenizer (TA-TiTok) for efficient image generation.  The images demonstrate MaskGen's ability to create high-quality images from text prompts, achieving state-of-the-art results on multiple benchmarks. Notably, MaskGen is trained using only publicly available data, making it more accessible and promoting wider use of this type of model.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.07730/x2.png", "caption": "Figure 2: \nOverview of TA-TiTok (Text-Aware Transformer-based 1-Dimensional Tokenizer).\n(a) TA-TiTok introduces three key enhancements to TiTok\u00a0[69]: First, an efficient one-stage training procedure replaces the need for a complex two-stage pipeline. Second, TA-TiTok supports 1D tokens in both discrete (VQ) and continuous (KL) formats. Third, it incorporates textual information (using CLIP\u2019s text encoder) during de-tokenization to improve semantic alignment with text captions.\n(b) A comparison of reconstruction results shows that TA-TiTok achieves superior reconstruction quality over TiTok.", "description": "Figure 2 illustrates the architecture of Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), highlighting key improvements over its predecessor, TiTok.  Panel (a) details the TA-TiTok's process: it uses a one-stage training method (more efficient than TiTok's two-stage approach), supports both discrete (VQ) and continuous (KL) 1D tokens, and incorporates text information (via CLIP's text encoder) during de-tokenization to improve the model's understanding of the text prompt and enhance image reconstruction. Panel (b) shows a qualitative comparison of reconstruction results, demonstrating TA-TiTok's superior image reconstruction quality compared to TiTok.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.07730/x3.png", "caption": "Figure 3: \nOverview of MaskGen.\nMaskGen is a family of text-to-image masked generative models that supports both discrete (VQ variant) and continuous (KL variant) token representations. For discrete tokens, MaskGen is trained with cross-entropy loss\u00a0[11], while for continuous tokens, it employs diffusion loss\u00a0[39].\nThe architecture is designed by concatenating text conditions with TA-TiTok\u2019s latent tokens (both masked and unmasked) and feeding them into Diffusion Transformer blocks\u00a0[44], with separate adaptive LayerNorm (adaLN) layers for text and image modalities, following MM-DiT\u00a0[23]. Additionally, aesthetic scores are incorporated as conditioning signals via adaLN. To encode captions, MaskGen uses the CLIP text encoder\u00a0[47] instead of the more resource-intensive T5-XXL\u00a0[48], making it more accessible to research groups with limited computational resources.", "description": "MaskGen is a family of text-to-image models supporting both discrete (VQ) and continuous (KL) image tokens.  For discrete tokens, training uses cross-entropy loss; for continuous tokens, diffusion loss is used. The architecture concatenates text and image tokens (masked and unmasked) and feeds them into Diffusion Transformer blocks.  Separate adaptive Layer Normalization (adaLN) layers handle text and image data, following the MM-DiT design.  Aesthetic scores are also included as conditioning signals. CLIP is used for text encoding, chosen over the more computationally intensive T5-XXL to improve accessibility.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.07730/x4.png", "caption": "Figure 4: \nPrompts Used for Recaptioning.\nOne of the four prompts are used when recpationing each image.\n{original_caption} is replaced with the original caption paired with the image.", "description": "This figure shows the four prompts used for re-captioning images in the dataset.  The goal was to improve the quality and detail of the captions associated with the images.  Each prompt uses the original caption associated with the image as a starting point and guides the improvement process by instructing how to write a better, more descriptive caption. The re-captioning task uses a state-of-the-art vision language model called Molmo to generate the improved captions.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.07730/extracted/6129647/fig/aes_varying.png", "caption": "Figure 5: \nRe-captioning Results.\nAugmented captions, generated by Molmo\u00a0[16], offer richer details and improved alignment with image content.", "description": "This figure displays examples of image re-captioning using the Molmo language model [16].  The original captions are compared to the new, improved captions generated by Molmo. The improvements demonstrate that Molmo produces more detailed and accurate descriptions that better align with the image content, reflecting a higher quality and more precise correspondence between image and text.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.07730/x5.png", "caption": "Figure 6: \nGenerated Images with Varying Aesthetic Score Conditioning. Conditioning on higher aesthetic scores produces generated images with enhanced fine-grained details.", "description": "This figure shows how the aesthetic score influences the quality of generated images in the MaskGen model.  Higher aesthetic scores lead to images with more refined detail and improved visual quality, demonstrating that the aesthetic score acts as a control mechanism for image fidelity and detail.  The example images illustrate the effect of different aesthetic scores on the level of detail and overall visual richness.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.07730/x6.png", "caption": "Figure 7: \nGenerated Images by MaskGen with Different Tokenizer Types.\nFor each caption, the top row displays images generated using continuous tokens (KL), while the bottom row shows images generated using discrete tokens (VQ). Long prompts are truncated for brevity.", "description": "This figure compares image generation results from MaskGen using two different types of image tokenizers: continuous (KL) and discrete (VQ).  For several example text prompts, it shows the generated images produced by both types of tokenizers. This allows for a visual comparison of the image quality and style differences produced by KL and VQ methods, highlighting potential advantages and disadvantages of each approach. Note that only a portion of the original prompts are shown due to space constraints.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.07730/x7.png", "caption": "Figure 8: \nQualitative examples of Text-to-Image (T2I) Generation with MaskGen.\nMaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.", "description": "This figure showcases several examples of images generated by the MaskGen model.  The images demonstrate the model's ability to generate high-quality, visually diverse results across a range of prompts.  The variety in style and subject matter highlights MaskGen's capacity to interpret and translate different textual descriptions into corresponding visuals. The high fidelity of the images underscores the effectiveness of the model's architecture, particularly the compact, text-aware 1D tokenizer (TA-TiTok).", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.07730/x8.png", "caption": "Figure 9: \nQualitative examples of Text-to-Image (T2I) Generation with MaskGen.\nMaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.", "description": "This figure showcases various examples of images generated by the MaskGen model.  The images demonstrate the model's ability to produce high-quality and diverse outputs, reflecting a wide range of styles, subjects, and levels of detail.  MaskGen's effectiveness stems from its use of the TA-TiTok tokenizer, a compact and efficient algorithm that allows for rapid and high-quality image generation. The variety of the images shows MaskGen's ability to accurately interpret and generate images according to varied textual prompts.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.07730/x9.png", "caption": "Figure 10: \nQualitative examples of Text-to-Image (T2I) Generation with MaskGen.\nMaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.", "description": "This figure showcases several examples of images generated by the MaskGen model.  Each image demonstrates the model's ability to generate high-fidelity, diverse images from a wide range of text prompts.  The results highlight the model's effectiveness in capturing fine details and complex scene descriptions.  The use of the compact, text-aware 1D tokenizer, TA-TiTok, contributes to the efficiency and high-quality image generation.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.07730/x10.png", "caption": "Figure 11: \nQualitative examples of Text-to-Image (T2I) Generation with MaskGen.\nMaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.", "description": "This figure showcases examples of images generated using the MaskGen model.  MaskGen leverages the TA-TiTok tokenizer, which is designed for efficiency and to facilitate high-quality image generation from text prompts. The examples demonstrate the model's ability to produce diverse and high-fidelity images across a range of styles and subjects, reflecting the effectiveness of the TA-TiTok tokenizer and the MaskGen architecture.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.07730/x11.png", "caption": "Figure 12: \nQualitative examples of Text-to-Image (T2I) Generation with MaskGen.\nMaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.", "description": "This figure showcases various text-to-image generation results produced by MaskGen.  Each image is accompanied by the text prompt used to generate it. The results highlight MaskGen's ability to generate high-quality, diverse, and visually appealing images based on a wide range of text descriptions, demonstrating its effectiveness at both image reconstruction and generation. This is enabled by its utilization of the efficient and compact text-aware 1D tokenizer, TA-TiTok.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.07730/x12.png", "caption": "Figure 13: \nQualitative examples of Text-to-Image (T2I) Generation with MaskGen.\nMaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images.", "description": "This figure showcases several examples of images generated using the MaskGen model.  MaskGen, which utilizes the efficient TA-TiTok tokenizer, demonstrates its ability to create high-quality, diverse images from various text prompts. The examples highlight the model's capacity to handle different artistic styles, levels of detail, and scene complexities, all while maintaining visual fidelity.", "section": "5. Experimental Results"}]