[{"figure_path": "https://arxiv.org/html/2504.04010/x2.png", "caption": "Figure 1: \nWe introduce DiTaiListener, a DiT-based listener generation model that synthesizes high-fidelity human portrait videos of listener behaviors from speaker audio and facial motion inputs in an end-to-end manner. DiTaiListener-Gen generates customizable listener responses in short segments, while DiTaiListener-Edit ensures seamless transitions between segments, producing continuous and natural listener behaviors. Together, DiTaiListener supports user-friendly customizable listener behavior generation for variable speaker inputs.", "description": "DiTaiListener is a novel, end-to-end video generation model that creates high-fidelity videos of a listener's reactions based on speaker audio and facial expressions.  It consists of two main components: DiTaiListener-Gen, which generates short, customizable video segments of listener responses, and DiTaiListener-Edit, which seamlessly blends these segments together to produce natural-looking, continuous listener behavior.  The figure shows examples of diverse listener responses generated by the model for different speaker inputs, demonstrating its ability to generate a wide range of believable listener reactions.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.04010/x3.png", "caption": "Figure 2: Overview of DiTaiListener. a) Given the listener\u2019s appearance (reference frame), speaker\u2019s motion, encoded via EMOCA 3DMM coefficients, speech (Wav2Vec2) and an input text control, DiTaiListener learns to generate listener face and head motions in pixel space through a video diffusion model powered by a modified DiT. b) We introduce a Causal Temporal Multimodal Adapter for seamless integration of multimodal speaker input in a temporally causal manner. c) Our long video generation pipeline consists of two video generation models. DiTaiListener-Gen generates video blocks that are fused by the DiTaiListener-Edit model that facilitates the smooth transition between two blocks, improving smoothness and reducing computational cost compared to existing long video generation strategies, e.g., prompt traveling and teacher forcing.", "description": "Figure 2 illustrates the architecture and functionality of DiTaiListener, a system for generating high-fidelity listener videos.  Panel (a) details the core generation process: input modalities (listener's appearance, speaker's audio from Wav2Vec2, speaker's motion from EMOCA 3DMM, and optional text control) are fed into a modified Diffusion Transformer (DiT) enhanced by a Causal Temporal Multimodal Adapter (CTM-Adapter). This DiT generates listener head and face motions directly in the pixel space, creating realistic videos. Panel (b) focuses on the CTM-Adapter, highlighting its role in seamlessly integrating multimodal speaker inputs in a temporally coherent way. This ensures the listener's responses are naturally timed and consistent with the speaker's cues. Finally, panel (c) explains how DiTaiListener handles long video sequences. It uses two models: DiTaiListener-Gen for generating short video segments and DiTaiListener-Edit for smoothly stitching these segments together. This approach produces natural, continuous listener behavior, unlike methods like prompt traveling or teacher forcing, which can result in unnatural transitions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.04010/x4.png", "caption": "Figure 3: Qualitative Comparison on ViCo test set. Our method generates high-quality, photorealistic facial images with diverse and natural social behaviors, including head movements and blinks, whereas baseline methods often produce less varied and expressive responses.", "description": "Figure 3 presents a qualitative comparison of listener video generation results on the ViCo test set. It showcases the superior performance of the proposed DiTaiListener model against existing baselines.  DiTaiListener generates high-quality, photorealistic facial images that exhibit a wide range of natural and nuanced social behaviors, including realistic head movements and blinks. In contrast, the baseline methods produce less diverse and expressive responses, lacking the same level of visual fidelity and realism.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04010/x5.png", "caption": "Figure 4: Listener generation from DiTaiListener on out-of-domain identities. Our method can integrate expressions from text conditions and synthesize diverse responses to the speakers.", "description": "This figure showcases the capability of the DiTaiListener model to generate diverse listener responses based on text-based emotional cues.  It demonstrates the model's ability to generate realistic and varied facial expressions, even when given identities not included in the original training dataset (out-of-domain identities). The figure visually presents a diverse array of listener reactions, highlighting the model's flexibility and controllability through text prompts. Each example demonstrates how different text prompts, describing emotions and behaviors, lead to different and nuanced listener responses, reflecting the successful integration of text conditions into the video generation process.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04010/extracted/6337681/figs/User_Study_Example.png", "caption": "Figure 5: Qualitative comparison of long video generation. Our method generates smoother videos with fewer transition artifacts compared to prompt traveling and teacher forcing methods.", "description": "Figure 5 displays a qualitative comparison of techniques for generating long videos.  The figure directly compares the smoothness and transition quality of videos produced using DiTaiListener-Edit (the authors' proposed method) versus those generated by two common alternative approaches: prompt traveling and teacher forcing.  The results visually demonstrate that DiTaiListener-Edit generates smoother, more natural-looking videos with significantly fewer abrupt transitions or artifacts between individual video segments compared to the other methods.", "section": "4.3. Ablation Analysis"}]