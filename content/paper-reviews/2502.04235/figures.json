[{"figure_path": "https://arxiv.org/html/2502.04235/x1.png", "caption": "Figure 1: Overview of MAGA framework. Our method expands the original corpus through a two-stage synthesis process. Each document is reformulated to 5 new documents, achieving 3.9\u00d7 token number expansion while maintaining diversity through massive (genre, audience) pairs.", "description": "The MAGA framework expands an original corpus using a two-stage synthesis process.  First, (genre, audience) pairs are generated for each document. Then, each document is reformulated into 5 new documents based on these pairs. This approach achieves a 3.9x increase in the number of tokens while maintaining diversity in the data.", "section": "3 MASSIVE GENRE-AUDIENCE REFORMULATION"}, {"figure_path": "https://arxiv.org/html/2502.04235/x2.png", "caption": "Figure 2: Implementation details. From a high-quality corpus, we sample a subset to serve as input for the LLM labeler and judger.\nThrough iterative filtering, we train and quantize SLM tool models for each stage to improve inference efficiency,\nwhich are used to generate the reformulated corpus.", "description": "This figure illustrates the MAGA framework's implementation details.  It begins with a high-quality corpus from which a subset is sampled to serve as input for an LLM that acts as both labeler and judger. This LLM evaluates the quality of data generated by tool models (SLMs). The SLMs are trained iteratively through a two-stage filtering process and then quantized to improve efficiency.  The output of this process is the reformulated corpus, created by the improved, quantized SLMs.", "section": "3 MASSIVE GENRE-AUDIENCE REFORMULATION"}, {"figure_path": "https://arxiv.org/html/2502.04235/x3.png", "caption": "Figure 3: The first and last two figures illustrate the training dynamics of EntireSet and SubSet data recipes, respectively.\nBenchmark details are provided in\u00a0Appendix\u00a0D.", "description": "This figure displays the training dynamics observed during experiments using different data recipes: EntireSet and Subset.  The EntireSet recipe involves training with a full dataset, while the Subset recipe uses a smaller subset.  The graphs likely show training curves, plotting metrics like validation loss or accuracy against the number of training tokens processed. The trends revealed in these curves illustrate how the different data strategies impact model performance across various model sizes. Detailed results and analysis of these benchmarks are provided in Appendix D of the paper. ", "section": "4.3 SCALING RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.04235/x4.png", "caption": "Figure 4: Benchmark results and validation losses in the MAGA-Mix setting. The sensitivity to data repetition varies across capability domains, with knowledge dimension showing greater resilience.", "description": "Figure 4 presents the results of experiments using the MAGA-Mix approach, which combines real and synthetic data for model training.  The figure shows training dynamics (scores and validation losses) across different model sizes (377M, 1.7B and 13B parameters) for multiple domains (knowledge, reasoning, math). The key observation is the varying sensitivity of different domains to data repetition. The knowledge domain exhibits greater resilience to data repetition than the reasoning and math domains, where the negative effects of repetition become more pronounced.  Different prompt engineering strategies (SLM-Base, SLM-Strict, and SLM-Relaxed) are compared. This comparison helps illustrate how the method\u2019s sensitivity to data repetition changes depending on the prompt engineering strategy used.", "section": "5 ABLATIONS AND DISCUSSIONS"}, {"figure_path": "https://arxiv.org/html/2502.04235/extracted/6183569/figures/ablation-pe-tsne.png", "caption": "Figure 5: t-SNE visualization results. Base (left) maintains a distribution that overlaps with but extends beyond the original data.\nStrict (middle) clusters also extend original data, but indicating limited diversity compared to left. Relaxed (right) shows significant distributional shift, explaining its poor performance.", "description": "This figure displays t-SNE visualizations illustrating the distribution of data embeddings generated under three different prompt engineering strategies: Base, Strict, and Relaxed.  The Base model shows a distribution that shares some overlap with the original data but also extends beyond it, indicating successful expansion while maintaining data quality. The Strict method produces a distribution that also expands beyond the original but exhibits less diversity than the Base model, suggesting more constrained data generation. The Relaxed approach, however, shows a significantly different distribution compared to both the Base model and the original data, hinting at a substantial shift in data characteristics that is correlated with poor model performance.", "section": "5.1 How Important Is to Construct a Proper Prompt Engineering Target?"}, {"figure_path": "https://arxiv.org/html/2502.04235/x5.png", "caption": "Figure 6: validation losses of experiments in Section\u00a04.2.", "description": "Figure 6 presents a detailed analysis of validation losses observed during the experiments described in Section 4.2.  The plots illustrate the trends of validation loss across different datasets (Fineweb-Edu, Cosmopedia-V2, Open-web-math, and Python-edu) for various model sizes (134M, 377M, and 1.7B parameters). This visualization helps to understand the impact of using the MAGA-Corpus in comparison to baseline models trained solely on the original dataset, providing insights into the effectiveness and potential limitations of MAGA's synthetic data expansion strategy.  The differing trends across datasets showcase dataset-specific characteristics and highlight the complexity of evaluating model performance using solely validation loss.", "section": "4.2 EFFECTIVENESS OF MAGACORPUS"}, {"figure_path": "https://arxiv.org/html/2502.04235/extracted/6183569/figures/ablation2_losses.png", "caption": "Figure 7: Losses pattern analysis. Subfigures 1 and 3 shows comparison between models trained on different data settings, with l\u2062o\u2062s\u2062sreal\ud835\udc59\ud835\udc5c\ud835\udc60subscript\ud835\udc60realloss_{\\text{real}}italic_l italic_o italic_s italic_s start_POSTSUBSCRIPT real end_POSTSUBSCRIPT on y-axis and l\u2062o\u2062s\u2062ssynt\ud835\udc59\ud835\udc5c\ud835\udc60subscript\ud835\udc60syntloss_{\\text{synt}}italic_l italic_o italic_s italic_s start_POSTSUBSCRIPT synt end_POSTSUBSCRIPT on x-axis.\nSubfigures 2 and 4 track the position where l\u2062o\u2062s\u2062ssynti\u2212l\u2062o\u2062s\u2062sreali\ud835\udc59\ud835\udc5c\ud835\udc60subscriptsuperscript\ud835\udc60\ud835\udc56synt\ud835\udc59\ud835\udc5c\ud835\udc60subscriptsuperscript\ud835\udc60\ud835\udc56realloss^{i}_{\\text{synt}}-loss^{i}_{\\text{real}}italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT synt end_POSTSUBSCRIPT - italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT real end_POSTSUBSCRIPT (l\u2062o\u2062s\u2062sdiffi\ud835\udc59\ud835\udc5c\ud835\udc60subscriptsuperscript\ud835\udc60\ud835\udc56diffloss^{i}_{\\text{diff}}italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT) first becomes significantly higher than the sequence\u2019s average difference (detailed definition in Appendix\u00a0D.3).", "description": "Figure 7 presents a detailed analysis of token-level loss patterns in models trained on real and synthetic data.  Subfigures 1 and 3 compare the loss for each token in the real and synthetic datasets. Subfigures 2 and 4 analyze where the difference between the synthetic and real token loss first surpasses the average difference across a sequence. This analysis helps to understand the model's behavior and potential issues, such as model collapse, during training on synthetic data.", "section": "5.2 Fine-Grained Pattern Analysis"}, {"figure_path": "https://arxiv.org/html/2502.04235/x6.png", "caption": "Figure 8: Detail evaluation results of EntireSet described in\u00a0Table\u00a04. MAGACorpus group demonstrats advantages over other groups across most evaluation sets, consistently across models of sizes.", "description": "Figure 8 presents a detailed performance comparison across multiple benchmark datasets.  It shows the results of using different training data configurations (EntireSet) as described in Table 4. Notably, it highlights the consistent superior performance of the MAGACorpus across various model sizes (134M, 377M, 1.7B, 7B, and 13B parameters), demonstrating its effectiveness as a training dataset in improving the overall performance of language models.", "section": "4.2 EFFECTIVENESS OF MAGACORPUS"}, {"figure_path": "https://arxiv.org/html/2502.04235/x7.png", "caption": "Figure 9: Detail evaluation results of Subset described in\u00a0Table\u00a04. As the model size increases, the performance gap between the upsampling group and MAGACorpus gradually widens in ARC, DROP, GSM8K, RACE, but with some variations observed in TriviaQA and WinoGrande.", "description": "Figure 9 presents a detailed performance comparison of different data strategies for training language models, focusing on a subset of data as described in Table 4 of the paper.  The graph shows how the performance gap between upsampling and MAGACorpus (the proposed method) changes as model size increases.  Specifically, it highlights that the advantage of MAGACorpus over upsampling becomes more pronounced for larger models on certain benchmarks (ARC, DROP, GSM8K, RACE), while the difference is less consistent or even reversed on others (TriviaQA, Winogrande). This suggests that MAGACorpus is a more effective scaling method for language models, particularly when dealing with larger sizes.", "section": "4.3 SCALING RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.04235/x8.png", "caption": "Figure 10: Corresponding benchmark results described in Section\u00a05.1.", "description": "Figure 10 presents a detailed comparison of benchmark results across different model variations, specifically focusing on the impact of prompt engineering strategies on model performance.  Each subplot likely represents a specific benchmark task (e.g., knowledge, reasoning, or math-related). The x-axis likely represents the amount of training data or another relevant metric, while the y-axis likely represents the model's performance on the task. By comparing the performance of different model variants (SLM-Base, SLM-Strict, SLM-Relaxed), the figure illustrates the trade-offs involved in balancing flexibility and information preservation during prompt engineering.  Differences in the trends across various tasks likely showcase the different sensitivities of model behavior to strict versus relaxed prompt strategies.", "section": "5.1 How important is to construct a proper prompt engineering target?"}, {"figure_path": "https://arxiv.org/html/2502.04235/extracted/6183569/figures/token-diff-examples.png", "caption": "Figure 11: Random examples sampling from where mean\u2062(l\u2062o\u2062s\u2062sdiffi)>0.5mean\ud835\udc59\ud835\udc5c\ud835\udc60subscriptsuperscript\ud835\udc60\ud835\udc56diff0.5\\text{mean}(loss^{i}_{\\text{diff}})>0.5mean ( italic_l italic_o italic_s italic_s start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT ) > 0.5, the synthetic-trained model fail to predict the tokens in later sequence positions.", "description": "This figure showcases examples where the mean difference between the loss of the synthetically trained model and the real model exceeds 0.5.  The analysis focuses on the token-level loss, revealing that the synthetic model struggles to predict tokens accurately, particularly in later sequence positions. This suggests a potential limitation of the synthetic training data or a model behavior difference when handling noisy data.", "section": "5.2 Multi-Perspective Validation Analysis"}, {"figure_path": "https://arxiv.org/html/2502.04235/x9.png", "caption": "Figure 12: Corresponding cases sampled from Fineweb-Edu, which align with the loss patterns shown in Figure\u00a011, with higher loss by synthetic-trained model highlighted in red.", "description": "Figure 12 presents example text excerpts from the Fineweb-Edu dataset that exhibit loss patterns similar to those illustrated in Figure 11.  Specifically, it highlights instances where the synthetically trained model demonstrates significantly higher loss compared to the model trained on real data. These examples are visually presented, with the higher-loss segments of text marked in red, thereby providing a visual correlation between the model's performance and the characteristics of specific text samples within the Fineweb-Edu dataset.", "section": "5.1 How important is to construct a proper prompt engineering target?"}, {"figure_path": "https://arxiv.org/html/2502.04235/x10.png", "caption": "Figure 13: Chinese corpus samples with higher loss by synthetic-trained model in red.", "description": "Figure 13 shows examples of Chinese text samples where a synthetically trained language model exhibits higher prediction loss compared to a model trained on real data.  The red highlighting indicates segments with the elevated loss. This visualization helps illustrate the challenges in achieving perfect parity between synthetic and real data in model training.  It suggests that even with well-designed synthetic data generation methods, there can still be disparities that lead to varying performance on different text samples. These discrepancies are further analyzed in Section 5.2 of the paper to explore potential causes such as differences in data distributions and model learning strategies.", "section": "5.2 Multi-Perspective Validation Analysis"}]