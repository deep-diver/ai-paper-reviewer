[{"figure_path": "https://arxiv.org/html/2412.15216/x2.png", "caption": "Figure 1: Unsupervised InstructPix2Pix. Our approach achieves more precise and coherent edits while preserving the structure of the scene. UIP2P outperforms state-of-the-art models in both real images (a. and b.) and synthetic images (c. and d.).", "description": "Figure 1 showcases the results of unsupervised instruction-based image editing using the proposed UIP2P method compared to the InstructPix2Pix approach.  The figure demonstrates the superior performance of UIP2P in terms of edit precision, coherence, and preservation of the original image structure across both real-world (a and b) and synthetic (c and d) image examples.  Each row presents an original image, the result of editing with InstructPix2Pix, and the result of editing with UIP2P, illustrating the improvement in accuracy and consistency. The different examples highlight UIP2P's ability to handle various types of edits, including adding elements to a scene, transforming objects, and making global changes to the image's appearance while maintaining overall scene integrity.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.15216/x3.png", "caption": "Figure 2: Examples of biases introduced by Prompt-to-Prompt in the InstructPix2Pix dataset. Each example shows an input image and its corresponding edited image (generated by Prompt-to-Prompt) along with the associated edit instruction. (a) Attribute-entangled edits: modifying the lady\u2019s dress also unintentionally changes the background. (b) Scene-entangled edits: transforming the cottage into a castle affects surrounding elements. (c) Global scene changes: converting the image to black and white alters the entire scene.", "description": "Figure 2 illustrates the limitations of the Prompt-to-Prompt method used to generate the InstructPix2Pix dataset. The figure showcases three instances where the method introduces biases into the editing process. (a) Attribute-entangled edits: Modifying a specific attribute (the lady's dress) unintentionally alters other aspects of the image (the background). This highlights the limitations of the model in isolating edits to specific regions. (b) Scene-entangled edits: Editing one part of a scene (transforming a cottage into a castle) unintentionally affects other elements within that scene. This shows the challenges of performing local edits without impacting other parts of the scene. (c) Global scene changes: A simple instruction (converting the image to black and white) results in a drastic global alteration, revealing a lack of control over the scope of edits.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15216/x4.png", "caption": "Figure 3: Overview of the UIP2P training framework. The model learns instruction-based image editing by utilizing forward and reverse instructions. Starting with an input image and a forward instruction, the model generates an edited image using IP2P. A reverse instruction is then applied to reconstruct the original image, enforcing Cycle Edit Consistency (CEC).", "description": "The figure illustrates the training process of the Unsupervised Instruction-based Image Editing via Cycle Edit Consistency (UIP2P) model.  The process begins with an input image and a forward instruction (e.g., \"add a mountain range\").  The UIP2P model, using InstructPix2Pix (IP2P) as a base, generates an edited image reflecting the forward instruction.  Then, a reverse instruction (e.g., \"remove the mountain range\") is applied to the edited image. The model's success is measured by its ability to reconstruct the original input image from this reverse process. This cycle of forward and reverse edits enforces Cycle Edit Consistency (CEC), a key element of the UIP2P training strategy.  The figure shows the flow of data and instructions through the model, highlighting the key components (CLIP embedding, forward/reverse attention maps, and the final loss functions) contributing to the learning process and achieving high-fidelity results.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2412.15216/x5.png", "caption": "Figure 4: Qualitative Examples. UIP2P performance is shown across various tasks and datasets, compared to InstructPix2Pix, MagicBrush, HIVE, MGIE, and SmartEdit. Our method demonstrates either comparable or superior results in terms of accurately applying the requested edits while preserving visual consistency.", "description": "Figure 4 presents a qualitative comparison of UIP2P against several state-of-the-art instruction-based image editing methods across various editing tasks.  It showcases the superior performance of UIP2P by visually demonstrating its ability to accurately execute diverse editing instructions, such as changing object colors, adding or removing elements, and modifying the overall scene, while simultaneously preserving visual consistency and image integrity.  Each row displays an example with the input image, the instruction, and the generated output from each method, highlighting the differences in accuracy, coherence, and visual quality.", "section": "5. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2412.15216/x6.png", "caption": "(a) Zero-shot Quantitative Comparison on MagicBrush [50] test set. Instruction-based editing methods that are not fine-tuned on MagicBrush are presented. In the multi-turn setting, target images are iteratively edited from the initial images.", "description": "This figure presents a quantitative comparison of different instruction-based image editing methods on the MagicBrush test set.  The methods compared were not specifically fine-tuned on the MagicBrush dataset.  The single-turn results show the performance of each method in one editing pass.  The multi-turn results illustrate performance when the same method is repeatedly applied to iteratively refine the image.  The comparison uses metrics to assess the quality of the editing and alignment with the given instructions. These metrics help to evaluate how well each method performs zero-shot edits, meaning without being trained specifically on the MagicBrush dataset.", "section": "5.3.3 MagicBrush Test Dataset"}, {"figure_path": "https://arxiv.org/html/2412.15216/x7.png", "caption": "(b) Evaluation on the IP2P test dataset. UIP2P outperforms IP2P in both CLIP image similarity and CLIP text-image similarity metrics, demonstrating better visual fidelity and instruction alignment.", "description": "This figure shows a quantitative comparison of UIP2P and InstructPix2Pix (IP2P) on the IP2P test dataset.  Two key metrics are used to evaluate the models: CLIP image similarity, measuring how visually similar the edited image is to the original, and CLIP text-image similarity, assessing how well the edits align with the given textual instructions.  The results demonstrate that UIP2P surpasses IP2P in both metrics, indicating improved visual fidelity and better adherence to the instructions during image editing.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15216/x8.png", "caption": "Figure 5: Evaluation on MagicBrush and IP2P test datasets.", "description": "Figure 5 presents a quantitative comparison of the UIP2P model against several baselines on two distinct test datasets: MagicBrush and InstructPix2Pix (IP2P).  The left panel displays numerical results, showing the performance of different methods across multiple metrics (L1, L2, CLIP image similarity, DINO similarity, and CLIP text-image similarity).  These metrics assess various aspects of image editing quality including pixel-wise difference, semantic alignment, and overall fidelity.  The right panel shows a graph specifically illustrating the performance on the IP2P test set, highlighting the improvements achieved by UIP2P in both CLIP image similarity and CLIP text-image direction similarity. This visualization helps to understand the model's ability to preserve image details while accurately reflecting the semantic changes specified in the textual instruction.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15216/extracted/6082894/figures/user_study_setting.png", "caption": "Figure 6: Ablation study on the number of steps. UIP2P achieves high fidelity edits on the input image with fewer steps, whereas IP2P struggles to maintain quality.", "description": "This ablation study compares UIP2P and InstructPix2Pix (IP2P) by varying the number of diffusion steps used during image editing.  The results show that UIP2P maintains high-fidelity edits even with a significantly smaller number of steps (e.g., 5 steps), while IP2P requires considerably more steps (e.g., 50 steps) to achieve comparable results.  This demonstrates UIP2P's superior efficiency and ability to produce high-quality edits with reduced computational cost.", "section": "5.4. Ablation Study"}]