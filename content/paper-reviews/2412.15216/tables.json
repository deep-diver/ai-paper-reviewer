[{"content": "| Input Caption | Edit Instruction | Edited Caption | Reverse Instruction |\n|---|---|---|---| \n| <div style=\"text-align:center;\"> <img src=\"https://arxiv.org/html/2412.15216/IP2P.png\" alt=\"IP2P\" width=\"20\" height=\"20\"> </div> | A man wearing a denim jacket | make the jacket a rain coat | A man wearing a rain coat | make the coat a denim jacket |\n| A sofa in the living room | add pillows | A sofa in the living room with pillows | remove the pillows |\n| \u2026 | \u2026 | \u2026 | \u2026 |\n| <div style=\"text-align:center;\"> <img src=\"https://arxiv.org/html/2412.15216/CCXM.png\" alt=\"CCXM\" width=\"28\" height=\"28\"> </div> | Person on the cover of a magazine | make the person a cat | Cat on the cover of the magazine | make the cat a person |\n| A tourist rests against a concrete wall | give him a backpack | A tourist with a backpack rests against a concrete wall | remove his backpack |\n| \u2026 | \u2026 | \u2026 | \u2026 |", "caption": "Table 1: Reverse Instruction Generation. Our method generates reverse instructions for the IP2P dataset, eliminating the need for manually edited images. Additionally, edit instructions, edited captions, and reverse instructions are generated for CC3M and CC12M datasets\u2014denoted as CCXM. The texts are generated by LLMs such as GEMINI Pro, and GEMMA2.", "description": "This table presents examples of input captions, edit instructions, corresponding edited captions, and generated reverse instructions.  The data demonstrates the capability of Large Language Models (LLMs) to generate coherent reverse instructions for image editing tasks. This eliminates the need for manually edited images in the training data, making the process more efficient and scalable.  The examples are drawn from both the InstructPix2Pix (IP2P) dataset and the combined CC3M and CC12M datasets (referred to as CCXM). The LLMs used for generating text are GEMINI Pro and GEMMA2.", "section": "5.1 Experimental Setup"}, {"content": "| Models | (Q1) | (Q2) |\n|---|---|---|\n| IP2P | 8% | 12% |\n| MagicBrush | 17% | 18% |\n| HIVE | 14% | 13% |\n| MGIE | 20% | 19% |\n| SmartEdit | 19% | 18% |\n| UIP2P | 22% | 20% |", "caption": "Table 2: User Study.", "description": "This table presents the results of a user study conducted to evaluate the performance of six different image editing methods.  Participants were asked to select their top two preferred methods based on two criteria: (Q1) how well the edit matched the instruction and intended location, and (Q2) how accurately the edit was applied to the intended area. The table shows the percentage of times each method was selected as a top performer for each criterion. This allows for a comparison of the methods' accuracy and effectiveness across various image editing tasks.", "section": "5.3. Quantitative Results"}, {"content": "| Settings | Methods | L1 \u2193 | L2 \u2193 | CLIP-I \u2191 | DINO \u2191 | CLIP-T \u2191 |\n|---|---|---|---|---|---|---|\n| **Single-turn** | HIVE [51] | 0.1092 | 0.0341 | 0.8519 | 0.7500 | 0.2752 |\n|  | InstructPix2Pix [3] | 0.1122 | 0.0371 | 0.8524 | 0.7428 | 0.2764 |\n|  | UIP2P w/ IP2P Dataset | 0.0722 | 0.0193 | 0.9243 | 0.8876 | 0.2944 |\n|  | UIP2P w/ CC3M Dataset | 0.0680 | 0.0183 | 0.9262 | 0.8924 | 0.2966 |\n|  | UIP2P w/ CC12M Dataset | 0.0619 | 0.0174 | 0.9318 | 0.9039 | 0.2964 |\n| **Multi-turn** | HIVE [51] | 0.1521 | 0.0557 | 0.8004 | 0.6463 | 0.2673 |\n|  | InstructPix2Pix [3] | 0.1584 | 0.0598 | 0.7924 | 0.6177 | 0.2726 |\n|  | UIP2P w/ IP2P Dataset | 0.1104 | 0.0358 | 0.8779 | 0.8041 | 0.2892 |\n|  | UIP2P w/ CC3M Dataset | 0.1040 | 0.0337 | 0.8816 | 0.8130 | 0.2909 |\n|  | UIP2P w/ CC12M Dataset | 0.0976 | 0.0323 | 0.8857 | 0.8235 | 0.2901 |", "caption": "Table 3: Ablation study on loss functions. Adding additional loss functions to the base loss functions enhances performance on the MagicBrush benchmark.", "description": "This ablation study investigates the impact of individual loss functions on the overall performance of the model on the MagicBrush benchmark.  It starts with a base model using only two loss functions and then progressively adds other loss functions (Lsim and Lattn) to evaluate their contributions to improving the model's performance.  The results, measured by L1, L2, CLIP-I, DINO, and CLIP-T metrics, show how each added loss function enhances the performance, indicating the effectiveness of the proposed multi-loss function approach.", "section": "5.4 Ablation Study"}, {"content": "| Loss | L1 \u2193 | L2 \u2193 | CLIP-I \u2191 | DINO \u2191 | CLIP-T \u2191 |\n|---|---|---|---|---|---| \n| Base | 0.117 | 0.032 | 0.878 | 0.806 | **0.309** |\n| + \u2112sim | 0.089 | 0.024 | 0.906 | 0.872 | 0.301 |\n| + \u2112attn | **0.062** | **0.017** | **0.932** | **0.904** | 0.296 |", "caption": "Table 4: Examples of Four Possible Edits for Two Different Input Captions. Our dataset generation process showcases the flexibility of the reverse instruction dataset by demonstrating multiple transformations for the same caption.", "description": "This table demonstrates the diversity and flexibility of the reverse instruction dataset used in the UIP2P model.  It shows four different edits applied to two example input captions, highlighting the model's capacity to handle diverse and complex image transformations. Each example shows an input caption, an edit instruction, the resulting edited caption, and the instruction to reverse the edit, illustrating the cycle edit consistency (CEC) framework.", "section": "7.7. More Examples from Reverse Instructions Dataset"}, {"content": "| Input Caption | Edit Instruction | Edited Caption | Reverse Instruction |\n|---|---|---|---| \n| A dog sitting on a couch | change the dog\u2019s color to brown | A brown dog sitting on a couch | change the dog\u2019s color back to white |\n|  | add a ball next to the dog | A dog sitting on a couch with a ball | remove the ball |\n|  | remove the dog | An empty couch | add the dog back |\n|  | move the dog to the floor | A dog sitting on the floor | move the dog back to the couch |\n| A car parked on the street | change the car color to red | A red car parked on the street | change the car color back to black |\n|  | add a bicycle next to the car | A car parked on the street with a bicycle | remove the bicycle |\n|  | remove the car | An empty street | add the car back |\n|  | move the car to the garage | A car parked in the garage | move the car back to the street |", "caption": "Table 5: Quantitative comparison on MagicBrush [50] test set. In the multi-turn setting, target images are iteratively edited from the initial source images.\nBest results are in bold.", "description": "Table 5 presents a quantitative comparison of different image editing methods on the MagicBrush test dataset.  It evaluates the performance of these methods across various metrics, including L1, L2, CLIP-I, DINO, and CLIP-T, providing a comprehensive assessment of their accuracy, visual fidelity, and semantic alignment.  The table is divided into single-turn and multi-turn settings. In single-turn, the edits are performed in a single step. In the multi-turn setting, edits are applied iteratively, refining the image across multiple steps. The best results in each metric are highlighted in bold, offering a clear comparison of model performance across different scenarios.", "section": "5.3.3 MagicBrush Test Dataset"}]