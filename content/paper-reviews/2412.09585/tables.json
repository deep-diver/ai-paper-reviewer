[{"content": "| Method | Encoder | Count<sup>2D</sup> | Depth<sup>3D</sup> | Relation<sup>2D</sup> | Distance<sup>3D</sup> | MMStar | RWQA | OK-VQA | Avg |\n|---|---|---|---|---|---|---|---|---|---| \n| *Phi3-4k-mini* |  |  |  |  |  |  |  |  |  |\n| LLaVA-1.5 | CLIP-ViT-L | **52.4** | 67.2 | 75.2 | 56.3 | **36.5** | 57.1 | **56.7** | 57.3 |\n| **OLA-VLM (ours)** | CLIP-ViT-L | **52.4** | **68.7** | **76.0** | **56.7** | 36.0 | **58.0** | 56.4 | **57.7** |\n| LLaVA-1.5 | CLIP-ConvNeXT-XXL | **51.8** | 70.8 | 74.0 | 55.3 | 36.4 | 58.0 | 55.9 | 57.4 |\n| **OLA-VLM (ours)** | CLIP-ConvNeXT-XXL | 49.4 | **72.5** | **77.2** | **60.3** | **38.4** | **58.4** | **56.5** | **58.9** |\n| *Llama3-8b* |  |  |  |  |  |  |  |  |  |\n| LLaVA-1.5 | CLIP-ViT-L | 50.4 | 73.3 | 64.9 | 48.7 | 38.8 | 57.8 | 56.9 | 55.1 |\n| LLaVA-1.5 (feat concat.) | CLIP-ViT-L + \ud835\udc04<sup>depth</sup> + \ud835\udc04<sup>seg</sup> + \ud835\udc04<sup>gen</sup> | 45.3 | 75.5 | **70.9** | **54.3** | 36.1 | 57.5 | **58.3** | 56.8 |\n| LLaVA-1.5 (token concat.) | CLIP-ViT-L + \ud835\udc04<sup>depth</sup> + \ud835\udc04<sup>seg</sup> + \ud835\udc04<sup>gen</sup> | 45.9 | **75.7** | 68.9 | 52.7 | 37.8 | 56.5 | **59.3** | 56.7 |\n| **OLA-VLM (ours)** | CLIP-ViT-L | **51.3** | 74.2 | 69.4 | **54.3** | **39.5** | **57.9** | 56.6 | **57.6** |\n| LLaVA-1.5 | CLIP-ConvNeXT-XXL | 54.1 | 62.8 | **69.5** | 49.8 | 37.4 | **57.5** | 56.3 | 55.3 |\n| **OLA-VLM (ours)** | CLIP-ConvNeXT-XXL | **57.4** | **71.5** | 66.8 | **52.8** | **38.5** | 55.0 | **59.0** | **57.3** |", "caption": "Table 1: Comparisons to Single and Multi-Encoder Baselines. We present results across different base encoders and decoder LLMs. Our OLA-VLM outperforms the single encoder and multi encoder LLaVA-1.5\u00a0[41] by up to 2.5% and 0.9% on average across various benchmarks, respectively. The best numbers are set in bold for every base-encoder and decoder LLM combination.", "description": "Table 1 compares the performance of OLA-VLM against single and multi-encoder baseline models on various vision and language benchmarks.  It shows the performance across different combinations of base vision encoders (CLIP-ViT-L, CLIP-ConvNeXT-XXL) and decoder LLMs (LLaMA-3-8b).  The results demonstrate that OLA-VLM consistently outperforms both single-encoder and multi-encoder baselines, achieving an average improvement of up to 2.5% over the single-encoder models and 0.9% over the multi-encoder models across the benchmarks. The best-performing models for each encoder/decoder combination are highlighted in bold.", "section": "5. Experiments"}, {"content": "| Method | LLM | Count<sup>2D</sup> | Depth<sup>3D</sup> | Relation<sup>2D</sup> | Distance<sup>3D</sup> | Avg |\n|---|---|---|---|---|---|---|\n| LLaVA-1.5 | Phi3-4k-mini | 49.7 | 70.0 | 72.6 | 58.7 | 61.8 |\n| OLA-VLM (ours) | Phi3-4k-mini | 53.7 | 72.0 | 73.1 | 58.5 | 63.4 |\n| LLaVA-1.5 | Llama3-8b | 56.3 | 76.8 | 73.1 | 50.3 | 63.3 |\n| OLA-VLM (ours) | Llama3-8b | 60.0 | 75.0 | 70.8 | 55.2 | 64.6 |", "caption": "Table 2: Scalability over more data with VPT. Our OLA-VLM outperforms LLaVA-1.5 on average across different CV-Bench tasks. We use CLIP-ConvNeXT-XXL\u00a0[13] as the base encoder.", "description": "Table 2 demonstrates the impact of additional visual data on the performance of OLA-VLM and LLaVA-1.5.  Both models underwent pre-training (PT) and instruction fine-tuning (IFT) but OLA-VLM also included an additional visual pre-training (VPT) stage using the ALLaVA-Caption dataset.  The table compares the performance of both models on various subtasks within the CV-Bench benchmark using CLIP-ConvNeXT-XXL as the base vision encoder.  The results show that OLA-VLM consistently outperforms LLaVA-1.5 across all tasks, suggesting that the embedding distillation technique employed in OLA-VLM is particularly effective when combined with larger visual datasets.", "section": "5. Experiments"}, {"content": "| \ud835\udd3b | \ud835\udd4a | \ud835\udd3e | CV-Bench<sup>2D</sup> | CV-Bench<sup>3D</sup> | MMStar | Avg |\n|---|---|---|---|---|---|---|\n|{18}|{18}|{20}|**58.9**|60.9|37.9|52.6|\n|{20}|{18}|{20}|57.6|60.8|38.8|52.4|\n|{8;14}|{10;16}|{12;18}|56.5|56.4|38.8|50.6|\n|{8;20}|{10;18}|{12;20}|58.6|**64.2**|39.5|**54.1**|\n|{18;20}|{18;20}|{16;20}|55.8|59.5|**40.8**|52.0|\n|{16;18;20}|{16;18;20}|{16;18;20}|56.8|61.3|37.0|51.7|", "caption": "Table 3: Ablations on Layer sets for embedding losses. Setting \ud835\udd3b\ud835\udd3b\\mathbb{D}blackboard_D={8,20}820\\{8,20\\}{ 8 , 20 }, \ud835\udd4a\ud835\udd4a\\mathbb{S}blackboard_S={10,18}1018\\{10,18\\}{ 10 , 18 }, and \ud835\udd3e\ud835\udd3e\\mathbb{G}blackboard_G={12,20}1220\\{12,20\\}{ 12 , 20 } performs the best.", "description": "This table presents ablation studies on different layer sets used for computing embedding losses in the OLA-VLM model.  The goal is to determine the optimal layers within the Language Model (LLM) to incorporate visual information from target encoders.  The results show the performance on various metrics across different sets of layers, with the combination of layers 8 and 20 for depth features (D), layers 10 and 18 for segmentation features (S), and layers 12 and 20 for generation features (G) yielding the best overall performance.", "section": "5.3 Ablations"}, {"content": "| PT | IFT | CV-Bench<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">2D</span></sup> | CV-Bench<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_medium ltx_font_italic\">3D</span></sup> | MMStar | RWQA | Avg |\n|---|---|---|---|---|---|---|\n| single-encoder |  | 56.0 | 61.0 | 38.8 | 57.8 | 53.4 |\n|  |  | 57.7 | 62.9 | 38.8 | 57.5 | 54.2 |\n| \u2713 |  | 58.6 | 64.2 | 39.5 | 57.9 | 55.1 |\n| \u2713 | \u2713 | 59.1 | 58.3 | 38.3 | 56.2 | 53.0 |", "caption": "Table 4: Ablations on training stages for embedding losses. Using the embedding losses only during PT is optimal.", "description": "This table presents the results of ablation experiments performed to determine the optimal training stages for applying embedding losses during the pre-training of the OLA-VLM model.  The experiments compare the performance of the model when embedding losses are used only during the pre-training (PT) stage, during both pre-training (PT) and instruction fine-tuning (IFT) stages, and during only the IFT stage. The results show that utilizing the embedding losses exclusively during the pre-training phase yields the best performance across various benchmarks, indicating that embedding loss optimization in the early training stages is crucial for effective visual representation learning.", "section": "4. Embedding Visual Information into LLM"}, {"content": "| **N<sup>seek</sup>** | **CV-Bench<sup>2D</sup>** | **CV-Bench<sup>3D</sup>** | **MMStar** | **RWQA** | **Avg** |\n|---|---|---|---|---|---| \n| single-encoder | 56.0 | 61.0 | 38.8 | 57.8 | 53.4 |\n| 0 | 56.1 | 62.0 | **40.1** | 56.3 | 53.6 |\n| 4 | **60.3** | 57.9 | 38.1 | 56.6 | 53.2 |\n| 8 | 58.6 | **64.2** | 39.5 | **57.9** | **55.1** |\n| 12 | 58.7 | 60.7 | 37.9 | 57.5 | 53.7 |\n| 16 | 56.6 | 63.6 | 37.1 | 54.5 | 52.9 |\n| 24 | 55.7 | 60.0 | 39.3 | 57.4 | 53.1 |", "caption": "Table 5: Ablations on \ud835\udc0dseeksuperscript\ud835\udc0dseek\\mathbf{N}^{\\text{seek}}bold_N start_POSTSUPERSCRIPT seek end_POSTSUPERSCRIPT. Setting number of special tokens (\u27e8t\u27e9delimited-\u27e8\u27e9\ud835\udc61\\langle t\\rangle\u27e8 italic_t \u27e9) to 8 for each task performs best. The setting with no special tokens (\ud835\udc0dseek=0superscript\ud835\udc0dseek0\\mathbf{N}^{\\text{seek}}=0bold_N start_POSTSUPERSCRIPT seek end_POSTSUPERSCRIPT = 0) also outperforms the single encoder baseline.", "description": "This table presents ablation studies on the impact of varying the number of special tokens used in the OLA-VLM model.  The special tokens, denoted as \u27e8t\u27e9, carry task-specific information (depth, segmentation, generation). The table shows the performance on various CV-Bench tasks (Count2D, Depth3D, MMStar, RWQA) for different numbers of special tokens (Nseek).  Results demonstrate that using 8 special tokens per task yields the best performance, even surpassing the single-encoder baseline's performance when no special tokens are used (Nseek=0).", "section": "5.3 Ablations"}, {"content": "| <math alttext=\"\\langle t\\rangle\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T6.1.1.1.1.m1.1\"><semantics id=\"S5.T6.1.1.1.1.m1.1a\"><mrow id=\"S5.T6.1.1.1.1.m1.1.2.2\" xref=\"S5.T6.1.1.1.1.m1.1.2.1.cmml\"><mo id=\"S5.T6.1.1.1.1.m1.1.2.2.1\" stretchy=\"false\" xref=\"S5.T6.1.1.1.1.m1.1.2.1.1.cmml\">\u27e8</mo><mi id=\"S5.T6.1.1.1.1.m1.1.1\" xref=\"S5.T6.1.1.1.1.m1.1.1.cmml\">t</mi><mo id=\"S5.T6.1.1.1.1.m1.1.2.2.2\" stretchy=\"false\" xref=\"S5.T6.1.1.1.1.m1.1.2.1.1.cmml\">\u27e9</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.1.1.1.1.m1.1b\"><apply id=\"S5.T6.1.1.1.1.m1.1.2.1.cmml\" xref=\"S5.T6.1.1.1.1.m1.1.2.2\"><csymbol cd=\"latexml\" id=\"S5.T6.1.1.1.1.m1.1.2.1.1.cmml\" xref=\"S5.T6.1.1.1.1.m1.1.2.2.1\">delimited-\u27e8\u27e9</csymbol><ci id=\"S5.T6.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.1.1.1.1.m1.1.1\">\ud835\udc61</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.1.1.1.1.m1.1c\">\\langle t\\rangle</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T6.1.1.1.1.m1.1d\">\u27e8 italic_t \u27e9</annotation></semantics></math> during IFT | **CV-Bench<sup>2D</sup>** | **CV-Bench<sup>3D</sup>** | **MMStar** | **RWQA** | **Avg** |\n|---|---|---|---|---|---| \n| frozen | **58.6** | **64.2** | **39.5** | **57.9** | **55.1** | \n| learnable | 56.9 | 56.1 | 39.0 | 57.3 | 52.3 |", "caption": "Table 6: Ablation on the nature of special tokens during IFT. Keeping \u27e8t\u27e9delimited-\u27e8\u27e9\ud835\udc61\\langle t\\rangle\u27e8 italic_t \u27e9 frozen during IFT aids in keeping their task-specific nature intact, resulting in better performance.", "description": "This table presents an ablation study on the impact of training the special tokens during instruction finetuning (IFT).  The study compares the performance of the model when the special tokens are frozen versus when they are learnable during the IFT stage. The goal is to assess whether keeping the tokens frozen helps maintain their task-specific information, leading to better performance, or if allowing them to be updated during IFT provides additional benefits. The results are shown in terms of performance on various downstream vision tasks from the CV-Bench benchmark.", "section": "5.3 Ablations"}, {"content": "| Method | PT | IFT | CV-Bench<sup>2D</sup> | CV-Bench<sup>3D</sup> | MMStar | OK-VQA | Avg |\n|---|---|---|---|---|---|---|---| \n| LLaVA-1.5 | LLaVA-558K | LLaVA-665k | 60.0 | 56.3 | 37.4 | 56.0 | 52.4 |\n| LLaVA-1.5 | LLaVA-558K + ALLaVA-Caption-663K | LLaVA-665k | 56.8 | 60.8 | 37.1 | 57.5 | 53.1 |\n| **OLA-VLM** | LLaVA-558K | LLaVA-665k | **60.8** | **62.2** | **38.5** | **59.0** | **55.1** |", "caption": "Table 7: Using additional data during PT v/s embedding optimization. Our OLA-VLM demonstrates superior performance than the LLaVA-1.5 model trained on with additional ALLaVA-Caption\u00a0[6] data during the PT stage, underscoring the effectiveness of our approach.", "description": "This table compares the performance of the OLA-VLM model and the baseline LLaVA-1.5 model.  Both models are trained with the standard LLaVA-665K dataset for Instruction Fine-Tuning (IFT). However, the key difference lies in the pre-training (PT) phase.  While LLaVA-1.5 utilizes only the LLaVA-558K dataset for PT, OLA-VLM adds the ALLaVA-Caption-663K dataset for an additional visual pre-training (VPT) stage. This table demonstrates that even with the additional training data used by LLaVA-1.5 during PT, OLA-VLM still achieves superior results across multiple benchmarks (CV-Bench2D, CV-Bench3D, MMStar, OK-VQA). The improved performance of OLA-VLM highlights the effectiveness of its embedding optimization technique over simply increasing training data.", "section": "5.2 Main Results"}, {"content": "| Probed Model | FID (<span>\u2193</span>) | DA-2K % Acc. (<span>\u2191</span>) | % mIoU (<span>\u2191</span>) |\n|---|---|---|---|\n| LLaVA-1.5 | 23.1 | 66.4 | 39.3 |\n| **OLA-VLM (ours)** | **22.4** | **77.8** | **45.4** |\n| <span style=\"color:#808080;\">Target Encoder</span> | <span style=\"color:#808080;\">18.1</span> | <span style=\"color:#808080;\">97.3</span> | <span style=\"color:#808080;\">64.5</span> |", "caption": "Table 8: Quantitative Evaluation on target probing task. Probes trained for our OLA-VLM perform significantly better as compared to the probes trained on baseline LLaVA-1.5\u00a0[41].", "description": "This table presents a quantitative comparison of the performance of probes trained on OLA-VLM and the baseline LLaVA-1.5 model for three downstream visual tasks: image generation (FID score), depth estimation (accuracy), and image segmentation (mIoU). Lower FID scores indicate better image generation quality.  Higher accuracy and mIoU values represent improved performance in depth estimation and segmentation, respectively. The results show that probes trained on OLA-VLM achieved significantly better scores across all three tasks, demonstrating the effectiveness of the proposed OLA-VLM approach in improving the quality of visual representations within the language model.", "section": "5.4 Evaluating Probes on Downstream Target Tasks"}, {"content": "| key input to emb. predictor | CV-Bench<sup>2D</sup> | CV-Bench<sup>3D</sup> | MMStar | RWQA | Avg |\n|---|---|---|---|---|---| \n| \u27e8img\u27e9 | \u27e8t\u27e9 | 53.0 | 54.6 | 38.4 | 56.7 | 50.7 |\n| \u27e8sys\u27e9 | \u27e8img\u27e9 | \u27e8t\u27e9 | 58.7 | 63.0 | 38.8 | 57.4 | 54.5 |\n| \u27e8sys\u27e9 | \u27e8img\u27e9 | \u27e8t\u27e9 | \u27e8txt\u27e9 | 58.6 | 64.2 | 39.5 | 57.9 | 55.1 |", "caption": "Table I: Key input to the Embedding Predictor.. Feeding the tokens corresponding to the system prompt, image embeddings, corresponding special tokens, and the text query is optimal.", "description": "This table investigates the optimal input tokens for the embedding predictor in the OLA-VLM model.  The embedding predictor uses a set of learnable queries (Qtask) and a token sequence as input to generate predictions. The table explores different combinations of tokens to determine which input provides the best performance. The results show that including tokens corresponding to the system prompt, image embeddings, special tokens, and the text query yields optimal results.", "section": "4. Embedding Visual Information into LLM"}, {"content": "| mode | CV-Bench<sup>2D</sup> | CV-Bench<sup>3D</sup> | MMStar | Avg |\n|---|---|---|---|---|\n| LLaVA-1.5 | 56.0 | 61.0 | 38.8 | 51.9 |\n| depth | 58.6 | 63.5 | 38.8 | 53.6 |\n| seg | 56.2 | 57.6 | 38.2 | 50.7 |\n| gen | 56.2 | 65.8 | 39.3 | 53.8 |\n| depth + seg | 58.6 | 61.8 | 38.6 | 53.0 |\n| depth + gen | 53.6 | 61.8 | 38.8 | 51.4 |\n| seg + gen | 54.2 | 60.2 | 39.3 | 51.2 |\n| depth + seg + gen | 58.6 | 64.2 | 39.5 | 54.1 |", "caption": "Table II: Embedding Optimization Modes. Using the depth, seg, and gen embedding losses at the same time is optimal.", "description": "This table explores different combinations of embedding losses (depth, segmentation, and generation) used during the pre-training stage of the model.  It investigates the impact of using these losses individually and in combination on the overall performance of the model. The results indicate that the best performance is achieved when all three types of embedding losses are used simultaneously, suggesting a synergistic effect among them in improving the model's ability to learn and utilize visual information.", "section": "4. Embedding Visual Information into LLM"}, {"content": "| \n | **order** | **Count<sup>2D</sup>** | **Depth<sup>3D</sup>** | **Relation<sup>2D</sup>** | **Distance<sup>3D</sup>** | **Overall** |\n|---|---|---|---|---|---|---|\n| LLaVA-1.5 | 50.4 | 73.3 | 64.9 | 48.7 | 58.5 |\n| \u27e8d\u27e9 | \u27e8s\u27e9 | \u27e8g\u27e9 | 49.4 | 68.7 | 69.2 | 56.2 | 59.9 |\n| \u27e8d\u27e9 | \u27e8g\u27e9 | \u27e8s\u27e9 | 51.6 | 72.8 | 70.3 | 54.5 | 61.4 |\n| \u27e8s\u27e9 | \u27e8d\u27e9 | \u27e8g\u27e9 | 48.7 | 71.3 | 65.2 | 52.5 | 58.5 |\n| \u27e8s\u27e9 | \u27e8g\u27e9 | \u27e8d\u27e9 | 46.7 | 71.3 | 71.2 | 50.8 | 58.9 |\n| \u27e8g\u27e9 | \u27e8d\u27e9 | \u27e8s\u27e9 | 51.3 | 74.2 | 69.4 | 54.3 | 61.4 |\n| \u27e8g\u27e9 | \u27e8s\u27e9 | \u27e8d\u27e9 | 50.9 | 68.8 | 70.0 | 50.5 | 59.2 |", "caption": "Table III: Order of different special tokens in the input sequence to the LLM. Appending the gen, depth, and seg tokens (in that order) in the LLM\u2019s input sequence after the image tokens is the optimal setup.", "description": "This table investigates the optimal order for appending special tokens representing image generation (gen), depth, and segmentation (seg) information to the input sequence of a large language model (LLM).  The experiment modifies the order of these tokens, placing them after the image tokens. The results show that appending the tokens in the order gen, depth, seg yields the best performance.", "section": "5.3 Ablations"}, {"content": "| $\n\u03bb_{depth}$ | $\n\u03bb_{seg}$ | $\n\u03bb_{gen}$ | **CV-Bench<sup>2D</sup>** | **CV-Bench<sup>3D</sup>** | **MMStar** | **Avg** |\n|---|---|---|---|---|---|---|\n| LLaVA-1.5 | 56.0 | 61.0 | 38.8 | 51.9 |\n| 0.10 | 0.10 | 0.10 | **60.5** | 61.3 | 38.3 | 53.4 |\n| 0.25 | 0.25 | 0.25 | 56.3 | 59.4 | 37.1 | 50.9 |\n| 0.50 | 0.50 | 0.50 | 58.6 | **64.2** | **39.5** | **54.1** |\n| 0.75 | 0.75 | 0.75 | 57.9 | 59.4 | 37.6 | 51.6 |\n| 1.00 | 1.00 | 1.00 | 55.8 | 61.7 | 38.1 | 51.9 |", "caption": "Table IV: Embedding Loss weights during PT. Setting each embedding loss\u2019 weight to 0.5 is optimal.", "description": "This table presents the results of an ablation study on the weights assigned to each of the three embedding losses (depth, segmentation, and generation) during the pre-training (PT) phase of the OLA-VLM model. The study varied the weights (0.1, 0.25, 0.5, 0.75, 1.0) for each loss, and evaluated the performance on various benchmarks (CV-Bench2D, CV-Bench3D, and MMStar). The results indicate that setting the weight of each embedding loss to 0.5 yields optimal performance.", "section": "4. Embedding Visual Information into LLM"}, {"content": "| $\nmathcal{L}_{sL1}$ | $\nmathcal{L}_{contrastive}$ | **CV-Bench<sup>2D</sup>** | **CV-Bench<sup>3D</sup>** | **MMStar** | **Avg** |\n|---|---|---|---|---|---| \n| \u2713 |  | 56.8 | 62.3 | 38.3 | 52.5 |\n| \u2713 | \u2713 | **58.6** | **64.2** | **39.5** | **54.1** |", "caption": "Table V: Ablations on components of embedding losses. Using both smooth-L1-loss and contrastive loss to compute the final embedding loss is optimal.", "description": "This table presents ablation studies on the components of the embedding loss function used in the OLA-VLM model.  The embedding loss combines smooth L1 loss and contrastive loss to optimize the model's visual representation learning.  The experiment varies the usage of these loss components (smooth L1 only, contrastive only, both) to determine the optimal combination for the best performance. The results show that using both smooth L1 loss and contrastive loss together leads to the best performance.", "section": "4.2 Predictive Embedding Optimization"}]