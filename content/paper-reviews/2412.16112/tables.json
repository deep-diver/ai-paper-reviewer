[{"content": "| Method | Locality | Formulation | High-Rank Attention Maps | Feature Integrity |\n|---|---|---|---|---|\n| Linear Attention [12, 38, 65, 30] | Yes | No | No | Yes |\n| Sigmoid Attention [48] | Yes | No | Yes | Yes |\n| PixArt-Sigma [6] | Yes | Yes | Yes | No |\n| Agent Attention [20] | Maybe | Yes | Yes | No |\n| Strided Attention [7] | No | Yes | Yes | Yes |\n| Swin Transformer [39] | Yes | Yes | No | Yes |\n| Neighborhood Attention [21] | Yes | Yes | Yes | Yes |", "caption": "Table 1: Summary of existing efficient attention mechanisms based on the four factors crucial for linearizing DiTs.", "description": "This table categorizes several efficient attention mechanisms based on four key aspects that are essential for successfully linearizing pre-trained Diffusion Transformers (DiTs). These four crucial factors are locality, formulation consistency, high-rank attention maps, and feature integrity.  Each method is evaluated based on whether it satisfies each of these four criteria (Yes/No/Maybe). This helps to understand which existing methods are suitable for linearizing pre-trained DiTs and highlights the specific design choices that are needed.", "section": "2. Efficient Attention: A Taxonomic Overview"}, {"content": "| Formulation | Consistency |\n|---|---|", "caption": "Table 2: Quantitative results of the original FLUX-1.dev, previous efficient attention methods, and CLEAR proposed in this paper with various r\ud835\udc5fritalic_r on 5,000 images from the COCO2014 validation dataset at a resolution of 1024\u00d71024102410241024\\times 10241024 \u00d7 1024.", "description": "Table 2 presents a quantitative comparison of different text-to-image generation models.  It evaluates the performance of the original FLUX-1.dev model against several other efficient attention mechanisms, including the proposed CLEAR method. The evaluation is performed using 5,000 images from the COCO2014 validation set, all at a resolution of 1024x1024 pixels.  The results are presented in terms of several metrics: FID (Fr\u00e9chet Inception Distance), LPIPS (Learned Perceptual Image Patch Similarity), CLIP-I (CLIP Image Similarity), DINO (DINO Image Similarity), and GFLOPS (floating point operations per second). Different values for the parameter 'r' (radius of the local attention window in the CLEAR method) are used to assess its performance. This allows analysis of the trade-off between computational efficiency and image quality across different model variations.", "section": "4. Experiments"}, {"content": "| High-Rank | Attention Maps |\n|---|---|", "caption": "Table 3: Quantitative results of the original FLUX-1.dev and our CLEAR with various r\ud835\udc5fritalic_r on 1,000 images from the COCO2014 validation dataset at resolutions of 2048\u00d72048204820482048\\times 20482048 \u00d7 2048 and 4096\u00d74096409640964096\\times 40964096 \u00d7 4096.", "description": "This table presents a quantitative comparison of image generation performance between the original FLUX-1.dev model and the proposed CLEAR model at different resolutions (2048x2048 and 4096x4096).  It shows the FID, LPIPS, CLIP-I, DINO, PSNR and SSIM scores for each model and different values of the radius parameter (r) used in the CLEAR model.  These metrics assess the quality and fidelity of the generated images against ground truth images and the original model. The table helps demonstrate the effectiveness of CLEAR in producing high-resolution images while maintaining visual quality and reducing computational cost.", "section": "4. Experiments"}, {"content": "| Feature | Integrity |\n|---|---|", "caption": "Table 4: Quantitative zero-shot generalization results to FLUX-1.schnell using CLEAR layers trained on FLUX-1.dev.", "description": "This table presents the results of a zero-shot generalization experiment.  The CLEAR (Convolution-like Linearization for Efficient Attention) layers, trained on the FLUX-1.dev model, were applied without further training to the FLUX-1.schnell model. The table evaluates the performance of this zero-shot transfer by comparing key metrics such as FID (Fr\u00e9chet Inception Distance), LPIPS (Learned Perceptual Image Patch Similarity), CLIP-I (CLIP Image Similarity), and DINO (DINO Image Similarity) against the original FLUX-1.schnell model and the ground truth.  This demonstrates the ability of the CLEAR method to generalize across different models.", "section": "4.3 Empirical Studies"}, {"content": "| Method/Setting | Against Original |  |  |  | Against Real |  | CLIP-T (\u2191) | IS (\u2191) | GFLOPS (\u2193) |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Original FLUX-1.dev | - | - | - | - | 34.93 | 0.81 | 31.06 | 38.25 | 260.9 |\n| Sigmoid Attention [48] | 447.80 | 0.91 | 41.34 | 0.25 | 457.69 | 0.84 | 17.53 | 1.15 | 260.9 |\n| Linear Attention [12, 38, 65, 30] | 324.54 | 0.85 | 51.37 | 2.17 | 325.58 | 0.87 | 19.16 | 2.91 | 174.0 |\n| PixArt-Simga [6] | 30.64 | 0.56 | 86.43 | 71.45 | 33.38 | 0.88 | 31.12 | 32.14 | 67.7 |\n| Agent Attention [20] | 69.85 | 0.65 | 78.18 | 56.09 | 54.31 | 0.87 | 30.38 | 21.03 | 80.5 |\n| Strided Attention [7] | 24.88 | 0.61 | 85.50 | 70.72 | 35.27 | 0.89 | 30.62 | 32.05 | 67.7 |\n| Swin Transformer [39] | 18.90 | 0.65 | 85.72 | 73.43 | 32.20 | 0.87 | 30.64 | 34.68 | 67.7 |\n| CLEAR (r=8) | 15.53 | 0.64 | 86.47 | 74.36 | 32.06 | 0.83 | 30.69 | 34.47 | 63.5 |\n| w. distill | 13.07 | 0.62 | 88.56 | 77.66 | 33.06 | 0.82 | 30.82 | 35.92 | 63.5 |\n| CLEAR (r=16) | 14.27 | 0.60 | 88.51 | 78.35 | 32.36 | 0.89 | 30.90 | 37.13 | 80.6 |\n| w. distill | 13.72 | 0.58 | 88.53 | 77.30 | 33.63 | 0.88 | 30.65 | 37.84 | 80.6 |\n| CLEAR (r=32) | 11.07 | 0.52 | 89.92 | 81.20 | 33.47 | 0.82 | 30.96 | 37.80 | 154.1 |\n| w. distill | 8.85 | 0.46 | 92.18 | 85.44 | 34.88 | 0.81 | 31.00 | 39.12 | 154.1 |", "caption": "Table 5: Quantitative zero-shot generalization results of the proposed CLEAR to a pre-trained ControlNet with grayscale image conditions on 1,000 images from the COCO2014 validation dataset. RMSE here denotes Root Mean Squared Error computed against condition images.", "description": "This table presents the results of a zero-shot generalization experiment. The model CLEAR, which uses a convolution-like local attention mechanism, is evaluated on its ability to work with a pre-trained ControlNet plugin.  The experiment uses grayscale images as input conditions, and the performance is assessed using standard metrics for image generation: FID, LPIPS, CLIP-I, DINO, CLIP-T, IS, and RMSE. The metrics compare the generated images to both the original images and to the grayscale condition images.  The RMSE (Root Mean Squared Error) specifically measures the difference between the generated image and the grayscale condition image. The data is based on 1,000 images from the COCO2014 validation dataset, and the table demonstrates that CLEAR generalizes well to the ControlNet plugin without any fine-tuning on the new dataset.", "section": "4. Experiments"}, {"content": "| Setting | PSNR (\u2191) | SSIM (\u2191) | FID (\u2193) | LPIPS (\u2193) | CLIP-I (\u2191) | DINO (\u2191) | CLIP-T (\u2191) | IS (\u2191) | GFLOPS (\u2193) |\n|---|---|---|---|---|---|---|---|---|---| \n| **\u20131024\u00d71024\u21922048\u00d72048\u2013** |  |  |  |  |  |  |  |  |  |\n| FLUX-1.dev | - | - | - | - | - | - | 31.11 | 24.53 | 3507.9 |\n| CLEAR (r=8) | 27.57 | 0.91 | 13.55 | 0.12 | 98.97 | 98.37 | 31.09 | 25.05 | 246.2 |\n| CLEAR (r=16) | 27.60 | 0.92 | 13.43 | 0.12 | 98.97 | 98.34 | 31.08 | 25.46 | 352.6 |\n| CLEAR (r=32) | 28.95 | 0.94 | 10.87 | 0.10 | 99.23 | 98.82 | 31.09 | 25.48 | 724.3 |\n| **\u20132048\u00d72048\u21924096\u00d74096\u2013** |  |  |  |  |  |  |  |  |  |\n| FLUX-1.dev | - | - | - | - | - | - | 31.29 | 24.36 | 53604.4 |\n| CLEAR (r=8) | 26.19 | 0.87 | 20.87 | 0.22 | 98.02 | 96.56 | 31.16 | 25.87 | 979.3 |\n| CLEAR (r=16) | 26.98 | 0.88 | 16.20 | 0.19 | 98.48 | 97.64 | 31.25 | 25.13 | 1433.2 |\n| CLEAR (r=32) | 27.70 | 0.90 | 13.56 | 0.17 | 98.72 | 98.21 | 31.20 | 24.81 | 3141.7 |", "caption": "Table 6: Results of patch-wise multi-GPU parallel inference with various numbers of patches using the approximation in Eq.\u00a07.", "description": "This table presents the results of a multi-GPU parallel inference experiment using the CLEAR method.  The experiment varies the number of image patches distributed across multiple GPUs.  A key aspect is the use of an approximation (Equation 7 from the paper) to aggregate attention results from each GPU for text tokens, which is crucial for efficient parallel processing. The table shows the effect of this approximation on the performance of the model as the number of GPUs increases, demonstrating the scalability of the CLEAR approach for high-resolution image generation.", "section": "3.4 Multi-GPU Parallel Inference"}, {"content": "| Setting | Against Original |  |  |  | Against Real |  | CLIP-T (\u2191) | IS (\u2191) |\n|---|---|---|---|---|---|---|---|---|\n|  | FID (\u2193) | LPIPS (\u2193) | CLIP-I (\u2191) | DINO (\u2191) | FID (\u2193) | LPIPS (\u2193) |  |  |\n|---|---|---|---|---|---|---|---|---|\n| FLUX-1.dev | - | - | - | - | 29.19 | 0.83 | 31.53 | 36.41 |\n| CLEAR (r=8) | 13.62 | 0.62 | 88.91 | 78.36 | 33.51 | 0.81 | 31.35 | 38.42 |\n| CLEAR (r=16) | 12.51 | 0.58 | 90.43 | 81.32 | 34.43 | 0.82 | 31.38 | 39.66 |\n| CLEAR (r=32) | 12.43 | 0.57 | 90.70 | 82.61 | 33.57 | 0.83 | 31.48 | 39.68 |", "caption": "Table 7: Raw data for Fig.\u00a02 on efficiency comparisons.", "description": "This table provides the detailed numerical data used to generate Figure 2 in the paper.  Figure 2 visually compares the speed and computational cost (GFLOPS) of the proposed linearized DiT model with the original FLUX-1-dev model across different image resolutions. This table offers the underlying raw data points used to create that figure, allowing for a more precise and detailed understanding of the performance improvements achieved through linearization. The data includes the execution time in seconds per image and the GFLOPS per layer for each model and resolution.", "section": "Experiments"}, {"content": "| Setting | PSNR (\u2191) | SSIM (\u2191) | FID (\u2193) | LPIPS (\u2193) | CLIP-I (\u2191) | DINO (\u2191) | Against Real FID (\u2193) | Against Real LPIPS (\u2193) | CLIP-T (\u2191) | IS (\u2191) | RMSE (\u2193) |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| FLUX-1.dev | - | - | - | - | - | - | 40.25 | 0.32 | 30.16 | 22.22 | 0.0385 |\n| CLEAR (r=8) | 25.95 | 0.93 | 26.14 | 0.19 | 93.39 | 94.24 | 43.82 | 0.31 | 29.90 | 21.29 | 0.0357 |\n| CLEAR (r=16) | 28.24 | 0.95 | 16.86 | 0.13 | 96.00 | 96.73 | 40.45 | 0.31 | 30.19 | 22.34 | 0.0395 |\n| CLEAR (r=32) | 30.59 | 0.97 | 11.57 | 0.09 | 97.33 | 98.12 | 40.21 | 0.31 | 30.21 | 21.94 | 0.0419 |", "caption": "Table 8: Quantitative results of the original SD3-Large and its linearized version by CLEAR proposed in this paper on 5,000 images from the COCO2014 validation dataset at a resolution of 1024\u00d71024102410241024\\times 10241024 \u00d7 1024.", "description": "This table presents a quantitative comparison of the performance of the original Stable Diffusion 3.5-Large (SD3.5-Large) model and its version linearized using the CLEAR method.  The evaluation is based on 5,000 images from the COCO2014 validation dataset, all at a resolution of 1024x1024 pixels.  The comparison uses several metrics to assess both the visual quality and the efficiency of the models.  These metrics likely include FID (Fr\u00e9chet Inception Distance), LPIPS (Learned Perceptual Image Patch Similarity), CLIP (Contrastive Language\u2013Image Pre-training) scores for image-text similarity, and potentially others to evaluate generation quality.  Furthermore, it likely includes computational metrics such as GFLOPS (floating-point operations per second) indicating the computational efficiency of each model.", "section": "4. Main Comparisons"}, {"content": "| Setting | Against Original | Against Real | CLIP-T (\u2191) | IS (\u2191) |  |  | \n|---|---|---|---|---|---|---|\n| **FID (\u2193)** | **LPIPS (\u2193)** | **CLIP-I (\u2191)** | **DINO (\u2191)** | **FID (\u2193)** | **LPIPS (\u2193)** | \n|---|---|---|---|---|---|---|\n| CLEAR (r=16) | - | - | - | - | 33.63 | 0.88 |\n|  N=2 | 11.55 | 0.51 | 90.46 | 80.89 | 33.74 | 0.81 |\n| N=4 | 12.78 | 0.54 | 89.74 | 79.99 | 33.07 | 0.81 |\n| N=8 | 14.21 | 0.57 | 88.92 | 78.65 | 32.26 | 0.80 |", "caption": "Table 9: Quantitative zero-shot generalization results of the proposed CLEAR to a pre-trained ControlNet with tiled image conditions and blur image conditions on 1,000 images from the COCO2014 validation dataset. RMSE here denotes Root Mean Squared Error computed against condition images.", "description": "This table presents a quantitative evaluation of CLEAR's zero-shot generalization capabilities when used with a pre-trained ControlNet model.  Two types of conditional images were tested: tiled images and blurred images. The evaluation is performed on 1000 images from the COCO2014 validation set. The metrics used include FID (Fr\u00e9chet Inception Distance), LPIPS (Learned Perceptual Image Patch Similarity), CLIP-I (CLIP Image Similarity), DINO (DINO Image Similarity),  and RMSE (Root Mean Squared Error), which is computed against the conditional images. This shows how well CLEAR maintains image quality and alignment with the ControlNet when it has not been specifically trained for these conditions. Lower FID and LPIPS scores indicate better visual quality compared to the original, higher CLIP-I and DINO scores indicate better similarity, while a lower RMSE indicates better alignment with the condition image.", "section": "4. Experiments"}, {"content": "| Setting | Running Time (Sec. / 50 Steps) |  |  |  | TFLOPS / Layer |  |  |  | \n|---|---|---|---|---|---|---|---|---| \n|  | 1024x1024 | 2048x2048 | 4096x4096 | 8192x8192 | 1024x1024 | 2048x2048 | 4096x4096 | 8192x8192 | \n| **Setting** |  |  |  |  |  |  |  |  | \n| FLUX-1.dev | 4.45 | 20.90 | 148.97 | 1842.48 | 0.26 | 3.51 | 53.60 | 847.73 | \n| CLEAR (r=8) | 4.40 | 15.67 | 69.41 | 293.50 | 0.06 | 0.25 | 0.98 | 3.92 | \n| CLEAR (r=16) | 4.56 | 17.19 | 83.13 | 360.83 | 0.09 | 0.35 | 1.43 | 5.79 | \n| CLEAR (r=32) | 5.45 | 19.95 | 109.57 | 496.22 | 0.15 | 0.72 | 3.14 | 13.09 |", "caption": "Table 10: Efficiency of multi-GPU parallel inference measured by sec./50 denoising steps on a HGX H100 8-GPU server. We adapt Distrifusion\u00a0[34] to FLUX-1.dev here for asynchronous communication. The ratios of acceleration are highlighted with red. Results of CLEAR with r=16\ud835\udc5f16r=16italic_r = 16 at the 1024\u00d71024102410241024\\times 10241024 \u00d7 1024 resolution are not available (NA) because the patch size processed by each GPU is smaller than the boundary size. OOM denotes encountering out-of-memory error.", "description": "This table presents a performance comparison of multi-GPU parallel inference for image generation using different models and settings.  The metric is the time taken (in seconds) to complete 50 denoising steps.  The models compared include the original FLUX-1.dev model and its variants using the CLEAR method with different radius values (r=8, r=16, r=32).  The experiments were conducted on an 8-GPU HGX H100 server, employing asynchronous communication as implemented in Distrifusion [34]. The table shows the speedup achieved by using multiple GPUs. Note that results for the CLEAR method with r=16 at a resolution of 1024x1024 are not provided because, at this resolution, the patch size was smaller than the GPU boundary size. 'OOM' indicates cases where the memory capacity of the GPU was exceeded.  Speedup factors are highlighted in red.", "section": "3.4. Multi-GPU Parallel Inference"}]