[{"content": "| Method | MMLU (30%) | MMLU (50%) | MMLU (70%) | ARC (30%) | ARC (50%) | ARC (70%) | PubMedQA (30%) | PubMedQA (50%) | PubMedQA (70%) | Drop (30%) | Drop (50%) | Drop (70%) | FPB (30%) | FPB (50%) | FPB (70%) |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Vanilla | 65.3 | 65.3 | 65.3 | 82.7 | 82.7 | 82.7 | 72.0 | 72.0 | 72.0 | 87.2 | 87.2 | 87.2 | 75.5 | 75.5 | 75.5 |\n| Hermes-3 | 65.5 | 65.5 | 65.5 | 68.7 | 68.7 | 68.7 | 64.8 | 64.8 | 64.8 | 87.1 | 87.1 | 87.1 | 59.4 | 59.4 | 59.4 |\n| Tulu-3 | 55.7 | 55.7 | 55.7 | 73.3 | 73.3 | 73.3 | 63.3 | 63.3 | 63.3 | 85.3 | 85.3 | 85.3 | 54.5 | 54.5 | 54.5 |\n| SelfLabel | 64.7 | 64.7 | 64.7 | 82.1 | 82.1 | 82.1 | 71.8 | 71.8 | 71.8 | 86.8 | 86.8 | 86.8 | 82.8 | 82.8 | 82.8 |\n| SFT | 59.5 | 47.5 | 37.3 | 70.7 | 61.7 | 47.5 | 66.4 | 36.7 | 32.8 | 85.3 | 78.6 | 66.4 | 79.7 | 58.4 | 34.9 |\n| NoiseAL | 66.3 | 65.5 | 66.1 | 84.0 | 83.6 | 83.4 | 74.2 | 72.2 | 71.8 | 86.8 | 84.3 | 82.1 | 81.1 | 78.5 | 72.8 |\n| SelfRAG | 65.3 | 65.4 | 64.1 | 83.1 | 82.7 | 82.0 | 63.2 | 60.2 | 57.0 | 86.5 | 85.5 | 83.1 | 83.8 | 76.2 | 68.2 |\n| SelfSelect | 59.1 | 53.4 | 44.0 | 76.8 | 72.1 | 62.6 | 57.8 | 46.0 | 22.6 | 86.2 | 78.8 | 64.4 | 79.8 | 58.4 | 32.0 |\n| **Ours** | **68.2** | **68.0** | **67.6** | **84.9** | **84.7** | **84.1** | **75.8** | **75.6** | **75.0** | **90.3** | **88.5** | **87.9** | **84.4** | **80.5** | **76.2** |\n| \u2191 vs. Vanilla | 4.4 | 4.1 | 3.5 | 2.7 | 2.4 | 1.7 | 5.3 | 5.0 | 4.2 | 3.6 | 1.5 | 0.8 | 11.8 | 6.6 | 0.9 |\n| \u2191 vs. SFT | 14.6 | 43.2 | 81.2 | 20.1 | 37.3 | 77.1 | 14.2 | 106 | 129 | 5.9 | 12.6 | 32.4 | 5.9 | 37.8 | 110 |", "caption": "Table 1: Performance comparison under different noise rates with Llama-3.1 8B. Best results are shown in bold. Numbers in the last two rows show relative improvements (%).", "description": "This table presents a performance comparison of different methods for fine-tuning a Llama-3.1 8B language model under varying levels of noise in the training data.  The methods compared include a vanilla approach (no fine-tuning), standard supervised fine-tuning (SFT), several noise-robust SFT approaches (NoiseAL, SelfRAG, SelfSelect), and the proposed ROBUSTFT method.  Performance is evaluated across five downstream tasks (MMLU, ARC, PubMedQA, Drop, and FPB) using three noise levels (30%, 50%, and 70% noisy data).  The best performance for each condition is highlighted in bold, and the last two rows quantify the relative improvement of each method compared to the Vanilla and SFT baselines.", "section": "4 Experiment"}, {"content": "| Model | MMLU 30% | MMLU 50% | MMLU 70% | ARC 30% | ARC 50% | ARC 70% | PubMedQA 30% | PubMedQA 50% | PubMedQA 70% | Drop 30% | Drop 50% | Drop 70% | FPB 30% | FPB 50% | FPB 70% |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| *Llama3.2 3B* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Vanilla | 54.9 | 54.9 | 54.9 | 72.4 | 72.4 | 72.4 | 57.8 | 57.8 | 57.8 | 71.0 | 71.0 | 71.0 | 39.9 | 39.9 | 39.9 |\n| SFT | 55.0 | 48.4 | 38.3 | 66.1 | 58.5 | 42.9 | 63.2 | 49.2 | 37.5 | 77.3 | 73.7 | 61.3 | 56.2 | 49.4 | 31.3 |\n| **Ours** | **58.5** | **58.2** | **57.9** | **74.6** | **74.3** | **72.6** | **68.9** | **67.9** | **67.9** | **78.9** | **77.6** | **75.6** | **66.1** | **59.4** | **46.8** |\n| *Llama3.1 8B* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Vanilla | 65.3 | 65.3 | 65.3 | 82.7 | 82.7 | 82.7 | 72.0 | 72.0 | 72.0 | 87.2 | 87.2 | 87.2 | 75.5 | 75.5 | 75.5 |\n| SFT | 59.5 | 47.5 | 37.3 | 70.7 | 61.7 | 47.5 | 66.4 | 36.7 | 32.8 | 85.3 | 78.6 | 66.4 | 79.7 | 58.4 | 34.9 |\n| **Ours** | **68.2** | **68.0** | **67.6** | **84.9** | **84.7** | **84.1** | **75.8** | **75.6** | **75.0** | **90.3** | **88.5** | **87.9** | **84.4** | **80.5** | **73.2** |\n| *Gemma2 9B* |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| Vanilla | 70.3 | 70.3 | 70.3 | 90.2 | 90.2 | 90.2 | 66.4 | 66.4 | 66.4 | 90.7 | 90.7 | 90.7 | 83.1 | 83.1 | 83.1 |\n| SFT | 63.6 | 52.1 | 40.3 | 77.9 | 64.6 | 55.0 | 61.7 | 39.8 | 30.4 | 88.8 | 80.5 | 67.3 | 88.1 | 60.7 | 35.6 |\n| **Ours** | **72.5** | **72.1** | **71.3** | **91.8** | **91.5** | **90.4** | **70.8** | **68.8** | **66.8** | **91.9** | **91.8** | **90.9** | **91.8** | **80.8** | **87.7** |", "caption": "Table 2: Performance comparison across different model architectures and noise rates. Best results for each model are shown in bold.", "description": "This table presents a comparison of the performance of three different large language models (LLMs) - Llama 3.2 3B, Llama 3.1 8B, and Gemma2 9B - across various downstream tasks.  The performance is evaluated under three different noise levels (30%, 50%, and 70%) in the training data. For each LLM and noise level, the table shows the accuracy achieved by a vanilla (untuned) model, a model fine-tuned using supervised fine-tuning (SFT), and a model fine-tuned using the proposed ROBUSTFT method.  The best result for each model configuration is highlighted in bold, allowing for a direct comparison of the effectiveness of each approach under different conditions.", "section": "4.2 Main Result"}, {"content": "| Variant | MMLU 30% | MMLU 50% | MMLU 70% | ARC 30% | ARC 50% | ARC 70% |\n|---|---|---|---|---|---|---|\n| Llama3.1-8B |  |  |  |  |  |  |\n| RobustFT | 68.2 | 68.0 | 67.6 | 84.9 | 84.7 | 84.1 |\n| *w/o* Selection | 65.7 | 65.1 | 64.6 | 83.2 | 83.0 | 82.8 |\n| *w/o* Checker | 65.3 | 65.0 | 64.9 | 82.7 | 82.6 | 82.2 |\n| *w/o* Reviewer | 68.0 | 67.7 | 67.1 | 84.5 | 84.3 | 84.0 |\n| *w/o* CER | 67.7 | 67.7 | 67.0 | 84.6 | 84.1 | 83.9 |\n| *w/o* REL | 67.4 | 67.2 | 66.9 | 84.1 | 83.9 | 83.6 |", "caption": "Table 3: Ablation study showing the impact of different noise rates (30%, 50%, 70%) on model variants across MMLU and ARC benchmarks.", "description": "This ablation study analyzes the performance of different ROBUSTFT model variants on the MMLU and ARC benchmarks under varying levels of noise (30%, 50%, and 70%).  It shows the impact of removing or modifying key components of the ROBUSTFT framework (such as the data selection mechanism, the noise detection component, the denoising mechanism, and the review agent), providing insights into the contributions of each part to the overall robustness and performance of the model in handling noisy data.", "section": "4.3 Ablation Study"}]