{"references": [{"fullname_first_author": "Gantavya Bhatt", "paper_title": "An experimental design framework for label-efficient supervised finetuning of large language models", "publication_date": "2024-01-06", "reason": "This paper is crucial because it introduces a framework for efficient fine-tuning of LLMs using limited labeled data, which directly addresses the challenges of noisy and limited data resources often encountered in practical applications."}, {"fullname_first_author": "Christopher Clark", "paper_title": "Boolq: Exploring the surprising difficulty of natural yes/no questions", "publication_date": "2019-06-01", "reason": "This paper's contribution is its exploration of the challenges of question answering with yes/no questions, which relates to the noisy data problem by demonstrating difficulties in generating and interpreting simple answers, which can be a source of noise."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper is important because it introduces a set of LLMs which is used as a basis for the experiments conducted in the paper, demonstrating various model capabilities and providing a foundation for comparison."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-03", "reason": "This paper presents a benchmark for evaluating LLMs across various tasks, which helps to demonstrate the performance degradation caused by noisy training data and is used to evaluate models in the paper."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-01", "reason": "This paper is highly relevant because it introduces a unified text-to-text transformer model that is widely used in LLM applications, and therefore its optimization in different scenarios like noisy data is important for the field."}]}