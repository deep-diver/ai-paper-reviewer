{"references": [{"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper introduces Direct Preference Optimization (DPO), which is used for training MLLMs in this paper."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-04-01", "reason": "This paper introduces visual instruction tuning, a method used to improve cross-modal alignment and increase the variety of tasks handled by MLLMs."}, {"fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "publication_date": "2024-01-01", "reason": "This paper proposes MMBench, a benchmark used in this paper for evaluating VQA capabilities of multimodal models."}, {"fullname_first_author": "Jeffrey Zhou", "paper_title": "Instruction-following evaluation for large language models", "publication_date": "2023-11-01", "reason": "This paper proposes instruction-following evaluation, which is used in this paper for evaluating the capability of language models to follow complex instructions."}, {"fullname_first_author": "Yusu Qian", "paper_title": "MIA-Bench: Towards better instruction following evaluation of multimodal llms", "publication_date": "2025-01-01", "reason": "This paper introduces MIA-Bench, an instruction following benchmark that this paper compares its results to."}]}