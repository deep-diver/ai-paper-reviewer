{"references": [{"fullname_first_author": "Zheng", "paper_title": "ProcessBench: Identifying Process Errors in Mathematical Reasoning", "publication_date": "2024-01-01", "reason": "This paper introduces the ProcessBench benchmark, which is used to evaluate the effectiveness of PRMs, making it a crucial reference for assessing the models discussed in the current paper."}, {"fullname_first_author": "Zhang", "paper_title": "The Lessons of Developing Process Reward Models in Mathematical Reasoning", "publication_date": "2025-01-01", "reason": "This paper emphasizes the importance of addressing chain-of-thought reasoning errors by proposing process reward models (PRMs)."}, {"fullname_first_author": "Lightman", "paper_title": "Let's Verify Step by Step", "publication_date": "2023-01-01", "reason": "This paper pioneers PRM training by employing human experts to label questions at the step level, demonstrating high-quality results and serving as a foundation for the current research."}, {"fullname_first_author": "Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "publication_date": "2021-03-01", "reason": "This paper introduces the MATH dataset, which is referenced to illustrate the need for models trained on diverse questions to consistently outperform those with limited question types."}, {"fullname_first_author": "Tan", "paper_title": "Aurora:automated training framework of universal process reward models via ensemble prompting and reverse verification", "publication_date": "2025-01-01", "reason": "This paper establishes a new state-of-the-art (SOTA) performance on ProcessBench, making it a crucial benchmark for comparisons."}]}