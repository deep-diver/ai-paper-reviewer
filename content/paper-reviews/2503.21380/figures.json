[{"figure_path": "https://arxiv.org/html/2503.21380/x1.png", "caption": "Figure 1: Performance comparisons of mainstream reasoning models between our OlymMATH (English version) and other Olympiad-level mathematical benchmarks. OlymMATH-HARD emerges as the most challenging, with significantly higher difficulty than existing evaluation benchmarks.", "description": "This figure compares the performance of several large language models (LLMs) on various Olympiad-level mathematics benchmarks, including the OlymMATH benchmark (English version) introduced in this paper.  The x-axis shows the different models tested, and the y-axis displays their accuracy (or percentile) on each benchmark.  The bars are grouped by benchmark: OlymMATH-EN-EASY, OlymMATH-EN-HARD (the new benchmark proposed in the paper), and other existing Olympiad-level benchmarks like AIME 2024, AIME 2025, and HMMT 202502.  The figure highlights that the OlymMATH-HARD subset is significantly more challenging than existing benchmarks, with even the state-of-the-art models showing considerably lower accuracy on this subset.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.21380/x2.png", "caption": "Figure 2: Examples from the MATH dataset and our OlymMATH dataset.", "description": "This figure shows a comparison of problem examples from the MATH dataset and the newly proposed OlymMATH dataset.  It highlights the differences in problem complexity and style between the two benchmarks. The MATH examples showcase relatively straightforward problems, while the OlymMATH examples present more complex and challenging problems at an Olympiad level, demonstrating the increased difficulty of the new benchmark.", "section": "2 Benchmark Construction"}]