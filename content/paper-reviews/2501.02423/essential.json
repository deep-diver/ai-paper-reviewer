{"importance": "This paper is crucial for researchers working on **large language model (LLM) training efficiency** and **low-precision computation**.  It provides a novel scaling law for floating-point quantization, offering **practical guidance for optimizing LLM training costs** and **hardware resource allocation**.  The findings are particularly relevant to current trends in reducing computational expenses and improving LLM deployment on resource-constrained platforms.  It opens up new avenues for exploring **optimal exponent-mantissa bit ratios** and **critical data size thresholds** in low-precision LLM training.", "summary": "New scaling laws for efficient floating-point quantization training in LLMs are presented, showing optimal bit allocation and critical data size.", "takeaways": ["A new unified scaling law accurately predicts the performance of LLMs under various floating-point quantization settings.", "Optimal exponent-mantissa bit ratios and a critical data size threshold for preventing performance degradation in low-precision LLM training were identified.", "The study suggests that the best cost-performance precision in LLM training lies between 4-8 bits, depending on computational power."], "tldr": "Low-precision training, especially using floating-point quantization, is crucial for efficient large language model (LLM) training. However, existing scaling laws primarily focus on integer quantization, which isn't well-suited for the nuances of floating-point methods. This lack of understanding hinders efforts to optimize training costs and predict model performance. This paper addresses these issues by deeply investigating the effects of different floating-point configurations (exponent bits, mantissa bits, scaling factor granularity) on LLM training performance.  The research uses a comprehensive experimental setup involving various data and model sizes, along with multiple precision settings, to establish a robust and accurate scaling law for predicting performance under low-precision training. This new scaling law incorporates these crucial floating-point parameters, unlike prior work that treated precision in a less nuanced way. \nThe core contribution of this work is the development of a novel, unified scaling law that accurately predicts LLM performance under various data sizes, model sizes, and floating-point configurations (exponent and mantissa bits).  This law allows researchers to efficiently select optimal parameter settings before running costly experiments and assists in predicting model behavior across a wide range of conditions.  Key insights from the scaling law include discovering the optimal exponent-mantissa bit ratio for various precision levels and determining the critical training data size to prevent performance degradation. This research demonstrates that cost-effective performance can be achieved between 4-8 bits of precision and proposes guidelines for selecting optimal hardware configurations.", "affiliation": "Tencent AI Lab", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2501.02423/podcast.wav"}