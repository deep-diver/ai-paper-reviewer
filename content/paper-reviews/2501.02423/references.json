{"references": [{"fullname_first_author": "Kaplan, J.", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-27", "reason": "This paper is foundational for understanding the scaling laws of LLMs, providing a basis for further research in low-precision training and establishing core relationships between model size, data size and performance."}, {"fullname_first_author": "Hoffmann, J.", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-03-22", "reason": "This paper significantly advanced the understanding of compute-optimal training for LLMs, influencing the design and optimization of efficient training strategies, including low-precision techniques."}, {"fullname_first_author": "Kumar, T.", "paper_title": "Scaling laws for precision", "publication_date": "2024-11-21", "reason": "This paper directly addresses scaling laws concerning precision in LLMs, providing valuable insights into the relationship between model performance and different quantization strategies, informing the current work's investigation into floating-point quantization."}, {"fullname_first_author": "Dubey, A.", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper introduces the LLaMA model family, which serves as the foundation model used in the experiments of the current study, directly impacting the results and conclusions of the research."}, {"fullname_first_author": "Micikevicius, P.", "paper_title": "FP8 formats for deep learning", "publication_date": "2022-09-21", "reason": "This paper introduces FP8 formats as a viable low-precision format for deep learning, directly relevant to the current work's focus on floating-point quantization training in LLMs."}]}