[{"figure_path": "https://arxiv.org/html/2503.16418/x2.png", "caption": "Figure 1: InfiniteYou generates identity-preserved images with exceptional identity similarity, text-image alignment, quality, and aesthetics.", "description": "Figure 1 showcases example images generated by InfiniteYou.  The figure demonstrates the model's ability to faithfully recreate a person's likeness in different scenarios and outfits while adhering closely to textual descriptions.  This highlights the model's success in preserving identity, accurately reflecting the given text prompts, achieving high-quality output, and maintaining aesthetic appeal.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.16418/x3.png", "caption": "Figure 2: The superiority of the DiT-based method over the U-Net-based one and the side effects of IP-Adapter\u00a0(IPA)\u00a0[54].", "description": "Figure 2 demonstrates the improved image generation quality achieved by using Diffusion Transformers (DiTs) compared to U-Net based models.  The left panel shows examples generated by U-Net based Stable Diffusion XL (SDXL) and DiT-based FLUX models. The right panel highlights the negative impacts of using the IP-Adapter method for identity injection in DiT-based methods. It visually compares the results generated with and without the IP-Adapter, showcasing issues such as compromised image quality and loss of text-image alignment.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2503.16418/x4.png", "caption": "Figure 3: The main framework of InfiniteYou (InfU) and the detailed architecture of InfuseNet. The projected identity features and an optional control image are injected by InfuseNet into text-to-image DiTs via residual connections. Specifically, each DiT block in InfuseNet predicts the output residuals of the corresponding i\ud835\udc56iitalic_i DiT blocks in the base model. Only InfuseNet and the projection network are trainable.", "description": "Figure 3 illustrates the architecture of InfiniteYou (InfU), a framework for identity-preserved image generation.  The main components are: a Face Identity Encoder that processes an identity image; a Text Encoder that processes text prompts; a DiT (Diffusion Transformer) base model, which is a pre-trained text-to-image generation model; and InfuseNet, a novel module designed to inject identity information and optionally a control image into the DiT base model. InfuseNet injects these features via residual connections, meaning that it adds its output to the output of corresponding blocks within the main DiT model.  This approach allows the model to preserve the generative capabilities of the DiT while adding identity information. A key aspect of InfuseNet's design is that each of its blocks modifies the outputs of a multiple of the base model's blocks (i blocks, where i is a multiplication factor), allowing it to scale and improve identity similarity.  Only InfuseNet and the projection network (which transforms the identity features) are trainable; the DiT base model is kept frozen to preserve its generation capabilities. The generated image reflects the injected identity while adhering to the given text prompt and control image information.", "section": "3.2 Network Architecture"}, {"figure_path": "https://arxiv.org/html/2503.16418/x5.png", "caption": "Figure 4: The introduced multi-stage training strategy with synthetic single-person-multiple-sample (SPMS) data and supervised fine-tuning (SFT).", "description": "Figure 4 illustrates the multi-stage training process employed to enhance the InfiniteYou model.  It begins with pretraining using real single-person-single-sample (SPSS) data.  Then, it leverages the pretrained model to generate synthetic single-person-multiple-sample (SPMS) data, which is used in a supervised fine-tuning (SFT) stage. The entire process aims to improve aspects like text-image alignment, image quality, and aesthetics, while maintaining identity preservation.", "section": "3.3 Multi-Stage Training Strategy"}, {"figure_path": "https://arxiv.org/html/2503.16418/x6.png", "caption": "Figure 5: Qualitative comparison results of InfU with the state-of-the-art baselines, FLUX.1-dev IP-Adapter\u00a0[20] and PuLID-FLUX\u00a0[14].", "description": "Figure 5 presents a qualitative comparison of image generation results from three different methods: InfU (the proposed method), FLUX.1-dev IP-Adapter [20], and PuLID-FLUX [14].  Each method is applied to generate images based on the same set of text prompts and identity images. The figure allows for a visual comparison of the image quality,  the accuracy of identity preservation, and the degree of alignment between generated images and the corresponding text prompts. This visual comparison demonstrates the superiority of InfU in terms of generating high-quality, identity-preserved images that closely match the given text descriptions.", "section": "4.2 Main Results"}]