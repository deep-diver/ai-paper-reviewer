{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This paper outlines the technical specifications for the GPT-4 model, a foundational model upon which the entire CodeArena project relies on, especially regarding prompt engineering, test case generation, and overall LLM evaluation."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating Large Language Models Trained on Code", "publication_date": "2021-07-03", "reason": "This paper introduces the HumanEval benchmark, which is extensively used for evaluating the functional correctness of code generated by LLMs, serving as a cornerstone for the development of CodeArena."}, {"fullname_first_author": "Mingzhe Du", "paper_title": "Mercury: An Efficiency Benchmark for LLM Code Synthesis", "publication_date": "2024-02-07", "reason": "This paper serves as a prior study that discusses measuring code execution efficiency, forming part of the motivation for the functionality introduced in the current work."}, {"fullname_first_author": "Terry Yue Zhuo", "paper_title": "BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions", "publication_date": "2024-06-19", "reason": "This paper introduces an important benchmark for code generation that tests more complex capabilities of LLMs, which is used as a standard in CodeArena."}, {"fullname_first_author": "Baptiste Roziere", "paper_title": "Code Llama: Open Foundation Models for Code", "publication_date": "2023-08-01", "reason": "This paper serves as an important prior study, that proposes an open-source LLM for code generation which is included in the LLMs listed in this paper."}]}