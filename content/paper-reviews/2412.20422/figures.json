[{"figure_path": "https://arxiv.org/html/2412.20422/x2.png", "caption": "Figure 1: \n(Click to view Video online)\nOur method, 3to4D, takes a static 3D object and a textual prompt describing a desired action. It then adds dynamics to the object based on the prompt to create a 4D animation, essentially a video viewable from any perspective. On the right, we display four 3D frames from the generated 4D animation. Each 3D frame is split into an RGB image and a corresponding depth map on its top right.", "description": "This figure showcases the capabilities of the 3to4D method.  Starting with a static 3D object (shown on the left of each example), a textual prompt is provided to specify the desired animation. The 3to4D system then generates a 4D animation, effectively a short video, which can be viewed from any perspective.  The image displays four frames extracted from one such 4D animation; for each frame, the RGB image is shown along with the corresponding depth map in the top right corner.  This illustrates how the system animates the provided 3D model according to a natural language description.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.20422/x3.png", "caption": "Figure 2: \nWorkflow of our 3to4D approach, designed to optimize a 4D radiance field using a neural representation that captures both static and dynamic elements.\nFirst, a 4D NeRF is trained to represent the static object (plant, left), having the same input structure at each time step.\nThen, we introduce dynamics to the 4D NeRF by distilling the prior from a pre-trained image-to-video model. At each SDS step, we select a viewpoint and render both the input object and the 4D NeRF from the same selected viewpoint. These renders, along with the textual prompts, are then fed into the image-to-video model, and the SDS loss is calculated to guide the generation of motion while preserving the object\u2019s identity.\nThe attention-masked SDS, focuses learning on the relevant parts of the object, improving identity preservation.", "description": "Figure 2 illustrates the 3to4D method's workflow for animating a 3D object using a text prompt.  It begins by training a static 4D Neural Radiance Field (NeRF) from the input 3D object to capture its appearance consistently across time. Dynamics are then added using an image-to-video diffusion model and a novel incremental viewpoint selection protocol that gradually expands the sampling range of viewpoints.  This enhances motion realism.  The process also incorporates a masked Score Distillation Sampling (SDS) loss, leveraging attention maps to focus optimization on the object and prevent unwanted background effects. The model learns to generate animation based on the textual prompt while maintaining the object\u2019s visual identity.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.20422/x6.png", "caption": "Figure 3: \n\n(click-to-view-online)\n3to4D brings various objects to life. On the left, we display the input object along with a textual prompt describing the desired action. On the right, we present four frames from the generated object, viewed from the front. Each 3D frame is split into an RGB image and its corresponding depth map, shown in the top right corner.", "description": "This figure showcases the capabilities of the 3to4D model to animate static 3D objects based on textual prompts.  The left side displays the initial 3D object along with a descriptive text prompt outlining the desired action (e.g., a blooming plant, a turtle with its head in its shell). The right side presents a sequence of four frames from the resulting 4D animation (video).  Each frame is further divided into an RGB image and a depth map in its top-right corner for better visual representation of the 3D scene. The 4D animation shows the object moving realistically based on the prompt.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2412.20422/x7.png", "caption": "Figure 4: Comparison between our method and baselines, across all objects tested. We consistently achieve better LPIPS scores across all objects.", "description": "Figure 4 presents a comparative analysis of the LPIPS scores achieved by 3to4D and various baseline methods across a comprehensive set of 3D objects.  The LPIPS (Learned Perceptual Image Patch Similarity) score serves as a metric to assess the visual similarity between the generated 4D animation and the original 3D input object.  The plot visually demonstrates the superior performance of the proposed 3to4D approach, consistently yielding lower LPIPS scores, indicating significantly better preservation of the input object's identity throughout the generated animation. This enhanced visual fidelity highlights the effectiveness of 3to4D compared to existing text-to-4D generation techniques.", "section": "Results"}, {"figure_path": "https://arxiv.org/html/2412.20422/x8.png", "caption": "Figure 5: Qualitative comparison with competing methods. A rendered image of the input object is shown on the left, alongside rendered images from our and other methods. While our method preserves the identity of the input object, all other baselines generate different objects.  \u00a0.", "description": "Figure 5 presents a qualitative comparison of the proposed 3to4D method against three baseline methods (4D-fy, Animate124, and STAG3D) for generating 4D content from a 3D object. For each method, a rendered image of the input 3D object is displayed on the left, followed by the 4D content generated by each method. The figure highlights that while 3to4D successfully maintains the identity and visual features of the input 3D object, the baseline methods produce significantly different outputs, sometimes generating completely different objects. This visually demonstrates 3to4D's superior ability in preserving object identity while adding dynamic motion compared to existing methods.", "section": "5. Results"}]