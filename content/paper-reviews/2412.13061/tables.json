[{"content": "| Method | Regularizer | Param. | MCL-JCV | | | | WebVid-Val | | | |\n|---|---|---|---|---|---|---|---|---|---|---| \n| | | | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 | FVD\u2193 | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 | FVD\u2193 |\n| MAGVIT-v2\u2217 | LFQ - 262,144 | - | 26.18 | - | 0.104 | - | - | - | - | - |\n| OmniTokenizer | VQ - 8,192 | 51M | 26.93 | 0.841 | 0.165 | 232.7 | 26.26 | **0.883** | 0.112 | 48.46 |\n| Cosmos-DV | FSQ - 64,000 | 101M | 28.07 | 0.743 | 0.212 | 227.7 | 29.39 | 0.741 | 0.170 | 57.97 |\n| Ours-FSQ | FSQ - 32,768 | 157M | **29.16** | **0.854** | **0.117** | **196.9** | **31.04** | **0.883** | **0.089** | **45.34** |\n| Ours-FSQ | FSQ - 262,144 | 157M | **29.82** | **0.867** | **0.106** | **160.1** | **31.76** | **0.896** | **0.080** | **38.17** |\n| CV-VAE | KL - 4chn | 182M | 28.56 | 0.823 | 0.163 | 334.2 | 30.79 | 0.863 | 0.116 | 70.39 |\n| Open-Sora-v1.2 | KL - 4chn | 393M | **29.44** | 0.766 | 0.164 | 350.7 | **31.02** | 0.764 | 0.137 | 112.34 |\n| Open-Sora-Plan-v1.2 | KL - 4chn | 239M | 29.07 | **0.839** | **0.131** | **201.7** | 30.85 | **0.869** | **0.101** | **44.76** |\n| Ours-KL | KL - 4chn | 157M | **29.64** | **0.852** | **0.114** | **194.2** | **31.53** | **0.878** | **0.087** | **36.88** |\n| CogVideoX | KL - 16chn | 206M | **33.76** | **0.930** | **0.076** | **93.2** | **36.22** | **0.952** | **0.049** | **15.30** |\n| Cosmos-CV | AE - 16chn | 101M | 31.27 | 0.817 | 0.149 | 153.7 | 33.04 | 0.818 | 0.107 | 23.85 |\n| Ours-KL | KL - 16chn | 157M | **35.04** | **0.942** | **0.047** | **78.9** | **37.53** | **0.961** | **0.032** | **9.12** |", "caption": "Table 1: Quantitative comparison with the state-of-the-art video tokenizers.\nAll evaluated models are causal and have a video compression ratio of 4\u00d78\u00d784884\\times 8\\times 84 \u00d7 8 \u00d7 8.\nThe input resolution for most models is 17\u00d7256\u00d72561725625617\\times 256\\times 25617 \u00d7 256 \u00d7 256, except for MAGVIT-v2\u2217, which is evaluated on 17\u00d7360\u00d76401736064017\\times 360\\times 64017 \u00d7 360 \u00d7 640 as reported in the original study.\nThe sample rate of testing data is 30 FPS.\nWe highlight the best and the second-best numbers in bold and underline respectively.", "description": "This table presents a quantitative comparison of VidTok with state-of-the-art causal video tokenizers, using a video compression ratio of 4x8x8.  Metrics include PSNR, SSIM, LPIPS, and FVD, evaluated on MCL-JCV and WebVid-Val datasets at 30 FPS. Input resolution is 17x256x256, except for MAGVIT-v2*, which uses 17x360x640 as reported in its original study.  Best and second-best results are highlighted.", "section": "4.2 COMPARISON WITH BASELINES"}, {"content": "| Method | Param. | FLOPs | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 | FVD\u2193 |\n|---|---|---|---|---|---|---| \n| Variant 1 | 245M | 16.98 T | 29.39 | 0.847 | 0.117 | 176.9 |\n| Variant 2 | 142M | 7.17 T | 29.36 | 0.846 | 0.119 | 185.7 |\n| Variant 3 | 126M | 10.18 T | 29.26 | 0.846 | 0.120 | 200.6 |\n| Ours | 157M | 10.35 T | 29.64 | 0.852 | 0.114 | 194.2 |", "caption": "Table 2: Ablation study on the model architecture. Variant 1: fully 3D architecture. Variant 2: w/o AlphaBlender. Variant 3: w/o 3D architecture. We use \u2018KL - 4chn\u2019 as regularizer for all settings.", "description": "This table presents an ablation study on the model architecture for video tokenization, comparing the performance of four different architectures. All models are trained using \u2018KL - 4chn\u2019 as regularizer. **Variant 1** uses a fully 3D architecture for spatial and temporal sampling. **Variant 2** separates spatial and temporal sampling using 2D and 1D convolutions but without AlphaBlender. **Variant 3** replaces all 3D convolutions with 2D convolutions. The proposed **Ours** model utilizes a combination of 2D and 1D convolutions with the AlphaBlender operator for temporal upsampling and downsampling, while other parts leverage 3D convolutions for information fusion. The table compares the number of parameters (Param.), FLOPS (floating point operations per second), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), Learned Perceptual Image Patch Similarity (LPIPS), and Fr\u00e9chet Video Distance (FVD).  The results demonstrate that the proposed architecture strikes a balance between computational efficiency and reconstruction performance.", "section": "3. VIDTOK"}, {"content": "| Regularizer | w/ R.L. | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 | FVD\u2193 | U.R.\u2191 |\n|---|---|---|---|---|---|---| \n| VQ - 262,144 | \\usym2613 | - | - | - | - | - |\n| VQ - 262,144 | \u2713 | 23.22 | 0.657 | 0.336 | 960.5 | 0.2% |\n| LFQ - 262,144 | \\usym2613 | 23.91 | 0.688 | 0.251 | 619.8 | 4.2% |\n| LFQ - 262,144 | \u2713 | 28.04 | 0.833 | 0.133 | 208.1 | 99.9% |\n| FSQ - 262,144 | \\usym2613 | 29.75 | 0.866 | 0.109 | 167.5 | 99.8% |\n| FSQ - 262,144 | \u2713 | 29.82 | 0.867 | 0.106 | 160.1 | 99.8% |\n| FSQ - 32,768 | \u2713 | 29.16 | 0.854 | 0.117 | 196.9 | 100.0% |\n| FSQ - 4,096 | \u2713 | 28.36 | 0.832 | 0.133 | 218.1 | 100.0% |", "caption": "Table 3: Analysis of the impact of discrete techniques on model performance. R.L. denotes Regularization Loss, while U.R. represents Utilization Rate.", "description": "This table shows the ablation study conducted on discrete techniques including VQ, LFQ, and FSQ, demonstrating FSQ's significant advantages over existing discrete tokenization techniques in terms of codebook utilization, reconstruction quality, and training stability. The presence or absence of regularization loss is also considered during the ablation analysis.", "section": "4.3.2 ABLATION ON THE DISCRETE TECHNIQUES"}, {"content": "| Sample Rate | First Stage | Second Stage | Fix Enc. | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 | FVD\u2193 | GPU Hours |\n|---|---|---|---|---|---|---|---|---| \n| 3 FPS | 256x256 | - | - | 29.19 | 0.843 | 0.127 | 174.9 | 3,072 | \n| 3 FPS | 128x128 | - | - | 29.02 | 0.838 | 0.130 | 221.7 | 960 |\n| 3 FPS | 128x128 | 256x256 | \\usym2613 | 29.15 | 0.842 | 0.127 | 203.2 | 1,728 |\n| 3 FPS | 128x128 | 256x256 | \u2713 | 29.21 | 0.843 | 0.125 | 189.8 | 1,536 |\n| 8 FPS | 128x128 | 256x256 | \u2713 | 29.02 | 0.839 | 0.126 | 219.2 | 1,536 |", "caption": "Table 4: Ablation study on the proposed training strategy. To ensure a fair comparison, both stages use training data from Training Set 1. Across all configurations, the regularizer \u2018KL - 4chn\u2019 is employed. The training computational cost, measured in GPU hours, is evaluated using NVIDIA A100 GPUs.", "description": "This table presents the ablation study on the two-stage training strategy proposed in the paper. The first stage involves training the full model on low-resolution videos (128x128), while the second stage involves fine-tuning only the decoder on high-resolution videos (256x256). The table compares different training settings, including single-stage training on high-resolution videos, two-stage training with or without fixed encoder, and varying sample rates. The results demonstrate that the two-stage training strategy with a fixed encoder achieves similar performance to single-stage training on high resolution but at roughly half the training time.", "section": "4.3.3 ABLATION ON THE TRAINING STRATEGIES"}, {"content": "| Regularizer | Causal | Input Size | VCR | Latent Size | Param. | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 | FVD\u2193 |\n|---|---|---|---|---|---|---|---|---|---|\n| KL - 4chn | \u2713 | 17\u00d7256\u00d7256 | 4\u00d78\u00d78 | 5\u00d732\u00d732 | 157M | 29.64 | 0.852 | 0.114 | 194.2 |\n| KL - 4chn | \u2713 | 17\u00d7256\u00d7256 | 4\u00d716\u00d716 | 5\u00d716\u00d716 | 199M | 25.05 | 0.711 | 0.228 | 549.1 |\n| KL - 4chn | \\usym2613 | 16\u00d7256\u00d7256 | 4\u00d78\u00d78 | 4\u00d732\u00d732 | 158M | 30.60 | 0.876 | 0.098 | 157.9 |\n| KL - 4chn | \\usym2613 | 16\u00d7256\u00d7256 | 4\u00d716\u00d716 | 4\u00d716\u00d716 | 199M | 26.06 | 0.751 | 0.190 | 423.2 |\n| KL - 8chn | \u2713 | 17\u00d7256\u00d7256 | 4\u00d78\u00d78 | 5\u00d732\u00d732 | 157M | 31.83 | 0.897 | 0.083 | 109.3 |\n| KL - 16chn | \u2713 | 17\u00d7256\u00d7256 | 4\u00d78\u00d78 | 5\u00d732\u00d732 | 157M | 35.04 | 0.942 | 0.047 | 78.9 |\n| KL - 8chn | \u2713 | 17\u00d7256\u00d7256 | 2\u00d78\u00d78 | 9\u00d732\u00d732 | 149M | 33.86 | 0.928 | 0.057 | 80.7 |\n| KL - 4chn | \u2713 | 17\u00d7256\u00d7256 | 4\u00d74\u00d74 | 5\u00d764\u00d764 | 155M | 34.78 | 0.941 | 0.051 | 87.2 |\n| FSQ - 4,096 | \u2713 | 17\u00d7256\u00d7256 | 4\u00d78\u00d78 | 5\u00d732\u00d732 | 157M | 28.36 | 0.832 | 0.133 | 218.1 |\n| FSQ - 32,768 | \u2713 | 17\u00d7256\u00d7256 | 4\u00d78\u00d78 | 5\u00d732\u00d732 | 157M | 29.16 | 0.854 | 0.117 | 196.9 |\n| FSQ - 262,144 | \u2713 | 17\u00d7256\u00d7256 | 4\u00d78\u00d78 | 5\u00d732\u00d732 | 157M | 29.82 | 0.867 | 0.106 | 160.1 |\n| FSQ - 262,144 | \u2713 | 17\u00d7256\u00d7256 | 4\u00d716\u00d716 | 5\u00d716\u00d716 | 199M | 25.38 | 0.738 | 0.206 | 430.1 |\n| FSQ - 262,144 | \\usym2613 | 16\u00d7256\u00d7256 | 4\u00d78\u00d78 | 4\u00d732\u00d732 | 157M | 30.78 | 0.889 | 0.091 | 132.1 |\n| FSQ - 262,144 | \\usym2613 | 16\u00d7256\u00d7256 | 4\u00d716\u00d716 | 4\u00d716\u00d716 | 199M | 26.37 | 0.772 | 0.171 | 357.0 |", "caption": "Table 5: Model summary. We offer a suite of models with diverse configurations, encompassing both continuous and discrete tokenization, various video compression ratios (VCR), and options for causal and non-causal scenarios. These configurations are designed to address the distinct requirements of various downstream tasks.", "description": "This table provides a comprehensive overview of the different VidTok model configurations.  It shows variations based on continuous vs. discrete tokenization, different video compression ratios (how much the video is compressed), whether the model is causal (processes information sequentially) or non-causal (processes information in any order), the latent space size, model parameters, and performance metrics (PSNR, SSIM, LPIPS, and FVD). This variety allows users to select the best model for their specific needs, whether it's high-fidelity reconstruction or efficient compression.", "section": "4.4 MODEL SUMMARY"}]