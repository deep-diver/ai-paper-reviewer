[{"figure_path": "https://arxiv.org/html/2501.09686/x1.png", "caption": "Figure 1: Illustrating different paradigms for annotating LLM reasoning data.", "description": "This figure contrasts two main approaches for creating training datasets for LLMs focused on reasoning: human annotation and automated LLM-based annotation.  Human annotation, while producing high-quality data, is expensive and difficult to scale.  In contrast, automated methods using LLMs are more cost-effective, but the quality of the resulting data can be less reliable, especially for complex, multi-step reasoning tasks.  The figure visually illustrates the process of each annotation method, highlighting their respective costs and scalability.", "section": "3 Data Construction: from Human Annotation to LLM Automation"}, {"figure_path": "https://arxiv.org/html/2501.09686/x2.png", "caption": "Figure 2: Reward models for Train-time Reinforcement of LLM Reasoning.", "description": "This figure illustrates three different reward model approaches used in train-time reinforcement learning for enhancing LLM reasoning capabilities.  (a) shows a direct reinforcement learning setup where rewards are given immediately after an action. (b) demonstrates multi-step reinforcement learning incorporating an outcome reward model (ORM), where a reward is assigned based on the final outcome.  (c) displays a more refined multi-step reinforcement learning approach using a process reward model (PRM), providing step-wise rewards to guide the LLM's reasoning process during intermediate steps.", "section": "Optimizing Pre-trained LLMs: Reinforcement Learning"}, {"figure_path": "https://arxiv.org/html/2501.09686/extracted/6136677/Figures/search.png", "caption": "Figure 3: Diagrams of Different Search Algorithms for Test-time Reasoning Enhancement.", "description": "This figure illustrates four different search algorithms used to enhance reasoning capabilities during the inference phase of large language models (LLMs).  Each algorithm explores different solution paths in a search space, aiming to find the optimal answer given a limited budget of compute during inference.  The diagrams visually represent the search process, showcasing how each algorithm navigates through various reasoning paths to reach a final decision.  The algorithms shown are: Majority Vote, Tree Search, Beam Search, and Lookahead Search.  These algorithms vary in their exploration strategy and computational cost.", "section": "5 Test-time Scaling: from CoTs to PRM Guided Search"}, {"figure_path": "https://arxiv.org/html/2501.09686/extracted/6136677/Figures/Group_4.png", "caption": "Figure 4: Typical training-free test-time enhancing methods: verbal reinforcement search, memory-based reinforcement, and agentic system search.", "description": "Figure 4 illustrates three test-time methods for enhancing Large Language Model (LLM) reasoning without additional training.  These methods leverage the inherent capabilities of LLMs and do not require further fine-tuning or retraining.  Verbal Reinforcement Search (VRS) uses iterative feedback and interaction to refine solutions. Memory-based Reinforcement leverages past experiences stored in an external memory module to inform the next reasoning step. Agentic System Search uses LLMs to search and optimize the structure and parameters of an agentic system, which consists of modules for planning, reasoning, tool usage, and memory.", "section": "7 Other Test-time Enhancing Techniques"}, {"figure_path": "https://arxiv.org/html/2501.09686/x3.png", "caption": "Figure 5: A Taxonomy for LLM Reasoning Benchmarks.", "description": "This figure presents a taxonomy of benchmarks used to evaluate the reasoning capabilities of Large Language Models (LLMs). It categorizes benchmarks into five main problem types: Math Problems, Logic Problems, Commonsense Problems, Coding Problems, and Agent Problems. Each category is further divided into subcategories based on the specific aspects of reasoning being assessed. For example, Math Problems include subcategories like Math Word Problems, Advanced Mathematical Reasoning, Geometric Reasoning, and Cross-Modal Mathematical Reasoning.  The taxonomy visually displays the hierarchical relationships among different benchmark types and sub-types, offering a comprehensive overview of the evaluation landscape for LLM reasoning.", "section": "8 Evaluation Benchmarks"}]