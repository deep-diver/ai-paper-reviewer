[{"figure_path": "https://arxiv.org/html/2504.13146/x1.png", "caption": "Figure 1: Reasoning traces generated via antidistillation sampling poison distillation attempts, while simultaneously preserving the teacher\u2019s downstream performance. The top and bottom rows shows results for MATH\u00a0[7] and GSM8K\u00a0[8], respectively. The left columns shows teacher accuracies under different sampling schemes, and the right column shows the performance of students distilled on traces sampled via these different sampling schemes. Notably, for the same teacher performance on both datasets, antidistillation sampling significantly degrades the performance of the distilled models relative to temperature sampling.", "description": "This figure demonstrates the effectiveness of antidistillation sampling in hindering model distillation while maintaining the teacher model's performance.  It presents the results of experiments on two benchmark datasets: MATH and GSM8K.  For each dataset, two rows display teacher model accuracies (left column) and student model accuracies (right column) achieved through distillation.  The sampling methods compared are baseline, temperature sampling, and antidistillation sampling.  The key finding is that antidistillation sampling substantially reduces student model accuracy compared to temperature sampling, even when the teacher model's accuracy remains similar across both methods.", "section": "3 Empirical Results"}, {"figure_path": "https://arxiv.org/html/2504.13146/x2.png", "caption": "Figure 2: Antidistillation sampling uses a tunable parameter \u03bb\ud835\udf06\\lambdaitalic_\u03bb to control the trade-off between teacher accuracy and distillability.\nThe baseline involves sampling from the teacher with increasing temperature \u03c4\ud835\udf0f\\tauitalic_\u03c4 to show that we can produce traces that are bad for distillation at some cost in teacher accuracy. One important feature of the blue temperature sampling curve is that to bring the student accuracy down below the undistilled accuracy, the teacher performance has to drop to 20%. On the other hand, with antidistillation sampling, the teacher model can still get 70% accuracy while producing traces that bring the student\u2019s performs down below the undistilled accuracy.", "description": "Figure 2 illustrates the tunable parameter \u03bb in antidistillation sampling, which balances teacher accuracy and the effectiveness of distillation.  A baseline using temperature sampling (\u03c4) shows a trade-off: significantly reducing student accuracy requires a substantial drop (to 20%) in teacher accuracy.  In contrast, antidistillation sampling achieves comparable or better student accuracy reduction while maintaining a much higher (70%) teacher accuracy. This highlights the effectiveness of antidistillation sampling in protecting the teacher model's performance.", "section": "3 Empirical Results"}, {"figure_path": "https://arxiv.org/html/2504.13146/x3.png", "caption": "Figure 3: Permutation sampling is a strong baseline where we destroy the information in antidistillation sampling while preserving statistical properties via random permutation and sign flipping.", "description": "Figure 3 compares the performance of three sampling methods: antidistillation sampling, permutation sampling, and temperature sampling, on the GSM8K benchmark.  Antidistillation sampling aims to poison distillation by strategically modifying the model's next-token probability distribution. Permutation sampling acts as a control, demonstrating the impact of destroying information within the antidistillation method, while preserving statistical properties through random permutation and sign-flipping. Temperature sampling is a standard sampling technique used for comparison. The graph shows student accuracy (the accuracy of a model trained via distillation on the teacher model's output) as a function of teacher accuracy (the accuracy of the original teacher model).  The results show that antidistillation sampling significantly reduces student accuracy compared to temperature sampling while maintaining relatively high teacher accuracy. Permutation sampling shows an intermediate performance, suggesting that the strategic poisoning introduced by antidistillation sampling is necessary for its effectiveness.", "section": "3 Empirical Results"}, {"figure_path": "https://arxiv.org/html/2504.13146/x7.png", "caption": "Figure 4: Relative error (ErrorError\\mathrm{Error}roman_Error) between the finite difference and the JVP results.", "description": "This figure displays the relative error between the Jacobian-vector product (JVP) and the finite difference approximation used in the antidistillation sampling method.  The x-axis represents the step size (epsilon) used in the finite difference calculation, and the y-axis represents the relative error between the JVP result (exact gradient calculation) and the finite difference approximation. The graph shows that for a suitably chosen step size epsilon, the finite difference approximation accurately estimates the JVP, demonstrating the validity of this efficient method.", "section": "C Hyperparameters"}]