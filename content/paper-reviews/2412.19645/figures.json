[{"figure_path": "https://arxiv.org/html/2412.19645/x2.png", "caption": "Figure 1: Visualization for our VideoMaker. Our method achieves high-fidelity zero-shot customized human and object video generation based on AnimateDiff\u00a0[26].", "description": "Figure 1 visualizes the capabilities of VideoMaker, a novel framework for zero-shot customized video generation.  The figure showcases example video sequences generated by the method, demonstrating its ability to create high-fidelity videos of both humans and objects.  The top row shows examples of customized human video generation where the model generates videos of a person performing various actions (drinking coffee, reading, working on a laptop, playing guitar), based only on a single reference image. The bottom row demonstrates customized object video generation with examples of panda and dog videos generated from a single reference image, displaying various actions. These results highlight the model's capacity to synthesize realistic and diverse video content based on minimal input, showcasing a significant advancement in zero-shot video generation.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.19645/x3.png", "caption": "Figure 2: Compared with the existing zero-shot customized generation framework. Our framework does not require any additional modules to extract or inject subject features. It only needs simple concatenation of the reference image and generated video, and VDM\u2019s inherent force is used to generate custom video.", "description": "This figure compares the proposed VideoMaker framework with existing zero-shot customized video generation methods.  Existing methods typically employ additional modules for subject feature extraction and injection, often involving complex processes like using a separate ReferenceNet or cross-modal alignment models. In contrast, VideoMaker leverages the inherent capabilities of Video Diffusion Models (VDMs). It directly inputs the reference image and the generated video frames into the VDM, relying on the VDM's internal mechanisms (like spatial self-attention) to extract and inject the subject's features naturally without extra modules.  This streamlined approach is highlighted to demonstrate the simplicity and efficiency of the proposed method.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2412.19645/x4.png", "caption": "Figure 3: Overall pipeline of VideoMaker. We directly input the reference image into VDM and use VDM\u2019s modules for fine-grained feature extraction. We modified the computation of spatial self-attention to enable feature injection. Additionally, to distinguish between reference features and generated content, we designed the Guidance Information Recognition Loss to optimize the training strategy.", "description": "The figure illustrates the architecture of VideoMaker, a novel framework for zero-shot customized video generation.  It leverages the inherent capabilities of Video Diffusion Models (VDMs) rather than relying on external modules. The process begins by directly inputting a reference image into the VDM.  The VDM's internal mechanisms are then used for fine-grained feature extraction from this image.  A key modification involves altering the spatial self-attention mechanism within the VDM to effectively inject these extracted features into the video generation process.  Finally, a novel 'Guidance Information Recognition Loss' is introduced to help the model distinguish between the reference image features and the newly generated video content, thus improving the overall quality and consistency of the generated video.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2412.19645/x5.png", "caption": "Figure 4: Qualitative comparison for customized object video generation. Compared with the blurry videos generated by VideoBooth\u00a0[32], our generated videos have more details.", "description": "Figure 4 presents a qualitative comparison of customized object video generation results between the VideoBooth method and the proposed VideoMaker method.  The figure shows that VideoMaker produces videos with significantly more detail and clarity than VideoBooth.  VideoBooth's output appears blurry and lacks fine details, while VideoMaker generates sharper, more defined videos, highlighting the superior performance of the proposed approach in capturing the nuances of the object appearance and movement.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19645/x6.png", "caption": "Figure 5: Qualitative comparison for customized human video generation. We compare our method with IP-Adapter\u00a0[71], ID-Animator\u00a0[23] and PhotoMaker\u00a0[38]. We observe that our method achieves high-quality generation, promising editability, and subject fidelity.", "description": "Figure 5 presents a qualitative comparison of customized human video generation results from four different methods: the proposed VideoMaker approach and three existing techniques\u2014IP-Adapter, ID-Animator, and PhotoMaker.  For three distinct video generation prompts, the figure showcases sample video frames produced by each method. This allows for a visual assessment of the quality of video generation, including editability and subject fidelity.  The results suggest that VideoMaker outperforms the others in generating high-quality, editable videos that accurately preserve the appearance of the subject.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19645/x7.png", "caption": "Figure 1: The overview of the celebrity dataset we use to test customized human video generation.", "description": "This figure displays a subset of the CelebV-Text dataset used in the paper to evaluate customized human video generation.  It shows a selection of images from various individuals in different poses, demonstrating the diversity of the dataset's human subjects used to test video generation.  The images are intended to illustrate the quality and range of appearances found within the training dataset that the proposed model was trained and tested on.", "section": "5.1. Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2412.19645/x8.png", "caption": "Figure 2: The overview of the dataset we use to test customized object video generation.", "description": "This figure shows the nine object categories used in the VideoBooth dataset for evaluating customized object video generation.  Each category includes two example videos, demonstrating the diverse range of objects and actions covered in the dataset.  The objects depicted include bears, cats, cars, dogs, elephants, horses, lions, pandas, and tigers. The images showcase variations in the environment (e.g., snowy landscape, jungle, beach) and object activities (e.g., walking, running, resting). These images represent the reference videos used as input for the model to generate customized videos.", "section": "5.1. Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2412.19645/extracted/6097614/figures/user_study_human.png", "caption": "Figure 3: The overview of the non-celebrity dataset we used for testing customized human video generation.", "description": "This figure displays the non-celebrity dataset used for evaluating the customized human video generation method.  The dataset consists of 16 images of diverse individuals, each depicting a unique pose and style. The images were sourced from the Unsplash50 dataset and selected to ensure they had not appeared in the training data for the model. These images served as reference inputs for the model, which was tasked with generating videos based on the provided textual prompts. This subset was used to assess the model's generalizability beyond the celebrity-centric data used in primary training and evaluation, providing a more robust measure of performance and capability.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19645/extracted/6097614/figures/user_study_object.png", "caption": "Figure 4: User Study for Customized Human Video Generation.", "description": "This figure displays the results of a user study comparing different methods for customized human video generation.  The study evaluated the generated videos based on four criteria: Text Alignment (how well the generated video matches the text prompt), Subject Fidelity (how accurately the generated video reflects the subject's appearance), Motion Alignment (the quality of movement in the generated video), and Overall Quality (a general assessment of the video's quality).  The bar chart shows the percentage of user preference for each method across these four evaluation criteria. This allows for a direct comparison of the effectiveness of different approaches to customized video generation, highlighting the strengths and weaknesses of each method in terms of satisfying user expectations for realism and accuracy.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19645/x9.png", "caption": "Figure 5: User Study for Customized Object Video Generation.", "description": "This figure displays the results of a user study comparing the quality of customized object video generation between the proposed VideoMaker method and the VideoBooth baseline method.  The user study evaluated four aspects: Subject Fidelity, Text Alignment, Motion Alignment, and Overall Quality.  Each bar represents the percentage of user preference for each method across these four aspects.  The results visually demonstrate the relative performance of VideoMaker and VideoBooth in generating high-quality videos that accurately reflect the specified subject and text prompts, while also displaying smooth and realistic motion.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19645/x10.png", "caption": "Figure 6: More Qualitative comparison for customized human video generation on celebrity dataset.", "description": "This figure provides a qualitative comparison of customized human video generation results on a celebrity dataset.  It shows the input image prompts (a person performing a specific action or wearing certain clothing), and the videos generated by four different methods: IP-Adapter, ID-Animator, PhotoMaker, and the authors' proposed VideoMaker method.  The comparison highlights the differences in video quality, subject fidelity, action consistency, and overall visual realism across the different approaches, showcasing VideoMaker's improved performance in terms of maintaining consistent appearance and accurate action generation.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19645/x11.png", "caption": "Figure 7: More Qualitative comparison for customized human video generation on non-celebrity dataset.", "description": "This figure provides a qualitative comparison of customized human video generation results on a non-celebrity dataset.  It showcases the outputs generated by different methods (IP-Adapter, ID-Animator, PhotoMaker, and the proposed VideoMaker approach) for various prompts. The goal is to visually demonstrate the relative performance of each method in terms of subject fidelity, text alignment, and overall video quality when applied to individuals not included in the training data. The comparison highlights the strengths and weaknesses of each approach when dealing with unseen subjects.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.19645/x12.png", "caption": "Figure 8: More Qualitative comparison for customized human video generation on non-celebrity dataset.", "description": "This figure presents a qualitative comparison of customized human video generation results on a non-celebrity dataset.  It showcases the performance of several methods, including IP-Adapter, ID-Animator, PhotoMaker, and the authors' proposed method (Ours), across various prompts and scenarios.  Each row demonstrates results for a specific prompt, providing a visual comparison of video quality, subject fidelity, and overall realism. This comparison helps to highlight the strengths and weaknesses of each approach in generating videos of non-famous individuals, focusing on aspects like facial details, background consistency, and the accurate depiction of clothing and actions.", "section": "5. Experiments"}]