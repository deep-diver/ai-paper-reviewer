[{"figure_path": "https://arxiv.org/html/2502.15086/x1.png", "caption": "Figure 1: Motivating examples of user-specific safety.", "description": "This figure illustrates two scenarios highlighting the importance of user-specific safety evaluations for Large Language Models (LLMs).  The first example shows a simple question about the dangerous dosage of a drug (Zolpidem). While a general response is considered safe for most, the same response could be harmful to a user with depression. The second example involves a seemingly innocuous request to buy Heineken. For a general user, this is safe.  However, this action could be extremely risky for a user with alcoholism or a pregnant individual. These examples demonstrate that safety evaluations for LLMs must consider individual user profiles and not solely rely on general safety standards.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.15086/x2.png", "caption": "Figure 2: Evaluation protocol of U-SafeBench. (a) and (b) illustrate the protocols for user-specific safety and user-specific helpfulness, respectively. An LLM agent receives a user profile along with an instruction and generates a response considering the provided profile. Subsequently, LLM-as-a-Judge accurately assesses the response\u2019s refusal intent.", "description": "The figure showcases the evaluation protocol used in the U-SafeBench benchmark.  Panel (a) demonstrates the process for assessing user-specific safety. An LLM receives a user profile (e.g., someone with depression) and an instruction. The LLM generates a response.  Then, another LLM, referred to as the \"LLM-as-a-Judge,\" determines if the response was safe for that specific user.  A safe response would be one where the LLM refuses to answer a potentially harmful query. Panel (b) illustrates the parallel process for evaluating user-specific helpfulness, where the LLM-as-a-Judge assesses whether the LLM provided a helpful and appropriate response, taking into account the user's profile.", "section": "2 User-Specific Safety Benchmark: U-SAFEBENCH"}, {"figure_path": "https://arxiv.org/html/2502.15086/x3.png", "caption": "Figure 3: Prompt provided to LLM for the automated harmful instruction collection.", "description": "This figure shows the prompt used to instruct the large language model (LLM) to generate harmful instructions for the U-SAFEBENCH dataset.  The prompt instructs the LLM to act as a user with specific profiles (e.g., someone with depression, an alcohol use disorder, or a criminal record) and to generate questions that, while seemingly innocuous to the general public, could pose a safety risk to individuals with those specific profiles. The goal is to create a diverse set of user instructions for testing the safety of LLMs.", "section": "2.4.2 Harmful Instruction Collection"}, {"figure_path": "https://arxiv.org/html/2502.15086/x6.png", "caption": "Figure 4: Distribution of (a) risk scenarios addressed, (b) task domains U-SafeBench\u00a0targets, and (c) instruction types.", "description": "Figure 4 presents a breakdown of the data included in the U-SAFEBENCH dataset, categorized into three key aspects: risk scenarios, task domains, and instruction types.  Panel (a) illustrates the distribution of user-specific safety risks addressed in the benchmark, showcasing the proportions of instances related to physical health, mental health, and illegal/unethical activity assistance.  Panel (b) highlights the proportions of tasks belonging to question-answering (QA) and autonomous agent scenarios.  Finally, panel (c) shows the distribution of instructions, differentiating between harmful and benign instances within the dataset.", "section": "2 User-Specific Safety Benchmark: U-SAFEBENCH"}, {"figure_path": "https://arxiv.org/html/2502.15086/x7.png", "caption": "Figure 5: Comparison of instruction-following LLM performance in user-specific safety (x\ud835\udc65xitalic_x-axis) and helpfulness (y\ud835\udc66yitalic_y-axis). Model details, such as \u201cit,\u201d are omitted from names due to space constraints.", "description": "Figure 5 is a scatter plot visualizing the relationship between user-specific safety and helpfulness scores achieved by various LLMs (Large Language Models) on the U-SAFEBENCH dataset.  The x-axis represents the user-specific safety score, indicating how well the model avoids generating unsafe responses for specific user profiles. The y-axis represents the user-specific helpfulness score, measuring the usefulness of the model's responses in the context of individual user needs. Each point on the plot represents a different LLM, and its position shows the trade-off between safety and helpfulness. For instance, models positioned in the upper-left quadrant exhibit low safety but high helpfulness, while models in the lower-right quadrant demonstrate high safety but low helpfulness.  The figure reveals a general negative correlation, suggesting that as models are trained to be safer (higher safety scores), their helpfulness tends to decrease.", "section": "3 Benchmarking Results"}, {"figure_path": "https://arxiv.org/html/2502.15086/x8.png", "caption": "Figure 6: Failure cases of Claude-3.5-sonnet on U-SafeBench.", "description": "Figure 6 presents instances where the Claude-3.5-sonnet model, despite exhibiting high overall safety scores, fails to account for user-specific safety considerations.  The examples illustrate instances of responses deemed unsafe due to the specific user profile (e.g., providing information about obtaining alcohol to a user with gout, or suggesting ways to hide smoke to a user with a smoking addiction). The figure showcases vulnerabilities in current LLMs' ability to consistently adhere to user-specific safety standards, even for models with generally high safety performance.", "section": "3.4 Case Studies"}, {"figure_path": "https://arxiv.org/html/2502.15086/x9.png", "caption": "Figure 7: Safety scores of Claude-3.5-sonnet across diverse user profiles. We select profiles with the top 30 and bottom 30 safety scores for analysis.", "description": "This figure visualizes the safety scores achieved by the Claude-3.5-sonnet language model across a diverse range of user profiles.  To highlight the model's performance variations, the profiles are ranked and divided into two groups: the top 30 profiles with the highest safety scores and the bottom 30 profiles with the lowest safety scores. Each bar represents a specific user profile and its corresponding safety score, providing a clear visual representation of how the model's safety performance differs based on user characteristics and potential risk factors.", "section": "3.4 Case Studies"}, {"figure_path": "https://arxiv.org/html/2502.15086/x10.png", "caption": "Figure 8: Complete list of user profiles in U-SafeBench.", "description": "Figure 8 presents a comprehensive list of the 157 user profiles included in the U-SafeBench dataset.  These profiles encompass a wide range of attributes, categorized into 'Criminal Records' and 'Medical Conditions', designed to represent diverse user demographics and potential vulnerabilities.  The profiles are crucial to evaluating the user-specific safety of LLMs, as they represent a variety of individuals and situations that might pose heightened risk for harm when interacting with an LLM. The inclusion of a 'General Population' category provides a baseline for comparison.", "section": "2.4 Dataset Construction"}]