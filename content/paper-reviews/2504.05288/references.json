{"references": [{"fullname_first_author": "Dongzhi Jiang", "paper_title": "MMsearch: Benchmarking the potential of large models as multi-modal search engines", "publication_date": "2024-09-12", "reason": "This paper benchmarks the potential of large models as multimodal search engines, which aligns with the task of visual knowledge-seeking."}, {"fullname_first_author": "Bowen Jin", "paper_title": "Search-r1: Training Ilms to reason and leverage search engines with reinforcement learning", "publication_date": "2025-03-13", "reason": "This paper focuses on the use of search engines in AI, which relates to the LIVEVQA project."}, {"fullname_first_author": "Kenneth Marino", "paper_title": "Ok-vqa: A visual question answering benchmark requiring external knowledge", "publication_date": "2019-01-01", "reason": "This paper presents a visual question answering benchmark requiring external knowledge, similar to LIVEVQA."}, {"fullname_first_author": "OpenAI", "paper_title": "GPT-4o", "publication_date": "2024-01-01", "reason": "This paper introduces the gpt-4o model which is used to generate QA pairs based on raw news document."}, {"fullname_first_author": "An Yang", "paper_title": "Qwen2. 5 technical report", "publication_date": "2024-12-23", "reason": "This paper presents the Qwen2.5, a large multimodal model which is tested in the LIVEVQA."}]}