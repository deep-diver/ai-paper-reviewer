[{"figure_path": "https://arxiv.org/html/2504.15271/x1.png", "caption": "Figure 1: Performance comparison of Eagle 2.5 with leading vision-language models on the Video-MME benchmark. Eagle 2.5 demonstrates consistent improvement as the number of input frames increases.", "description": "This figure showcases a performance comparison between Eagle 2.5 and other leading vision-language models on the Video-MME benchmark.  The x-axis represents the number of input frames used for video comprehension, while the y-axis shows the accuracy achieved on the benchmark.  Eagle 2.5 models of varying sizes (8B and others) are plotted, demonstrating a consistent improvement in performance as the number of input frames increases. This highlights Eagle 2.5's capability to effectively handle and understand long-context video data, outperforming or matching the performance of larger commercial models like GPT-40 and open-source alternatives.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.15271/x2.png", "caption": "Figure 2: Tiling-based general multimodal system.", "description": "This figure illustrates the architecture of the Eagle 2.5 multimodal system.  The system is designed to handle long-context inputs, which consist of text and multiple images or a long video.  The text is first processed through an LLM (Large Language Model). The visual input, which can be a high-resolution image or long video sequence, is divided into tiles using a tiling mechanism (details not shown here, but described in the paper) before undergoing feature extraction. The visual embeddings (extracted features) from SigLIP are then projected into the same representation space as the LLM outputs using an MLP connector layer. These text and vision representations are then combined to enable multimodal understanding and response generation.", "section": "3.1 Model Architecture"}, {"figure_path": "https://arxiv.org/html/2504.15271/x3.png", "caption": "Figure 3: Image area preservation. Compared to the tiling strategy (a) from InternVL\u00a0Team (2024a), our method (b) effectively retains a larger portion of the original image, especially for high-resolution inputs. This ensures that more comprehensive visual information is preserved, benefiting tasks that require fine-grained details.", "description": "This figure compares two image tiling strategies for processing high-resolution images in a vision-language model.  InternVL's method (a) divides the image into a fixed grid of tiles, potentially losing a significant portion of the original image, especially with high-resolution inputs due to rigid aspect ratio constraints.  Eagle 2.5's approach (b), in contrast, prioritizes preserving the original image area and aspect ratio.  The visualization shows how Eagle 2.5's method retains a considerably larger portion of the original image, preserving more comprehensive visual information. This is crucial for tasks demanding fine-grained visual detail.", "section": "3.2.1 Information-First Sampling"}, {"figure_path": "https://arxiv.org/html/2504.15271/x4.png", "caption": "Figure 4: Comparison of video duration between open-source data and Eagle-Video-110K.", "description": "This histogram illustrates the distribution of video durations within the Eagle-Video-110K dataset and a collection of open-source datasets.  The x-axis represents video duration in seconds (log scale), and the y-axis shows the frequency of videos of that length.  The plot allows for a visual comparison of the length of videos present in Eagle-Video-110K versus those found in other, more readily available open-source datasets. This comparison highlights the difference in video lengths, illustrating that Eagle-Video-110K contains significantly more longer videos than typical open-source data.", "section": "3.3 Data Recipe"}, {"figure_path": "https://arxiv.org/html/2504.15271/x5.png", "caption": "Figure 5: Overview of our video annotation framework combining bottom-up clip-level and top-down story-level approaches. The diagram illustrates our dual annotation strategy. In the bottom-up approach (left), short video clips are processed by GPT-4o to generate clip-level QA pairs enhanced with time anchors and textural context anchors. In the top-down approach (right), human annotators create story-level segmentations of longer videos, which are then captioned and processed by GPT-4 to generate comprehensive story-level QA pairs. This hierarchical methodology enables both fine-grained temporal understanding and high-level semantic comprehension of video content.", "description": "Figure 5 illustrates the dual annotation strategy used in the Eagle-Video-110K dataset creation.  The bottom-up approach uses GPT-4 to automatically generate clip-level question-answer (QA) pairs from short video clips.  These QA pairs include time and textual context anchors to provide richer spatiotemporal information. The top-down approach leverages human annotators to segment longer videos into meaningful story-level chapters.  These chapters are then captioned, and GPT-4 is used to generate comprehensive story-level QA pairs. This combined approach ensures both fine-grained temporal and high-level semantic understanding of the video content.", "section": "3.3 Data Recipe"}, {"figure_path": "https://arxiv.org/html/2504.15271/x6.png", "caption": "Figure 6: The impact of Eagle-Video-110K dataset and different post-training schedules on the performance of Video-MME.", "description": "This figure illustrates the performance of Eagle 2.5 on the Video-MME benchmark under different training conditions.  Specifically, it shows how the inclusion of the Eagle-Video-110K dataset and the use of a progressive mixed post-training schedule affect the model's ability to handle varying numbers of input frames (context length).  The results demonstrate that both the additional dataset and the progressive training strategy lead to significant improvements, particularly for longer videos with many frames. The graph likely shows consistent performance gains as the number of frames increases, highlighting the model's enhanced long-context capabilities.", "section": "4. Experiments"}]