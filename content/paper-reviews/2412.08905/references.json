{"references": [{"fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-04-14", "reason": "This paper is the predecessor to the current work and provides foundational context and methodology."}, {"fullname_first_author": "Suriya Gunasekar", "paper_title": "Textbooks are all you need", "publication_date": "2023-06-11", "reason": "This paper introduces the Phi family of models, which the current work builds upon and improves upon."}, {"fullname_first_author": "Mojan Javaheripi", "paper_title": "The surprising power of small language models", "publication_date": "2023-00-00", "reason": "This paper demonstrates the surprising capabilities of smaller language models, which motivates the focus on data quality in the current work."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-00-00", "reason": "This paper introduces the DPO technique, which is a crucial part of the post-training process for phi-4."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This foundational paper introduces the transformer architecture, which is the basis for the phi-4 model architecture."}]}