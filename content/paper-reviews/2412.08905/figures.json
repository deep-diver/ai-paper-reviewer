[{"figure_path": "https://arxiv.org/html/2412.08905/x1.png", "caption": "Figure 1: Average performance of different models on the November 2024 AMC-10 and AMC-12 tests. This is the average score (with maximum score 150) over the four tests on 100 runs with temperature t=0.5\ud835\udc610.5t=0.5italic_t = 0.5. We chose t=0.5\ud835\udc610.5t=0.5italic_t = 0.5 to follow simple-evals\u00a0[24]. Error bars are 2\u2062\u03c32\ud835\udf0e2\\sigma2 italic_\u03c3 of the estimate. On competition math, phi-4\u00a0scores well above its weight-class even compared to non\u2013open-weight models.", "description": "Figure 1 presents a comparison of various large language models' performance on the November 2024 AMC 10/12 mathematics competitions.  The average score (out of 150) across four tests, each run 100 times with a temperature setting of 0.5, is shown for each model.  Phi-4's performance is highlighted, showcasing its superior score relative to its size, even when compared to larger models. Error bars indicate the 2-sigma confidence interval of the average scores.", "section": "1.1 Addressing Overfitting and Data Contamination"}, {"figure_path": "https://arxiv.org/html/2412.08905/extracted/6060244/figures/mmlu_synth_epochs.png", "caption": "Figure 2: 5-shot MMLU score for phase 2 pretraining runs with 4 and 12 epochs of synthetic data. All models are trained for the same token horizon, thus the model with 4 epochs of synthetic has seen more (unique) web tokens. We see that despite many epochs on synthetic data, we do not see overfitting behavior and in fact the 12 epoch models perform better than those that have seen more unique web tokens.", "description": "This figure displays the results of an ablation study on the effect of training epochs on synthetic data in comparison to the number of unique web tokens seen by the model during phase 2 pretraining. The experiment used two model sizes (7B and 14B parameters) and two conditions (4 and 12 epochs of training on the same synthetic data).  The x-axis represents training progress (checkpoints), while the y-axis displays the 5-shot MMLU score. Despite more epochs, increased training on synthetic data does not lead to overfitting; instead, the 12-epoch models consistently outperform the 4-epoch models, indicating that increased exposure to synthetic data is beneficial.", "section": "3.1 Data Composition in Pretraining"}, {"figure_path": "https://arxiv.org/html/2412.08905/x2.png", "caption": "Table 7: Data Mixture for Pivotal Token DPO", "description": "This table shows the composition of the dataset used for the first round of Direct Preference Optimization (DPO).  The DPO process aims to align the model's outputs with human preferences by providing pairs of preferred and less-preferred model responses.  This specific dataset uses the \"pivotal token search\" technique to identify key tokens that significantly impact the model's overall performance. The dataset includes various categories of data, such as data related to unknown topics and safety, generic multiple-choice questions, mathematical problems, and code in various programming languages.", "section": "4.2 Direct Preference Optimization"}]