{"references": [{"fullname_first_author": "James Betker", "paper_title": "Improving image generation with better captions", "publication_date": "2023-01-01", "reason": "This paper is important because it demonstrates how improving image generation with better captions, similar to the approach in the cited paper, benefits the overall performance of the model."}, {"fullname_first_author": "Junsong Chen", "paper_title": "Pixart-a: Fast training of diffusion transformer for photorealistic text-to-image synthesis", "publication_date": "2024-01-01", "reason": "This paper is significant as it provides a baseline architecture (PixArt) that the current paper compares against and builds upon, particularly in the aspects of architecture, text conditioning, and training strategies."}, {"fullname_first_author": "Patrick Esser", "paper_title": "SDXL: Improving latent diffusion models for high-resolution image synthesis", "publication_date": "2023-01-01", "reason": "This paper is important because it details techniques for scaling rectified flow transformers, which directly informs the paper's own approach to high-resolution image synthesis and architectural design."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Auto-encoding variational bayes", "publication_date": "2022-01-01", "reason": "This paper is fundamental for its introduction of variational autoencoders (VAEs), a key component used in the latent diffusion framework that is a building block of the image generation pipeline."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "This paper proposes Diffusion Transformers (DiTs) which is a core component and the primary object of study in the reviewed paper, focusing on architectural choices, text-conditioning strategies, and training protocols in the paper."}]}