[{"content": "| Index | L |  | R |  |\n|---|---|---|---|---|\n| | S | ALS | S | ALS |\n| 1 | 34.06 | 37.18 | 34.53 | 35.52 |\n| 2 | 33.12 | 34.33 | 34.67 | 36.63 |\n| 3 | 32.90 | 33.37 | 31.62 | 34.26 |\n| 4 | 33.47 | 34.41 | 33.68 | 34.29 |\n| 5 | 34.07 | 35.68 | 32.77 | 35.31 |\n| 6 | 32.65 | 34.41 | 32.05 | 32.77 |\n| 7 | 32.71 | 33.43 | 32.68 | 34.66 |", "caption": "Table 1: The quantitative results of the proposed ordering algorithms. S: the simple greedy algorithm, ALS: the adaptive-length subsequence. L and R denote the PSNR of the left and right image in two image pairs from Fig.\u00a04.", "description": "This table presents a quantitative comparison of two proposed image ordering algorithms for 3D super-resolution: the simple greedy algorithm (S) and the adaptive-length subsequence algorithm (ALS).  The algorithms aim to arrange input images into a sequence suitable for video super-resolution (VSR) processing.  The table shows the Peak Signal-to-Noise Ratio (PSNR) values for both the left and right images within two image pairs selected from Figure 4. This illustrates how the different ordering strategies impact the visual quality of the resulting 3D model, specifically concerning the consistency and smoothness of neighboring views.", "section": "4.2 Results"}, {"content": "|       | S     | ALS   |\n|-------|-------|-------|\n| Chair | 32.11 | 32.74 |\n| Drums | 29.74 | 30.26 |\n| Ficus | 35.31 | 35.96 |\n| Hotdog | 37.85 | 38.32 |\n| Lego  | 33.30 | 34.73 |\n| Materials | 35.24 | 35.85 |\n| Mic   | 31.38 | 31.62 |\n| Ship  | 30.03 | 30.48 |", "caption": "Table 2: The comparison of the proposed ordering algorithms in the NeRF-synthetic dataset.", "description": "This table presents a quantitative comparison of two proposed image ordering algorithms: the simple greedy algorithm and the adaptive-length subsequence algorithm.  The algorithms are evaluated on the NeRF-synthetic dataset, a benchmark dataset commonly used for 3D super-resolution tasks.  The comparison is based on the PSNR (Peak Signal-to-Noise Ratio) metric.  Each row represents a different scene from the dataset, and the columns show the PSNR scores achieved by each of the algorithms for that scene. Higher PSNR values indicate better image quality and thus, more effective ordering of the images for subsequent processing. This table demonstrates the improved performance of the adaptive-length subsequence algorithm over the simple greedy algorithm.", "section": "4.2 Results"}, {"content": "| Method | PSNR\u2191 | SSIM\u2191 | LPIPS\u2193 |\n|---|---|---|---| \n| Bicubic | 27.56 | 0.9150 | 0.1040 |\n| SwinIR | 30.77 | 0.9501 | 0.0550 |\n| Render-SR | 28.90 | 0.9346 | 0.0683 |\n| NeRF-SR | 28.46 | 0.9210 | 0.0760 |\n| ZS-SRT\u2020 | 29.69 | 0.9290 | 0.0690 |\n| CROP\u2020 | 30.71 | 0.9459 | 0.0671 |\n| FastSR-NeRF\u2020 | 30.47 | 0.9440 | 0.0750 |\n| DiSR-NeRF | 26.00 | 0.8898 | 0.1226 |\n| SRGS\u2020 | 30.83 | 0.9480 | 0.0560 |\n| GaussianSR\u2020 | 28.37 | 0.9240 | 0.0870 |\n| SuperGaussian\u2020 | 28.44 | 0.9459 | 0.0670 |\n| Ours-ALS | **31.41** | **0.9520** | **0.0540** |\n| 3DGS-HR | 33.31 | 0.9695 | 0.0303 |", "caption": "Table 3: Comparison of different methods for 3D super-resolution (\u00d74\u2192\u00d71\\times 4\\rightarrow\\times 1\u00d7 4 \u2192 \u00d7 1) in Blender Dataset. The numbers marked with \u2020 are sourced from their respective paper, as the code is not available at this time.", "description": "This table presents a quantitative comparison of various 3D super-resolution methods on the Blender dataset.  The task is to upscale low-resolution (LR) images (downsampled by a factor of 4) to high-resolution (HR) images.  The comparison focuses on three key metrics: Peak Signal-to-Noise Ratio (PSNR), Structural SIMilarity index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).  Higher PSNR and SSIM values generally indicate better fidelity to the ground truth, while lower LPIPS indicates better perceptual similarity.  Note that some results are marked with a \u2020 symbol, indicating that those numbers were taken from the original publications and not reproduced by the authors of this paper due to unavailability of the original code.", "section": "4 Experiment"}, {"content": "|       | VRT                     |       |       | IART                    |       |       | PSRT                    |       |       |\n| :---- | :----------------------- | :---- | :---- | :----------------------- | :---- | :---- | :----------------------- | :---- | :---- |\n| SISR  | 31.20                    | 0.9497 | 0.0567 | 31.10                    | 0.9484 | 0.0590 | 31.10                    | 0.9516 | 0.0543 |\n| S     | 31.25                    | 0.9505 | 0.0557 | 31.32                    | 0.9513 | 0.0550 | 31.35                    | 0.9513 | 0.0548 |\n| ALS   | 31.37                    | 0.9516 | 0.0544 | 31.35                    | 0.9514 | 0.0548 | 31.41                    | 0.9520 | 0.0540 |", "caption": "Table 4: Ablation comparison of Blender dataset (\u00d74\u2192\u00d71\\times 4\\rightarrow\\times 1\u00d7 4 \u2192 \u00d7 1) on various VSR models. SISR refers to Single-Image Super-Resolution (single image VSR), S refers to ordering by simple greedy algorithm (order: feature), and ALS refers to using adaptive-length subsequence (order: feature) with multi-threshold (threshold: pose).", "description": "This table presents an ablation study comparing different video super-resolution (VSR) models and ordering strategies for 3D super-resolution.  It shows the performance (PSNR, SSIM, LPIPS) of three VSR models (VRT, IART, PSRT) under three conditions: Single-Image Super-Resolution (SISR) which processes each image independently, a simple greedy ordering algorithm (S) using features for similarity, and an adaptive-length subsequence generation algorithm (ALS) that combines features and pose for ordering, and uses multiple thresholds. The results are evaluated on the Blender dataset with a 4x downsampling factor. This helps to understand which VSR model and ordering approach works best for 3D super-resolution.", "section": "3 Method"}, {"content": "| Metric | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---| \n| S (last 25%) | 31.32 | 0.9511 | 0.0552 |\n| ALS | 31.41 | 0.9520 | 0.0540 |", "caption": "Table 5: Impact of misalignment on 3D super-resolution.", "description": "This table demonstrates the effect of image misalignment on the quality of 3D super-resolution.  It compares the results of a simple greedy algorithm (S) and an adaptive-length subsequence algorithm (ALS) applied to a set of unordered images. The PSNR, SSIM, and LPIPS metrics are presented to assess the image quality resulting from the two different approaches.  The data showcases how the adaptive-length subsequence approach better handles the challenges posed by misaligned sequences, leading to improved 3D super-resolution results.", "section": "3.3 Adaptive-Length Subsequence"}, {"content": "|       | chair | drums | ficus | hotdog | lego | materials | mic | ship | average |\n| :---- | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: | :----: |\n| Bicubic | 29.02 | 23.75 | 28.24 | 31.86 | 27.46 | 26.47 | 27.97 | 25.71 | 27.56 |\n| PSRT (SISR) | 30.94 | 25.56 | 33.49 | 35.82 | 32.20 | 30.06 | 31.75 | 28.96 | 31.10 |\n| SwinIR+3DGS | 31.02 | 25.48 | 32.49 | 35.60 | 32.05 | 29.58 | 31.75 | 28.20 | 30.77 |\n| Render-SR | 30.23 | 24.04 | 28.63 | 33.78 | 29.23 | 27.34 | 30.53 | 27.35 | 28.90 |\n| NeRF-SR | 30.16 | 23.46 | 26.64 | 34.40 | 29.13 | 28.02 | 27.25 | 26.61 | 28.21 |\n| DiSR-NeRF | 27.55 | 22.63 | 25.64 | 30.07 | 26.43 | 24.71 | 26.49 | 24.47 | 26.00 |\n| CROP\u2020 | 31.53 | 24.99 | 31.50 | 35.62 | 32.88 | 29.16 | 31.76 | 28.23 | 30.71 |\n| Ours-S | 31.33 | 25.58 | 33.71 | 35.95 | 32.98 | 30.09 | 31.91 | 29.26 | 31.35 |\n| Ours-ALS | 31.36 | 25.65 | 33.69 | 36.18 | 33.03 | 30.17 | 31.93 | 29.26 | 31.41 |\n| HR-3DGS | 35.79 | 26.14 | 34.84 | 37.72 | 35.77 | 29.97 | 35.36 | 30.89 | 33.31 |", "caption": "Table 6: Per-object PSNR comparison on the synthetic Blender dataset (\u00d74absent4\\times 4\u00d7 4 \u2192\u2192\\rightarrow\u2192 \u00d71absent1\\times 1\u00d7 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).", "description": "This table presents a quantitative comparison of per-object Peak Signal-to-Noise Ratio (PSNR) values on the synthetic Blender dataset.  The dataset was downsampled by a factor of 4 (4x to 1x) before processing.  The comparison includes several baseline methods (Bicubic, SwinIR + 3DGS, Render-SR, NeRF-SR, DiSR-NeRF, CROP) and the authors' method, with and without adaptive-length subsequencing (ALS).  Higher PSNR values indicate better reconstruction quality.", "section": "4.2 Results"}, {"content": "|       | chair | drums | ficus | hotdog | lego | materials | mic | ship | average |\n|-------|-------|-------|-------|--------|------|-----------|-----|------|---------|\n| Bicubic | 0.9194 | 0.9003 | 0.9430 | 0.9526 | 0.9059 | 0.9220 | 0.9481 | 0.8291 | 0.9150 |\n| PSRT (SISR) | 0.9475 | 0.9386 | 0.9762 | 0.9721 | 0.9572 | 0.9544 | 0.9732 | 0.8688 | 0.9516 |\n| SwinIR+3DGS | 0.9469 | 0.9412 | 0.9760 | 0.9728 | 0.9601 | 0.9558 | 0.9747 | 0.8731 | 0.9501 |\n| Render-SR | 0.9432 | 0.9163 | 0.9539 | 0.9677 | 0.9379 | 0.9322 | 0.9671 | 0.8582 | 0.9346 |\n| NeRF-SR | 0.9366 | 0.9019 | 0.9026 | 0.9629 | 0.9292 | 0.9319 | 0.9432 | 0.8357 | 0.9180 |\n| DiSR-NeRF | 0.9035 | 0.8618 | 0.9117 | 0.9332 | 0.8875 | 0.8816 | 0.9335 | 0.8053 | 0.8898 |\n| CROP\u2020 | 0.9513 | 0.9236 | 0.9709 | 0.9725 | 0.9641 | 0.9468 | 0.9740 | 0.8637 | 0.9459 |\n| Ours-S | 0.9538 | 0.9391 | 0.9779 | 0.9738 | 0.9646 | 0.9541 | 0.9747 | 0.8724 | 0.9513 |\n| Ours-ALS | 0.9539 | 0.9405 | 0.9777 | 0.9744 | 0.9649 | 0.9555 | 0.9750 | 0.8741 | 0.9520 |\n| HR-3DGS | 0.9874 | 0.9544 | 0.9872 | 0.9853 | 0.9828 | 0.9603 | 0.9914 | 0.9067 | 0.9694 |", "caption": "Table 7: Per-object SSIM comparison on the synthetic Blender dataset (\u00d74absent4\\times 4\u00d7 4 \u2192\u2192\\rightarrow\u2192 \u00d71absent1\\times 1\u00d7 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).", "description": "This table presents a per-object comparison of the Structural Similarity Index (SSIM) metric. The comparison is made for different methods on the synthetic Blender dataset, where low-resolution images (LR) are upscaled to high-resolution (HR) images with a factor of 4 (4x super-resolution).  The methods compared include Bicubic interpolation, SwinIR (a state-of-the-art single-image super-resolution model combined with 3D Gaussian Splatting), Render-SR (a method that renders smooth video using LR images and upsamples), NeRF-SR, DiSR-NeRF (both methods specialized for Neural Radiance Fields), CROP, and the authors' methods (Ours-S and Ours-ALS).  Ours-ALS uses their proposed adaptive-length subsequencing approach for improved results. The SSIM values are shown for eight different objects in the dataset (chair, drums, ficus, hotdog, lego, materials, mic, ship), along with the average SSIM across all objects. This allows readers to evaluate and compare the performance of each method on an object-by-object basis and overall.", "section": "4.2 Results"}, {"content": "|             | chair | drums | ficus | hotdog | lego  | materials | mic  | ship  | average |\n|-------------|-------|-------|-------|--------|-------|-----------|------|-------|---------|\n| Bicubic     | 0.0899 | 0.1106 | 0.0619 | 0.0768 | 0.1272 | 0.0892     | 0.0626 | 0.2136 | 0.1040  |\n| PSRT (SISR) | 0.0553 | 0.0609 | 0.0237 | 0.0421 | 0.0595 | 0.0480     | 0.0254 | 0.1567 | 0.0544  |\n| SwinIR+3DGS | 0.0577 | 0.0565 | 0.0221 | 0.0401 | 0.0498 | 0.0420     | 0.0203 | 0.1511 | 0.0550  |\n| Render-SR   | 0.0563 | 0.0743 | 0.0396 | 0.0462 | 0.0691 | 0.0597     | 0.0312 | 0.1698 | 0.0683  |\n| NeRF-SR     | 0.0687 | 0.1091 | 0.1014 | 0.0591 | 0.0976 | 0.0770     | 0.0805 | 0.1984 | 0.0990  |\n| DiSR-NeRF   | 0.0943 | 0.1429 | 0.0905 | 0.1001 | 0.1378 | 0.1293     | 0.0751 | 0.2106 | 0.1226  |\n| CROP\u2020       | 0.0567 | 0.0856 | 0.0317 | 0.0481 | 0.0496 | 0.0622     | 0.0251 | 0.1776 | 0.0671  |\n| Ours-S      | 0.0478 | 0.0585 | 0.0216 | 0.0395 | 0.0470 | 0.0488     | 0.0240 | 0.1509 | 0.0547  |\n| Ours-ALS    | 0.0478 | 0.0576 | 0.0216 | 0.0388 | 0.0465 | 0.0464     | 0.0233 | 0.1501 | 0.0540  |\n| HR-3DGS     | 0.0117 | 0.0371 | 0.0116 | 0.0199 | 0.0154 | 0.0341     | 0.0060 | 0.1063 | 0.0303  |", "caption": "Table 8: Per-object LPIPS comparison on the synthetic Blender dataset (\u00d74absent4\\times 4\u00d7 4 \u2192\u2192\\rightarrow\u2192 \u00d71absent1\\times 1\u00d7 1). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).", "description": "This table presents a quantitative comparison of different methods for 3D super-resolution on the synthetic Blender dataset, specifically focusing on the perceptual similarity metric LPIPS (Learned Perceptual Image Patch Similarity).  Lower LPIPS values indicate better perceptual similarity to the ground truth.  The results are broken down by object category within the dataset, and each method's performance is reported.  The 'Ours-ALS' column represents the performance of the authors' proposed method using an adaptive-length subsequence algorithm.", "section": "4.2 Results"}, {"content": "| Method | bicycle | flowers | garden | stump | treehill | room | counter | kitchen | bonsai | average |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Bicubic | 24.02 | 21.24 | 25.14 | 26.30 | 22.25 | 30.47 | 28.15 | 28.23 | 30.21 | 26.22 |\n| SwinIR + 3DGS | 24.54 | 21.18 | 25.81 | 26.38 | 22.16 | 31.30 | 28.71 | 29.82 | 31.26 | 26.80 |\n| Ours-S | 24.42 | 21.13 | 26.04 | 26.40 | 22.26 | 31.47 | 28.96 | 30.79 | 31.69 | 27.02 |\n| Ours-ALS | 24.50 | 21.17 | 25.99 | 26.46 | 22.26 | 31.52 | 28.90 | 30.73 | 31.68 | 27.02 |\n| HR-3DGS | 24.41 | 20.59 | 26.58 | 26.28 | 22.27 | 31.52 | 29.12 | 31.57 | 32.36 | 27.19 |", "caption": "Table 9: Per-scene PSNR comparison on the Mip-NeRF 360 dataset (\u00d78absent8\\times 8\u00d7 8 \u2192\u2192\\rightarrow\u2192\u00d72absent2\\times 2\u00d7 2). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).", "description": "Table 9 presents a per-scene comparison of Peak Signal-to-Noise Ratio (PSNR) values for the Mip-NeRF 360 dataset. The comparison is made between different super-resolution methods and focuses on the results achieved with a downsampling factor of 8 and upsampling factor of 2.  The table shows PSNR values obtained for various scenes within the Mip-NeRF 360 dataset, including \"bicycle,\" \"flowers,\" \"garden,\" \"stump,\" \"treehill,\" \"room,\" \"counter,\" \"kitchen,\" \"bonsai.\"  The results for the proposed method using adaptive-length subsequence generation (Ours-ALS) are highlighted. Additionally, results for a baseline bicubic interpolation, SwinIR + 3DGS, and the ground truth (HR-3DGS) are included for comparison. This detailed per-scene analysis allows assessing the effectiveness of the proposed super-resolution technique across different scene types in the Mip-NeRF 360 dataset. ", "section": "4.2 Results"}, {"content": "| |bicycle|flowers|garden|stump|treehill|room|counter|kitchen|bonsai|average|\n|---|---|---|---|---|---|---|---|---|---|---|\n|Bicubic|0.6401|0.5321|0.6648|0.7324|0.5880|0.8877|0.8573|0.8128|0.8980|0.7348|\n|SwinIR + 3DGS|0.6810|0.5498|0.7259|0.7468|0.6020|0.9063|0.8837|0.8724|0.9235|0.7657|\n|Ours-S|0.6752|0.5512|0.7476|0.7481|0.6048|0.9123|0.8936|0.9071|0.9328|0.7747|\n|Ours-ALS|0.6783|0.5503|0.7462|0.7467|0.6028|0.9123|0.8918|0.9062|0.9323|0.7741|\n|HR-3DGS|0.7007|0.5445|0.8173|0.7571|0.6269|0.9263|0.9144|0.9325|0.9465|0.7962|", "caption": "Table 10: Per-scene SSIM comparison on the Mip-NeRF 360 dataset (\u00d78absent8\\times 8\u00d7 8 \u2192\u2192\\rightarrow\u2192\u00d72absent2\\times 2\u00d7 2). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).", "description": "This table presents a comparison of the per-scene Structural Similarity Index (SSIM) scores achieved by different methods on the Mip-NeRF 360 dataset.  The dataset's resolution was reduced by a factor of 8 (8x downsampling) and then upsampled to half the original resolution (2x upsampling).  The methods compared include a bicubic upsampling baseline, SwinIR + 3DGS, the authors' proposed method using a simple greedy algorithm (Ours-S), the authors' proposed method with adaptive-length subsequence generation (Ours-ALS), and finally the ground truth result from the full-resolution 3DGS model (HR-3DGS). The comparison shows how different methods perform across various scenes from the Mip-NeRF 360 dataset in terms of SSIM.  The results demonstrate that the authors' method, especially the version with adaptive-length subsequencing, achieves comparable performance to SwinIR + 3DGS and surpasses bicubic upsampling, while falling slightly below the ground truth SSIM scores.", "section": "4.2 Results"}, {"content": "|                   | bicycle | flowers | garden | stump | treehill | room | counter | kitchen | bonsai | average |\n|-------------------|----------|----------|---------|--------|-----------|-------|----------|----------|--------|---------|\n| Bicubic            | 0.3688   | 0.4315   | 0.3469  | 0.3334 | 0.4391    | 0.275 | 0.2671   | 0.2598   | 0.2392 | 0.3290  |\n| SwinIR + 3DGS     | 0.3220   | 0.4065   | 0.2784  | 0.3098 | 0.4116    | 0.235 | 0.2216   | 0.1973   | 0.2035 | 0.2873  |\n| Ours-S             | 0.3344   | 0.4091   | 0.2613  | 0.3142 | 0.4162    | 0.222 | 0.2074   | 0.1536   | 0.1927 | 0.2790  |\n| Ours-ALS           | 0.3261   | 0.4062   | 0.2607  | 0.3117 | 0.4134    | 0.222 | 0.2104   | 0.1542   | 0.1925 | 0.2774  |\n| HR-3DGS           | 0.3230   | 0.4188   | 0.1777  | 0.3130 | 0.3997    | 0.193 | 0.1800   | 0.1136   | 0.1758 | 0.2550  |", "caption": "Table 11: Per-scene LPIPS comparison on the Mip-NeRF 360 dataset (\u00d78absent8\\times 8\u00d7 8 \u2192\u2192\\rightarrow\u2192\u00d72absent2\\times 2\u00d7 2). Ours-ALS refers to our method using adaptive-length subsequencing (ALS).", "description": "This table presents a per-scene comparison of the LPIPS (Learned Perceptual Image Patch Similarity) metric on the Mip-NeRF 360 dataset.  The dataset's low-resolution images were upscaled by a factor of 4 (8x to 2x).  The table compares the LPIPS scores achieved by different methods, including bicubic upsampling, SwinIR + 3DGS (Swin Transformer + 3D Gaussian Splatting), the proposed method (Ours-S using a simple greedy algorithm, and Ours-ALS using an adaptive-length subsequence), and the ground truth (HR-3DGS).  LPIPS is a measure of perceptual similarity; lower scores represent better visual quality.", "section": "4.2 Results"}, {"content": "| Method | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 |\n|---|---|---|---| \n| Bicubic | 26.22 | 0.7349 | 0.3290 |\n| SwinIR | 26.80 | 0.7657 | 0.2873 |\n| SRGS<sup>\u2020</sup> | 26.88 | 0.7670 | 0.2860 |\n| Ours | **27.02** | **0.7747** | **0.2790** |\n| 3DGS-HR | 27.19 | 0.7710 | 0.2802 |", "caption": "Table 12: Comparison with baseline models in Mip-NeRF 360 dataset (\u00d78absent8\\times 8\u00d7 8 \u2192\u2192\\rightarrow\u2192 \u00d72absent2\\times 2\u00d7 2).", "description": "This table presents a quantitative comparison of the proposed method's performance against several baseline models on the Mip-NeRF 360 dataset.  The models' performance is evaluated on the task of 3D super-resolution, specifically upscaling low-resolution (LR) images by a factor of 8 to higher resolution (HR) images (8x to 2x). The metrics used for comparison include PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and LPIPS (Learned Perceptual Image Patch Similarity). Higher PSNR and SSIM values indicate better reconstruction quality, while a lower LPIPS value signifies a better perceptual similarity to the ground truth.", "section": "4 Experiment"}, {"content": "| Method | FVD\u2193 | PSNR\u2191 |\n|---|---|---|\n| Bicubic | 195 | 27.56 |\n| SwinIR | 113 | 30.77 |\n| Render-SR | 134 | 28.90 |\n| NeRF-SR | 169 | 28.21 |\n| DiSR-NeRF | 304 | 26.00 |\n| Ours-S | 110 | 31.35 |\n| Ours-ALS | 109 | 31.41 |", "caption": "Table 13: Temporal Consistency and Spatial Quality Metrics on Blender Dataset.", "description": "This table presents a comparison of different methods for 3D super-resolution, focusing on temporal consistency and spatial quality.  It shows the Fr\u00e9chet Video Distance (FVD) scores, which measure how temporally consistent the generated videos are, and the Peak Signal-to-Noise Ratio (PSNR) values, which indicate the spatial quality of the reconstructed 3D models. Lower FVD indicates better temporal consistency, and higher PSNR signifies better spatial quality. The table helps to assess the effectiveness of each method in producing temporally coherent and visually pleasing 3D reconstructions.", "section": "Experiment"}]