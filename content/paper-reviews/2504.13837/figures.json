[{"figure_path": "https://arxiv.org/html/2504.13837/x1.png", "caption": "Figure 1: \n(Left) The effect of RLVR on LLM\u2019s reasoning ability.\nSearch trees are generated by repeated sampling from the base and RLVR-trained models for a given problem.\nGrey indicates paths that are unlikely to be sampled by the model, while black indicates paths that are likely to be sampled. Green indicates correct paths, which has positive rewards.\nOur key finding is that all reasoning paths in the RLVR model are already present in the base model.\nFor certain problems like Problem A, RLVR training biases the distribution toward rewarded paths, improving sampling efficiency.\nHowever, this comes at the cost of reduced scope of reasoning capacity: For other problems like Problem B,\nthe base model contains the correct path, whereas that of the RLVR model does not.\n(Right)\nAs RLVR training progresses, the average performance (i.e., pass@1) improves, but the coverage of solvable problems (i.e., pass@256) decreases, indicating a reduction in the model\u2019s reasoning upper bound.", "description": "Figure 1 illustrates the impact of Reinforcement Learning with Verifiable Rewards (RLVR) on Large Language Model (LLM) reasoning.  The left panel shows search trees for two example problems (A and B).  Nodes represent steps in a reasoning process.  Black nodes represent frequently sampled paths by both the base and RLVR-trained models, grey indicates rarely sampled paths. Green nodes represent correct solutions. The figure demonstrates that RLVR doesn't add new reasoning paths; all paths in the RLVR model already exist in the base model.  RLVR improves efficiency for certain problems (A) by biasing the model toward rewarded paths. However, this comes at the cost of reduced exploration, as seen with problem B, where the base model still contains the correct solution path, even though the RLVR model does not. The right panel shows pass@k curves for several LLMs. Pass@k measures the fraction of problems solvable with k samples.  It shows that as RLVR training progresses, pass@1 (average performance) improves, but pass@256 (coverage of all solvable problems) decreases, indicating that the model explores fewer unique reasoning paths.", "section": "3. RLVR's Effect on Reasoning Capacity Boundary"}, {"figure_path": "https://arxiv.org/html/2504.13837/x2.png", "caption": "Figure 2: \nPass@k\ud835\udc58kitalic_k curves of base models and their zero-RL-trained counterparts across multiple mathematical benchmarks.\nWhen k\ud835\udc58kitalic_k is small, RL-trained models outperform their base versions.\nHowever, as k\ud835\udc58kitalic_k increases to the tens or hundreds, base models consistently catch up with RL-trained models across all benchmarks and LLM families without exception.\nEventually, base models surpass RL-trained models.", "description": "Figure 2 presents a comparative analysis of the pass@k metric across various mathematical benchmarks, utilizing both base and reinforcement learning (RL)-trained large language models (LLMs).  The x-axis represents the number of samples (k), signifying the number of attempts the model gets to solve a problem. The y-axis displays the pass@k score, indicating the percentage of problems correctly solved within k attempts.  The figure illustrates that while RL-trained models initially demonstrate superior performance with a small number of attempts (low k), their advantage diminishes as the number of attempts increases. Notably, base models consistently catch up to and eventually outperform RL-trained models when a sufficiently large number of samples are allowed (k in the tens or hundreds), demonstrating that the reasoning capacity of RL-trained models does not inherently surpass that of base models. This effect is observed across diverse benchmarks and multiple LLM families.", "section": "3. RLVR's Effect on Reasoning Capacity Boundary"}, {"figure_path": "https://arxiv.org/html/2504.13837/x3.png", "caption": "Figure 3: \nOat-Zero for AIME24.", "description": "The figure shows the performance comparison between the base Qwen-2.5-Math-7B model and its RL-trained counterpart (Oat-Zero-7B) on the AIME24 mathematics benchmark.  The x-axis represents the number of samples (k) used in the pass@k metric, and the y-axis represents the coverage (pass@k).  The graph illustrates that while the RL-trained model initially outperforms the base model for small k values (meaning it solves more problems with fewer attempts), the base model eventually surpasses the RL model as k increases. This demonstrates that the RL training biases the model towards more efficient sampling of correct answers, but doesn't expand the overall reasoning ability beyond that already present in the base model.", "section": "3.1 RLVR for Mathematical Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.13837/x4.png", "caption": "Figure 4: RLVR for Coding.", "description": "Figure 4 presents the results of the experiments evaluating the effect of Reinforcement Learning with Verifiable Rewards (RLVR) on code generation tasks. The graph displays the pass@k curves for both the base and RLVR-trained models. The x-axis represents the number of samples (k), while the y-axis shows the coverage (pass@k). The figure demonstrates that although RLVR improves the performance at small k values (e.g., k=1), the base model consistently surpasses the RLVR-trained model at larger k values, suggesting that RLVR does not expand the reasoning capabilities of LLMs beyond those of the base model, but only improves the sampling efficiency.", "section": "3.2. RLVR for Code Generation"}, {"figure_path": "https://arxiv.org/html/2504.13837/x5.png", "caption": "Figure 5: \nPass@k\ud835\udc58kitalic_k curves of base models and zero-RL counterparts.\n(Left) Code Generation.\n(Right) Visual Reasoning.", "description": "This figure displays the performance of base LLMs and their counterparts trained with zero-RL (Reinforcement Learning) across two distinct tasks: code generation and visual reasoning.  The x-axis represents the number of samples (k) used to determine if a problem is solved (pass@k), while the y-axis indicates the pass@k score, showing the proportion of problems solved within k attempts. Separate graphs are provided for each task, illustrating the performance of base models and their corresponding zero-RL trained versions across a range of sample sizes (k values). This allows for a comparison of the reasoning capabilities between base models and RL-trained models, considering both the average-case performance (low k) and the ability to find correct solutions with increased effort (high k).", "section": "3. RLVR's Effect on Reasoning Capacity Boundary"}, {"figure_path": "https://arxiv.org/html/2504.13837/x6.png", "caption": "Figure 6: \n(Left) Perplexity distribution of responses from different sources, evaluated by the base and RL models. The conditioning problem x\ud835\udc65xitalic_x is omitted in the figure.\n(Right) Coverage comparison of base, Instruct, RL, and distilled models.", "description": "Figure 6 presents a comparative analysis of model performance using perplexity and pass@k metrics.  The left panel shows the perplexity distributions of responses generated by three different methods (base model, RL-trained model, and human-generated responses) as evaluated by both the base and RL models.  Lower perplexity indicates higher likelihood of generation, showing whether the RL model's responses align with patterns in the base model's generation. The right panel displays the pass@k scores for four model types: a base model, an instruction-tuned model, an RL-trained model, and a model created by knowledge distillation from a strong model. Pass@k represents the proportion of problems successfully solved when given k attempts. This part demonstrates the effects of different training methods on solving problems.", "section": "4. Deep Analysis"}, {"figure_path": "https://arxiv.org/html/2504.13837/x7.png", "caption": "Figure 7: \n(Top) Different RL algorithms.\n(Bottom) Different RL training steps.\nWe use a folded y-axis range to better highlight the details at k=1\ud835\udc581k=1italic_k = 1 and 256256256256.\nUnfolded version can be found in\u00a0Figure\u00a08.\nThe detailed values for each point at pass@1 and pass@256 are provided in\u00a0Table\u00a02 and\u00a0Table\u00a03.", "description": "This figure compares the performance of different reinforcement learning (RL) algorithms and different RL training durations on three tasks: Omni-MATH-Train, Omni-MATH-Test, and MATH500.  The top panel shows the pass@k curves for various RL algorithms, highlighting their differences in sampling efficiency. The bottom panel illustrates how the performance changes with varying numbers of RL training steps, showing that increasing steps doesn't necessarily lead to better reasoning. The y-axis uses a folded scale for better visualization of small k values (k=1) and large k values (k=256), with the unfolded version shown in Figure 8.  Detailed data is available in Tables 2 and 3.", "section": "4.3. Effects of Different RL Algorithms"}, {"figure_path": "https://arxiv.org/html/2504.13837/x8.png", "caption": "Figure 8: \nUnfolded y-axis version of\u00a0Figure\u00a07.", "description": "Figure 8 provides an expanded view of the data presented in Figure 7, showing the performance of different reinforcement learning algorithms across various metrics and datasets. The y-axis, representing performance metrics (such as pass@k), is no longer compressed, enabling a clearer view of the differences between the RL methods, particularly at lower values of k (number of samples). The figure is crucial for a detailed comparison of the algorithms' sampling efficiency and their ability to solve problems, both within and outside the training domain. It complements the main findings of the study.", "section": "4.3. Effects of Different RL Algorithms"}, {"figure_path": "https://arxiv.org/html/2504.13837/x9.png", "caption": "Figure 9: \nPass@k\ud835\udc58kitalic_k curves of the base and RL models in the filtered AIME24.", "description": "This figure compares the performance of base and reinforcement learning (RL)-trained LLMs on the AIME24 mathematical reasoning benchmark.  The x-axis represents the number of samples (k) used to determine if a problem is solved (pass@k). A problem is considered solved if at least one of the k samples produces a correct answer. The y-axis shows the pass@k score, indicating the percentage of problems solved within k samples. The curves for base and RL models illustrate how their problem-solving ability changes as the number of samples increases.  The goal is to assess whether RL training significantly expands the model's reasoning capacity beyond that of the base model.", "section": "3.1 RLVR for Mathematical Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.13837/x10.png", "caption": "Figure 10: \nWe found that the base model\u2019s performance drops when the temperature exceeds 1.0, as it tends to generate more random and less coherent tokens. In contrast, the RL model\u2019s performance remains relatively stable across different temperature settings. Therefore, we use T=0.6\ud835\udc470.6T=0.6italic_T = 0.6 in the main experiments, as it allows both models to demonstrate their best reasoning performance.", "description": "Figure 10 shows the impact of different temperature settings on the performance of both base and RL-trained LLMs.  The base model's performance significantly degrades when the temperature parameter exceeds 1.0, which is attributed to the generation of more random and incoherent text. In contrast, the RL model maintains relatively consistent performance across various temperature settings. This observation led the researchers to choose a temperature of 0.6 for their main experiments, as this setting allowed both models to showcase their optimal reasoning abilities.", "section": "3. RLVR's Effect on Reasoning Capacity Boundary"}, {"figure_path": "https://arxiv.org/html/2504.13837/x11.png", "caption": "Figure 11: \nPrompt for SimpleRL Training and Evaluation.\nThe base model uses the same prompt as the RL model during evaluation.", "description": "Figure 11 shows the prompt used for training and evaluating the SimpleRL model. The prompt instructs the model to act as a helpful assistant and to answer a given question using step-by-step reasoning, placing the final answer within a box.  The caption highlights that during evaluation, the base model (without reinforcement learning) is given the same prompt as the RL-trained model. This is crucial for a fair comparison of the reasoning capabilities between the models.  The uniformity in prompt ensures any observed differences are attributed to model training, not variations in the prompts.", "section": "B. Prompt Templates"}, {"figure_path": "https://arxiv.org/html/2504.13837/x12.png", "caption": "Figure 12: \nPrompt for Oat-Zero training and evaluation.", "description": "This figure shows the prompt template used for training and evaluating the Oat-Zero model.  The prompt instructs the model to act as a helpful assistant and to solve a given question by reasoning step-by-step. The final answer must be enclosed within a box.  This standardized prompt ensures consistent input formatting during both training and evaluation phases of the Oat-Zero model.", "section": "B. Prompt Templates"}, {"figure_path": "https://arxiv.org/html/2504.13837/x13.png", "caption": "Figure 13: \nPrompt for Code-R1 training.", "description": "The prompt instructs the language model to act as a helpful programming assistant.  It specifies that the user will pose a question, and the assistant should first determine the solution through reasoning before providing the answer. The reasoning process and the final answer must be clearly delineated using the tags `<think>...</think>` and `<answer>...</answer>`, respectively. This formatted structure helps to separate the model's thought process from the final output, making it easier to analyze and evaluate the model's reasoning abilities.", "section": "3.2. RLVR for Code Generation"}, {"figure_path": "https://arxiv.org/html/2504.13837/x14.png", "caption": "Figure 14: \nPrompt for Code-R1 Evaluation on LiveCodeBench.", "description": "This figure displays the prompt template used for evaluating the Code-R1 model on the LiveCodeBench benchmark.  The prompt instructs the model to act as a helpful programming assistant, providing a problem specification and generating correct Python code that passes all tests. The prompt includes formatting instructions and placeholders for the question and the code generated by the model. The structure of the prompt is designed to ensure the model generates only the code and nothing else, focusing the model's output strictly on the programming task itself.", "section": "3.2. RLVR for Code Generation"}, {"figure_path": "https://arxiv.org/html/2504.13837/x15.png", "caption": "Figure 15: \nPrompt for Code-R1 Evaluation on HumanEval+ and MBPP+.", "description": "This figure displays the prompt used for evaluating the Code-R1 model on the HumanEval+ and MBPP+ benchmarks.  The prompt instructs the model to act as a helpful programming assistant, providing a self-contained Python script that solves a given problem. The solution should be presented within a markdown code block, ensuring the script's integrity and testability. This setup emphasizes the model's ability to generate functional code while adhering to specific formatting guidelines.", "section": "3.2. RLVR for Code Generation"}, {"figure_path": "https://arxiv.org/html/2504.13837/x16.png", "caption": "Figure 16: \nPrompt for EasyR1 training and evaluation.", "description": "This figure shows the prompt template used for training and evaluating the EasyR1 model.  The prompt instructs the model to first engage in a chain-of-thought reasoning process, which is enclosed within `<think></think>` tags, before providing the final answer within a \\boxed{} tag. The prompt also includes an image token `<vision_start>{image_token}</vision_end>` indicating that the model is designed for visual reasoning tasks where both textual and visual inputs are provided.", "section": "3. RLVR's Effect on Reasoning Capacity Boundary"}, {"figure_path": "https://arxiv.org/html/2504.13837/x17.png", "caption": "Figure 17: \n(1)\nPrompt for VeRL training and evaluation.", "description": "This figure displays the prompt template used for training and evaluating the VeRL (Visual Reinforcement Learning) model.  It shows the instructions given to the language model, specifying that it should act as a helpful and harmless assistant, think step-by-step, and provide its final answer within a boxed area. The template guides the model to generate responses that clearly lay out the reasoning process before presenting the final answer. This structured prompt helps to ensure consistent and easily-analyzed responses, facilitating a fair and effective evaluation of the model's performance.", "section": "VeRL Training and Evaluation Prompt"}, {"figure_path": "https://arxiv.org/html/2504.13837/x18.png", "caption": "Figure 18: Qwen-2.5-Base-7B Correct Response - Case 1.", "description": "This figure shows a sample correct response generated by the Qwen-2.5-Base-7B model for question 16 of the AIME24 benchmark.  The response demonstrates a multi-step reasoning process using the principle of inclusion-exclusion to solve a problem involving the number of residents in a town who own specific items. The solution includes multiple attempts, showing the model's process of refinement and correction of errors in its initial calculation. The final answer is correctly identified as 73.", "section": "3.1 RLVR for Mathematical Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.13837/x19.png", "caption": "Figure 19: Qwen-2.5-Base-7B Correct Response - Case 2.", "description": "This figure displays a sample correct response generated by the Qwen-2.5-Base-7B model for question 24 of the AIME24 benchmark.  The question involves determining the number of ways to arrange digits in a 2x3 grid such that the sum of the two numbers (read left to right) is 999 and the sum of three numbers (read top to bottom) is 99. The figure shows the model's detailed step-by-step reasoning process which uses the principle of inclusion-exclusion to solve this mathematical problem.  The solution method and the final answer (45) are presented.", "section": "3.1 RLVR for Mathematical Reasoning"}]