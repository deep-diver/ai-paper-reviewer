[{"heading_title": "RLVR Limits", "details": {"summary": "**RLVR's potential is capped by the base model's inherent knowledge.** It can refine existing reasoning but struggles to introduce truly novel skills. The vast action space makes exploration challenging, favoring exploitation of prior knowledge. This leads to **diminishing returns** as the model becomes too specialized, **sacrificing broader problem-solving capabilities.** Distillation, by contrast, transfers knowledge, extending reasoning boundaries."}}, {"heading_title": "Pass@K Boundary", "details": {"summary": "The concept of a \"Pass@K Boundary\" is crucial when evaluating reasoning capabilities of language models. It moves beyond simple success rates, recognizing that a model's potential isn't fully captured by single-attempt failures. Instead, it measures performance across multiple attempts (K), revealing a model's ability to solve difficult problems with sufficient sampling. **This boundary defines the limit of a model's reasoning capacity**, showing what it can achieve given ample opportunities. Comparing base models to reinforced learning models using this boundary highlights whether RL truly expands reasoning or merely improves efficiency. **A higher boundary suggests a broader reasoning scope**, while a lower boundary indicates limitations, regardless of initial success. This metric is important for understanding the real impact of training strategies on LLMs."}}, {"heading_title": "Sample Efficiency", "details": {"summary": "Sample efficiency is crucial in reinforcement learning (RL), especially within large language models (LLMs), given the computational costs. Improving it means achieving optimal performance with minimal data, which directly impacts training time and resource consumption. The research paper highlights that **RL with verifiable rewards can improve sampling efficiency.** But it is also highlighted that the LLM has a better output for small k, and not for large k. RL trains by biasing the model which makes a more **efficient search** for the higher number of rewards, thus **improving the result for pass@1**. This indicates that sample efficiency is not just about the quantity of data but how effectively the RL algorithm utilizes the data and explores space."}}, {"heading_title": "Base Model Bound", "details": {"summary": "The base model's inherent capabilities place a significant bound on what RLVR can achieve. **RLVR primarily refines the sampling efficiency within the pre-existing solution space of the base model, rather than expanding its capacity**. The base model's knowledge and reasoning skills acts as a ceiling, since **RLVR struggles to discover fundamentally new strategies that the base model could not, in principle, generate**. It is vital to consider the limitations in applying RLVR alone and **investigate innovative methods to enhance the base model's capabilities before RLVR-finetuning**. "}}, {"heading_title": "Distill vs. RLVR", "details": {"summary": "**Distillation offers a distinct advantage over RLVR in enhancing LLM reasoning**. While RLVR primarily improves sampling efficiency by biasing the model towards high-reward paths already within its existing capacity, distillation introduces genuinely *new knowledge* into the model. By learning from a more powerful 'teacher' model, the distilled model can *expand its reasoning capability beyond the limitations of the base model*. This contrast highlights that RLVR's focus on *refining existing capabilities* differs fundamentally from distillation's ability to *impart novel reasoning patterns*, making distillation a more promising avenue for transcending the inherent boundaries of a given LLM architecture."}}]