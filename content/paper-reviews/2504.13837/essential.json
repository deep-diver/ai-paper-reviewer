{"importance": "**This work challenges current understanding of Reinforcement Learning(RL)'s role in advancing LLM reasoning.** It opens new avenues for rethinking RL training and exploring better paradigms to push the boundary of reasoning abilities in LLMs, marking a shift in focus for future AI research.", "summary": "RLVR doesn't truly incentivize new reasoning in LLMs; it boosts efficiency within existing base model capabilities.", "takeaways": ["RLVR-trained models underperform base models at large k values.", "RLVR improves sampling efficiency but narrows reasoning capacity.", "RLVR differs fundamentally from distillation in expanding reasoning capabilities."], "tldr": "**Reinforcement Learning with Verifiable Rewards (RLVR) is believed to enhance reasoning in Large Language Models(LLMs).** This study challenges this, finding that RLVR doesn't create new reasoning patterns. While RL-trained models initially outperform base models, the latter catch up at large k values. Analysis reveals RL-trained models' reasoning paths are within the base models' distribution, suggesting no novel abilities acquired through RLVR.\n\nRLVR biases the model towards reward-yielding paths, improving sampling efficiency but limiting exploration. Unlike RLVR, distillation introduces new knowledge. **The study underscores RLVR's limitation in advancing LLM reasoning, suggesting a need to rethink its impact and seek superior training methods.** A project page is available at https://limit-of-RLVR.github.io for further details.", "affiliation": "Tsinghua University", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2504.13837/podcast.wav"}