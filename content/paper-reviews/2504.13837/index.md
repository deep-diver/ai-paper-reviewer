---
title: "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?"
summary: "RLVR doesn't truly incentivize new reasoning in LLMs; it boosts efficiency within existing base model capabilities."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Machine Learning", "Reinforcement Learning", "üè¢ Tsinghua University",]
showSummary: true
date: 2025-04-18
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.13837 {{< /keyword >}}
{{< keyword icon="writer" >}} Yang Yue et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-04-21 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.13837" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.13837" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.13837/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

**Reinforcement Learning with Verifiable Rewards (RLVR) is believed to enhance reasoning in Large Language Models(LLMs).** This study challenges this, finding that RLVR doesn't create new reasoning patterns. While RL-trained models initially outperform base models, the latter catch up at large k values. Analysis reveals RL-trained models' reasoning paths are within the base models' distribution, suggesting no novel abilities acquired through RLVR.



RLVR biases the model towards reward-yielding paths, improving sampling efficiency but limiting exploration. Unlike RLVR, distillation introduces new knowledge. **The study underscores RLVR's limitation in advancing LLM reasoning, suggesting a need to rethink its impact and seek superior training methods.** A project page is available at https://limit-of-RLVR.github.io for further details.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} RLVR-trained models underperform base models at large k values. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} RLVR improves sampling efficiency but narrows reasoning capacity. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} RLVR differs fundamentally from distillation in expanding reasoning capabilities. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
**This work challenges current understanding of Reinforcement Learning(RL)'s role in advancing LLM reasoning.** It opens new avenues for rethinking RL training and exploring better paradigms to push the boundary of reasoning abilities in LLMs, marking a shift in focus for future AI research.

------
#### Visual Insights



![](https://arxiv.org/html/2504.13837/x1.png)

> üîº Figure 1 illustrates the impact of Reinforcement Learning with Verifiable Rewards (RLVR) on Large Language Model (LLM) reasoning.  The left panel shows search trees for two example problems (A and B).  Nodes represent steps in a reasoning process.  Black nodes represent frequently sampled paths by both the base and RLVR-trained models, grey indicates rarely sampled paths. Green nodes represent correct solutions. The figure demonstrates that RLVR doesn't add new reasoning paths; all paths in the RLVR model already exist in the base model.  RLVR improves efficiency for certain problems (A) by biasing the model toward rewarded paths. However, this comes at the cost of reduced exploration, as seen with problem B, where the base model still contains the correct solution path, even though the RLVR model does not. The right panel shows pass@k curves for several LLMs. Pass@k measures the fraction of problems solvable with k samples.  It shows that as RLVR training progresses, pass@1 (average performance) improves, but pass@256 (coverage of all solvable problems) decreases, indicating that the model explores fewer unique reasoning paths.
> <details>
> <summary>read the caption</summary>
> Figure 1:  (Left) The effect of RLVR on LLM‚Äôs reasoning ability. Search trees are generated by repeated sampling from the base and RLVR-trained models for a given problem. Grey indicates paths that are unlikely to be sampled by the model, while black indicates paths that are likely to be sampled. Green indicates correct paths, which has positive rewards. Our key finding is that all reasoning paths in the RLVR model are already present in the base model. For certain problems like Problem A, RLVR training biases the distribution toward rewarded paths, improving sampling efficiency. However, this comes at the cost of reduced scope of reasoning capacity: For other problems like Problem B, the base model contains the correct path, whereas that of the RLVR model does not. (Right) As RLVR training progresses, the average performance (i.e., pass@1) improves, but the coverage of solvable problems (i.e., pass@256) decreases, indicating a reduction in the model‚Äôs reasoning upper bound.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S2.T1.1.1">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.1" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Task</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.2" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Start Model</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.3" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">RL Framework</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.4" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">RL Algorithm(s)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.1.5" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.5.1">Benchmark(s)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.1" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1.1">Mathematics</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.2" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.2.2.1"></span> <span class="ltx_text" id="S2.T1.1.1.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.2.2.2.1">
<span class="ltx_tr" id="S2.T1.1.1.2.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.2.2.1.1.1" style="padding:0.75pt 3.0pt;">LLaMA-3.1-8B</span></span>
<span class="ltx_tr" id="S2.T1.1.1.2.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.2.2.1.2.1" style="padding:0.75pt 3.0pt;">Qwen-2.5-7B/14B/32B-Base</span></span>
<span class="ltx_tr" id="S2.T1.1.1.2.2.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.2.2.1.3.1" style="padding:0.75pt 3.0pt;">Qwen-2.5-Math-7B</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.2.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.3" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.2.3.1"></span> <span class="ltx_text" id="S2.T1.1.1.2.3.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.2.3.2.1">
<span class="ltx_tr" id="S2.T1.1.1.2.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.3.2.1.1.1" style="padding:0.75pt 3.0pt;">SimpleRLZoo</span></span>
<span class="ltx_tr" id="S2.T1.1.1.2.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.3.2.1.2.1" style="padding:0.75pt 3.0pt;">Oat-Zero</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.2.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.4" style="padding:0.75pt 3.0pt;">GRPO</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.1.2.5" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.2.5.1"></span> <span class="ltx_text" id="S2.T1.1.1.2.5.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.2.5.2.1">
<span class="ltx_tr" id="S2.T1.1.1.2.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.5.2.1.1.1" style="padding:0.75pt 3.0pt;">GSM8K, MATH500</span></span>
<span class="ltx_tr" id="S2.T1.1.1.2.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.5.2.1.2.1" style="padding:0.75pt 3.0pt;">Minerva, Olympiad</span></span>
<span class="ltx_tr" id="S2.T1.1.1.2.5.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.2.5.2.1.3.1" style="padding:0.75pt 3.0pt;">AIME24, AMC23</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.2.5.3"></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3" style="background-color:#ECECEC;">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.1" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.3.1.1" style="background-color:#ECECEC;">Code Generation</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.2" style="padding:0.75pt 3.0pt;"><span class="ltx_text" id="S2.T1.1.1.3.2.1" style="background-color:#ECECEC;">Qwen-2.5-7B-Instruct</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.3" style="padding:0.75pt 3.0pt;"><span class="ltx_text" id="S2.T1.1.1.3.3.1" style="background-color:#ECECEC;">Code-R1</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.4" style="padding:0.75pt 3.0pt;"><span class="ltx_text" id="S2.T1.1.1.3.4.1" style="background-color:#ECECEC;">GRPO</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.3.5" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.3.5.1"></span><span class="ltx_text" id="S2.T1.1.1.3.5.2" style="background-color:#ECECEC;"> <span class="ltx_text" id="S2.T1.1.1.3.5.2.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.3.5.2.1.1">
<span class="ltx_tr" id="S2.T1.1.1.3.5.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.3.5.2.1.1.1.1" style="padding:0.75pt 3.0pt;">¬†¬†¬†¬†¬†LiveCodeBench</span></span>
<span class="ltx_tr" id="S2.T1.1.1.3.5.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.3.5.2.1.1.2.1" style="padding:0.75pt 3.0pt;">HumanEval+</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.3.5.2.2"></span></span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.1" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.4.1.1">Visual Reasoning</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.2" style="padding:0.75pt 3.0pt;">Qwen-2.5-VL-7B</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.3" style="padding:0.75pt 3.0pt;">EasyR1</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.4" style="padding:0.75pt 3.0pt;">GRPO</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.4.5" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.4.5.1"></span> <span class="ltx_text" id="S2.T1.1.1.4.5.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.4.5.2.1">
<span class="ltx_tr" id="S2.T1.1.1.4.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.4.5.2.1.1.1" style="padding:0.75pt 3.0pt;">MathVista</span></span>
<span class="ltx_tr" id="S2.T1.1.1.4.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.4.5.2.1.2.1" style="padding:0.75pt 3.0pt;">MathVision</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.4.5.3"></span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5" style="background-color:#ECECEC;">
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.1" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.5.1.1" style="background-color:#ECECEC;">Deep Analysis</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.2" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.5.2.1"></span><span class="ltx_text" id="S2.T1.1.1.5.2.2" style="background-color:#ECECEC;"> <span class="ltx_text" id="S2.T1.1.1.5.2.2.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.5.2.2.1.1">
<span class="ltx_tr" id="S2.T1.1.1.5.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.2.2.1.1.1.1" style="padding:0.75pt 3.0pt;">Qwen-2.5-7B-Base</span></span>
<span class="ltx_tr" id="S2.T1.1.1.5.2.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.2.2.1.1.2.1" style="padding:0.75pt 3.0pt;">Qwen-2.5-7B-Instruct</span></span>
<span class="ltx_tr" id="S2.T1.1.1.5.2.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.2.2.1.1.3.1" style="padding:0.75pt 3.0pt;">DeepSeek-R1-Distill-Qwen-7B</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.5.2.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.3" style="padding:0.75pt 3.0pt;"><span class="ltx_text" id="S2.T1.1.1.5.3.1" style="background-color:#ECECEC;">VeRL</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.4" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.5.4.1"></span><span class="ltx_text" id="S2.T1.1.1.5.4.2" style="background-color:#ECECEC;"> <span class="ltx_text" id="S2.T1.1.1.5.4.2.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.5.4.2.1.1">
<span class="ltx_tr" id="S2.T1.1.1.5.4.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.4.2.1.1.1.1" style="padding:0.75pt 3.0pt;">PPO, GRPO</span></span>
<span class="ltx_tr" id="S2.T1.1.1.5.4.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.4.2.1.1.2.1" style="padding:0.75pt 3.0pt;">Reinforce++</span></span>
<span class="ltx_tr" id="S2.T1.1.1.5.4.2.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.4.2.1.1.3.1" style="padding:0.75pt 3.0pt;">RLOO, ReMax, DAPO</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.5.4.2.2"></span></span>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.1.1.5.5" style="padding:0.75pt 3.0pt;">
<span class="ltx_text" id="S2.T1.1.1.5.5.1"></span><span class="ltx_text" id="S2.T1.1.1.5.5.2" style="background-color:#ECECEC;"> <span class="ltx_text" id="S2.T1.1.1.5.5.2.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.5.5.2.1.1">
<span class="ltx_tr" id="S2.T1.1.1.5.5.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.5.2.1.1.1.1" style="padding:0.75pt 3.0pt;">¬†¬†¬†¬†Omni-Math-Rule</span></span>
<span class="ltx_tr" id="S2.T1.1.1.5.5.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.5.5.2.1.1.2.1" style="padding:0.75pt 3.0pt;">MATH500</span></span>
</span></span><span class="ltx_text" id="S2.T1.1.1.5.5.2.2"></span></span>
</td>
</tr>
</table>{{< /table-caption >}}

> üîº This table details the experimental setup used to evaluate the impact of Reinforcement Learning with Verifiable Rewards (RLVR) on the reasoning capabilities of Large Language Models (LLMs).  It lists the specific LLMs used as base models, the reinforcement learning frameworks and algorithms applied, and the benchmark datasets employed for evaluation across different reasoning tasks: Mathematics, Code Generation, and Visual Reasoning.
> <details>
> <summary>read the caption</summary>
> Table 1: Experimental setup for assessing RLVR‚Äôs effect on the reasoning boundaries of LLMs across different tasks.
> </details>





### In-depth insights


#### RLVR Limits
**RLVR's potential is capped by the base model's inherent knowledge.** It can refine existing reasoning but struggles to introduce truly novel skills. The vast action space makes exploration challenging, favoring exploitation of prior knowledge. This leads to **diminishing returns** as the model becomes too specialized, **sacrificing broader problem-solving capabilities.** Distillation, by contrast, transfers knowledge, extending reasoning boundaries.

#### Pass@K Boundary
The concept of a "Pass@K Boundary" is crucial when evaluating reasoning capabilities of language models. It moves beyond simple success rates, recognizing that a model's potential isn't fully captured by single-attempt failures. Instead, it measures performance across multiple attempts (K), revealing a model's ability to solve difficult problems with sufficient sampling. **This boundary defines the limit of a model's reasoning capacity**, showing what it can achieve given ample opportunities. Comparing base models to reinforced learning models using this boundary highlights whether RL truly expands reasoning or merely improves efficiency. **A higher boundary suggests a broader reasoning scope**, while a lower boundary indicates limitations, regardless of initial success. This metric is important for understanding the real impact of training strategies on LLMs.

#### Sample Efficiency
Sample efficiency is crucial in reinforcement learning (RL), especially within large language models (LLMs), given the computational costs. Improving it means achieving optimal performance with minimal data, which directly impacts training time and resource consumption. The research paper highlights that **RL with verifiable rewards can improve sampling efficiency.** But it is also highlighted that the LLM has a better output for small k, and not for large k. RL trains by biasing the model which makes a more **efficient search** for the higher number of rewards, thus **improving the result for pass@1**. This indicates that sample efficiency is not just about the quantity of data but how effectively the RL algorithm utilizes the data and explores space.

#### Base Model Bound
The base model's inherent capabilities place a significant bound on what RLVR can achieve. **RLVR primarily refines the sampling efficiency within the pre-existing solution space of the base model, rather than expanding its capacity**. The base model's knowledge and reasoning skills acts as a ceiling, since **RLVR struggles to discover fundamentally new strategies that the base model could not, in principle, generate**. It is vital to consider the limitations in applying RLVR alone and **investigate innovative methods to enhance the base model's capabilities before RLVR-finetuning**. 

#### Distill vs. RLVR
**Distillation offers a distinct advantage over RLVR in enhancing LLM reasoning**. While RLVR primarily improves sampling efficiency by biasing the model towards high-reward paths already within its existing capacity, distillation introduces genuinely *new knowledge* into the model. By learning from a more powerful 'teacher' model, the distilled model can *expand its reasoning capability beyond the limitations of the base model*. This contrast highlights that RLVR's focus on *refining existing capabilities* differs fundamentally from distillation's ability to *impart novel reasoning patterns*, making distillation a more promising avenue for transcending the inherent boundaries of a given LLM architecture.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.13837/x2.png)

> üîº Figure 2 presents a comparative analysis of the pass@k metric across various mathematical benchmarks, utilizing both base and reinforcement learning (RL)-trained large language models (LLMs).  The x-axis represents the number of samples (k), signifying the number of attempts the model gets to solve a problem. The y-axis displays the pass@k score, indicating the percentage of problems correctly solved within k attempts.  The figure illustrates that while RL-trained models initially demonstrate superior performance with a small number of attempts (low k), their advantage diminishes as the number of attempts increases. Notably, base models consistently catch up to and eventually outperform RL-trained models when a sufficiently large number of samples are allowed (k in the tens or hundreds), demonstrating that the reasoning capacity of RL-trained models does not inherently surpass that of base models. This effect is observed across diverse benchmarks and multiple LLM families.
> <details>
> <summary>read the caption</summary>
> Figure 2:  Pass@kùëòkitalic_k curves of base models and their zero-RL-trained counterparts across multiple mathematical benchmarks. When kùëòkitalic_k is small, RL-trained models outperform their base versions. However, as kùëòkitalic_k increases to the tens or hundreds, base models consistently catch up with RL-trained models across all benchmarks and LLM families without exception. Eventually, base models surpass RL-trained models.
> </details>



![](https://arxiv.org/html/2504.13837/x3.png)

> üîº The figure shows the performance comparison between the base Qwen-2.5-Math-7B model and its RL-trained counterpart (Oat-Zero-7B) on the AIME24 mathematics benchmark.  The x-axis represents the number of samples (k) used in the pass@k metric, and the y-axis represents the coverage (pass@k).  The graph illustrates that while the RL-trained model initially outperforms the base model for small k values (meaning it solves more problems with fewer attempts), the base model eventually surpasses the RL model as k increases. This demonstrates that the RL training biases the model towards more efficient sampling of correct answers, but doesn't expand the overall reasoning ability beyond that already present in the base model.
> <details>
> <summary>read the caption</summary>
> Figure 3:  Oat-Zero for AIME24.
> </details>



![](https://arxiv.org/html/2504.13837/x4.png)

> üîº Figure 4 presents the results of the experiments evaluating the effect of Reinforcement Learning with Verifiable Rewards (RLVR) on code generation tasks. The graph displays the pass@k curves for both the base and RLVR-trained models. The x-axis represents the number of samples (k), while the y-axis shows the coverage (pass@k). The figure demonstrates that although RLVR improves the performance at small k values (e.g., k=1), the base model consistently surpasses the RLVR-trained model at larger k values, suggesting that RLVR does not expand the reasoning capabilities of LLMs beyond those of the base model, but only improves the sampling efficiency.
> <details>
> <summary>read the caption</summary>
> Figure 4: RLVR for Coding.
> </details>



![](https://arxiv.org/html/2504.13837/x5.png)

> üîº This figure displays the performance of base LLMs and their counterparts trained with zero-RL (Reinforcement Learning) across two distinct tasks: code generation and visual reasoning.  The x-axis represents the number of samples (k) used to determine if a problem is solved (pass@k), while the y-axis indicates the pass@k score, showing the proportion of problems solved within k attempts. Separate graphs are provided for each task, illustrating the performance of base models and their corresponding zero-RL trained versions across a range of sample sizes (k values). This allows for a comparison of the reasoning capabilities between base models and RL-trained models, considering both the average-case performance (low k) and the ability to find correct solutions with increased effort (high k).
> <details>
> <summary>read the caption</summary>
> Figure 5:  Pass@kùëòkitalic_k curves of base models and zero-RL counterparts. (Left) Code Generation. (Right) Visual Reasoning.
> </details>



![](https://arxiv.org/html/2504.13837/x6.png)

> üîº Figure 6 presents a comparative analysis of model performance using perplexity and pass@k metrics.  The left panel shows the perplexity distributions of responses generated by three different methods (base model, RL-trained model, and human-generated responses) as evaluated by both the base and RL models.  Lower perplexity indicates higher likelihood of generation, showing whether the RL model's responses align with patterns in the base model's generation. The right panel displays the pass@k scores for four model types: a base model, an instruction-tuned model, an RL-trained model, and a model created by knowledge distillation from a strong model. Pass@k represents the proportion of problems successfully solved when given k attempts. This part demonstrates the effects of different training methods on solving problems.
> <details>
> <summary>read the caption</summary>
> Figure 6:  (Left) Perplexity distribution of responses from different sources, evaluated by the base and RL models. The conditioning problem xùë•xitalic_x is omitted in the figure. (Right) Coverage comparison of base, Instruct, RL, and distilled models.
> </details>



![](https://arxiv.org/html/2504.13837/x7.png)

> üîº This figure compares the performance of different reinforcement learning (RL) algorithms and different RL training durations on three tasks: Omni-MATH-Train, Omni-MATH-Test, and MATH500.  The top panel shows the pass@k curves for various RL algorithms, highlighting their differences in sampling efficiency. The bottom panel illustrates how the performance changes with varying numbers of RL training steps, showing that increasing steps doesn't necessarily lead to better reasoning. The y-axis uses a folded scale for better visualization of small k values (k=1) and large k values (k=256), with the unfolded version shown in Figure 8.  Detailed data is available in Tables 2 and 3.
> <details>
> <summary>read the caption</summary>
> Figure 7:  (Top) Different RL algorithms. (Bottom) Different RL training steps. We use a folded y-axis range to better highlight the details at k=1ùëò1k=1italic_k = 1 and 256256256256. Unfolded version can be found in¬†Figure¬†8. The detailed values for each point at pass@1 and pass@256 are provided in¬†Table¬†2 and¬†Table¬†3.
> </details>



![](https://arxiv.org/html/2504.13837/x8.png)

> üîº Figure 8 provides an expanded view of the data presented in Figure 7, showing the performance of different reinforcement learning algorithms across various metrics and datasets. The y-axis, representing performance metrics (such as pass@k), is no longer compressed, enabling a clearer view of the differences between the RL methods, particularly at lower values of k (number of samples). The figure is crucial for a detailed comparison of the algorithms' sampling efficiency and their ability to solve problems, both within and outside the training domain. It complements the main findings of the study.
> <details>
> <summary>read the caption</summary>
> Figure 8:  Unfolded y-axis version of¬†Figure¬†7.
> </details>



![](https://arxiv.org/html/2504.13837/x9.png)

> üîº This figure compares the performance of base and reinforcement learning (RL)-trained LLMs on the AIME24 mathematical reasoning benchmark.  The x-axis represents the number of samples (k) used to determine if a problem is solved (pass@k). A problem is considered solved if at least one of the k samples produces a correct answer. The y-axis shows the pass@k score, indicating the percentage of problems solved within k samples. The curves for base and RL models illustrate how their problem-solving ability changes as the number of samples increases.  The goal is to assess whether RL training significantly expands the model's reasoning capacity beyond that of the base model.
> <details>
> <summary>read the caption</summary>
> Figure 9:  Pass@kùëòkitalic_k curves of the base and RL models in the filtered AIME24.
> </details>



![](https://arxiv.org/html/2504.13837/x10.png)

> üîº Figure 10 shows the impact of different temperature settings on the performance of both base and RL-trained LLMs.  The base model's performance significantly degrades when the temperature parameter exceeds 1.0, which is attributed to the generation of more random and incoherent text. In contrast, the RL model maintains relatively consistent performance across various temperature settings. This observation led the researchers to choose a temperature of 0.6 for their main experiments, as this setting allowed both models to showcase their optimal reasoning abilities.
> <details>
> <summary>read the caption</summary>
> Figure 10:  We found that the base model‚Äôs performance drops when the temperature exceeds 1.0, as it tends to generate more random and less coherent tokens. In contrast, the RL model‚Äôs performance remains relatively stable across different temperature settings. Therefore, we use T=0.6ùëá0.6T=0.6italic_T = 0.6 in the main experiments, as it allows both models to demonstrate their best reasoning performance.
> </details>



![](https://arxiv.org/html/2504.13837/x11.png)

> üîº Figure 11 shows the prompt used for training and evaluating the SimpleRL model. The prompt instructs the model to act as a helpful assistant and to answer a given question using step-by-step reasoning, placing the final answer within a box.  The caption highlights that during evaluation, the base model (without reinforcement learning) is given the same prompt as the RL-trained model. This is crucial for a fair comparison of the reasoning capabilities between the models.  The uniformity in prompt ensures any observed differences are attributed to model training, not variations in the prompts.
> <details>
> <summary>read the caption</summary>
> Figure 11:  Prompt for SimpleRL Training and Evaluation. The base model uses the same prompt as the RL model during evaluation.
> </details>



![](https://arxiv.org/html/2504.13837/x12.png)

> üîº This figure shows the prompt template used for training and evaluating the Oat-Zero model.  The prompt instructs the model to act as a helpful assistant and to solve a given question by reasoning step-by-step. The final answer must be enclosed within a box.  This standardized prompt ensures consistent input formatting during both training and evaluation phases of the Oat-Zero model.
> <details>
> <summary>read the caption</summary>
> Figure 12:  Prompt for Oat-Zero training and evaluation.
> </details>



![](https://arxiv.org/html/2504.13837/x13.png)

> üîº The prompt instructs the language model to act as a helpful programming assistant.  It specifies that the user will pose a question, and the assistant should first determine the solution through reasoning before providing the answer. The reasoning process and the final answer must be clearly delineated using the tags `<think>...</think>` and `<answer>...</answer>`, respectively. This formatted structure helps to separate the model's thought process from the final output, making it easier to analyze and evaluate the model's reasoning abilities.
> <details>
> <summary>read the caption</summary>
> Figure 13:  Prompt for Code-R1 training.
> </details>



![](https://arxiv.org/html/2504.13837/x14.png)

> üîº This figure displays the prompt template used for evaluating the Code-R1 model on the LiveCodeBench benchmark.  The prompt instructs the model to act as a helpful programming assistant, providing a problem specification and generating correct Python code that passes all tests. The prompt includes formatting instructions and placeholders for the question and the code generated by the model. The structure of the prompt is designed to ensure the model generates only the code and nothing else, focusing the model's output strictly on the programming task itself.
> <details>
> <summary>read the caption</summary>
> Figure 14:  Prompt for Code-R1 Evaluation on LiveCodeBench.
> </details>



![](https://arxiv.org/html/2504.13837/x15.png)

> üîº This figure displays the prompt used for evaluating the Code-R1 model on the HumanEval+ and MBPP+ benchmarks.  The prompt instructs the model to act as a helpful programming assistant, providing a self-contained Python script that solves a given problem. The solution should be presented within a markdown code block, ensuring the script's integrity and testability. This setup emphasizes the model's ability to generate functional code while adhering to specific formatting guidelines.
> <details>
> <summary>read the caption</summary>
> Figure 15:  Prompt for Code-R1 Evaluation on HumanEval+ and MBPP+.
> </details>



![](https://arxiv.org/html/2504.13837/x16.png)

> üîº This figure shows the prompt template used for training and evaluating the EasyR1 model.  The prompt instructs the model to first engage in a chain-of-thought reasoning process, which is enclosed within `<think></think>` tags, before providing the final answer within a \boxed{} tag. The prompt also includes an image token `<vision_start>{image_token}</vision_end>` indicating that the model is designed for visual reasoning tasks where both textual and visual inputs are provided.
> <details>
> <summary>read the caption</summary>
> Figure 16:  Prompt for EasyR1 training and evaluation.
> </details>



![](https://arxiv.org/html/2504.13837/x17.png)

> üîº This figure displays the prompt template used for training and evaluating the VeRL (Visual Reinforcement Learning) model.  It shows the instructions given to the language model, specifying that it should act as a helpful and harmless assistant, think step-by-step, and provide its final answer within a boxed area. The template guides the model to generate responses that clearly lay out the reasoning process before presenting the final answer. This structured prompt helps to ensure consistent and easily-analyzed responses, facilitating a fair and effective evaluation of the model's performance.
> <details>
> <summary>read the caption</summary>
> Figure 17:  (1) Prompt for VeRL training and evaluation.
> </details>



![](https://arxiv.org/html/2504.13837/x18.png)

> üîº This figure shows a sample correct response generated by the Qwen-2.5-Base-7B model for question 16 of the AIME24 benchmark.  The response demonstrates a multi-step reasoning process using the principle of inclusion-exclusion to solve a problem involving the number of residents in a town who own specific items. The solution includes multiple attempts, showing the model's process of refinement and correction of errors in its initial calculation. The final answer is correctly identified as 73.
> <details>
> <summary>read the caption</summary>
> Figure 18: Qwen-2.5-Base-7B Correct Response - Case 1.
> </details>



![](https://arxiv.org/html/2504.13837/x19.png)

> üîº This figure displays a sample correct response generated by the Qwen-2.5-Base-7B model for question 24 of the AIME24 benchmark.  The question involves determining the number of ways to arrange digits in a 2x3 grid such that the sum of the two numbers (read left to right) is 999 and the sum of three numbers (read top to bottom) is 99. The figure shows the model's detailed step-by-step reasoning process which uses the principle of inclusion-exclusion to solve this mathematical problem.  The solution method and the final answer (45) are presented.
> <details>
> <summary>read the caption</summary>
> Figure 19: Qwen-2.5-Base-7B Correct Response - Case 2.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="A1.T2.1.1">
<tr class="ltx_tr" id="A1.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="A1.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A1.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.2.1">Omni-MATH-Train</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A1.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.3.1">Omni-MATH-Test</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.1.4.1">MATH500</span></td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.2">
<td class="ltx_td ltx_border_r" id="A1.T2.1.1.2.1"></td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.2.2">pass@1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.2.3">pass@256</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.2.4">pass@1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.2.5">pass@256</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.2.6">pass@1</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.2.7">pass@256</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T2.1.1.3.1">Qwen2.5-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.1.3.2">9.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.1.1.3.3">67.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.1.3.4">10.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.1.1.3.5">69.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.1.3.6">34.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T2.1.1.3.7">96.2</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T2.1.1.4.1">GRPO</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.4.2">26.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.4.3">66.3</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.4.4">25.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.4.5">68.3</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.4.6">74.4</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.4.7">97.2</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T2.1.1.5.1">PPO</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.5.2">27.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.5.3">65.8</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.5.4">26.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.5.5">69.2</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.5.6">75.2</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.5.7">97.2</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T2.1.1.6.1">ReMax</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.6.2">24.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.6.3">65.5</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.6.4">23.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.6.5">67.5</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.6.6">73.5</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.6.7">96.6</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T2.1.1.7.1">RLOO</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.7.2">28.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.7.3">66.4</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.7.4"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.7.4.1">28.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.7.5">69.2</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.7.6">75.0</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.7.7"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.7.7.1">97.4</span></td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T2.1.1.8.1">Reinforce++</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.8.2">28.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.8.3"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.8.3.1">67.7</span></td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.8.4"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.8.4.1">28.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.1.1.8.5"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.8.5.1">69.7</span></td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.8.6">75.4</td>
<td class="ltx_td ltx_align_center" id="A1.T2.1.1.8.7">96.8</td>
</tr>
<tr class="ltx_tr" id="A1.T2.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A1.T2.1.1.9.1">DAPO</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.1.9.2"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.9.2.1">31.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T2.1.1.9.3">66.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.1.9.4">26.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T2.1.1.9.5">67.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.1.9.6"><span class="ltx_text ltx_font_bold" id="A1.T2.1.1.9.6.1">75.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T2.1.1.9.7">96.4</td>
</tr>
</table>{{< /table-caption >}}
> üîº This table presents a detailed breakdown of the pass@1 and pass@256 metrics for different reinforcement learning (RL) algorithms.  It shows the performance of each algorithm on three datasets: Omni-MATH-Train, Omni-MATH-Test (in-domain), and MATH500 (out-of-domain).  The pass@k metric indicates the percentage of problems solved correctly within k attempts. Pass@1 reflects the model's average-case performance. Pass@256 shows its reasoning boundary, examining how well the models solve problems given a higher number of attempts.
> <details>
> <summary>read the caption</summary>
> Table 2: Detailed values for each point at pass@1 and pass@256 across different RL algorithms in¬†Figure¬†7.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="A1.T3.1.1">
<tr class="ltx_tr" id="A1.T3.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="A1.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A1.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.2.1">Omni-MATH-Train</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="A1.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.3.1">Omni-MATH-Test</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A1.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.1.4.1">MATH500</span></td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.1.2">
<td class="ltx_td ltx_border_r" id="A1.T3.1.1.2.1"></td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.2.2">pass@1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.1.1.2.3">pass@256</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.2.4">pass@1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.1.1.2.5">pass@256</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.2.6">pass@1</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.2.7">pass@256</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A1.T3.1.1.3.1">Qwen2.5-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.1.3.2">9.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.1.1.3.3"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.3.3.1">67.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.1.3.4">10.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.1.1.3.5"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.3.5.1">69.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.1.3.6">34.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T3.1.1.3.7">96.2</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T3.1.1.4.1">GRPO-step150</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.4.2">26.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.1.1.4.3">66.3</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.4.4">25.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.1.1.4.5">68.3</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.4.6">74.4</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.4.7"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.4.7.1">97.2</span></td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A1.T3.1.1.5.1">GRPO-step300</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.5.2">33.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.1.1.5.3">65.3</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.5.4">27.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.1.1.5.5">66.6</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.5.6">75.4</td>
<td class="ltx_td ltx_align_center" id="A1.T3.1.1.5.7">96.0</td>
</tr>
<tr class="ltx_tr" id="A1.T3.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A1.T3.1.1.6.1">GRPO-step450</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T3.1.1.6.2"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.6.2.1">42.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T3.1.1.6.3">64.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T3.1.1.6.4"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.6.4.1">28.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T3.1.1.6.5">63.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T3.1.1.6.6"><span class="ltx_text ltx_font_bold" id="A1.T3.1.1.6.6.1">76.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T3.1.1.6.7">95.4</td>
</tr>
</table>{{< /table-caption >}}
> üîº Table 3 presents a detailed breakdown of the pass@1 and pass@256 metrics across various reinforcement learning (RL) training steps.  The data shown corresponds to the results visualized in Figure 7 of the paper.  The table allows for a precise comparison of model performance at different training durations, indicating how the model's ability to solve problems with one attempt (pass@1) and the model's broader reasoning capacity (pass@256) change as training progresses.  Different model versions are included, enabling analysis of how training duration impacts each.
> <details>
> <summary>read the caption</summary>
> Table 3: Detailed values at pass@1 and pass@256 across different RL training steps in¬†Figure¬†7.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.1">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">Problem Indices</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.2.1">Qwen-7B-Base</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T4.1.1.2.2">0, 1, 4, 6, 7, 8, 9, 11, 12, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.1.3.1">SimpleRL-Qwen-7B</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.1.3.2">0, 1, 6, 7, 8, 9, 12, 14, 15, 16, 18, 22, 23, 24, 25, 26, 27, 28, 29</td>
</tr>
</table>{{< /table-caption >}}
> üîº Table 4 presents the indices of problems in the AIME24 benchmark that were successfully solved by both the base and RL-trained models.  The indices start at 0.  The key observation is that the problems solved by the RL model are almost entirely a subset of those solved by the base model, indicating that the RL model does not solve problems that are beyond the base model's capabilities. This supports the paper's finding that reinforcement learning mainly improves sampling efficiency instead of enhancing the model's fundamental reasoning abilities.
> <details>
> <summary>read the caption</summary>
> Table 4: Indices of solvable problems in AIME24 (starting from 0). An approximate subset relationship can be observed: most problems solved by the RL model are also solvable by the base model.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="A1.T5.1.1">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1">Solvable Problem Indices</span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.2.1">Qwen-7B-Instruct-1M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T5.1.1.2.2">
<span class="ltx_text" id="A1.T5.1.1.2.2.1"></span> <span class="ltx_text" id="A1.T5.1.1.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.2.2.2.1">
<span class="ltx_tr" id="A1.T5.1.1.2.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.2.2.2.1.1.1">400, 402, 403, 407, 409, 412, 413, 417, 418, 419, 422, 423,</span></span>
<span class="ltx_tr" id="A1.T5.1.1.2.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.2.2.2.1.2.1">427, 432, 433, 436, 438, 439, 440, 444, 445, 448, 449</span></span>
</span></span><span class="ltx_text" id="A1.T5.1.1.2.2.3"></span></td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.1.1.3.1">Coder-R1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T5.1.1.3.2">
<span class="ltx_text" id="A1.T5.1.1.3.2.1"></span> <span class="ltx_text" id="A1.T5.1.1.3.2.2">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.3.2.2.1">
<span class="ltx_tr" id="A1.T5.1.1.3.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.3.2.2.1.1.1">400, 402, 403, 407, 412, 413, 417, 418, 419, 422, 423,</span></span>
<span class="ltx_tr" id="A1.T5.1.1.3.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.3.2.2.1.2.1">427, 430, 433, 438, 439, 440, 444, 445, 449</span></span>
</span></span><span class="ltx_text" id="A1.T5.1.1.3.2.3"></span></td>
</tr>
</table>{{< /table-caption >}}
> üîº Table 5 presents the indices of problems within the LiveCodeBench dataset that were successfully solved by two different Language Models (LMs). The dataset contains problems numbered from 400 to 450. The table shows which of these problems were solved by a Qwen-7B Instruct-1M model and which were solved by a Code-R1 model.  This allows for a comparison of the problem-solving capabilities of the two models. Note that the problem indices start at 0.
> <details>
> <summary>read the caption</summary>
> Table 5: Indices of solvable problems in LiveCodeBench (ranging from 400 to 450, starting from 0).
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.13837/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.13837/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}