{"references": [{"fullname_first_author": "Qi", "paper_title": "Fine-tuning aligned language models compromises safety, even when users do not intend to!", "publication_date": "2023-10-03", "reason": "This paper is foundational to the current work, as it first reveals the vulnerability of LLMs to harmful fine-tuning attacks and motivates the research on mitigation strategies."}, {"fullname_first_author": "Rosati", "paper_title": "Representation noising effectively prevents harmful fine-tuning on LLMs", "publication_date": "2024-05-14", "reason": "This paper introduces a defense mechanism against harmful fine-tuning attacks, which is directly relevant to the proposed attack method and provides a basis for comparison."}, {"fullname_first_author": "Huang", "paper_title": "Booster: Tackling harmful fine-tuning for large language models via attenuating harmful perturbation", "publication_date": "2024-09-01", "reason": "This paper proposes a defense method that is closely related to the attack method, allowing for a direct comparison of attack and defense strategies."}, {"fullname_first_author": "Hu", "paper_title": "LoRA: Low-rank adaptation of large language models", "publication_date": "2021-06-09", "reason": "This paper introduces the LoRA technique, a crucial component of the experimental setup for efficiently fine-tuning LLMs, demonstrating its practical importance."}, {"fullname_first_author": "Zou", "paper_title": "Universal and transferable adversarial attacks on aligned language models", "publication_date": "2023-07-15", "reason": "This paper introduces the GCG optimizer, a key algorithm used in the proposed attack, highlighting its role in achieving the attack's effectiveness."}]}