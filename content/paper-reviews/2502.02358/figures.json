[{"figure_path": "https://arxiv.org/html/2502.02358/x1.png", "caption": "Figure 1. Demonstration of our MotionLab\u2019s versatility, performance and efficiency. Previous SOTA refer to multiple expert models, including MotionLCM (Dai et\u00a0al., 2025), OmniControl (Xie et\u00a0al., 2023), MotionFix (Athanasiou et\u00a0al., 2024), CondMDI (Cohan et\u00a0al., 2024) and MCM-LDM (Song et\u00a0al., 2024). All motions are represented using SMPL (Loper et\u00a0al., 2023), where transparent motion indicates the source motion or condition, and the other represents the target motion. More qualitative results are available in the website and appendix.", "description": "Figure 1 showcases MotionLab's capabilities in human motion generation and editing.  It compares MotionLab's performance to several state-of-the-art (SOTA) models across various tasks, including text-based and trajectory-based motion generation and editing, as well as motion style transfer and in-betweening. The figure visually demonstrates MotionLab's ability to generate and edit human motions based on different types of conditions (text, trajectories, and other motions).  All motions are represented using the SMPL model, with transparent motions signifying the source motion or condition, and the opaque motions representing the resulting target motion.  The results highlight MotionLab's superior versatility, performance, and efficiency compared to previous SOTA methods.  Additional qualitative results can be found on the project's website and in the paper's appendix.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.02358/x2.png", "caption": "Figure 2. Demonstration of the difference trajectory between diffusion models and rectified flows. This difference lies in that the trajectory of diffusion models is based on xt=(1\u2212\u03b1t\u00af)\u2062x0+\u03b1t\u00af\u2062\u03f5subscript\ud835\udc65\ud835\udc611\u00afsubscript\ud835\udefc\ud835\udc61subscript\ud835\udc650\u00afsubscript\ud835\udefc\ud835\udc61italic-\u03f5x_{t}=\\sqrt{(1-\\overline{\\alpha_{t}})}x_{0}+\\sqrt{\\overline{\\alpha_{t}}}\\epsilonitalic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = square-root start_ARG ( 1 - over\u00af start_ARG italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ) end_ARG italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + square-root start_ARG over\u00af start_ARG italic_\u03b1 start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG end_ARG italic_\u03f5, while the trajectory of rectified flows is based on xt=(1\u2212t)\u2062x0+t\u2062x1subscript\ud835\udc65\ud835\udc611\ud835\udc61subscript\ud835\udc650\ud835\udc61subscript\ud835\udc651x_{t}=(1-t)x_{0}+tx_{1}italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ( 1 - italic_t ) italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_t italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT. This distinction leads to more robust learning by maintaining a constant velocity, contributing to model\u2019s efficiency (Zhao et\u00a0al., 2024).", "description": "Figure 2 illustrates the key difference between the trajectory in diffusion models and rectified flows.  Diffusion models generate a trajectory from noise (x\u2080) to data (x\u2081) that is not linear and changes velocity as the process evolves.  This is shown mathematically as  xt = \u221a(1-\u03b1t\u00af)x0 + \u221a\u03b1t\u00af\u03f5.  In contrast, rectified flows create a trajectory with constant velocity, moving linearly from x\u2080 to x\u2081. This is represented by xt = (1-t)x\u2080 + tx\u2081. This linear trajectory is what makes rectified flows more robust and efficient for learning, as indicated by Zhao et al. (2024).", "section": "3 PRELIMINARY: RECTIFIED FLOWS"}, {"figure_path": "https://arxiv.org/html/2502.02358/x3.png", "caption": "Figure 3. Illustration of our MotionLab and the detail of its MotionFlow Transformer (MFT).", "description": "Figure 3 is a two-part illustration detailing the architecture of MotionLab, a unified framework for human motion generation and editing.  (a) shows the overall system, highlighting the input modalities (source motion, condition, task instruction) fed into the core MotionFlow Transformer (MFT). The MFT processes these inputs to generate the target motion.  (b) zooms in on a single MFT block, showcasing its internal components: joint attention mechanisms for cross-modal interaction, condition paths for modality-specific processing, and aligned rotational position encoding for temporal synchronization.  The figure visually represents the flow of information within the MotionLab framework.", "section": "5 MOTIONLAB"}, {"figure_path": "https://arxiv.org/html/2502.02358/x4.png", "caption": "Figure 4. Qualitative results of MotionLab on the text-based motion generation. For clarity, as time progresses, motion sequences transit from light to dark colors.", "description": "This figure displays qualitative results from the MotionLab model for text-based human motion generation.  Multiple examples are shown, each demonstrating the model's ability to generate a realistic 3D human motion sequence from a given text prompt.  To enhance clarity and visualize the temporal progression of each generated motion, the color scheme of the 3D models changes gradually from light to dark colors as the motion unfolds.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.02358/x5.png", "caption": "Figure 5. Qualitative results of MotionLab on the text-based motion editing. The transparent motion is the source motion, and the other is the generated motion.", "description": "This figure showcases the capabilities of MotionLab in text-based motion editing.  It presents several examples where a source motion (shown transparently) is modified based on a textual instruction. The resulting edited motion, generated by MotionLab, is displayed alongside the source motion for direct comparison. This illustrates how MotionLab can accurately and naturally alter existing motion sequences according to textual descriptions, allowing for fine-grained control over the editing process.", "section": "Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.02358/x6.png", "caption": "Figure 6. Qualitative results of MotionLab on the trajectory-based motion generation. The red balls are the trajectory of the pelvis, right hand and right foot.", "description": "Figure 6 showcases examples of human motion generated by MotionLab using trajectory-based input.  The system takes a target trajectory as input (represented by red spheres for the pelvis, right hand, and right foot) and generates a full 3D human motion sequence that follows that trajectory. This demonstrates MotionLab's ability to generate realistic and accurate motion sequences conditioned on specified joint movements over time.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.02358/x7.png", "caption": "Figure 7. Comparison of the motion in-between with CondMDI (Cohan et\u00a0al., 2024) on HumanML3D (Guo et\u00a0al., 2022a), which shows that our model outperforms CondMDI.", "description": "Figure 7 presents a quantitative comparison of motion in-between generation results between the proposed MotionLab model and the CondMDI model (Cohan et al., 2024) using the HumanML3D dataset (Guo et al., 2022a).  The figure displays a comparison across four key metrics: Fr\u00e9chet Inception Distance (FID), R-precision (top 3), average error, and foot skating ratio. Lower FID values indicate better generation quality, higher R-precision indicates higher accuracy, lower average error reflects better fidelity to target motion, and lower foot skating ratio shows better physical plausibility.  The bar chart visually demonstrates that MotionLab significantly outperforms CondMDI across all metrics, showcasing its superior performance in generating smooth and realistic in-between motions.", "section": "A ADDITIONAL QUANTITATIVE RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.02358/extracted/6183938/Figure/style.png", "caption": "Figure 8. Comparison of the motion style transfer with MCM-LDM (Song et\u00a0al., 2024) on a subset of HumanML3D (Guo et\u00a0al., 2022a). This shows that our model has a stronger ability to preserve the semantics of source motion and a stronger ability to learn the style of style motion.", "description": "Figure 8 presents a quantitative comparison of motion style transfer performance between the proposed MotionLab model and the state-of-the-art MCM-LDM model.  The comparison uses a subset of the HumanML3D dataset and evaluates two key aspects of style transfer: semantic preservation (measured by Content Recognition Accuracy or CRA) and style accuracy (measured by Style Recognition Accuracy or SRA).  The bar chart visually demonstrates that MotionLab achieves superior performance in both CRA and SRA compared to MCM-LDM, indicating a stronger ability to retain the original meaning of the source motion while effectively adapting the style from a target motion.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.02358/x8.png", "caption": "Figure 9. Ablation results of MotionLab on the motion in-between. Beige motion is use 1D-learnable position encoding, purple motion use Aligned ROPE, and gray motions are the poses provided in keyframes, demonstrating the importance of Aligned ROPE.", "description": "This figure shows an ablation study on the motion in-between task, comparing different positional encoding methods used in MotionLab. Three versions of the same motion in-between task are shown, using different positional encoding strategies: 1D-learnable position encoding (beige), Aligned ROPE (purple), and the original keyframes from the input (gray). The visual differences highlight that Aligned ROPE significantly improves the quality of the generated motion compared to the other methods. The improved temporal alignment between the generated motion and the input keyframes demonstrates the importance of Aligned ROPE for motion in-between tasks.", "section": "6.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.02358/extracted/6183938/Figure/timesteps.png", "caption": "Figure 10. Comparison of the inference time on text-based motion generation. We calculate AITS on the test set of HumanML3D (Guo et\u00a0al., 2022a) without model or data loading parts. All tests are performed on the same RTX 4090D. The closer the model\u2019s points are to the lower left corner, the stronger the model is.", "description": "Figure 10 presents a comparison of inference time for text-based human motion generation.  The experiment measured Average Inference Time per Sample (AITS) using the HumanML3D dataset (Guo et al., 2022a).  The testing excluded model and data loading times, and all tests ran on the same RTX 4090D GPU for consistent comparison.  The graph plots FID (Fr\u00e9chet Inception Distance, lower is better) against AITS (lower is better).  The closer a model's point is to the lower-left corner of the plot, indicating both high generation quality (low FID) and fast inference (low AITS), the stronger the model is deemed to be.", "section": "6 EXPERIMENTS"}]