[{"figure_path": "https://arxiv.org/html/2412.15118/x1.png", "caption": "Figure 1: Comparison of code generation paradigms.", "description": "This figure compares three different approaches to code generation: Outcome Supervision, Process Supervision, and Outcome-Refining Process Supervision (the authors' proposed method). Outcome supervision uses only the final output to guide the model. Process supervision utilizes intermediate reasoning steps evaluated by a Process Reward Model (PRM). The authors' method combines both, using concrete execution signals from code execution to directly ground the supervision of reasoning steps, thus avoiding the need for training PRMs and improving reliability.  The figure illustrates the differences in model training, feedback mechanisms, and evaluation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.15118/x2.png", "caption": "Figure 2: Outcome-Refining Process Supervision framework overview. A language model serves as both programmer and critic in a step-by-step reasoning process. Through beam search, the framework maintains multiple solution trajectories, where each state contains reasoning chains, code implementations, and step reward.", "description": "The figure illustrates the Outcome-Refining Process Supervision (ORPS) framework.  The framework uses a language model to iteratively generate and refine code solutions.  The process starts with a problem statement and the model generates an initial reasoning chain and code. This solution is then executed, and feedback is collected on its correctness. This feedback is used to guide the next steps. The framework uses beam search to explore multiple solution trajectories simultaneously. Each state in the search tree includes the reasoning process up to that point, the generated code, and an associated reward score reflecting the progress made towards a correct solution. The model acts as both programmer (generating code) and critic (evaluating and refining solutions), leading to more robust and efficient code generation.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2412.15118/x3.png", "caption": "Figure 3: Multi-dimensional Performance Analysis. Metrics are normalized against the LBPP standard solutions (1.0\u00d7) and averaged across all backbone models. Higher values indicate better performance.", "description": "This figure presents a multi-dimensional performance comparison of different code generation methods on the LBPP benchmark.  Each metric (code length, AST nodes, cyclomatic complexity, cognitive complexity, execution speed, branch mispredictions, page faults, and CPU instructions) is normalized relative to the standard LBPP solutions (set to 1.0x) to facilitate comparison. The results are averaged across multiple backbone models. Higher values on each metric indicate better performance, signifying improved code quality and efficiency.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2412.15118/x4.png", "caption": "Figure 4: Performance vs. Inference Budget. Pass@1 scores on LBPP with varying inference budgets. Our method maintains superior performance across different computational constraints.", "description": "This figure demonstrates the impact of varying inference budgets on the performance of the Outcome-Refining Process Supervision (ORPS) method and baseline methods on the LBPP benchmark.  The x-axis represents the inference budget, and the y-axis represents the Pass@1 score (percentage of problems where the model generates code that passes all test cases).  The lines represent different model architectures.  The key takeaway is that ORPS consistently outperforms baseline methods across a wide range of computational resources, showcasing its robustness and efficiency.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.15118/x5.png", "caption": "Figure 5: Performance by Problem Class. Top-20 problem classes in LBPP showing success rates and unsolved cases for our method vs baseline.", "description": "This figure compares the performance of the proposed Outcome-Refining Process Supervision (ORPS) method against a baseline method across the top 20 problem classes in the LBPP dataset.  The bars represent the success rate (percentage of problems solved correctly) for each method within each problem class.  It visually demonstrates the relative strengths and weaknesses of ORPS compared to the baseline, especially highlighting problem categories where ORPS shows a significant improvement or where both methods struggle.", "section": "4 Experiments"}]