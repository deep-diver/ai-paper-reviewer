[{"content": "LBPP (2024)\n| Model/Method | Pass@1 \u2191 | Tests \u2191 | Valid \u2191 | Time \u2193 | \n|---|---|---|---|---|\n| Llama-3.1-8B-Instruct (2024) CoT | 30.9 | 44.3 | 63.0 | 176.8 | \n| Llama-3.1-8B-Instruct (2024) Reflexion | 34.0 | 49.3 | 67.3 | 148.5 | \n| Llama-3.1-8B-Instruct (2024) LDB (w/ T) | 25.9 | 39.8 | 58.0 | 252.2 | \n| Llama-3.1-8B-Instruct (2024) BoN | 46.9 | 64.7 | 84.6 | 107.6 | \n| Llama-3.1-8B-Instruct (2024) ORPS | 45.9 | 66.9 | 88.5 | 99.1 | \n| Llama-3.1-8B-Instruct (2024) ORPS (w/ T) | 67.1 | 81.4 | 93.7 | 89.4 | \nDeepSeek-Coder-7B-Instruct-v1.5 (2024)\n| Model/Method | Pass@1 \u2191 | Tests \u2191 | Valid \u2191 | Time \u2193 | \n|---|---|---|---|---|\n| DeepSeek-Coder-7B-Instruct-v1.5 (2024) CoT | 32.7 | 45.9 | 67.3 | 160.1 | \n| DeepSeek-Coder-7B-Instruct-v1.5 (2024) Reflexion | 25.9 | 41.9 | 63.0 | 153.0 | \n| DeepSeek-Coder-7B-Instruct-v1.5 (2024) LDB (w/ T) | 31.5 | 45.7 | 61.7 | 206.2 | \n| DeepSeek-Coder-7B-Instruct-v1.5 (2024) BoN | 49.4 | 63.9 | 80.2 | 123.4 | \n| DeepSeek-Coder-7B-Instruct-v1.5 (2024) ORPS | 56.3 | 71.1 | 88.0 | 89.4 | \n| DeepSeek-Coder-7B-Instruct-v1.5 (2024) ORPS (w/ T) | 63.7 | 80.8 | 96.9 | 74.4 | \nQwen-2.5-Coder-7B-Instruct (2024)\n| Model/Method | Pass@1 \u2191 | Tests \u2191 | Valid \u2191 | Time \u2193 | \n|---|---|---|---|---|\n| Qwen-2.5-Coder-7B-Instruct (2024) CoT | 40.1 | 55.3 | 72.2 | 118.6 | \n| Qwen-2.5-Coder-7B-Instruct (2024) Reflexion | 37.7 | 57.1 | 78.4 | 111.2 | \n| Qwen-2.5-Coder-7B-Instruct (2024) LDB (w/ T) | 35.8 | 49.9 | 65.4 | 187.8 | \n| Qwen-2.5-Coder-7B-Instruct (2024) BoN | 53.1 | 68.8 | 85.8 | 117.9 | \n| Qwen-2.5-Coder-7B-Instruct (2024) ORPS | 59.9 | 75.7 | 92.0 | 84.1 | \n| Qwen-2.5-Coder-7B-Instruct (2024) ORPS (w/ T) | 77.8 | 87.9 | 96.9 | 82.4 | \nQwen-2.5-Coder-14B-Instruct (2024)\n| Model/Method | Pass@1 \u2191 | Tests \u2191 | Valid \u2191 | Time \u2193 | \n|---|---|---|---|---|\n| Qwen-2.5-Coder-14B-Instruct (2024) CoT | 53.7 | 63.9 | 77.2 | 119.2 | \n| Qwen-2.5-Coder-14B-Instruct (2024) Reflexion | 60.5 | 70.5 | 82.1 | 113.3 | \n| Qwen-2.5-Coder-14B-Instruct (2024) LDB (w/ T) | 51.9 | 62.9 | 75.3 | 225.2 | \n| Qwen-2.5-Coder-14B-Instruct (2024) BoN | 61.7 | 74.9 | 90.7 | 115.6 | \n| Qwen-2.5-Coder-14B-Instruct (2024) ORPS | 61.7 | 77.4 | 90.7 | 84.8 | \n| Qwen-2.5-Coder-14B-Instruct (2024) ORPS (w/ T) | 85.8 | 90.7 | 95.7 | 64.2 | \nGPT-4o-Mini (2024)\n| Model/Method | Pass@1 \u2191 | Tests \u2191 | Valid \u2191 | Time \u2193 | \n|---|---|---|---|---|\n| GPT-4o-Mini (2024) CoT | 50.0 | 65.9 | 80.2 | 124.5 | \n| GPT-4o-Mini (2024) Reflexion | 62.3 | 73.9 | 87.7 | 93.2 | \n| GPT-4o-Mini (2024) LDB (w/ T) | 54.9 | 67.8 | 82.7 | 220.1 | \n| GPT-4o-Mini (2024) BoN | 64.2 | 78.6 | 93.8 | 88.9 | \n| GPT-4o-Mini (2024) ORPS | 67.9 | 81.2 | 94.4 | 81.5 | \n| GPT-4o-Mini (2024) ORPS (w/ T) | 88.9 | 94.3 | 98.1 | 61.6 | \nHumanEval (2021b)\n| Model/Method | Pass@1 \u2191 | Tests \u2191 | Valid \u2191 | Time \u2193 | \n|---|---|---|---|---|\nMBPP (2021)\n| Model/Method | Pass@1 \u2191 | Tests \u2191 | Valid \u2191 | Time \u2193 | \n|---|---|---|---|---|", "caption": "Table 1: Main Results on Code Generation Benchmarks. Pass@1: solutions passing all test cases. Tests: average test cases passed. Valid: solutions that compile and execute. Time: relative execution time, compared to the standard solution. Best results are in bold and second-best are underlined, every metric is in percentage.", "description": "This table presents the performance of different code generation models and methods across three benchmarks: LBPP, HumanEval, and MBPP.  For each model and method (including several baselines and the proposed ORPS method), the table shows the percentage of successful solutions (Pass@1), the average percentage of test cases passed, the percentage of solutions that compiled and ran successfully (Valid), and the relative execution time compared to a standard solution.  The best results for each metric are bolded, and the second-best are underlined. This allows for a comparison of the effectiveness and efficiency of various code generation approaches on different types and complexities of coding tasks.", "section": "4 Experiments"}, {"content": "|---|---|---|---|\n|  | **LBPP** | **HumanEval** | **MBPP** |\n|  | (Matton et al., 2024) | (Chen et al., 2021b) | (Austin et al., 2021) |\n| # Test Problems | 162 | 164 | 257<sup>\u2020</sup> |\n| # Unit Tests | 5.1 | 6.5 | 3.0 |\n| Solution Length<sup>\u00a7</sup> | 627 / 3039 | 169 / 622 | 130 / 589 |\n| Contamination | New Dataset | 18.9%<sup>\u2021</sup> | 20.8%<sup>\u2021</sup> |\n| Difficulty | **Competitive Programming** | **Basic Functions** | **Basic Functions** |\n| Task Type | Algorithms | Func. Completion | Basic Prog. |", "caption": "Table 2: Dataset Statistics. Characteristics of the programming benchmarks used in evaluation.", "description": "This table presents a comparative overview of three programming benchmarks (LBPP, HumanEval, and MBPP) used in the paper's evaluation.  For each benchmark, it details the number of programming problems, unit tests, the average and maximum length of solution code, the percentage of code contamination (data leakage from pre-training corpora), the general difficulty level (basic or competitive), and the task type (functions, algorithms, or function completion). This information is crucial for understanding the characteristics of the datasets and how they might influence the performance of different code generation models.", "section": "4.1 Experimental Setup"}, {"content": "| Method | Pass@1\u2191 | Tests\u2191 | Valid\u2191 | Time\u2193 |\n|---|---|---|---|---|\n| **ORPS** | 59.9 | 75.7 | 92.0 | 84.1 |\n|  - Execution | 43.8 | 56.4 | 72.8 | 200.5 |\n|  - Reasoning | 55.6 | 74.5 | 94.4 | 124.5 |", "caption": "Table 3: Ablation Study Results. - Execution: Remove execution feedback from our framework. - Reasoning: Remove in-depth reasoning process. Every metric is in percentage.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of two key components in the Outcome-Refining Process Supervision (ORPS) framework: execution feedback and in-depth reasoning.  It compares the performance of the complete ORPS model against two variants: one where execution feedback is removed, and another where the in-depth reasoning process is removed.  The performance metrics, all expressed as percentages, include the success rate (Pass@1), the average percentage of test cases passed, and the percentage of valid solutions generated. The time metric represents the relative execution time compared to a standard solution.", "section": "3.3 Execution-Guided Process Reward Model"}, {"content": "| Category | Metric | Description |\n|---|---|---|\n| **Dynamic Execution Profiling** |  |  |\n|  | Time Enabled | Total CPU time spent executing the code, measured in nanoseconds. Lower values indicate more efficient execution and better algorithmic optimization. |\n|  | Instruction Count | Number of CPU instructions executed during runtime. Reflects computational efficiency, with lower counts suggesting more optimized code paths and better algorithm implementation. |\n|  | Branch Misses | Frequency of incorrect branch predictions during execution. Lower values indicate better code predictability and CPU pipeline efficiency, resulting in faster execution times. |\n|  | Page Faults | Number of times the program needs to access virtual memory. Fewer page faults suggest better memory management and more efficient memory access patterns. |\n| **Static Analysis** |  |  |\n|  | Code Length | Total number of lines in the source code. Generally, shorter code length indicates more concise solutions while maintaining readability and functionality. |\n|  | AST Node Count | Number of nodes in the Abstract Syntax Tree. Measures structural complexity of the code, with fewer nodes suggesting simpler and more maintainable implementation. |\n|  | Cyclomatic Complexity | Quantifies the number of linearly independent paths through the code. Lower values indicate easier-to-maintain and test code, reducing potential bug sources. |\n|  | Cognitive Complexity | Measures how difficult the code is to understand, based on control flow structures and nesting. Lower scores suggest more readable and maintainable code that is easier to debug. |", "caption": "Table 4: Analysis of Process Reward Model.\u00a0Granularity refers to the level of detail in the reward signal (line-level or outcome-level). Train indicates whether the process reward model requires training.", "description": "This table presents an ablation study comparing different process reward model setups within the Outcome-Refining Process Supervision framework.  It examines the impact of reward signal granularity (line-level vs. outcome-level feedback) and whether the process reward model requires explicit training on the model's performance. Metrics such as Pass@1 (percentage of solutions passing all test cases), Tests (average number of test cases passed), Valid (percentage of solutions that compile and execute), and Time (relative execution time) are used to assess the effectiveness of each setup.", "section": "3.3 Execution-Guided Process Reward Model"}]