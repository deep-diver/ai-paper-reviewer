[{"figure_path": "https://arxiv.org/html/2504.14899/x1.png", "caption": "Figure 1: Given a single-view image across various domains (e.g., real-world, text-to-image, animation), we first extract the monocular depth and focal length of it via Depth-Pro\u00a0[6] and then achieve relative point clouds. Then, the proposed Uni3C can generate impressive videos under arbitrary camera trajectories (a), human motion characters (SMPL-X\u00a0[36]), or both of these conditions (b).\n(c) Uni3C further supports the camera-controlled motion transfer.", "description": "Figure 1 demonstrates the capabilities of the Uni3C framework. Starting with a single image from various sources (real-world scenes, text-to-image outputs, or animation), the system first estimates depth and focal length using Depth-Pro to generate a point cloud representation of the scene.  Uni3C then leverages this 3D information to generate videos controlled in three ways: (a) by specifying arbitrary camera trajectories; (b) by providing human motion data using SMPL-X characters, or (c) by combining both camera control and human motion control. Finally, (c) shows that Uni3C can transfer motion from a reference video to a new video with camera control.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.14899/x2.png", "caption": "Figure 2: The overview pipeline of PCDController. PCDController is built as a lightweight DiT trained from scratch. We first obtain the point clouds via the monocular depth extracted from the first view. Then, the point clouds are warped and rendered into the video Vp\u2062c\u2062dsubscript\ud835\udc49\ud835\udc5d\ud835\udc50\ud835\udc51V_{pcd}italic_V start_POSTSUBSCRIPT italic_p italic_c italic_d end_POSTSUBSCRIPT. The input conditions for PCDController comprise rendered Vp\u2062c\u2062dsubscript\ud835\udc49\ud835\udc5d\ud835\udc50\ud835\udc51V_{pcd}italic_V start_POSTSUBSCRIPT italic_p italic_c italic_d end_POSTSUBSCRIPT, Pl\u00fccker ray \ud835\udc0f\ud835\udc0f\\mathbf{P}bold_P, and the noisy latent ztsubscript\ud835\udc67\ud835\udc61z_{t}italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT.\nNote that only the PCDController and camera encoder are trainable in our framework.", "description": "Figure 2 illustrates the architecture of PCDController, a lightweight module designed for precise camera control in video generation.  It starts by extracting monocular depth from the initial video frame to generate a point cloud. This point cloud is then warped and rendered into a video representation (Vpcd).  PCDController takes this rendered video (Vpcd), Pl\u00fccker ray coordinates (P), and a noisy latent video representation (zt) as input.  Importantly, only PCDController itself and the camera encoder are trainable; the rest of the video generation model remains frozen. This design makes PCDController easily adaptable and generalizable.", "section": "4.1 PCDController with 3D Geometric Priors"}, {"figure_path": "https://arxiv.org/html/2504.14899/x3.png", "caption": "Figure 3: Results of PCDController with imperfect point clouds.\nBenefiting from the well-preserved capacity from VDM, PCDController enjoys robust generation with inferior point clouds.", "description": "This figure demonstrates the robustness of the PCDController module, a key component of the Uni3C framework.  Even when provided with imperfect or noisy point cloud data (due to limitations in depth estimation), the PCDController consistently produces high-quality video generations. This showcases the model's ability to generalize well beyond perfectly annotated data and its effective use of strong 3D priors from point clouds, even when those priors are not fully accurate. The preserved capacity from the underlying video diffusion model (VDM) enables this robustness and ensures that the generation process is not significantly impacted by inaccuracies in the input.", "section": "4.1 PCDController with 3D Geometric Priors"}, {"figure_path": "https://arxiv.org/html/2504.14899/x4.png", "caption": "Figure 4: The assignment of multi-modal conditions for Uni3C. Camera, point clouds, SMPL-X\u00a0[36] and Hamer\u00a0[37] are all presented as 3D conditions.", "description": "Figure 4 illustrates the input modalities used in the Uni3C framework for controlling both camera and human motion in video generation.  It shows how the model integrates multiple 3D representations: camera parameters (including trajectory), point clouds representing the scene's geometry, a SMPL-X model for human body shape and pose, and a Hamer model for hand details.  These 3D inputs are unified to generate consistent and coherent videos, demonstrating the core concept of combining diverse 3D data sources within Uni3C for superior control.", "section": "4 Method"}, {"figure_path": "https://arxiv.org/html/2504.14899/x5.png", "caption": "Figure 5: The overview of global 3D world guidance.\n(a) We first align the SMPL-X characters from the human world space Wh\u2062u\u2062msubscript\ud835\udc4a\u210e\ud835\udc62\ud835\udc5aW_{hum}italic_W start_POSTSUBSCRIPT italic_h italic_u italic_m end_POSTSUBSCRIPT to the environment world space We\u2062n\u2062vsubscript\ud835\udc4a\ud835\udc52\ud835\udc5b\ud835\udc63W_{env}italic_W start_POSTSUBSCRIPT italic_e italic_n italic_v end_POSTSUBSCRIPT comprising dense point clouds. (b) GeoCalib\u00a0[53] is used to calibrate the gravity direction of SMPL-X. (c) The rigid transformation coefficients s~,R~,t~~\ud835\udc60~\ud835\udc45~\ud835\udc61\\tilde{s},\\tilde{R},\\tilde{t}over~ start_ARG italic_s end_ARG , over~ start_ARG italic_R end_ARG , over~ start_ARG italic_t end_ARG are employed to align the whole SMPL-X sequence. We re-render all aligned conditions under specific camera trajectories as the global 3D world guidance.", "description": "Figure 5 illustrates the process of aligning human motion data (represented by SMPL-X characters) with environmental data (point clouds) to create a unified 3D scene for video generation.  Panel (a) shows the initial alignment of SMPL-X characters from their original coordinate system (Whum) to the environment's coordinate system (Wenv), which is based on the point clouds.  Panel (b) depicts the calibration of the gravity direction of the SMPL-X characters using GeoCalib, ensuring physically realistic motion.  Finally, panel (c) demonstrates the application of a rigid transformation (using coefficients ~s, ~R, ~t) to align the entire SMPL-X sequence, followed by re-rendering with the camera trajectories, thus creating a consistent global 3D world guidance for both camera and human motion.", "section": "4.3 Global 3D World Guidance"}, {"figure_path": "https://arxiv.org/html/2504.14899/x6.png", "caption": "Figure 6: Ablation results of Pl\u00fccker ray and point clouds during the training phase. Point clouds enjoy highly accurate camera control against Pl\u00fccker ray.", "description": "This figure displays ablation study results comparing the use of Pl\u00fccker ray and point cloud representations for camera control during the training phase of a video generation model.  The graph shows training loss curves for both methods over a number of iterations, demonstrating the superior performance of point clouds in achieving highly accurate camera control compared to Pl\u00fccker rays.  Point clouds provide strong 3D priors which significantly enhance camera control accuracy, as evidenced by the lower and more stable loss curve.", "section": "4.1 PCDController with 3D Geometric Priors"}, {"figure_path": "https://arxiv.org/html/2504.14899/x7.png", "caption": "Figure 7: Rendering results with and without gravity calibration by GeoCalib\u00a0[53].", "description": "Figure 7 demonstrates the impact of gravity calibration on the quality of video generation using the Uni3C framework.  The left side shows results without gravity calibration, highlighting issues like unnatural poses and movements, especially during challenging camera trajectories. The right side shows results with gravity calibration using GeoCalib [53], resulting in more natural and physically plausible human motion and overall improved video generation.", "section": "4.3 Global 3D World Guidance"}, {"figure_path": "https://arxiv.org/html/2504.14899/x8.png", "caption": "Figure 8: Our out-of-distribution benchmark for camera control. The validation set includes generative, human, scene-level, and object-level images with diverse aspect ratios.", "description": "This figure showcases a variety of images used as an out-of-distribution benchmark for testing the camera control capabilities of the proposed Uni3C framework.  The images are categorized into four main types: generative (computer-generated), human (featuring humans), scene-level (capturing broader scenes), and object-level (focused on specific objects). The diverse aspect ratios of these images highlight the model's robustness in handling various image formats and compositions.", "section": "5.2 Results of Camera Control"}, {"figure_path": "https://arxiv.org/html/2504.14899/x9.png", "caption": "Figure 9: Qualitative results of camera control on our benchmark.\nWe compare the proposed PCDController to ViewCrafter\u00a0[66], SEVA\u00a0[71], and our model without point cloud guidance. The leftmost image is the reference condition. \u201cfull\u201d indicates using both Pl\u00fccker ray and point clouds as conditions.", "description": "Figure 9 presents a qualitative comparison of camera control methods on a benchmark dataset.  The benchmark includes diverse scenes with various characteristics. Four methods are compared: ViewCrafter, SEVA, a baseline using only Pl\u00fccker rays (a type of camera control), and the proposed Uni3C model using both Pl\u00fccker rays and point clouds. For each scene, the leftmost image shows the reference image, while subsequent images show the results of each method. The 'full' label in the figure indicates that both Pl\u00fccker rays and point clouds were used in the camera control.", "section": "5.2 Results of Camera Control"}, {"figure_path": "https://arxiv.org/html/2504.14899/x10.png", "caption": "Figure 10: Results of the challenging orbital 360\u2218 rotations from PCDController. The leftmost images are the reference views.", "description": "This figure showcases the robustness of the PCDController module in handling challenging camera movements.  It presents results from a test set where the camera performs a 360-degree orbital rotation around various scenes. Each row displays a different scene, with the leftmost image being the reference view from which the depth is extracted to drive the camera trajectory in subsequent frames. The effectiveness of the controller is demonstrated by the consistent and smooth generation of the video across all 360-degree rotations. The results illustrate the controller's ability to maintain video quality while generating complex camera motions.", "section": "5.2 Results of Camera Control"}, {"figure_path": "https://arxiv.org/html/2504.14899/x11.png", "caption": "Figure 11: Results of unified camera and human motion controls. The leftmost images are the reference views, while the first row indicates the aligned 3D world guidance.", "description": "This figure demonstrates the results of the Uni3C framework, showcasing its ability to unify camera and human motion controls for video generation.  The leftmost column shows the reference image or video frame. The top row shows the aligned 3D world guidance, illustrating how the framework integrates camera trajectories (point clouds) and human motion (SMPL-X characters) into a unified 3D scene representation before video generation. Subsequent rows display the generated video sequences from different methods: RealisDance-DiT (a baseline focusing on human motion control), PCDController (Uni3C's camera control module), and Uni3C (I2V and T2V versions, representing the full framework). This visualization highlights how Uni3C effectively coordinates both camera and human motion controls to produce more coherent and controllable videos than baselines.", "section": "4.3 Global 3D World Guidance"}, {"figure_path": "https://arxiv.org/html/2504.14899/x12.png", "caption": "Figure 12: Results of unified camera, human motion, and Hamer controls. The leftmost images are the reference views, while the first and second rows indicate the aligned 3D world guidance and Hamer rendering, respectively.", "description": "Figure 12 presents a visual comparison of video generation results using Uni3C, focusing on the unified control of camera, human motion (SMPL-X), and hand animation (Hamer).  The leftmost column displays the original reference images. The next two rows demonstrate the aligned 3D world guidance utilized by Uni3C.  Specifically, the first row shows how the point cloud data representing the scene environment and the SMPL-X human character model are aligned into a shared 3D coordinate system before being used to control the video generation. The second row showcases the rendered results of the Hamer hand model integrated with the aligned 3D environment, highlighting the seamless integration of multiple control signals within Uni3C.", "section": "4.3 Global 3D World Guidance"}, {"figure_path": "https://arxiv.org/html/2504.14899/x13.png", "caption": "Figure 13: Results of motion transfer. The first row indicates the reference video, while others show our generated videos transferring motions from the reference sequence.", "description": "This figure demonstrates the motion transfer capabilities of the Uni3C framework.  The top row displays a reference video showcasing a specific set of motions.  The rows below show videos generated by Uni3C that have transferred the motions from the reference video, but applied to different characters and in different settings (different backgrounds, different camera angles). This highlights Uni3C's ability to transfer motions seamlessly across diverse video scenarios and character models.", "section": "4.3 Global 3D World Guidance"}, {"figure_path": "https://arxiv.org/html/2504.14899/x14.png", "caption": "Figure 14: Results transferred from random integrated motion clips of BABEL\u00a0[41]. The motion sequences are listed on the left, which are executed from light to dark colors.", "description": "Figure 14 showcases the motion transfer capabilities of Uni3C.  It demonstrates the model's ability to generate videos with motions sourced from random, integrated motion clips from the BABEL dataset [41]. The figure displays multiple rows, each starting with a reference motion sequence (left column) that is then transferred to generate corresponding videos. The order of execution follows a light-to-dark color scheme, with the lighter colored videos earlier in each row and the darker videos later.  This highlights how Uni3C can adapt various motions to the specified scenes and camera controls.", "section": "4.3 Global 3D World Guidance"}, {"figure_path": "https://arxiv.org/html/2504.14899/x15.png", "caption": "Figure 15: Failed cases generated by Uni3C. These results are primarily limited by the conflict between human motions and environments.", "description": "Figure 15 showcases examples where the Uni3C model failed to generate realistic videos.  These failures stem from inconsistencies between the human actions and their surrounding environment. For instance, a human character's movement might be physically impossible given the constraints of the scene geometry (e.g., a person walking through a wall), resulting in unnatural or unrealistic video output.", "section": "5.4 Ablation Study and Exploratory Discussions"}]