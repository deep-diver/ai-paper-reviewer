[{"figure_path": "https://arxiv.org/html/2504.14717/x1.png", "caption": "Figure 1: TAPIP3D performs long-term 3D point tracking in a persistent 3D world space of 3D feature clouds, which exceeds prior 3D point tracking methods\u00a0[44, 28] operating in camera-dependent UV pixels + Depth (UVD) spaces. We leverage the given / estimated depth map and camera pose from MegaSaM\u00a0[23] to compute a 3D space where camera motion is cancelled.\nTAPIP3D designs a Local Pair Attention for featurization and iterative motion estimation. The 3D motion trajectories for the sampled dynamic points in 3D XYZ world space are significantly smoother and more linear then UVD space.", "description": "Figure 1 illustrates TAPIP3D's core functionality: tracking 3D points over time in a consistent 3D world space.  Unlike previous methods that track points in camera-centric 2D image coordinates plus depth (UVD space), which are affected by camera movement, TAPIP3D leverages depth and camera pose estimations (from MegaSaM [23]) to transform the data into a 3D world space. This removes camera motion artifacts, resulting in smoother and more linear 3D trajectories.  The figure showcases how TAPIP3D uses Local Pair Attention to extract features and iteratively refine motion estimates for precise 3D point tracking.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.14717/x2.png", "caption": "Figure 2: Architecture of our\u00a0TAPIP3D. The model takes RGB frames and corresponding 3D point maps as input, computes features from the RGB frames, and transfers them to the 3D points, forming a feature cloud for each timestep.\nUsing either provided or estimated camera poses, these feature clouds can be arranged in either world space or camera space.\nWe then apply our Local Pair Attention module (Figure\u00a03) to extract correlation features, followed by our 3D Trajectory Transformer, which iteratively updates the estimated trajectories.\nTop right: Illustration of the difference between 3D k\ud835\udc58kitalic_k-NN (used in our approach) and fixed 2D neighborhoods (used in prior works\u00a0[17, 6]).", "description": "Figure 2 illustrates the TAPIP3D architecture.  It begins by taking RGB video frames and their corresponding 3D point maps as input.  Features are extracted from the RGB frames and transferred to the 3D points, creating a feature cloud for each time step.  These feature clouds can be represented in either world space or camera space, depending on whether camera pose information is available.  The core of the method is the Local Pair Attention module (detailed in Figure 3), which extracts correlation features that capture the spatiotemporal relationships among 3D points. These features are then processed by a 3D Trajectory Transformer to iteratively refine the estimated 3D point trajectories over time. The top-right inset highlights the key difference between TAPIP3D's 3D k-NN approach for finding neighboring points and the traditional fixed 2D neighborhood approach used in previous methods.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.14717/x3.png", "caption": "Figure 3: Local Pair Attention. Given a 3D query point at a specific timestep, we first identify its local 3D context using k\ud835\udc58kitalic_k-NN to form a query group. Then, within the point cloud at another timestep, we find K\ud835\udc3eKitalic_K nearest 3D neighbors to construct a key group. We apply bi-directional cross-attention between the query and key groups to capture spatio-temporal correspondences.", "description": "This figure illustrates the Local Pair Attention mechanism used in TAPIP3D for 3D point tracking.  A query point at a given timestep has its local 3D neighbors identified using k-NN to form a 'query group'.  At a future timestep, the K nearest 3D neighbors of this query group are located within the point cloud and form a 'key group'. Bi-directional cross-attention between these groups captures both spatial and temporal relationships for enhanced accuracy in tracking the trajectory of the 3D point across time.", "section": "3. Method"}]