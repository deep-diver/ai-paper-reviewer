[{"figure_path": "https://arxiv.org/html/2503.04808/x1.png", "caption": "Figure 1: Evaluation accuracy as a function of the number of allowed attempts during evaluation, averaged across five benchmarks: AIME 2024, MATH 500, AMC 2023, Minerva Math, and OlympiadBench. Both LLMs are based on Qwen 2.5 Math 1.5B and fine-tuned via RL on a small math dataset in either multi-attempt tasks or single-turn tasks (baseline).", "description": "This figure displays the evaluation accuracy of two large language models (LLMs) across five different math benchmarks (AIME 2024, MATH 500, AMC 2023, Minerva Math, and OlympiadBench).  The accuracy is shown as a function of the number of attempts allowed during evaluation. Both LLMs are based on the same foundation model (Qwen 2.5 Math 1.5B) but are trained differently: one using a multi-attempt reinforcement learning (RL) approach and the other using a standard single-turn RL approach (the baseline).  The graph allows for a direct comparison of how the models perform with an increasing number of attempts, highlighting the impact of the multi-attempt training strategy.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04808/extracted/6250029/fig/illust.png", "caption": "Figure 2: Illustration of the multi-attempt question-answer task. We extend the single-turn question-answer task from DeepSeek R1 to a multi-attempt setting, enabling iterative refinement.", "description": "This figure illustrates the core concept of the paper: a multi-attempt question answering task.  It contrasts this approach with the traditional single-turn question-answering task. The diagram shows how, in the multi-attempt scenario, the model receives feedback (whether its answer is correct or incorrect) after each attempt. If the answer is incorrect, the model is given another chance to refine its response. This process of iterative feedback and refinement is the key innovation of the paper, enabling the model to learn and improve reasoning capabilities through reinforcement learning. The single-turn task, in comparison, offers only one opportunity for the model to answer and receive feedback.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.04808/x2.png", "caption": "Figure 3: An example of a multi-attempt dialogue (N=2\ud835\udc412N=2italic_N = 2) from a fine-tuned LLM, where the LLM makes a mistake on the first attempt but learns to correct it in the second attempt.", "description": "This figure displays a dialogue between a user and a large language model (LLM) fine-tuned for a multi-attempt question-answering task.  The task involves answering a math problem. The LLM is given two attempts (N=2). In the first attempt, it provides an incorrect answer.  The user provides feedback, indicating the answer is wrong.  In the second attempt, guided by this feedback, the LLM correctly solves the problem. The dialogue shows the LLM's thought process and highlights its ability to learn from its mistakes and refine its answer through multiple attempts.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.04808/x3.png", "caption": "(a) Training Reward", "description": "This figure shows the training reward over training steps for both the multi-attempt and baseline LLMs.  The y-axis represents the average reward received per training step. The x-axis represents the number of training steps completed.  The plot visualizes the learning progress of both models, demonstrating the difference in reward accumulation between the multi-attempt model (which receives feedback and can refine its response) and the baseline single-attempt model.  The multi-attempt LLM consistently achieves higher rewards, indicating more effective learning.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04808/x4.png", "caption": "(b) Average Evaluation Accuracy", "description": "This figure shows the average evaluation accuracy across five benchmarks (AIME 2024, MATH 500, AMC 2023, Minerva Math, and OlympiadBench) as a function of training steps.  It compares the performance of two LLMs: one trained on a multi-attempt task and a baseline LLM trained on a standard single-turn task. The plot helps visualize how the accuracy of each model improves (or doesn't improve) over the course of training. This is important for assessing the effectiveness of the multi-attempt training approach.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04808/x5.png", "caption": "Figure 4: Training and evaluation performance of the LLMs. (a) Training reward as a function of training steps. (b) Average evaluation accuracy across five benchmarks as a function of training steps, evaluated under the standard single-attempt setting.", "description": "This figure displays the training and evaluation results for two LLMs: one trained with a multi-attempt reinforcement learning (RL) approach and a baseline model trained with a standard single-attempt RL approach.  Subfigure (a) shows the training reward over training steps, demonstrating that the multi-attempt LLM consistently receives higher rewards. Subfigure (b) presents the average evaluation accuracy across five different math benchmarks as a function of training steps, all evaluated using a single-attempt setting. This part highlights that even under the standard single-attempt evaluation protocol, the multi-attempt LLM achieves slightly better performance than the baseline model.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04808/x6.png", "caption": "(a) AIME 2024", "description": "This figure shows the evaluation accuracy for the AIME 2024 benchmark as a function of the number of allowed attempts during evaluation.  The accuracy is compared between a model trained with a multi-attempt approach and a baseline model trained on a standard single-turn task.  The x-axis represents the number of attempts allowed, and the y-axis represents the accuracy. The graph shows that the multi-attempt model consistently outperforms the baseline model, demonstrating the effectiveness of the multi-attempt training.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04808/x7.png", "caption": "(b) MATH500", "description": "The figure shows the evaluation accuracy of two different LLMs on the MATH500 benchmark as a function of the number of allowed attempts. One LLM was trained on a multi-attempt task, allowing it to refine responses based on feedback, and the other LLM was trained on a standard single-turn task. The graph compares their performance across different numbers of attempts during evaluation.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.04808/x8.png", "caption": "(c) AMC 2023", "description": "The figure shows the evaluation accuracy on the AMC 2023 benchmark as a function of the number of allowed attempts during evaluation.  It compares the performance of a model trained with a multi-attempt approach to a baseline model trained with a standard single-attempt approach. The x-axis represents the number of attempts, and the y-axis represents the accuracy.  The plot visually demonstrates the improvement in accuracy gained by allowing multiple attempts, especially for the multi-attempt trained model.", "section": "Experiments"}]