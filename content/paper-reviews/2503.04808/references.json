{"references": [{"fullname_first_author": "Daya Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-12", "reason": "This paper is a core reference as it outlines DeepSeek R1, a key example of using reinforcement learning to improve the reasoning capabilities of large language models, which the current paper builds upon."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces Proximal Policy Optimization (PPO), the reinforcement learning algorithm used in this work, making it fundamental to the methodology."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper describes the use of human feedback in training language models, a standard technique (RLHF) that the authors compare their approach to."}, {"fullname_first_author": "Aviral Kumar", "paper_title": "Training language models to self-correct via reinforcement learning", "publication_date": "2024-09-12", "reason": "This paper's approach to training language models to self-correct via reinforcement learning is very similar to the current paper's approach."}, {"fullname_first_author": "Noah Shinn", "paper_title": "Reflexion: Language agents with verbal reinforcement learning", "publication_date": "2023-01-01", "reason": "This paper's method of self-reflection in language agents is related to the self-correction ability that the authors want to test."}]}