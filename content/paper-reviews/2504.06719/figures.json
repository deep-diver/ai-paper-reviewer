[{"figure_path": "https://arxiv.org/html/2504.06719/x2.png", "caption": "Figure 1: Self-Supervised Feature Visualization using PCA.\nWe reduce the point features obtained with our self-supervised model to three dimensions using PCA and visualize them as colors.\nFeatures learned by our model are semantic-aware, which is visible from the color separation: Similar objects result in similar features, such as the sofas in the first figure or the chairs in the last one, while different objects result in different features, such as the counter and the tables in the second image or the crib and the curtains in the third one.", "description": "This figure visualizes features learned by a self-supervised model using Principal Component Analysis (PCA).  The model's point cloud features are reduced to three dimensions using PCA and mapped to colors.  The resulting color-coded point clouds show that semantically similar objects (e.g., sofas, chairs) have similar feature representations (similar colors), while dissimilar objects (e.g., counter, table; crib, curtains) have distinct feature representations (different colors). This demonstrates that the learned features possess semantic awareness.", "section": "Self-Supervised Feature Visualization"}, {"figure_path": "https://arxiv.org/html/2504.06719/x3.png", "caption": "Figure 2: Pilot study. Our hierarchical features uncover better performance in all self-supervised models.\nMoreover, our study shows that existing approaches exhibit a large performance gap between supervised and self-supervised training.", "description": "This figure shows the results of a pilot study comparing the performance of different self-supervised methods for 3D scene understanding using hierarchical and last-layer features.  The results demonstrate that hierarchical features, which incorporate information from multiple layers of the model, significantly improve the performance of all self-supervised models. The study also highlights a substantial gap in performance between self-supervised and supervised models, even when using the better-performing hierarchical features. This gap underscores the challenges in achieving comparable performance using self-supervised methods versus supervised methods in 3D scene understanding.", "section": "3. Feature Evaluation Protocol"}, {"figure_path": "https://arxiv.org/html/2504.06719/x4.png", "caption": "Figure 3: Hierarchical features", "description": "This figure illustrates the process of extracting hierarchical features from a UNet-like architecture commonly used in 3D scene understanding.  Instead of using only the output of the final layer, the method uses tri-linear interpolation to upsample feature maps from each decoder level. These upsampled features are then combined to create a rich, task-agnostic feature map that captures hierarchical information from all levels.  This approach provides a more comprehensive representation of the learned features than using the output of the last layer alone.", "section": "3. Feature Evaluation Protocol"}, {"figure_path": "https://arxiv.org/html/2504.06719/x5.png", "caption": "Figure 4: Overview. Our method receives as input a 3D scene represented as a pointcloud, (a). The scene is voxelized into two different views, (b), and then further cropped and masked, (c). The student model first encodes the cropped views and then adds the masked voxels with a learnable token, (d). The decoder processes the cropped views and reconstructs deep features of the masked tokens, (e). The loss is computed in a cross-view manner where the target features, (f), are obtained from a teacher model updated with EMA.", "description": "Figure 4 illustrates the Masked Scene Modeling framework.  It begins with a 3D point cloud (a) which is voxelized into two distinct views (b). These views are then cropped and randomly masked to create partial observations (c). A 'student' model processes the unmasked portions of the cropped views (d) and incorporates the masked voxel information using a learnable token. Next, a decoder reconstructs the deep features from the masked regions (e).  Finally, a 'teacher' model, trained using an exponential moving average (EMA) of the student model's parameters, provides the ground truth features for calculating a cross-view reconstruction loss (f).", "section": "4. Masked Scene Modeling"}, {"figure_path": "https://arxiv.org/html/2504.06719/x6.png", "caption": "Figure 5: Hierarchical reconstruction. The masked voxelization is processed by our hierarchical encoder. The decoder processes the encoded features in a bottom-up manner by first including the masked voxels with a learnable token. Each level is used in the loss computation before the decoded features are upscaled and combined with the skip connection from the previous level.", "description": "This figure illustrates the hierarchical reconstruction process within the Masked Scene Modeling framework.  The input is a voxelized 3D scene with masked regions.  A hierarchical encoder processes the unmasked portions. The decoder then reconstructs the masked regions in a bottom-up fashion, starting with the deepest layer's features.  At each decoder level, masked features are added using a learnable token, the loss is computed, and the output is upsampled and combined with skip connections from the preceding level. This process iterates until the final feature map is generated at the original voxel resolution, effectively integrating information from multiple levels of the network.", "section": "4. Masked Scene Modeling"}, {"figure_path": "https://arxiv.org/html/2504.06719/x7.png", "caption": "Figure 6: Qualitative results.\nFeature visualization of off-the-shelf features of our method and the baselines.\nOur learned features align with semantic classes better than existing methods.", "description": "This figure displays a qualitative comparison of feature visualizations from different self-supervised learning models, including the proposed method and several baselines.  Each image shows a 3D point cloud where points are colored according to their feature vectors reduced to three dimensions using Principal Component Analysis (PCA). The goal is to visually demonstrate the semantic awareness of learned features. The authors claim that their model's features exhibit better alignment with semantic classes (meaning similar objects have similar colors) than those learned by the compared methods.", "section": "5. Qualitative evaluation"}, {"figure_path": "https://arxiv.org/html/2504.06719/x8.png", "caption": "Figure 7: Qualitative results.\nFeature visualization of off-the-shelf features of our method and the baselines.\nOur learned features align with semantic classes better than existing methods.", "description": "This figure visualizes the semantic features learned by different self-supervised methods, including the proposed method and several baselines.  Principal Component Analysis (PCA) is used to reduce the high-dimensional feature vectors to three dimensions, which are then mapped to colors. Each point in the 3D point cloud is colored according to its feature vector. The visualization shows that features learned by the proposed method exhibit a clearer separation based on semantic class compared to the baselines, suggesting that the proposed method learns more semantically meaningful features.", "section": "Additional Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2504.06719/x9.png", "caption": "Figure 8: Masking ratio.\nLinear probing performance for different masking ratios.", "description": "This figure shows the impact of varying the masking ratio on the performance of the Masked Scene Modeling method.  The x-axis represents the percentage of voxels masked in the input point cloud, while the y-axis shows the mean Intersection over Union (mIoU) achieved on a semantic segmentation task using linear probing. The plot demonstrates the model's robustness to different masking ratios within a certain range (approximately 30% to 60%), with optimal performance around 40%. Outside this range, performance degrades significantly.", "section": "C. Ablation Studies"}]