{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "LoRA: Low-Rank Adaptation of Large Language Models", "publication_date": "2022-04-25", "reason": "This paper introduces LoRA, a key parameter-efficient fine-tuning technique that this work seeks to improve upon."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "QLORA: Efficient Finetuning of Quantized LLMs", "publication_date": "2023-12-10", "reason": "This paper introduces QLoRA, a technique that combines quantization with LoRA, that is related and complimentary to the approach in this paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-18", "reason": "This paper presents LLaMA-2, the large language model family used as the base model in this study."}, {"fullname_first_author": "Elias Frantar", "paper_title": "SparseGPT: Massive language models can be accurately pruned in one-shot", "publication_date": "2023-07-23", "reason": "This paper presents SparseGPT, a one-shot pruning method and important baseline used in the current study."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling Laws for Neural Language Models", "publication_date": "2020-01-28", "reason": "This paper describes scaling laws of neural language models, that are used to justify the approach in the current paper."}]}