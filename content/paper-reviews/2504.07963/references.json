{"references": [{"fullname_first_author": "Patrick Esser", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces Latent Diffusion Models (LDMs), a key method that has become a dominant approach in generative modeling, and the work the current paper aims to improve upon."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This is another mention of the LDM work, solidifying its importance to the current paper as a baseline and point of comparison."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable Diffusion Models with Transformers", "publication_date": "2023-01-01", "reason": "This paper introduces a diffusion model approach that uses Transformers, which the current paper uses as a base for its own architecture."}, {"fullname_first_author": "Yaron Lipman", "paper_title": "Flow Matching for Generative Modeling", "publication_date": "2023-01-01", "reason": "This paper introduces Flow Matching, the specific technique used in the current paper to model the generative process."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All You Need", "publication_date": "2017-01-01", "reason": "This foundational paper introduces the Transformer architecture, which serves as the base architecture upon which the model in the current work is built."}]}