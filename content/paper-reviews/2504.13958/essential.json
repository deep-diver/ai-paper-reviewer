{"importance": "This paper pioneers RL application to TIR, presenting insights for reward design. Its generalizable framework, validated by GRPO, boosts tool use, guiding future LLM-agent research and enhancing automated solutions. It opens avenues for exploring complex reasoning and collaboration.", "summary": "ToolRL: RL can significantly enhance tool learning in LLMs by careful reward design, improving performance and generalization.", "takeaways": ["Longer reasoning traces aren't always better; length rewards can hinder performance.", "Dynamic reward scaling helps models adapt from simple to complex behaviors.", "Detailed reward breakdown improves stability and learning effectiveness."], "tldr": "Current Large Language Models (LLMs) often struggle to generalize in tool use scenarios. While Supervised Fine-Tuning (SFT) is common, it falls short in unfamiliar situations. Reinforcement Learning (RL), especially with R1-like models, shows promise in reasoning and generalization. However, designing rewards for tool use is challenging due to multiple tools, diverse parameters, and the need for fine-grained feedback that coarse-grained reward signals like answer matching can't provide.\n\nThis paper introduces **ToolRL**, a comprehensive study on reward design for tool selection and application within RL.  The study systematically explores reward strategies, analyzing their types, scales, granularity, and temporal dynamics. A principled reward design, tailored for tool use tasks is proposed and applied to train LLMs using Group Relative Policy Optimization (GRPO). Empirical results across benchmarks demonstrate that **ToolRL** enables robust, scalable, and stable training, leading to substantial improvements over base and SFT models.", "affiliation": "University of Illinois Urbana-Champaign", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2504.13958/podcast.wav"}