[{"figure_path": "https://arxiv.org/html/2504.10481/x1.png", "caption": "Figure 1: Framework of xVerify: (1) Collecting LLM Responses: aggregate responses from multiple LLMs across datasets covering four question types. (2) VAR Dataset Construction: employ GPT-4o and human annotators for labeling and rechecking, and use data augmentation to refine the dataset. (3) xVerify Judge Pipeline: accurately evaluate multi-component answers from reasoning models on challenging questions.", "description": "This figure illustrates the xVerify framework's three main stages.  Stage 1 focuses on collecting a diverse set of responses from various large language models (LLMs) across a range of question types (multiple choice, math, short answer, and classification).  Stage 2 details the VAR dataset construction, emphasizing the use of GPT-40 and human annotators for labeling and rechecking the data, along with data augmentation techniques to improve the dataset's quality and robustness.  Finally, Stage 3 shows the xVerify judge pipeline, which is responsible for accurately evaluating the complex, multi-component answers generated by reasoning models, particularly those answers requiring nuanced reasoning and challenging questions.", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2504.10481/x2.png", "caption": "Figure 2: Data Augmentation Pipelines: (1) transformation of multiple-choice options through numbering conversion and noise injection, (2) diversification of mathematical answers via equivalent expression generation, and (3) final answer sentence transformation using prompt rephrasing, symbol wrapping, and gap token insertion.", "description": "This figure illustrates the data augmentation techniques used to enhance the VAR dataset.  Three main pipelines are shown: 1) For multiple-choice questions, the options are modified by converting their numbering system (e.g., letters to numbers) and adding or removing options to increase complexity and variability. 2) For math problems, the figure depicts how multiple mathematically equivalent forms of the correct answer are generated to improve robustness to varied answer formatting.  3) Finally, for all question types, the final answer sentences are augmented using different techniques, including rephrasing, adding symbols like boxes, and inserting gap tokens, thereby enhancing the dataset's diversity and making the model more resilient to variations in answer phrasing.", "section": "4.1 VAR Dataset"}, {"figure_path": "https://arxiv.org/html/2504.10481/extracted/6360676/figures/label-studio-interface.png", "caption": "Figure 3: Illustration of the Label Studio Interface.", "description": "This figure shows a screenshot of the Label Studio interface used for annotating the VAR dataset.  It highlights the key elements of the annotation process, including the question, LLM response, correct answer, and an answer range. The interface facilitates annotators to determine if the LLM's response accurately matches the correct answer, considering aspects like semantic equivalence, and handling cases with multiple answers or different answer formats.", "section": "4.1 VAR Dataset"}]