{"importance": "This paper introduces a new paradigm for **flexible scene understanding**, breaking away from the constraints of aligned multi-modal data. It inspires **new research directions in interactive mixed-reality experiences** and dynamic scene reconstruction with its adaptable method.", "summary": "CrossOver: Flexible scene-level cross-modal alignment via modality-agnostic embeddings, unlocking robust 3D scene understanding.", "takeaways": ["CrossOver enables robust scene retrieval and object localization, even with missing modalities.", "The framework employs dimensionality-specific encoders and multi-stage training.", "Emergent cross-modal behaviors are learned without explicit training on all modality pairs."], "tldr": "Multi-modal 3D object understanding is promising, but existing methods often rely on complete datasets with rigid alignments across modalities. Current approaches struggle in real-world settings where data is incomplete, noisy, or modalities lack consistent correspondence. They also require semantic instance segmentation, a labor intensive task. Thus there is a need to improve scene-level understanding.\n\nThis paper introduces a flexible framework: CrossOver, for cross-modal 3D scene understanding via scene-level alignment without aligned modality data. It learns a unified, modality-agnostic embedding space for scenes using RGB images, point clouds, CAD models, floorplans, and text descriptions. By using dimensionality-specific encoders and multi-stage training, CrossOver achieves robust scene retrieval and object localization, adapting to missing modalities and emergent cross-modal behaviors.", "affiliation": "Stanford University", "categories": {"main_category": "Computer Vision", "sub_category": "Scene Understanding"}, "podcast_path": "2502.15011/podcast.wav"}