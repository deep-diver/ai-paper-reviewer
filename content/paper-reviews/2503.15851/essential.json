{"importance": "This paper is important because it **provides a robust and data-efficient solution for creating lifelike head avatars**, a key component for AR/VR, films, and games. It addresses the critical challenge of reducing data requirements, making avatar creation more accessible. The innovative method also has a high potential for future research of 3D avatar synthesis and manipulation.", "summary": "Zero-1-to-A: Animatable avatars from a single image using video diffusion, robust to spatial & temporal inconsistencies!", "takeaways": ["Introduces SymGEN: a novel method for synthesizing spatial and temporal consistency datasets for 4D avatar reconstruction using video diffusion.", "Proposes a progressive learning strategy with Spatial and Temporal Consistency Learning for stable initialization and quality enhancement.", "Demonstrates superior fidelity, animation quality, and rendering speed in 4D avatar generation compared to baseline methods."], "tldr": "Creating animatable head avatars usually needs lots of training data. A natural way to fix this is by using existing methods that don't need data, like pre-trained diffusion models with score distillation sampling (SDS). But, directly making 4D avatars from video diffusion often leads to results that are too smooth because of spatial and temporal inconsistencies. This paper aims to solve this problem.\n\nThe paper introduces \"Zero-1-to-A\", a method that creates a spatial and temporal consistency dataset for 4D avatar reconstruction using video diffusion. Zero-1-to-A iteratively builds video datasets and optimizes animatable avatars, making sure avatar quality improves smoothly. It uses two learning stages: Spatial Consistency Learning (fixes expressions, learns from front-to-side views) and Temporal Consistency Learning (fixes views, learns from relaxed to exaggerated expressions).", "affiliation": "Zhejiang University", "categories": {"main_category": "Computer Vision", "sub_category": "3D Vision"}, "podcast_path": "2503.15851/podcast.wav"}