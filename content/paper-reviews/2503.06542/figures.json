[{"figure_path": "https://arxiv.org/html/2503.06542/x1.png", "caption": "Figure 1: ARMOR can output images, text, or interleaved text-image. We show some examples of chat with ARMOR.", "description": "This figure showcases ARMOR's multimodal capabilities.  It demonstrates ARMOR's ability to generate different output formats, including images only, text only, and a combination of interleaved text and images, in response to various prompts or questions.  The examples provided in the figure illustrate diverse interaction scenarios, highlighting ARMOR's flexibility and effectiveness in handling various multimodal tasks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.06542/x2.png", "caption": "Figure 2: Structural comparison of UniMs.", "description": "Figure 2 presents a structural comparison of several Unified Multimodal Models (UniMs).  It illustrates the architectural differences in how these models handle multimodal inputs (text and image) and outputs, highlighting distinct approaches such as fully autoregressive architectures, hybrid approaches using autoregressive and diffusion methods, and variations in encoder and decoder designs.  Key differences, including the use of asymmetric encoders and decoders, are highlighted to provide insights into the different strategies used for multimodal understanding and generation.", "section": "3. ARMOR Framework"}, {"figure_path": "https://arxiv.org/html/2503.06542/x3.png", "caption": "Figure 3: Architecture of ARMOR.", "description": "ARMOR's architecture is composed of an asymmetric encoder-decoder structure.  The encoder leverages the pre-trained components of the MLLM (multimodal large language model), preserving its strong understanding capabilities. The decoder is extended with a new visual output layer (VQGAN decoder) and a forward-switching mechanism, enabling the model to naturally generate text and images and seamlessly interleave them. This enables efficient generation of both images and text, as well as natural text-image interleaving, with minimal added computational overhead. The unified embedding space allows integration of textual and visual information for generating coherent outputs.", "section": "3. ARMOR Framework"}, {"figure_path": "https://arxiv.org/html/2503.06542/x4.png", "caption": "Figure 4: Forward switching mechanism.", "description": "This figure illustrates the forward-switching mechanism used in ARMOR.  It shows how the model dynamically selects between text and image generation based on the input.  Special tokens control which modality's answer space is used for prediction, enabling natural interleaved text-image generation.", "section": "3.2. Architecture"}, {"figure_path": "https://arxiv.org/html/2503.06542/x5.png", "caption": "(a) step 1.", "description": "This figure shows the first step of the three-stage WoHG training algorithm. In this stage, the model learns to decide what to generate (text, image, or both) based on different types of input questions.  The model is trained on various datasets including text-to-text (t2t), text-image-to-text (ti2t), text-to-image (t2i), and text-to-text-image (t2ti). The weights of the loss function are adjusted to \u03b1 = 1.0 and \u03b2 = 0.0 to emphasize text prediction, helping the model learn to differentiate between question types.", "section": "3.4 WoHG Training Algorithm"}, {"figure_path": "https://arxiv.org/html/2503.06542/x6.png", "caption": "(b) step 2.", "description": "This figure shows the second stage of the three-stage WoHG (What or How to Generate) training algorithm.  In this stage, the model focuses on enhancing its image generation capabilities.  The parameters related to image generation are unfrozen and trained to improve the accuracy and quality of image generation. The image generation loss weight (\u03b2) is set to 1.0, and the text generation loss weight (\u03b1) is set to 0.0.  The goal is to allow the model to generate high-quality images matching text input, while preserving its multimodal understanding capabilities from the first stage.", "section": "3. WoHG Training Algorithm"}, {"figure_path": "https://arxiv.org/html/2503.06542/x7.png", "caption": "(c) step 3.", "description": "This figure shows the third stage of the \"What or How to Generate\" (WoHG) training algorithm.  In this stage, the model is fine-tuned using a high-quality dataset of interleaved text and image data. The goal is to improve the model's ability to generate high-quality, integrated text and image responses that are contextually relevant and coherent.  The model learns to better integrate text and visual modalities, leading to more natural and effective multimodal output.", "section": "3.4 WoHG Training Algorithm"}, {"figure_path": "https://arxiv.org/html/2503.06542/extracted/6250560/figs/show-fig/1-1.jpg", "caption": "Figure 5: Demonstration of the proposed three-stage WoHG training algorithm.", "description": "This figure demonstrates the three-stage training process of the \"What or How to Generate\" (WoHG) algorithm. Each stage focuses on a specific objective: Stage 1 trains the model to determine what type of response to generate (text, image, or both); Stage 2 trains the model to improve its image generation capabilities; and Stage 3 trains the model to improve the quality and coherence of its multimodal responses. The figure visually illustrates the changes in the model's architecture and training data used in each stage.", "section": "3. WoHG Training Algorithm"}, {"figure_path": "https://arxiv.org/html/2503.06542/extracted/6250560/figs/show-fig/1-2.jpg", "caption": "Figure 6: Changes in image generation quality during part of the training process (epochs: 4, 6, 8\u2026 18, from left to right). Prompt 1: \u201cCould you generate an image of the aurora for me?\u201d; Prompt 2: \u201cPlease help me draw a picture of the tropical rainforest.\u201d.", "description": "This figure displays the evolution of image generation quality throughout the training process, focusing on two different prompts. The leftmost images correspond to epoch 4, and the quality progressively improves towards the rightmost images (epoch 18).  Each set of images shows side-by-side examples generated from the same prompt at different training epochs.  Prompt 1 focuses on the generation of an aurora, while Prompt 2 involves creating a depiction of a tropical rainforest. The progression demonstrates the model's learning ability to generate increasingly detailed and accurate images over time.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.06542/extracted/6250560/figs/show-fig/1-3.jpg", "caption": "Figure 7: The training loss of the two model structures.", "description": "This figure compares the training loss curves for two different model architectures.  The x-axis represents the training epoch, while the y-axis shows the loss value.  The plot allows for a visual comparison of the convergence speed and overall training loss between the two models, providing insights into their relative training efficiency and effectiveness.", "section": "3. ARMOR Framework"}, {"figure_path": "https://arxiv.org/html/2503.06542/extracted/6250560/figs/show-fig/1-4.jpg", "caption": "Figure 8: Generated images with the prompt \u201cCan you help me draw a picture of a teddy bear doll?\u201d. Images (a), (b) and (c) are from a model with one output layer, and images (d), (e) and (f) are from a model with two output layers.", "description": "This figure visualizes the impact of the model architecture on image generation quality.  Using the prompt \"Can you help me draw a picture of a teddy bear doll?\", the model was used to generate images. The first three images (a, b, and c) were generated by a model with a single output layer, while the next three images (d, e, and f) were created by a model with two output layers. The comparison highlights how the increased model complexity (two output layers) leads to improved image generation, resulting in more refined and detailed teddy bear depictions.", "section": "3.2 Architecture"}, {"figure_path": "https://arxiv.org/html/2503.06542/extracted/6250560/figs/show-fig/1-5.jpg", "caption": "Figure 9: The Generated Images Trained with Visual Output Layer and Adapter.", "description": "This figure shows example images generated by a model trained with a visual output layer and adapter.  The visual output layer is a component added to enable image generation capabilities, and the adapter is a set of additional transformer layers designed to help integrate the new image generation functionality with the pre-existing language model. The quality of the generated images likely reflects the effectiveness of the training process and the architecture modifications made to the model.", "section": "6.3 WoHG Algorithm Explore"}]