[{"figure_path": "https://arxiv.org/html/2501.01423/x1.png", "caption": "Figure 1: Optimization dilemma within latent diffusion models. In latent diffusion models, increasing the dimension of the visual tokenizer enhances detail reconstruction but significantly reduces generation quality. (In tokenizer specification, \u201cf\u201d and \u201cd\u201d represent the downsampling rate and dimension, respectively. All results are evaluated on ImageNet 256\u00d7\\times\u00d7256 dataset with a fixed compute budget during diffusion model training.)", "description": "This figure illustrates the optimization dilemma in latent diffusion models.  As the dimensionality ('d') of the visual tokenizer increases (while maintaining a fixed downsampling rate 'f'), the reconstruction quality improves (lower rFID values),  as shown by the decreasing rFID values from left to right. However, this improvement in reconstruction comes at the cost of significantly reduced generation quality, indicated by the increasing gFID values.  This trade-off is clearly demonstrated across three different tokenizer specifications, all evaluated under a fixed computational budget on the ImageNet 256x256 dataset.  This highlights the challenge of balancing reconstruction and generation performance when designing latent diffusion models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.01423/x2.png", "caption": "Figure 2: Reconstruction-generation frontier of latent diffusion models. VA-VAE improves the feature distribution of high-dimensional latent. Through alignment with vision foundation models, we expand the frontier between reconstruction and generation in latent diffusion models.", "description": "This figure illustrates the trade-off between reconstruction and generation capabilities in latent diffusion models, a key challenge in model design.  The x-axis represents reconstruction quality (measured by rFID, with lower values indicating better reconstruction), and the y-axis represents generation quality (measured by gFID, with lower values representing better generation).  The plot shows that increasing the dimensionality of the visual tokenizer improves reconstruction but significantly harms generation. The introduction of VA-VAE (Vision foundation model Aligned Variational AutoEncoder), however, shifts this trade-off, expanding the range of achievable performance. By aligning latent spaces with pre-trained vision foundation models, VA-VAE enhances the quality of high-dimensional latent representations, improving generation without sacrificing reconstruction capabilities.", "section": "3. Align VAE with Vision Foundation Models"}, {"figure_path": "https://arxiv.org/html/2501.01423/x3.png", "caption": "Figure 3: The proposed Vision foundation model Aligned VAE (VA-VAE). Vision foundation models are used to guide the training of high-dimensional visual tokenizers, effectively mitigating the optimization dilemma and improve generation performance.", "description": "This figure illustrates the architecture of the Vision Foundation model Aligned Variational Autoencoder (VA-VAE).  The VA-VAE addresses the optimization dilemma in latent diffusion models by incorporating pre-trained vision foundation models into the training process of the high-dimensional visual tokenizer. The vision foundation model guides the learning of the latent representations, enabling the tokenizer to achieve superior reconstruction quality without sacrificing generation performance.  The figure shows how the encoder of the VA-VAE receives input, processes it through the vision foundation model, and incorporates this information into the learning process, resulting in better aligned high-dimensional visual tokens.", "section": "3. Align VAE with Vision Foundation Models"}, {"figure_path": "https://arxiv.org/html/2501.01423/x4.png", "caption": "(a)", "description": "The figure shows training curves for FID scores of different models with varying tokenizer specifications.  The curves demonstrate that using Vision Foundation Model (VFM) alignment significantly improves convergence speed, reducing the number of training steps needed to reach a comparable FID score. Models without VFM alignment (baseline) show slower convergence, particularly with higher-dimensional tokenizers (f16d64). In contrast, those with VFM alignment converge much faster. This highlights that aligning latent space with pretrained vision foundation models improves the efficiency of training high-dimensional latent diffusion models.", "section": "3. Adaptive Weighting"}, {"figure_path": "https://arxiv.org/html/2501.01423/x5.png", "caption": "(b)", "description": "This figure shows the training curves of FID (Fr\u00e9chet Inception Distance) scores for various models, demonstrating the impact of the proposed Vision Foundation model Aligned Variational AutoEncoder (VA-VAE). The models were trained using different tokenizers (f16d32 and f16d64), both with and without VA-VAE. The plot shows the training curves for FID score on the y-axis and the number of training steps on the x-axis. The curves demonstrate a significant reduction in FID score with VA-VAE, indicating faster convergence speed during training and improved performance.", "section": "5.2 Foundation Models Improve Convergence"}, {"figure_path": "https://arxiv.org/html/2501.01423/x6.png", "caption": "(c)", "description": "This figure shows how the FID score changes with the increase of the model size (number of parameters) for three different tokenizers: f16d16, f16d32 (with and without VF DINOv2 loss), and f16d64 (with and without VF DINOv2 loss).  The x-axis is the model size on a logarithmic scale, representing the increasing model capacity.  The y-axis is the FID score (a lower score is better), indicating the image generation quality.  The plot demonstrates how VF DINOv2 loss significantly improves the scalability of high-dimensional tokenizers (f16d32 and f16d64), enabling better performance even with smaller models. The f16d16 tokenizer shows relatively stable performance regardless of model size, indicating that VF loss is not needed for lower dimensions.", "section": "5.3. Foundation Models Improve Scalability"}, {"figure_path": "https://arxiv.org/html/2501.01423/x7.png", "caption": "Figure 4: (a)&(b) VF Loss Improves Convergence. We train LightningDiT-B for 160 epochs on ImageNet at 256 resolution using different tokenizers. The VF loss significantly accelerates convergence, with a maximum speedup of up to 2.7 times. (c) VF Loss Improves Scalability. VF loss reduces the need for large parameters in generative models of high-dimensional tokenizer, enabling better scalability.", "description": "Figure 4 demonstrates the impact of the Vision Foundation model alignment Loss (VF Loss) on the training convergence and scalability of the LightningDiT model.  Subfigures (a) and (b) compare training curves (FID score over training steps) for LightningDiT-B trained with different tokenizers (f16d32 and f16d64) both with and without VF Loss. The results show that VF Loss significantly accelerates the convergence speed, achieving up to a 2.7x speedup. Subfigure (c) illustrates the impact of VF Loss on scalability by showing FID scores for models of varying sizes (parameter counts) using different tokenizers.  This graph shows VF Loss reduces the need for extremely large models to achieve good performance with higher-dimensional tokenizers.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.01423/x8.png", "caption": "Figure 5: Visualization Results. We visualize our latent diffusion system with proposed VA-VAE together with LightningDiT-XL trained on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256 resolution.", "description": "This figure showcases sample images generated by the proposed latent diffusion model, LightningDiT-XL, utilizing the Vision foundation model Aligned Variational AutoEncoder (VA-VAE).  The model was trained on the ImageNet dataset at a 256x256 resolution. The images demonstrate the model's ability to generate high-fidelity and diverse images across various categories. The figure visually represents the quality and variety achievable with the improved system.", "section": "5. Experiments"}]