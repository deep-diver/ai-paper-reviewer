{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-XX-XX", "reason": "This paper introduces the foundational latent diffusion model which the current research builds upon."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Scaling rectified flow transformers for high-resolution image synthesis", "publication_date": "2024-XX-XX", "reason": "This paper addresses a key challenge of scaling latent diffusion models to high resolutions, which the current work aims to improve upon."}, {"fullname_first_author": "Kaiming He", "paper_title": "Masked autoencoders are scalable vision learners", "publication_date": "2021-11-XX", "reason": "This paper introduces the masked autoencoder (MAE) architecture, a crucial component in the proposed VA-VAE model."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-XX-XX", "reason": "CLIP, introduced in this paper, is used as one of the vision foundation models to guide the training of the VA-VAE."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-XX-XX", "reason": "This paper introduces Diffusion Transformers (DiT), the core generative model used in the proposed LightningDiT system."}]}