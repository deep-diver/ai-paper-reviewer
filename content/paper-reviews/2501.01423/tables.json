[{"content": "| Training Trick | Training Sample | Epoch | FID-50k \u2193 | \n|---|---|---|---| \n| DiT-XL/2 [29] | 400k \u00d7 256 | 80 | 19.50 | \n| *Training Strategies* |  |  |  | \n| + Rectified Flow [23] | 400k \u00d7 256 | 80 | 17.20 | \n| + *batchsize* \u00d7 4 & *lr* \u00d7 2 | 100k \u00d7 1024 | 80 | 16.59 | \n| + AdamW \u03b2\u2082=0.95 [1] | 100k \u00d7 1024 | 80 | 16.61 | \n| + Logit Normal Sampling [7] | 100k \u00d7 1024 | 80 | 13.99 | \n| + Velocity Direction Loss [41] | 100k \u00d7 1024 | 80 | 12.52 | \n| *Architecture Improvements* |  |  |  | \n| + SwiGLU FFN [34] | 100k \u00d7 1024 | 80 | 10.10 | \n| + RMS Norm [44] | 100k \u00d7 1024 | 80 | 9.25 | \n| + Rotary Pos Embed [35] | 100k \u00d7 1024 | 80 | 7.13 | \n| + patch size=1 & VA-VAE (Sec. 3) | 100k \u00d7 1024 | 80 | 4.29 | ", "caption": "Table 1: Performance of LightningDiT. With SD-VAE\u00a0[33], LightningDiT\u00a0achieves FID-50k=7.13 on ImageNet class-conditional generation, using 94% fewer training samples compared to the original DiT\u00a0[29].\nWe show that the original DiT can also achieve exceptional performance by leveraging advanced design techniques.", "description": "This table presents the performance improvements achieved by LightningDiT, an enhanced version of Diffusion Transformers (DiT).  Using the SD-VAE (Stable Diffusion Variational Autoencoder) tokenizer, LightningDiT achieves a Fr\u00e9chet Inception Distance (FID) score of 7.13 on the ImageNet dataset for class-conditional image generation.  This represents a significant improvement, as it uses 94% fewer training samples compared to the original DiT model. The table further highlights that the exceptional performance of the original DiT model can be replicated by incorporating advanced architectural and training strategies, as demonstrated by the results within the table.", "section": "2.2 Fast Convergence of Diffusion Transformers"}, {"content": "| Tokenizer | Spec. | Reconstruction Performance |  |  |  | Generation Performance (FID-10K)\u2193 |  |  |  |\n|---|---|---|---|---|---|---|---|---|---|\n| **Tokenizer** | **Spec.** | **rFID\u2193** | **PSNR\u2191** | **LPIPS\u2193** | **SSIM\u2191** | **LightningDiT-B** | **LightningDiT-L** | **LightningDiT-XL** |\n| LDM [33] |  | 0.49 | 26.10 | 0.132 | 0.72 | 16.24 | 9.49 | 8.28 |\n| LDM+VF loss (MAE) [15] |  | 0.51 | 26.01 | 0.137 | 0.71 | 16.86 (+0.62) | 10.93 (+1.44) | 9.19 (+0.91) |\n| LDM+VF loss (DINOv2) [28] |  | 0.55 | 25.29 | 0.147 | 0.69 | 15.79 (-0.45) | 10.02 (+0.53) | 8.71 (+0.43) |\n| LDM [33] |  | 0.26 | 28.59 | 0.089 | 0.80 | 22.62 | 12.86 | 10.92 |\n| LDM+VF loss (MAE) [15] |  | 0.28 | 28.33 | 0.091 | 0.80 | 19.89 (-2.73) | 11.51 (-1.35) | 9.92 (-1.00) |\n| LDM+VF loss (DINOv2) [28] |  | 0.28 | 27.96 | 0.096 | 0.79 | 15.82 (-6.80) | 9.82 (-3.04) | 8.22 (-2.70) |\n| LDM [33] |  | 0.17 | 31.03 | 0.055 | 0.88 | 36.83 | 20.73 | 17.24 |\n| LDM+VF loss (MAE) [15] |  | 0.15 | 31.03 | 0.054 | 0.87 | 23.58 (-13.25) | 14.40 (-6.33) | 11.69 (-5.55) |\n| LDM+VF loss (DINOv2) [28] |  | 0.14 | 30.71 | 0.055 | 0.87 | 24.00 (-12.83) | 14.95 (-5.78) | 11.98 (-5.26) |", "caption": "Table 2: VF loss Improves Generation Performance. The f16d16 tokenizer specification is widely used\u00a0[33, 21]. As dimensionality increases, we observe that (1) higher dimensions improve reconstruction but reduce generation quality, highlighting an optimization dilemma within the latent diffusion framework; (2) VF Loss significantly enhances generative performance in high-dimensional tokenizers with minimal impact on reconstruction.", "description": "This table investigates the impact of increasing the dimensionality of visual tokenizers in latent diffusion models and the effect of incorporating Vision Foundation model alignment loss (VF Loss).  It shows that while higher dimensionality improves reconstruction quality (measured by rFID, PSNR, LPIPS, SSIM), it simultaneously hurts the generation quality (measured by FID). This demonstrates an optimization dilemma. The table then presents results showing how VF Loss significantly improves the generation performance (FID) of high-dimensional tokenizers while maintaining comparable reconstruction quality, effectively mitigating this dilemma.", "section": "3. Align VAE with Vision Foundation Models"}, {"content": "| Method | Tokenizer | rFID | gFID | #params | sFID | IS | Pre. | Rec. | gFID | sFID | IS | Pre. | Rec. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| **AutoRegressive (AR)** |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| MaskGIT [2] | MaskGiT | 2.28 | 555 | 227M | 6.18 | - | 182.1 | 0.80 | 0.51 | - | - | - | - |\n| LlamaGen [36] | VQGAN\u2020 | 0.59 | 300 | 3.1B | 9.38 | 8.24 | 112.9 | 0.69 | 0.67 | 2.18 | 5.97 | 263.3 | 0.81 | 0.58 |\n| VAR [38] | - | - | 350 | 2.0B | - | - | - | - | - | 1.80 | - | 365.4 | 0.83 | 0.57 |\n| MagViT-v2 [42] | - | - | 1080 | 307M | 3.65 | - | 200.5 | - | - | 1.78 | - | 319.4 | - | - |\n| MAR [21] | LDM\u2020 | 0.53 | 800 | 945M | 2.35 | - | 227.8 | 0.79 | 0.62 | 1.55 | - | 303.7 | 0.81 | 0.62 |\n| **Latent Diffusion Models** |  |  |  |  |  |  |  |  |  |  |  |  |  |\n| MaskDiT [45] | SD-VAE [33] | 0.61 | 1600 | 675M | 5.69 | 10.34 | 177.9 | 0.74 | 0.60 | 2.28 | 5.67 | 276.6 | 0.80 | 0.61 |\n| DiT [29] | SD-VAE [33] | 0.61 | 1400 | 675M | 9.62 | 6.85 | 121.5 | 0.67 | 0.67 | 2.27 | 4.60 | 278.2 | 0.83 | 0.57 |\n| SiT [26] | SD-VAE [33] | 0.61 | 1400 | 675M | 8.61 | 6.32 | 131.7 | 0.68 | 0.67 | 2.06 | 4.50 | 270.3 | 0.82 | 0.59 |\n| FasterDiT [41] | SD-VAE [33] | 0.61 | 400 | 675M | 7.91 | 5.45 | 131.3 | 0.67 | 0.69 | 2.03 | 4.63 | 264.0 | 0.81 | 0.60 |\n| MDT [11] | SD-VAE [33] | 0.61 | 1300 | 675M | 6.23 | 5.23 | 143.0 | 0.71 | 0.65 | 1.79 | 4.57 | 283.0 | 0.81 | 0.61 |\n| MDTv2 [12] | SD-VAE [33] | 0.61 | 1080 | 675M | - | - | - | - | - | 1.58 | 4.52 | 314.7 | 0.79 | 0.65 |\n| REPA [43] |  |  | 800 | 675M | 5.90 | - | - | - | - | 1.42 | 4.70 | 305.7 | 0.80 | 0.65 |\n| **LightningDiT** | **VA-VAE** | **0.28** | 64 | 675M | 5.14 | 4.22 | 130.2 | 0.76 | 0.62 | 2.11 | 4.16 | 252.3 | 0.81 | 0.58 |\n|  |  |  | 800 | 675M | 2.17 | 4.36 | 205.6 | 0.77 | 0.65 | 1.35 | 4.15 | 295.3 | 0.79 | 0.65 |", "caption": "Table 3: System-Level Performance on ImageNet 256\u00d7\\times\u00d7256. Our latent diffusion system achieves state-of-the-art performance with rFID=0.28 and FID=1.35. Besides, our LightningDiT together with VA-VAE surpasses DiT\u00a0[29] and SiT\u00a0[26] in FID within only 64 training epochs, demonstrating a 21.8 \u00d7\\times\u00d7 faster convergence.", "description": "This table presents a comprehensive comparison of the system-level performance of various latent diffusion models on the ImageNet 256x256 dataset.  Key metrics evaluated include reconstruction quality (rFID), generation quality (FID, IS, and Precision/Recall), and training efficiency (training epochs and number of parameters).  The table highlights the superior performance of the proposed LightningDiT model combined with VA-VAE, demonstrating state-of-the-art results in terms of both generation quality (FID of 1.35) and training speed (achieving comparable performance to existing models in a fraction of the training time, specifically 64 epochs compared to others, indicating a more than 21 times speedup).  The comparison includes both autoregressive and latent diffusion models, allowing for a comprehensive assessment of the proposed method's advantages.", "section": "5. Experiments"}, {"content": "| Model Type | rFID\u2193 | PSNR\u2191 | LPIPS\u2193 | SSIM\u2191 | gFID\u2193 |\n|---|---|---|---|---|---| \n| naive | 0.26 | 28.59 | 0.089 | 0.80 | 22.62 |\n| DINOv2 [28] | 0.28 | 27.96 | 0.096 | 0.79 | 15.82 |\n| MAE [15] | 0.28 | 28.33 | 0.091 | 0.80 | 19.89 |\n| SAM [18] | 0.26 | 28.31 | 0.091 | 0.80 | 19.80 |\n| CLIP [32] | 0.33 | 28.39 | 0.091 | 0.80 | 18.93 |", "caption": "Table 4: Ablation on Foundation Models. We evaluate the impact of different VF losses on generative performance. Our results show that DINOv2 achieves the highest generative performance.", "description": "This ablation study investigates the effect of different vision foundation models on the performance of the Vision Foundation Model Aligned Variational AutoEncoder (VA-VAE).  The table compares the generative performance (measured by FID score) when using different pre-trained models (DINOv2, MAE, CLIP, and SAM) to guide the training of the VA-VAE.  The results demonstrate that the choice of foundation model significantly impacts the quality of image generation, with DINOv2 showing the best performance.", "section": "6.2 Ablations on Vision Foundation Models"}, {"content": "| Loss Type | rFID\u2193 | PSNR\u2191 | LPIPS\u2193 | SSIM\u2191 | gFID\u2193 |\n|---|---|---|---|---|---| \n| *NaN* | 0.26 | 28.59 | 0.089 | 0.80 | 22.62 |\n| *full* | 0.28 | 27.96 | 0.096 | 0.79 | 15.82 |\n| *mcos loss* | 0.27 | 28.52 | 0.090 | 0.80 | 21.87 |\n| *mdistmat loss* | 0.27 | 28.24 | 0.090 | 0.80 | 17.74 |\n| *margin* | 0.27 | 28.07 | 0.093 | 0.79 | 17.77 |", "caption": "Table 5: Ablation Study of VF Loss Formulations:\nComparison of different configurations on generative performance metrics using LightningDiT-B.", "description": "This table presents an ablation study analyzing the impact of different components within the Vision Foundation model alignment Loss (VF Loss) on the generative performance of the LightningDiT-B model.  It compares the reconstruction and generation performance metrics (rFID, PSNR, LPIPS, SSIM, and gFID) using different configurations of the VF loss. Specifically, it examines the effects of removing the marginal cosine similarity loss (mcos), the marginal distance matrix similarity loss (mdms), and the margin term from the loss function, providing insights into the contribution of each component to the overall performance.", "section": "6.3 Ablations on Loss Formulations"}, {"content": "| Tokenizer | VF Loss | density \u2193 | gini coefficient \u2193 | normalized entropy \u2191 | gFID (DiT-B) \u2193 |\n|---|---|---|---|---|---| \n| *f16d32* | NaN | 0.263 | 0.145 | 0.995 | 22.62 |\n|  | MAE | 0.193 | 0.101 | 0.997 | 19.89 |\n|  | **DINOv2** | **0.178** | **0.096** | **0.998** | **15.82** |\n| *f16d64* | NaN | 0.296 | 0.166 | 0.994 | 36.83 |\n|  | MAE | 0.256 | 0.143 | 0.995 | 23.58 |\n|  | **DINOv2** | **0.251** | **0.141** | **0.996** | 24.00 |", "caption": "Table 6: Relationship between uniformity and generative performance: We evaluate the uniformity of feature distribution. Results indicate a possible positive correlation between the uniformity of feature distribution and generative performance.", "description": "This table explores the relationship between the uniformity of feature distributions in latent spaces of different visual tokenizers and their impact on generative model performance.  Uniformity is assessed using two metrics: kernel density estimation standard deviation and Gini coefficient. The results suggest a positive correlation between a more uniform latent space (lower standard deviation and Gini coefficient) and better generative performance (lower FID score).  This indicates that well-structured, uniformly distributed latent spaces learned by the vision foundation model improve the quality of generated images.", "section": "6.4 Discuss on VF loss with Latent Distribution"}]