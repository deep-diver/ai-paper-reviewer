[{"heading_title": "RL for Tool Use", "details": {"summary": "Reinforcement learning (RL) has emerged as a promising avenue for enhancing tool use in large language models (LLMs). Unlike traditional methods that rely on imitation learning and struggle with generalization, **RL enables LLMs to autonomously discover and refine tool usage strategies through outcome-based feedback**. This approach fosters exploration, allowing models to learn when and how to invoke tools, recover from errors, and adapt to diverse problem settings.  The integration of external code interpreters in the RL framework not only empowers LLMs to handle intricate tasks that demand structured problem-solving but also helps to improve **code utilization proficiency and strategic tool usage development**. Furthermore, RL can foster metacognitive capabilities, **enabling models to self-correct, adaptively select tools, and optimize reasoning strategies**."}}, {"heading_title": "ReTool's Design", "details": {"summary": "**ReTool** is designed to enhance LLMs by integrating code interpreters into the reasoning loop, using a novel reinforcement learning framework.  A **high-quality cold-start dataset** is curated to equip the model with foundational code-invoking abilities. The framework supports **interleaved code execution** during rollouts, allowing for iterative refinement via tool-augmented interactions, all guided by a sandboxed code interpreter. ReTool allows LLMs to **explore, refine, and optimize** reasoning strategies. A rule-based outcome reward design, **combined with the interleaved code execution** and feedback, enables the models to discover more diverse problem-solving behaviors."}}, {"heading_title": "AIME Benchmark", "details": {"summary": "The paper evaluates **ReTool** on the challenging **MATH Olympiad** benchmarks **AIME2024** and **AIME2025**, demonstrating its superiority in mathematical reasoning tasks. The experiments compare **ReTool** against various strong baselines and ablations. The benchmark aims to validate the tool's effectiveness. **AIME** requires structured problem-solving. **ReTool** is able to integrate tool learning into reasoning. Experiments are performed in both **AIME2024** and **AIME2025** to demonstrate results."}}, {"heading_title": "Code Dynamics", "details": {"summary": "Analyzing code dynamics within the paper reveals interesting patterns. **Code ratio increases during training**, showing improved tool utilization. Furthermore, **code lines grow, suggesting complexity**. Analyzing *code pass rate* highlights that **executability matters**. Code invocation timing shifting indicates strategic tool awareness. The emergence of *self-correction* demonstrates *metacognitive capabilities*. This data suggests ReTool fosters code proficiency through RL."}}, {"heading_title": "Tool+Reasoning", "details": {"summary": "**Tool-integrated reasoning** is crucial for LLMs to solve computationally intensive tasks. Methods integrate Python interpreters to simplify and validate complex steps. Earlier approaches, like Chen et al.'s supervised fine-tuning, are limited by specific data distributions, lacking adaptive tool use. **ReTool** strategically determines when and how to invoke tools via reinforcement learning, outperforming models like Qwen-Math. This approach enhances training efficiency and enables better problem-solving strategies. The models show cognitive patterns in code invocation, leading to code self-correction. These findings demonstrate the power of tool-augmented reasoning in LLMs."}}]