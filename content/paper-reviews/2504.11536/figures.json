[{"figure_path": "https://arxiv.org/html/2504.11536/extracted/6364450/figures/code-rl.png", "caption": "Figure 1: AIME 2024 & 2025 scores of ReTool and text-based RL baseline on the Qwen2.5-32B-Instruct model.", "description": "This figure shows the performance comparison between ReTool and a text-based reinforcement learning (RL) baseline on the AIME (American Invitational Mathematics Examination) 2024 and 2025 datasets.  The results are presented using the Qwen2.5-32B-Instruct language model. The x-axis represents the number of training steps, and the y-axis represents the accuracy achieved.  The graph visually demonstrates that ReTool achieves significantly higher accuracy with fewer training steps, highlighting its efficiency and effectiveness in enhancing the reasoning capabilities of LLMs through tool integration.", "section": "3 Experiment"}, {"figure_path": "https://arxiv.org/html/2504.11536/x1.png", "caption": "Figure 2: \nDemonstration of text-based RL training process and ReTool\u2019s RL training process.", "description": "This figure compares the training processes of a standard text-based reinforcement learning (RL) model and ReTool.  The text-based RL approach shows a simple process: the LLM generates text, which is then evaluated to produce a reward signal that updates the model's policy.  In contrast, ReTool's RL training process involves interleaving code execution within the natural language reasoning process. The LLM generates text, and when appropriate, it also generates code. This code is executed in a sandboxed code interpreter, providing real-time feedback in the form of execution results and error messages. This feedback, combined with the textual output, is then used to compute a reward that refines the model's policy.  This demonstrates ReTool's key feature of integrating code execution within the RL loop, leading to more nuanced tool-use strategies.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2504.11536/extracted/6364450/figures/training_process_bold.png", "caption": "Figure 3: CI-related behavior evolution during RL training.", "description": "This figure visualizes the changes in code-related behaviors of the ReTool model throughout the reinforcement learning (RL) training process. It includes subfigures showing trends in response length, code ratio, code lines, total test set correct code counts, code pass rate, and code invocation timing across multiple training steps on the AIME2024 and AIME2025 datasets.  These trends illustrate how the model learns to use code more effectively and strategically during the RL training.", "section": "3.3 Cognitive Analysis"}, {"figure_path": "https://arxiv.org/html/2504.11536/x2.png", "caption": "Figure 4: The case of \u201caha moment\u201d about code self-correction.", "description": "During the reinforcement learning process, the model initially generated incorrect code due to a missing import statement.  Upon receiving feedback from the code interpreter indicating an error, the model recognized its mistake and autonomously corrected the code by adding the necessary import, demonstrating an emergent capability of self-correction.", "section": "3.3 Cognitive Analysis"}, {"figure_path": "https://arxiv.org/html/2504.11536/extracted/6364450/figures/pie_chart.png", "caption": "Figure 5: Code purpose analysis.", "description": "This figure shows a comparison of code purposes used by the model before and after reinforcement learning (RL) training.  The left chart (a) illustrates the distribution of code purposes before RL training, revealing that calculation and verification were the primary uses. The right chart (b) displays the code purpose distribution after RL training, demonstrating a more diverse range of code purposes including calculation, verification, solution search, optimization, and data processing. This visualization highlights the model's enhanced ability and adaptation in using tools after training.", "section": "3.3 Cognitive Analysis"}, {"figure_path": "https://arxiv.org/html/2504.11536/x3.png", "caption": "Figure 6: Case of CI-powered Reasoning vs. Text-based Reasoning.", "description": "This figure showcases a comparison between the reasoning processes employed by a large language model (LLM) with and without the aid of a code interpreter.  The example demonstrates a mathematical problem-solving scenario.  The text-based reasoning approach shows a lengthy, step-by-step calculation process that's prone to errors, reflecting the limitations of LLMs in purely textual numerical tasks. In contrast, the CI-powered reasoning process uses concise Python code to perform the calculations accurately and efficiently, demonstrating the effectiveness of integrating code interpretation to improve LLMs' mathematical abilities and reduce reasoning errors.", "section": "3.3 Cognitive Analysis"}, {"figure_path": "https://arxiv.org/html/2504.11536/x4.png", "caption": "Figure 7: Template prompt for ReTool rollout.", "description": "This figure shows the template prompt used in the ReTool reinforcement learning process.  The prompt instructs the model to solve a problem step-by-step, using Python code where appropriate. The code will be executed in a sandbox environment, and the results will be provided back to the model to aid in its reasoning. The prompt specifies the format for code input (wrapped in `<code>` tags), the format for the final answer (wrapped in `<answer>` tags), and includes placeholders for the user's question and the model's response.  This structure ensures a standardized interaction between the model and the external code interpreter during training.", "section": "2.3 ReTool: Reinforcement Learning for Strategic Tool Use"}, {"figure_path": "https://arxiv.org/html/2504.11536/x5.png", "caption": "Figure 8: Template Prompt for Data Curation.", "description": "This figure shows the template prompt used in the data curation process for ReTool.  The prompt instructs an AI assistant to convert a given reasoning process into a code-augmented version. The AI should identify parts of the original reasoning that can be sped up or made more accurate with Python code and replace those sections with code snippets and their execution results.  Importantly, the original logical flow of reasoning, including any failed attempts, must be preserved. The final answer should be enclosed in <answer> tags.  The prompt specifies code formatting requirements, indicating that code snippets should be wrapped with <code> tags and be executable Python code.", "section": "2.2 Cold Start for Tool-Integrated Reasoning Foundation"}]