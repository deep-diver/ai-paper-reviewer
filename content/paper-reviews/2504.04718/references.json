{"references": [{"fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper demonstrates strong emergent abilities through large-scale pretraining, and is fundamental to the capabilities of modern LLMs."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper showcases the chain-of-thought prompting approach, which is vital to complex reasoning tasks and improves interpretability in step-wise verification."}, {"fullname_first_author": "Hyung Won Chung", "paper_title": "Scaling instruction-finetuned language models", "publication_date": "2024-01-01", "reason": "This paper demonstrates scaling instruction-finetuned language models, which is related to instruction prompts used for generating and verifying text."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-01-01", "reason": "This paper's approach to training verifiers for math word problems helps to motivate the value of reliable verification of generated solutions."}, {"fullname_first_author": "Geoffrey E. Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-01-01", "reason": "This paper's knowledge distillation method is vital to transfer verification capabilities from larger verifiers to sLMs."}]}