[{"figure_path": "https://arxiv.org/html/2504.04718/x1.png", "caption": "(a) Concept figure", "description": "This figure demonstrates how small language models (SLMs) struggle with complex reasoning tasks due to their limited capacity.  Panel (a) illustrates an example calculation where an SLM fails to reliably verify the result. However, when an external tool such as a code interpreter is used, the SLM significantly improves its verification accuracy. Panel (b) shows an experiment on the Llama 1B model, confirming that tool integration effectively mitigates the drop in performance as the complexity of the calculation increases.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.04718/x2.png", "caption": "(b) Concept-proof results", "description": "The figure displays experimental results demonstrating the impact of tool integration on the reliability of small language models (SLMs) during self-verification.  Specifically, it shows the accuracy of a Llama 1B model in verifying arithmetic calculations with varying numbers of digits. The results illustrate that while the 1B model's accuracy decreases significantly as the number of digits increases when it performs verification without using external tools, the accuracy remains consistently high when the model uses an external code interpreter to assist with the verification task.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.04718/x3.png", "caption": "Figure 1: (a) Concept figure. Small language models (sLMs) often fail due to their limited capacity. However, when sLMs utilize external tools, their reliability significantly improves. (b) Concept-proof experimental results. We evaluate Llama 1B model on their ability to verify arithmetic calculations of N\ud835\udc41Nitalic_N 3-digit numbers. The performance of 1B model consistently drops as N\ud835\udc41Nitalic_N increases. However, enabling code generation and execution for verification largely mitigates the performance drop. See\u00a0Appendix\u00a0A for details of concept-proof experiments.", "description": "Figure 1(a) illustrates the core concept: small language models (SLMs) struggle with complex verification tasks due to limited capacity.  The diagram shows an SLM attempting a calculation and then failing verification.  However, when the same SLM uses an external tool (like a code interpreter), it successfully verifies the answer, highlighting the reliability improvement through tool integration. Figure 1(b) presents experimental results supporting this concept. It shows the accuracy of a Llama 1B model verifying arithmetic calculations with increasing complexity (number of 3-digit numbers involved). The accuracy drops without tool integration. However, when the model is enhanced to generate and execute code to check answers, the performance drop is significantly mitigated.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.04718/x4.png", "caption": "Figure 2: Tool-integrated self-verification for mathematical reasoning.\n(a) Generator: A small language model (sLM) may produce incorrect solutions due to calculation errors.\n(b) Tool-based Verifier (ToolV): The sLM generates executable code based on its reasoning; the output of the code is used to verify the solution\u2019s correctness.\n(c) Reward Model (RM)-based Verifier: The reward model (GenRM / PRM) still evaluates the solution as before, but its verdict only contributes to the final decision if the solution passes the tool-assisted filter.\nConcrete examples are in\u00a0Appendix\u00a0F.", "description": "Figure 2 illustrates the Tool-integrated Self-verification (T1) process for mathematical reasoning tasks.  It shows three stages: (a) a small language model (sLM) generates a solution, which may contain calculation errors; (b) a Tool-based Verifier (ToolV) checks the solution's correctness by executing code generated by the sLM based on its reasoning steps; (c) a Reward Model (RM)-based Verifier (GenRM or PRM) provides a final assessment of the solution, but only after it has passed the ToolV filter.  Appendix F provides concrete examples.", "section": "4 Method"}, {"figure_path": "https://arxiv.org/html/2504.04718/x5.png", "caption": "Figure 3: MATH500 with PRM. Weighted Best-of-N performance of three small language models, emphasizing the benefits of ToolV on college-level math problems. ToolV significantly enhances PRM, enabling small models to outperform or match much larger models. Qwen2.5-1.5B and Llama3.1-8B performances are reported as N=1\ud835\udc411N=1italic_N = 1 greedy decoding.", "description": "This figure displays the results of an experiment on the MATH500 dataset, comparing the performance of three small language models (sLMs) using a process reward model (PRM) for verification. The models tested are SmolLM2-360M-Instruct, Qwen2.5-0.5B-Instruct, and Llama-3.2-1B-Instruct. The key finding is that the addition of ToolV (Tool-integrated self-verification) significantly boosts the performance of the PRM, allowing the small models to either match or surpass the performance of considerably larger models (Qwen2.5-1.5B and Llama-3.1-8B) which only use a single (N=1) prediction. The x-axis represents the number of solutions generated per problem, and the y-axis shows the accuracy. The graph vividly demonstrates the substantial improvement in accuracy achieved through the integration of ToolV in small language models for complex mathematical reasoning.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x6.png", "caption": "Figure 4: MATH500 with GenRM. Weighted Best-of-N performance of three small language models, showcasing the effectiveness of ToolV with GenRM, where even generative verification cannot supplement the calculation error which can be easily filtered out by using a tool.", "description": "Figure 4 presents the results of an experiment evaluating the performance of three small language models (SLMs) on the MATH500 benchmark using a generative reward model (GenRM) for verification.  The graph displays the accuracy of each model across different numbers of solution attempts (Best-of-N).  A key finding highlighted is that the proposed Tool-integrated Self-verification (T1) method significantly improves the models' ability to correctly solve math problems, even surpassing the performance of larger language models. This improvement is attributed to T1's ability to filter out incorrect solutions stemming from calculation errors, a weakness that GenRM alone cannot effectively address.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x7.png", "caption": "Figure 5: GSM8K with GenRM. Weighted Best-of-N performance comparison across three small language models. The results show that ToolV also improves model performance on graduate-level arithmetic problems. However, the gains are smaller on this simpler task, where existing verifiers already perform reliably compared to more challenging tasks.", "description": "Figure 5 presents a comparative analysis of three small language models' performance on the GSM8K dataset when employing a generative reward model (GenRM) for verification, with and without the Tool-integrated self-verification (ToolV) method. The results illustrate that ToolV enhances the performance of all three models, particularly on graduate-level arithmetic problems. However, the improvement is less pronounced on GSM8K compared to more challenging datasets like MATH500, suggesting that the baseline GenRM verifier already achieves satisfactory performance on simpler arithmetic tasks. The x-axis represents the number of solutions per problem considered in the Best-of-N approach, while the y-axis displays the accuracy achieved. This visualization effectively demonstrates ToolV's impact on improving verification accuracy, particularly for complex reasoning tasks.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x8.png", "caption": "Figure 6: MMLU-Pro with PRM. Weighted Best-of-N (N=64\ud835\udc4164N=64italic_N = 64) performance of Llama-3.2-1B-Instruct on three knowledge-intensive domains, illustrating the effect of different document sources in ToolV + Distilled PRM (retrieved and gold documents). ToolV extends beyond math, improving PRM on multi-domain knowledge-intensive reasoning tasks.", "description": "Figure 6 presents the performance of the Llama-3.2-1B-Instruct model on three knowledge-intensive domains from the MMLU-Pro benchmark using the Process Reward Model (PRM) combined with the Tool-integrated Self-verification (ToolV) method.  The results show the impact of using different document sources (retrieved vs. gold standard documents) within the ToolV approach.  The figure illustrates that ToolV enhances the performance of PRM on various multi-domain tasks, extending its benefits beyond the mathematical reasoning tasks shown in other figures.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x9.png", "caption": "Figure 7: Analysis with problem types and levels. We perform analysis on the effect of tool-based verifier with problem types and levels in MATH500 dataset. The results are from Llama-3.2-1B-Instruct with PRM using weighted Best-of-N (N=64\ud835\udc4164N=64italic_N = 64). This analysis shows ToolV is most effective on mid-level problems and calculational domains.", "description": "Figure 7 presents a detailed analysis of the impact of the tool-based verifier (ToolV) on the performance of the Llama-3.2-1B-Instruct model with the process reward model (PRM) on the MATH500 dataset. The analysis specifically focuses on the effects of ToolV across different problem types and difficulty levels (N=64). The results indicate that ToolV significantly improves the performance of the model, particularly for mid-level problems that involve complex calculations. This suggests that tool integration is especially beneficial for enhancing the reasoning capabilities of language models on computationally intensive tasks.", "section": "6.3 Analysis"}, {"figure_path": "https://arxiv.org/html/2504.04718/x10.png", "caption": "Figure 8: Effects of ToolV on sizes of GenRM. Weighted Best-of-N (N=64\ud835\udc4164N=64italic_N = 64) performance of GenRM based on different sizes of Llama 3\u00a0(Dubey et\u00a0al., 2024) on MATH500. For ToolV, we use 1B and only scale up the GenRM.", "description": "This figure demonstrates the impact of Tool-Integrated Self-verification (ToolV) on the performance of different-sized language models in mathematical reasoning.  Specifically, it shows the weighted average of the best-of-64 results for the Generative Reward Model (GenRM) using Llama 3 language models of varying sizes (1B, 3B, and 8B parameters).  Importantly, ToolV is consistently used with a 1B parameter model, highlighting its effect when paired with larger GenRMs.  The results illustrate how ToolV can improve the performance of even smaller models compared to larger models without ToolV, demonstrating its efficiency and effectiveness in improving accuracy and reducing the reliance on large models.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x11.png", "caption": "Figure 9: Correct solutions ratio among N=64\ud835\udc4164N=64italic_N = 64 generations to show how the tool-based verifier works.", "description": "Figure 9 presents a comparison of the success rates in generating correct solutions using the tool-based verifier and standard GenRM model across different model sizes (1B, 3B, 8B parameters). The success rate is defined as the percentage of instances where at least one out of 64 generated solutions is correct after verification using the tool-based verifier. The figure shows that even small language models can achieve high accuracy if the tool-based verifier is used, especially with a larger number of generations.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x12.png", "caption": "Figure 10: Confusion matrix of verification results from GenRM and GenRM + ToolV, where True denotes the correct solution. This result indicates ToolV improves the performance on removing false positive cases. Results are from experiments with Llama-3.2-1B-Instruct on MATH500 benchmark.", "description": "This figure presents confusion matrices for the performance of the GenRM (Generative Reward Model) and GenRM+ToolV (GenRM with Tool-integrated self-verification) in verifying the correctness of solutions to MATH500 problems.  Each matrix shows the counts of true positives (correctly identified correct solutions), false positives (incorrectly identified as correct), true negatives (correctly identified incorrect solutions), and false negatives (incorrectly identified as incorrect). The results show that GenRM+ToolV significantly reduces false positives (incorrectly identified as correct solutions), indicating improved verification accuracy by using the ToolV method.  The improvement comes from the ToolV filtering out incorrect solutions before reaching GenRM.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x13.png", "caption": "Figure 11: Data-scale experiment. Performance comparison with varying distillation data sizes. In each plot, one verifier is distilled with 10% or 1% of data, while the other uses the full dataset. ToolV remains competitive even with only 10% of data, highlighting its data efficiency. Results are from experiments with Llama-3.2-1B-Instruct on MATH500.", "description": "This figure displays the results of an experiment evaluating the data efficiency of Tool-integrated Self-verification (T1).  The experiment varied the amount of training data used to distill the verifier models (ToolV and PRM).  Three different data conditions are compared: both verifiers trained on the full dataset, the ToolV trained on 10% of the data and PRM on 100%, and the ToolV trained on 1% of the data and PRM on 100%. The plot shows the accuracy of the models under these data conditions.  The key finding is that the ToolV method remains surprisingly accurate even with substantially less training data (10% or 1%), demonstrating its data efficiency compared to the PRM method.", "section": "6 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.04718/x14.png", "caption": "Figure 12: MMLU-Pro with PRM (Line Plot). Weighted Best-of-N performance of Llama-3.2-1B-Instruct on three knowledge-intensive domains from MMLU-Pro.", "description": "This line plot displays the weighted best-of-N performance of the Llama-3.2-1B-Instruct language model on three distinct knowledge-intensive domains from the MMLU-Pro benchmark.  The x-axis represents the number of solutions per problem, illustrating how performance changes as the number of generated solutions increases. The y-axis shows the accuracy achieved on each domain (Health, Economics, and History).  The plot includes multiple lines, each representing a different method or baseline:  Majority voting (a baseline without verification), Distilled PRM (a distilled process reward model), ToolV+PRM (retrieved documents), and ToolV+PRM (gold documents).  The use of gold documents versus retrieved documents allows a comparison of the method's performance under ideal versus realistic conditions. The graph effectively demonstrates the impact of ToolV (tool-integrated self-verification) on the accuracy of the PRM, especially when using retrieved documents instead of ideal gold standard documents, which highlights its practical value. ", "section": "6 Experiments"}]