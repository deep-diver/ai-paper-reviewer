[{"figure_path": "https://arxiv.org/html/2412.12571/x1.png", "caption": "Figure 1: Overview of the ChatDiT multi-agent framework. The framework consists of three core agents operating sequentially: the Instruction-Parsing Agent interprets user instructions and analyzes inputs, the Strategy-Planning Agent formulates in-context generation strategies, and the Execution Agent performs the planned actions using pretrained diffusion transformers. An optional Markdown Agent integrates the outputs into cohesive, illustrated articles. Sub-agents handle specialized tasks within each core agent, ensuring flexibility and precision in generation.", "description": "The ChatDiT framework employs a multi-agent system for image generation.  It has three core agents: Instruction-Parsing, Strategy-Planning, and Execution.  The Instruction-Parsing agent analyzes user instructions and input images; the Strategy-Planning agent formulates generation strategies; and the Execution agent uses pretrained diffusion transformers to generate images. An optional Markdown agent can create illustrated articles from the output.  Sub-agents within each core agent handle specific tasks.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.12571/x2.png", "caption": "Figure 2: Selected single-round generation examples of ChatDiT on IDEA-Bench (Liang et\u00a0al., 2024). ChatDiT demonstrates versatility by handling diverse tasks, instructions, and input-output configurations in a zero-shot manner through free-form natural language interactions. The user messages shown here are condensed summaries of the original detailed instructions from IDEA-Bench to conserve space.", "description": "This figure showcases ChatDiT's zero-shot image generation capabilities on a variety of tasks from the IDEA-Bench dataset.  Each example includes the user's condensed instruction and the generated image(s). The tasks demonstrated include color variations of a game controller, 3D rendering from a sketch, image zooming and cropping, pose modification, character design transfer to a new object (tote bag), image-based product design (water bottle), scene generation based on an object (speaker), image sequence generation (sunflower growth), and style transfer (witch portrait).  These examples highlight ChatDiT's ability to handle diverse instructions, input types, and desired output formats without any specific training or adaptation.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.12571/x5.png", "caption": "Figure 3: Selected illustrated article generation examples of ChatDiT. ChatDiT is able to generate interleaved text-image articles based on users\u2019 natural language instructions. It autonomously estimates the required number of images, plans and executes the generation process using in-context capabilities, and seamlessly integrates the outputs into coherent and visually engaging illustrated articles.", "description": "ChatDiT generates illustrated articles based on natural language instructions, estimating the number of images, planning, and executing generation using in-context capabilities to seamlessly integrate outputs into coherent articles. The examples showcase ChatDiT's ability to generate diverse illustrated content, including a horror-thriller comic about a detective investigating murders at a haunted amusement park, an article on a futuristic sports event featuring high-tech athletes, and an adventurer's quest for treasure in a jungle temple.", "section": "4 Experiments"}]