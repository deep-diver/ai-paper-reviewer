[{"heading_title": "ATP Scaling", "details": {"summary": "**ATP (Automated Theorem Proving) scaling** is explored within the context of Large Language Models (LLMs), drawing parallels to the scaling advancements observed in natural language reasoning models. The paper investigates **post-training scaling** techniques for ATP, aiming to bridge the performance gap between LLMs in natural language tasks and their application in formal reasoning with Lean 4 codes. This involves continual training of ATP models using a hybrid dataset, including statement-proof pairs and data designed to emulate human reasoning processes. Reinforcement learning (RL) is also employed, utilizing the Lean 4 compiler's outcome reward to further refine the models. The goal is to enhance the ability of existing formal provers to generate complete and valid proofs, achieving state-of-the-art performance in whole-proof generation. The investigation addresses challenges in formal language reasoning and aims to improve performance in ATP tasks, such as increasing the pass rate on benchmarks like MiniF2F."}}, {"heading_title": "Hybrid Training", "details": {"summary": "While \"Hybrid Training\" isn't explicitly mentioned, the paper does incorporate several elements that suggest a hybrid approach to training formal reasoning models.  The core of the work involves **continual training**, where pre-existing models (DeepSeek-Prover and Goedel-Prover) are further trained on a mixed dataset. This dataset itself is a hybrid, containing both existing formal statement-proof pairs and **synthetically generated data** designed to instill cognitive behaviors like self-reflection. This combination of real and synthetic data is a key characteristic of hybrid training.  Furthermore, the paper utilizes **reinforcement learning (RL)**, which could be seen as another element in the hybrid strategy.  The RL component uses rewards derived from the Lean 4 compiler to guide the model towards generating valid proofs. This blends supervised learning (the continual training) with reinforcement learning, creating a comprehensive and adaptive training regime. The design choices reflect a deliberate effort to integrate various learning paradigms."}}, {"heading_title": "Lean4 as Reward", "details": {"summary": "**Lean4, acting as a reward function**, is a compelling strategy for improving automated theorem proving. **By leveraging the Lean4 compiler to verify proof validity**, the system obtains valuable feedback on whether the generated proofs are syntactically correct and logically sound. This binary feedback, acting as a reward signal, enables reinforcement learning agents to optimize their proof generation strategies. **The use of Lean4's feedback is advantageous as it offers deterministic and objective evaluation**. However, this approach solely relies on final proof verification. **It might miss opportunities for intermediate rewards during proof construction**, potentially leading to less efficient exploration. Further, designing reward functions that incorporate intermediate steps could boost the performance."}}, {"heading_title": "Cognitive Models", "details": {"summary": "From the context, the work emphasizes **integrating human-like reasoning** (verification, backtracking, subgoal setting) into formal theorem provers. The paper suggests that without these cognitive abilities, reinforcement learning (RL) algorithms struggle to achieve significant improvements. It also highlights the idea that directly instilling meta-thinking capabilities into Language Models (LLMs) may be more effective than relying solely on RL, and suggests a paradigm shift towards **zero RL training**. A key point is the authors' suspicion that the **underperformance of RL strategies** in formal reasoning might stem from neglecting these fundamental cognitive aspects, paving the way for integrating the cognitive behavior to achieve zero RL training."}}, {"heading_title": "RL Limitations", "details": {"summary": "The paper acknowledges limitations related to Reinforcement Learning (RL) in formal reasoning. The base prover model appears weaker compared to LLMs used in natural language post-training, hindering full RL potential. Despite integrating cognitive behaviors and careful prompt selection, stronger RL performance didn't fully materialize. The observed weakening of self-reflection capacities after RL suggests challenges in effectively integrating these behaviors into weaker base LLMs. **This implies that the success of RL is highly dependent on the capabilities of the underlying model**, and simply applying RL techniques without addressing the core model's limitations may not yield significant improvements. **Further research is needed to improve the base model itself** before expecting substantial gains from RL."}}]