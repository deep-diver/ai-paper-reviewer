---
title: "DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception"
summary: "DocLayout-YOLO: Blazing-fast document layout analysis via diverse synthetic data and adaptive perception, exceeding state-of-the-art speed and accuracy."
categories: ["AI Generated"]
tags: ["üîñ 24-10-16", ]
showSummary: true
date: 2024-10-16
draft: false
---

{{< keyword >}} 2410.12628 {{< /keyword >}}

### TL;DR


{{< lead >}}

DocLayout-YOLO tackles the speed-accuracy tradeoff in document layout analysis (DLA).  It uses a novel, optimized YOLO-based architecture paired with a large, diverse synthetic dataset (DocSynth-300K).  DocSynth-300K was created using a unique algorithm that frames document synthesis as a 2D bin-packing problem, resulting in realistic and varied layouts.  DocLayout-YOLO also includes a 'Global-to-Local Controllable Receptive Module' to better handle differently sized document components.  Testing shows it surpasses existing methods in both speed and accuracy on diverse benchmarks.  This research significantly contributes to improving DLA efficiency and robustness, especially for real-world documents with variable layouts and component sizes.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.12628" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.12628" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is crucial for researchers in document layout analysis and computer vision.  It introduces a novel, high-speed model (DocLayout-YOLO) that outperforms existing methods, along with a new large-scale synthetic dataset (DocSynth-300K) for training. This advances research in robust, efficient document understanding, vital for many applications, and opens avenues for improving model generalization across diverse document types.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} DocLayout-YOLO significantly improves the speed and accuracy of document layout analysis compared to existing methods. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The new DocSynth-300K dataset, generated by a novel Mesh-candidate BestFit algorithm, substantially improves model performance. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} The Global-to-Local Controllable Receptive Module (GL-CRM) effectively handles the varied scales of document elements. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](https://ai-paper-reviewer.com/2410.12628/figures_2_0.png)

> üîº The figure compares the speed and accuracy of DocLayout-YOLO against other state-of-the-art document layout analysis methods on the DocStructBench dataset, showing that DocLayout-YOLO excels in both speed and accuracy.
> <details>
> <summary>read the caption</summary>
> Figure 1: Comparisons between DocLayout-YOLO and existing state-of-the-art (SOTA) DLA methods. DocLayout-YOLO surpasses unimodal and multimodal methods in both speed and accuracy.
> </details>





![](https://ai-paper-reviewer.com/2410.12628/charts_2_0.png)

> üîº The chart compares the speed and accuracy of DocLayout-YOLO against other state-of-the-art document layout analysis methods, showing its superiority in both aspects.
> <details>
> <summary>read the caption</summary>
> Figure 1: Comparisons between DocLayout-YOLO and existing state-of-the-art (SOTA) DLA methods. DocLayout-YOLO surpasses unimodal and multimodal methods in both speed and accuracy.
> </details>





{{< table-caption >}}
<table id='1' style='font-size:14px'><tr><td rowspan="2">Improvement GL-CRM Pretrain</td><td colspan="2">D4LA</td><td colspan="2">DocLayNet</td><td colspan="2">Academic</td><td colspan="2">Textbook</td><td colspan="2">Market Analysis</td><td colspan="2">Financial</td></tr><tr><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td></tr><tr><td></td><td>68.6</td><td>80.7</td><td>76.7</td><td>93.4</td><td>80.5</td><td>95.0</td><td>70.2</td><td>88.0</td><td>68.9</td><td>79.2</td><td>89.8</td><td>95.9</td></tr><tr><td></td><td>69.8</td><td>81.7</td><td>77.7</td><td>93.0</td><td>81.4</td><td>95.4</td><td>71.5</td><td>88.8</td><td>70.2</td><td>80.0</td><td>90.0</td><td>95.8</td></tr><tr><td></td><td>69.8</td><td>82.1</td><td>79.3</td><td>93.6</td><td>82.1</td><td>95.8</td><td>71.5</td><td>88.5</td><td>69.3</td><td>79.6</td><td>90.3</td><td>95.5</td></tr><tr><td>V</td><td>70.3</td><td>82.4</td><td>79.7</td><td>93.4</td><td>81.8</td><td>95.8</td><td>73.7</td><td>90.3</td><td>69.4</td><td>79.4</td><td>90.1</td><td>95.9</td></tr><tr><td>‚Üë‚ñ≥</td><td>1.7</td><td>1.7</td><td>3.0</td><td>-</td><td>1.3</td><td>0.8</td><td>3.5</td><td>2.3</td><td>0.5</td><td>0.2</td><td>0.3</td><td>-</td></tr></table>{{< /table-caption >}}

> üîº Table 1 presents the performance improvements of DocLayout-YOLO using different optimization strategies, showing significant gains over the baseline YOLO-v10 model, particularly with DocSynth-300K pre-training and the GL-CRM.
> <details>
> <summary>read the caption</summary>
> Table 1: Results of DocLayout-YOLO with different optimization strategies. Pretrain denotes DocSynth-300K pre-training. Resulting DocLayout-YOLO significantly outperforms the baseline model. ‚Üë‚ñ≥ denotes improvements compared with baseline YOLO-v10 model.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](https://ai-paper-reviewer.com/2410.12628/figures_3_0.png)

> üîº Figure 2 illustrates the Mesh-candidate BestFit algorithm, showing the preprocessing steps of creating an element pool and the layout generation process of iteratively finding the best fit between candidate elements and grid spaces.
> <details>
> <summary>read the caption</summary>
> Figure 2: Illustration of Mesh-candidate BestFit. Initially, in (A) Preprocessing, a category-wise element pool is created from a small initial dataset. During (B) Layout Generation, Mesh-candidate BestFit iteratively searches for the optimal candidate-grid match.
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_4_0.png)

> üîº Figure 3 shows example synthetic documents generated by the proposed Mesh-candidate BestFit algorithm, illustrating the diversity of layouts and elements achieved.
> <details>
> <summary>read the caption</summary>
> Figure 3: Examples of synthetic document data. Synthetic documents demonstrate comprehensive layout diversity (multiple layout formats) and element diversity (incorporating varied elements).
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_5_0.png)

> üîº The figure illustrates the Controllable Receptive Module (CRM) which extracts and fuses features with multiple scales and granularities to handle the scale-varying challenge in document images.
> <details>
> <summary>read the caption</summary>
> Figure 4: Illustration of Controllable Receptive Module (CRM), which extracts and fuses features of varying scales and granularities.
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_5_1.png)

> üîº The figure illustrates the hierarchical perception process of the Global-to-Local Design (GL) architecture in DocLayout-YOLO, showing how features are extracted at multiple scales (global, block, and local).
> <details>
> <summary>read the caption</summary>
> Figure 5: Illustration of Global-to-local design.
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_6_0.png)

> üîº The figure compares the speed and accuracy of DocLayout-YOLO against other state-of-the-art document layout analysis methods, demonstrating its superior performance.
> <details>
> <summary>read the caption</summary>
> Figure 1: Comparisons between DocLayout-YOLO and existing state-of-the-art (SOTA) DLA methods. DocLayout-YOLO surpasses unimodal and multimodal methods in both speed and accuracy.
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_9_0.png)

> üîº Figure 7 visualizes document images generated using three different synthetic methods: Random, Diffusion, and the proposed Mesh-candidate BestFit, showing the differences in layout and rendering quality.
> <details>
> <summary>read the caption</summary>
> Figure 7: Visualization of generated document images using different document synthetic methods.
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_14_0.png)

> üîº Figure 8 shows examples of generated document layouts with varying element sizes and arrangements, along with their rendered counterparts.
> <details>
> <summary>read the caption</summary>
> Figure 8: Visualization of generated diverse layouts and corresponding pages after rendering. S, M, L respectively denote small, medium, and large elements, indicating the components that are relatively abundant on the page.
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_15_0.png)

> üîº The figure displays example pages from the four subsets of the DocStructBench dataset (Academic, Textbook, Market Analysis, and Financial) to illustrate the diversity and complexity of document layouts.
> <details>
> <summary>read the caption</summary>
> Figure 6: Examples of complex documents with different formats and structures in DocStructBench.
> </details>



![](https://ai-paper-reviewer.com/2410.12628/figures_15_1.png)

> üîº Figure 1 compares the speed and accuracy of DocLayout-YOLO against other state-of-the-art document layout analysis methods, demonstrating its superiority in both aspects.
> <details>
> <summary>read the caption</summary>
> Figure 1: Comparisons between DocLayout-YOLO and existing state-of-the-art (SOTA) DLA methods. DocLayout-YOLO surpasses unimodal and multimodal methods in both speed and accuracy.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table id='3' style='font-size:16px'><tr><td rowspan="2" colspan="2">Methods</td><td rowspan="2">Backbone</td><td rowspan="2">Pretrain Dataset</td><td colspan="2">D4LA</td><td colspan="2">DocLayNet</td></tr><tr><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td></tr><tr><td rowspan="2">Unimodal</td><td>YOLO-v10</td><td>v10m</td><td>-</td><td>68.6</td><td>80.7</td><td>76.2</td><td>93.0</td></tr><tr><td>DINO-4scale</td><td>R50</td><td>ImageNet1K</td><td>64.7</td><td>76.9</td><td>77.7</td><td>93.5</td></tr><tr><td rowspan="4">Multimodal</td><td>VGT</td><td>ViT-B</td><td>IIT-CDIP, 42M</td><td>68.8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LayoutLMv3-B</td><td>ViT-B</td><td>IIT-CDIP, 42M</td><td>60.0</td><td>72.6</td><td>75.4</td><td>92.1</td></tr><tr><td>DiT-Cascade-B</td><td>ViT-B</td><td>IIT-CDIP, 42M</td><td>67.7</td><td>79.8</td><td>73.2</td><td>87.6</td></tr><tr><td>DiT-Cascade-L</td><td>ViT-L</td><td>IIT-CDIP, 42M</td><td>68.2</td><td>80.1</td><td>72.6</td><td>84.9</td></tr><tr><td>Ours</td><td>DocLayout-YOLO</td><td>v10m++</td><td>DocSynth, 300K</td><td>70.3</td><td>82.4</td><td>79.7</td><td>93.4</td></tr></table>{{< /table-caption >}}
> üîº Table 2 presents a comparison of the performance of DocLayout-YOLO against other unimodal and multimodal methods on the D4LA and DocLayNet datasets, highlighting the superior performance of DocLayout-YOLO.
> <details>
> <summary>read the caption</summary>
> Table 2: Performance comparison on D4LA and DocLayNet. v10m++ denotes the original v10m bottleneck enhanced by our proposed GL-CRM bottleneck. Best and second best are highlighted.
> </details>

{{< table-caption >}}
<table id='1' style='font-size:14px'><tr><td colspan="2">Method</td><td>Backbone</td><td colspan="2">Academic</td><td colspan="2">Textbook</td><td colspan="2">Market Analysis</td><td colspan="2">Financial</td><td rowspan="2">FPS</td></tr><tr><td></td><td></td><td></td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td></tr><tr><td rowspan="2">Unimodal</td><td>YOLO-v10</td><td>v10m</td><td>80.5</td><td>95.0</td><td>70.2</td><td>88.0</td><td>68.9</td><td>79.2</td><td>89.9</td><td>95.9</td><td>144.9‚Ä†</td></tr><tr><td>DINO-4scale</td><td>R50</td><td>80.5</td><td>95.4</td><td>70.5</td><td>85.6</td><td>68.6</td><td>79.2</td><td>89.1</td><td>95.6</td><td>26.7¬±</td></tr><tr><td rowspan="4">Multimodal</td><td>DiT-Cascade-B</td><td>ViT-B</td><td>79.7</td><td>95.1</td><td>69.7</td><td>86.1</td><td>63.7</td><td>71.0</td><td>88.7</td><td>94.1</td><td>14.1*</td></tr><tr><td>DiT-Cascade-L</td><td>ViT-L</td><td>81.0</td><td>96.0</td><td>70.8</td><td>86.8</td><td>70.8</td><td>80.8</td><td>89.3</td><td>94.5</td><td>6.0*</td></tr><tr><td>LayoutLMv3-B</td><td>ViT-B</td><td>76.5</td><td>94.9</td><td>66.0</td><td>82.3</td><td>65.7</td><td>75.2</td><td>85.7</td><td>90.4</td><td>9.0*</td></tr><tr><td>LayoutLMv3-BC</td><td>ViT-B</td><td>77.7</td><td>93.5</td><td>68.0</td><td>82.8</td><td>67.9</td><td>75.7</td><td>87.6</td><td>92.1</td><td>9.0*</td></tr><tr><td>Ours</td><td>DocLayout-YOLO</td><td>v10m++</td><td>81.8</td><td>95.8</td><td>73.7</td><td>90.3</td><td>69.4</td><td>79.4</td><td>90.1</td><td>95.9</td><td>85.5‚Ä†</td></tr></table>{{< /table-caption >}}
> üîº Table 3 compares the performance of DocLayout-YOLO against other state-of-the-art methods on the DocStructBench dataset, showing mAP, AP50, and FPS across four document types.
> <details>
> <summary>read the caption</summary>
> Table 3: Performance comparison on DocStructBench. v10m++ denotes original v10m bottleneck enhanced by our proposed GL-CRM bottleneck. FPS is tested on the same single A100 GPU machine. LayoutLMv3-BC denotes pre-trained on additional Chinese document data. * denotes FPS tested under Detectron2, ‚Ä† denotes FPS tested under Ultralytics (Jocher et al., 2023) and + denotes tested under MMDetection. Best and second best are highlighted.
> </details>

{{< table-caption >}}
<table id='3' style='font-size:14px'><tr><td rowspan="2">Data Type</td><td rowspan="2">Pretrain Dataset</td><td rowspan="2">Volume</td><td colspan="2">Academic</td><td colspan="2">Textbook</td><td colspan="2">Market Analysis</td><td colspan="2">Financial</td></tr><tr><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td><td>mAP</td><td>AP50</td></tr><tr><td colspan="3">baseline</td><td>80.5</td><td>95.0</td><td>70.2</td><td>88.0</td><td>68.9</td><td>79.2</td><td>89.9</td><td>95.9</td></tr><tr><td rowspan="3">Public</td><td>M6Doc</td><td>2k</td><td>80.4</td><td>94.9</td><td>70.0</td><td>87.7</td><td>68.9</td><td>79.1</td><td>89.7</td><td>95.8</td></tr><tr><td>DocBank</td><td>400k</td><td>81.6</td><td>95.5</td><td>70.9</td><td>89.6</td><td>69.1</td><td>79.5</td><td>90.1</td><td>95.9</td></tr><tr><td>PubLayNet</td><td>300k</td><td>81.0</td><td>95.3</td><td>71.5</td><td>88.8</td><td>69.1</td><td>78.8</td><td>89.7</td><td>95.7</td></tr><tr><td rowspan="3">Synthetic</td><td>Random</td><td>300k</td><td>80.5</td><td>95.1</td><td>71.2</td><td>88.8</td><td>68.1</td><td>78.6</td><td>89.6</td><td>95.7</td></tr><tr><td>Diffusion</td><td>300k</td><td>80.7</td><td>95.2</td><td>71.9</td><td>89.4</td><td>68.9</td><td>79.3</td><td>89.3</td><td>95.8</td></tr><tr><td>DocSynth</td><td>300k</td><td>82.1</td><td>95.8</td><td>71.5</td><td>88.5</td><td>69.3</td><td>79.6</td><td>90.3</td><td>95.5</td></tr></table>{{< /table-caption >}}
> üîº Table 4 presents a comparison of downstream fine-tuning performance using different pre-training datasets (public and synthetic), highlighting DocSynth-300K's superior adaptability across various document types.
> <details>
> <summary>read the caption</summary>
> Table 4: Donwstream fine-tuning performance of different document dataset pre-trained model (baseline YOLO-v10m is utilized). baseline row indicates from scratch training results. Results show that compared with public and synthetic document datasets, DocSynth-300K shows better adaptability across all document types. Best and second best are highlighted.
> </details>

{{< table-caption >}}
<br><table id='8' style='font-size:14px'><tr><td>Data</td><td>Type</td><td>Volume</td></tr><tr><td>DSSE200</td><td>Academic</td><td>271</td></tr><tr><td>CHN</td><td>Wikipedia</td><td>10K</td></tr><tr><td>DocBank</td><td>Academic</td><td>400K</td></tr><tr><td>PubLayNet</td><td>Academic</td><td>300K</td></tr><tr><td>DocLayNet</td><td>Multiple</td><td>80K</td></tr><tr><td>D4LA</td><td>Multiple</td><td>9K</td></tr><tr><td>Prima-LAD</td><td>Multiple</td><td>478</td></tr></table>{{< /table-caption >}}
> üîº This table lists the different document datasets used to train the LACE (Layout Augmented generation using diffusion model) model for generating synthetic document layouts.
> <details>
> <summary>read the caption</summary>
> Table 5: Data used in LACE.
> </details>

{{< table-caption >}}
<br><table id='8' style='font-size:14px'><tr><td colspan="2">Ablation</td><td colspan="5">D4LA</td></tr><tr><td>Global-level</td><td>Block-level</td><td>mAP</td><td>AP50</td><td>APs</td><td>APm</td><td>APt</td></tr><tr><td></td><td></td><td>68.6</td><td>80.7</td><td>47.0</td><td>53.2</td><td>68.8</td></tr><tr><td rowspan="2">V</td><td rowspan="2"></td><td>69.2</td><td>81.2</td><td>47.1</td><td>53.9</td><td>69.6</td></tr><tr><td>‚Üë0.6</td><td>‚Üë0.5</td><td>‚Üë0.1</td><td>‚Üë0.7</td><td>‚Üë0.8</td></tr><tr><td rowspan="2"></td><td rowspan="2">V</td><td>69.3</td><td>81.5</td><td>47.2</td><td>55.0</td><td>69.4</td></tr><tr><td>‚Üë0.7</td><td>‚Üë0.8</td><td>‚Üë0.2</td><td>‚Üë1.8</td><td>‚Üë0.6</td></tr><tr><td rowspan="2">V</td><td rowspan="2">V</td><td>69.8</td><td>81.7</td><td>47.2</td><td>55.3</td><td>70.2</td></tr><tr><td>‚Üë1.2</td><td>‚Üë1.0</td><td>‚Üë0.2</td><td>‚Üë2.1</td><td>‚Üë1.4</td></tr></table>{{< /table-caption >}}
> üîº The table shows the ablation study of the proposed GL-CRM, demonstrating the impact of global and local components on the detection accuracy for different object sizes.
> <details>
> <summary>read the caption</summary>
> Table 6: Ablation studies on GL-CRM.
> </details>

{{< table-caption >}}
<br><table id='4' style='font-size:14px'><tr><td>Type</td><td>Source</td><td>Training</td><td>Testing</td></tr><tr><td>Academic</td><td>Academic papers</td><td>1605</td><td>402</td></tr><tr><td>Textbook</td><td>Textbooks & Test papers</td><td>2345</td><td>587</td></tr><tr><td>Analysis Report</td><td>Industry & market analysis report</td><td>2660</td><td>651</td></tr><tr><td>Financial</td><td>Financial business document</td><td>2472</td><td>592</td></tr></table>{{< /table-caption >}}
> üîº Table 7 shows the document source and training/testing data split for the four subsets of the DocStructBench dataset.
> <details>
> <summary>read the caption</summary>
> Table 7: Document source and train/test split of Docstructbench.
> </details>

{{< table-caption >}}
<table id='7' style='font-size:16px'><tr><td>Category</td><td>Interpretation</td><td>Training</td><td>Testing</td></tr><tr><td>Title</td><td>Includes multi-level headings, separate lines, bolded, and in a distinct font from the text.</td><td>11384</td><td>2943</td></tr><tr><td>Plain text</td><td>Main body text of the document.</td><td>45243</td><td>12455</td></tr><tr><td>Abandon</td><td>Includes headers, footers, page numbers, page footnotes, and marginal notes.</td><td>16640</td><td>4379</td></tr><tr><td>Figure</td><td>Isolate figure floating in the document.</td><td>5164</td><td>1296</td></tr><tr><td>Figure caption</td><td>Corresponding caption interpreting the figure.</td><td>2660</td><td>715</td></tr><tr><td>Table</td><td>Isolate table floating in the document.</td><td>1389</td><td>407</td></tr><tr><td>Table caption</td><td>Corresponding caption interpreting the table.</td><td>911</td><td>271</td></tr><tr><td>Table footnote</td><td>The footnote of a table, typically provides additional explanations and clarifications about the table.</td><td>1490</td><td>370</td></tr><tr><td>Isolate formula</td><td>A standalone equation (excluding equations embedded within the text)</td><td>795</td><td>221</td></tr><tr><td>Formula caption</td><td>The caption of a formula, typically refers to the label or numbering of the formula.</td><td>385</td><td>86</td></tr></table>{{< /table-caption >}}
> üîº Table 8 presents the fine-grained category and the number of instances annotated in the DocStructBench dataset.
> <details>
> <summary>read the caption</summary>
> Table 8: Fine-grained category and number of instances annotated in DocStructBench.
> </details>

{{< table-caption >}}
<table id='0' style='font-size:18px'><tr><td></td><td>Algorithm 1: Mesh-candidate BestFit Algorithm</td></tr><tr><td></td><td>Input: Element pool P, Cset = {e1, e2, .... eN} sampled from P, matching threshold frthr; Output: Generated layout L; insert into L;</td></tr><tr><td>1</td><td>sample e* from Cset and</td></tr><tr><td>2</td><td>while ÎØ∏ < N do</td></tr><tr><td>3</td><td>Mg = MeshEngine(L);</td></tr><tr><td>4</td><td>foreach candidate ei E Cset do</td></tr><tr><td>5</td><td>foreach meshgrid gj E Mset do</td></tr><tr><td>6</td><td>fr = match(ei, gj);</td></tr><tr><td>7</td><td>if fr > frmax then</td></tr><tr><td>8</td><td>fr max ‚Üê fr, Cbest ‚Üê ei, Mbest ‚Üê gj;</td></tr><tr><td>9</td><td>end</td></tr><tr><td>10</td><td>end</td></tr><tr><td>11</td><td>end</td></tr><tr><td>12</td><td>if fr max < frthr then</td></tr><tr><td>13</td><td>break</td></tr><tr><td>14</td><td>else</td></tr><tr><td>15</td><td>I remove Cbest from Cset and insert Cbest into L;</td></tr><tr><td>16</td><td>end</td></tr><tr><td></td><td></td></tr><tr><td>17 18</td><td>end return L;</td></tr></table>{{< /table-caption >}}
> üîº Table 1 presents the performance improvements of DocLayout-YOLO using different optimization strategies, showing significant gains over the baseline YOLO-v10 model.
> <details>
> <summary>read the caption</summary>
> Table 1: Results of DocLayout-YOLO with different optimization strategies. Pretrain denotes DocSynth-300K pre-training. Resulting DocLayout-YOLO significantly outperforms the baseline model. ‚ÜëŒî denotes improvements compared with baseline YOLO-v10 model.
> </details>

{{< table-caption >}}
<br><table id='9' style='font-size:14px'><tr><td>Layout Generation</td><td>Align‚Üì</td><td>Density‚Üë</td></tr><tr><td>Random</td><td>0.0171</td><td>0.259</td></tr><tr><td>Diffusion (LACE)</td><td>0.0032</td><td>0.476</td></tr><tr><td>Mesh-candidate BestFit (ours)</td><td>0.0009</td><td>0.645</td></tr></table>{{< /table-caption >}}
> üîº Table 9 quantitatively compares different layout generation methods based on their alignment and density scores, showing Mesh-candidate BestFit's superior performance.
> <details>
> <summary>read the caption</summary>
> Table 9: Quantative comparison between different layout generation methods.
> </details>

</details>


### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2410.12628/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.12628/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}