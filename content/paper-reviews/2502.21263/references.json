{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-01-01", "reason": "This paper introduces BERT, a foundational model in NLP, which the current study uses as a baseline for information extraction."}, {"fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "publication_date": "2020-04-01", "reason": "This paper introduces Longformer, a model capable of processing long sequences, which the current study uses for diagnosis prediction due to its strong performance in clinical tasks."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-01", "reason": "This paper introduces LLaMA, a large language model used in this study with parameter-efficient fine-tuning and retrieval-augmented generation for clinical coding."}, {"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-01", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning technique used to adapt large language models for the clinical coding task."}, {"fullname_first_author": "Olivier Bodenreider", "paper_title": "The unified medical language system (UMLS): integrating biomedical terminology", "publication_date": "2004-01-01", "reason": "This paper describes the UMLS, a comprehensive resource of biomedical terminologies, which is used in the study for transferring knowledge across domains and terminologies."}]}