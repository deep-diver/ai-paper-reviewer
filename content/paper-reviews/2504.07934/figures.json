[{"figure_path": "https://arxiv.org/html/2504.07934/extracted/6351589/figures/mainfigure_v2.png", "caption": "Figure 1: Recent \u201cReasoning VLMs\u201d studies finetune \u201cBase VLMs\u201d with extra reasoning training data to improve visual reasoning. This paper presents a data-efficient self-improving method for better training reasoning VLMs. (Left) Comparison of VLMs with different parameter sizes on MathVista. Our model ThinkLite-VL-7B achieves the state-of-the-art (SoTA) accuracy of 75.1, surpassing Qwen2.5-VL-72B-Instruct, GPT-4o, O1, and other 7B-level reasoning VLMs. (Right) Comparison of the reasoning training data size used by 7B-level reasoning models. Our model achieves SoTA performance using only 11k data, and without any additional knowledge distillation.", "description": "Figure 1 demonstrates the effectiveness of the proposed data-efficient self-improvement method for training visual reasoning VLMs.  The left panel shows a comparison of various VLMs' performance on the MathVista benchmark, highlighting that ThinkLite-VL-7B achieves state-of-the-art accuracy (75.1%) with significantly fewer parameters than competing models.  The right panel contrasts the amount of reasoning training data used by different 7B-level reasoning models.  ThinkLite-VL-7B stands out by achieving top performance using only 11,000 data points, a substantial reduction compared to other models, and without employing knowledge distillation techniques.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.07934/extracted/6351589/figures/mainfigure_datasize_v2.png", "caption": "Figure 2: Performance comparison on 8 visual benchmarks. Our model significantly outperforms Qwen2.5-VL-7b-Instruct and other 7b-level reasoning models.", "description": "Figure 2 presents a detailed performance comparison of various visual reasoning models across eight benchmark datasets.  It demonstrates the superior performance of the proposed ThinkLite-VL-7B model compared to other 7B-level reasoning models, notably surpassing the Qwen2.5-VL-7B-Instruct model.  The figure visually highlights the relative strengths of each model across multiple metrics, providing a clear and comprehensive overview of their visual reasoning capabilities.", "section": "4.1 Benchmark Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.07934/extracted/6351589/figures/maincompare.png", "caption": "Figure 3: Data statistic of ThinkLite-VL-70k training dataset. We find that converting all answers to open-ended format is critical in reliably assessing question difficulty and effective model training.", "description": "Figure 3 presents a breakdown of the 70,000 training samples used in the ThinkLite-VL model.  These samples are categorized into three main visual reasoning tasks: mathematical reasoning, natural image understanding, and chart understanding.  Each category further subdivides into question types (e.g., open-ended, multiple choice). The figure visually displays the number of samples in each subcategory, showing the dataset composition.  A key finding highlighted in the caption is the importance of converting all answer formats to open-ended responses. This standardization is crucial for accurately determining question difficulty and achieving optimal model performance during training.", "section": "3.1 Data Collection"}, {"figure_path": "https://arxiv.org/html/2504.07934/extracted/6351589/figures/piechart_11k_v2.png", "caption": "Figure 4: Data difficulty distribution of our 11k training set after MCTS-based data filtration. Unsolved refers to data that VLM cannot solve after 50 MCTS iterations.", "description": "This figure shows the distribution of difficulty levels in the 11,000 training samples selected by the MCTS-based data filtration method.  The x-axis represents the number of MCTS iterations required to solve a problem. The y-axis shows the percentage of samples at each iteration count. The samples requiring more than 50 iterations, categorized as \"Unsolved,\" represent the most challenging problems for the VLM, while lower iteration counts indicate relatively easier problems.  The distribution is skewed toward the \"Unsolved\" category, indicating that this subset of the data contains the most difficult samples. This suggests that using a data filtering strategy based on MCTS, enabling the retention of challenging and unsolved samples for training, is beneficial for enhancing VLM reasoning abilities.", "section": "3.2 MCTS-based Sample Selection"}, {"figure_path": "https://arxiv.org/html/2504.07934/extracted/6351589/figures/R1-reward.png", "caption": "Figure 5: Comparison of reward curves of models trained with different data during RFT. Iter5+Unsolved 11k dataset presents the most challenging learning setting for VLM, highlighting the difficulty of the samples selected by MCTS-based sample selection.", "description": "This figure displays the reward curves obtained during reinforcement fine-tuning (RFT) for several variations of the ThinkLite-VL model. Each curve represents a model trained on a different dataset: a randomly selected subset (ThinkLite-VL-Random11k), the full dataset (ThinkLite-VL-Fullset), a subset containing only samples requiring more than 5 iterations to solve using Monte Carlo Tree Search (MCTS) (ThinkLite-VL-Iter5Only), the combination of samples that are unsolved and those requiring more than 5 iterations (ThinkLite-VL-Iter5+Unsolved which is the ThinkLite-VL model itself), and a self-consistency filtered set. The Iter5+Unsolved 11k dataset, used for the main ThinkLite-VL model, shows the lowest initial reward, indicating the highest difficulty, but also demonstrates a steeper curve suggesting that the more challenging data leads to better final performance. This supports the claim that including difficult samples in the RFT process improves the model's reasoning ability more effectively.", "section": "4.2 Importance of MCTS-based Sample Selection"}]