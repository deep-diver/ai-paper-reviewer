{"importance": "This paper introduces a data-efficient visual reasoning method, enabling VLM self-improvement with fewer samples. **It offers a novel MCTS-guided sample selection strategy**, outperforming existing 7B-level models and providing insights for future research in VLM training and reasoning.", "summary": "SOTA Visual Reasoning with Less Data via MCTS-Guided Sample Selection!", "takeaways": ["MCTS can be repurposed for sample difficulty quantification to improve data efficiency.", "High-quality, challenging training data is crucial for VLM self-improvement.", "ThinkLite-VL achieves SoTA performance using only 11k training samples."], "tldr": "Enhancing the reasoning capabilities of Vision-Language Models (VLMs) often requires large-scale reinforcement fine-tuning (RFT), yet progress is limited due to the mismatch between text-focused pre-training and multimodal post-training. Existing methods often involve knowledge distillation via supervised fine-tuning, which is cumbersome and prevents VLMs from truly self-improving. The key issue is identifying appropriately challenging training data that aligns with the VLM's skill level. \n\nTo address these issues, this paper introduces ThinkLite-VL, a data-efficient self-improving method that uses Monte Carlo Tree Search (MCTS) to select challenging training samples. By repurposing MCTS, the method quantifies sample difficulty based on the iterations required for the VLM to solve each problem. The framework filters and retains 11k samples, achieving state-of-the-art accuracy on MathVista and improving average performance by 7% with no additional knowledge distillation. **This demonstrates a data-efficient approach to enhancing visual reasoning.**", "affiliation": "University of Maryland, College Park", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2504.07934/podcast.wav"}