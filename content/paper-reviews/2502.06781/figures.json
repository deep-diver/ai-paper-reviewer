[{"figure_path": "https://arxiv.org/html/2502.06781/x1.png", "caption": "Figure 1: Overall performance between OREAL-32B and some competitive baselines.", "description": "This bar chart compares the performance of the OREAL-32B model against several other state-of-the-art large language models on the MATH-500 benchmark dataset.  The chart displays the pass@1 accuracy (the percentage of times the model correctly answered the question on the first try) for each model.  Models are compared across multiple benchmarks including MATH-500, AIME2024, AIME2025-1, LiveMathBench, and OlympiadBench, showcasing performance across different question difficulty levels and mathematical topics.  The figure highlights that OREAL-32B significantly outperforms many existing models, achieving the best performance overall.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06781/x2.png", "caption": "Figure 2: Average test accuracy of 7B models across different training steps.", "description": "This figure shows the average test accuracy on MATH-500 of 7B parameter models across different training steps using various reinforcement learning (RL) settings.  The x-axis represents the training iteration steps and the y-axis represents the average accuracy across different benchmark datasets. The plot allows for comparison of model performance using different RL approaches by showing how accuracy changes with training. This is an ablation study, showing the impact of adding components such as reward shaping and behavioral cloning to a baseline REINFORCE approach.", "section": "4.3 Ablation Study"}]