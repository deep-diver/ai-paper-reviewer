[{"figure_path": "https://arxiv.org/html/2412.16849/extracted/6088342/figs/fig1.jpg", "caption": "Figure 1: OpenRFT framework.", "description": "The OpenRFT framework consists of three main modules: data augmentation, SFT-based imitation, and RL-based exploration and self-improvement.  Data augmentation generates more domain-specific data by rewriting questions and shuffling options. SFT-based imitation uses a stronger reasoning foundation model to synthesize reasoning process data, which is then used to pre-adapt the student policy model. RL-based exploration and self-improvement incorporates domain-specific samples via few-shot ICL and uses a process reward model (PRM) to supervise the rationality of the reasoning process.", "section": "2 METHOD"}, {"figure_path": "https://arxiv.org/html/2412.16849/extracted/6088342/figs/data_scale.jpg", "caption": "Figure 2: Task instructions for generating distinct expressions", "description": "This figure details the task instructions used for data augmentation in the OpenRFT framework.  The goal is to generate multiple distinct yet semantically equivalent versions of a scientific multiple-choice question, preserving its original meaning.  Instructions specify that the paragraph should remain a scientific multiple-choice question without options. The meaning should be unchanged, without adding extra or unrelated information. Sentence structures can be adjusted, but the meaning must remain consistent. The final output should include five variations of the original question, each separated by a specified delimiter.", "section": "2.1 DATA AUGMENTATION"}, {"figure_path": "https://arxiv.org/html/2412.16849/extracted/6088342/figs/fig3.png", "caption": "Figure 3: Performance with different sizes of domain-specific data. The light green dashed line represents the performance of SFT with 100 samples.", "description": "Figure 3 illustrates how the model's performance changes with varying amounts of domain-specific training data.  The x-axis represents the number of domain-specific samples used in the reinforcement learning phase of the OpenRFT model. The y-axis shows the accuracy of the model's predictions. Multiple lines represent different model variations (ReFT, SFT+RL(PRM), SFT+RL(PRM)+DA). The graph reveals that the accuracy generally improves as the number of samples increases, indicating the effectiveness of the proposed data augmentation techniques. A light green dashed line serves as a baseline, showing the accuracy achieved by Supervised Fine-Tuning (SFT) using only 100 samples.", "section": "3 Experiments"}]