[{"content": "| Model/ Method | Biology | Chemistry | Chemistry | Chemistry | Physics | Materials | Materials | Materials | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| GPT-4o-mini | **0.37** | 0.69 | 0.84 | **0.32** | 0.53 | 0.49 | 0.90 | 0.525 | 0.583 |\n| o1-mini | 0.35 | **0.86** | **0.87** | 0.23 | **0.73** | **0.70** | **0.87** | 0.50 | **0.639** |\n| Vanilla | 0.28 | 0.55 | 0.52 | 0.23 | 0.45 | 0.34 | 0.41 | 0.41 | 0.403 |\n| ReFT | 0.27 | 0.50 | 0.52 | 0.23 | 0.44 | 0.33 | 0.41 | 0.50 | 0.402 |\n| ReFT+PRM | 0.30 | 0.57 | 0.49 | 0.23 | 0.44 | 0.36 | 0.37 | 0.48 | 0.405 |\n| SFT | <ins>0.33</ins> | 0.53 | 0.49 | 0.20 | 0.45 | 0.37 | 0.43 | 0.49 | 0.415 |\n| SFT+RL(PRM) | 0.29 | 0.59 | 0.52 | 0.24 | <ins>0.47</ins> | 0.36 | 0.46 | 0.57 | 0.437 |\n| SFT+RL(PRM)+DA | 0.29 | 0.63 | <ins>0.53</ins> | 0.21 | <ins>0.47</ins> | <ins>0.38</ins> | 0.48 | <ins>0.59</ins> | <ins>0.447</ins> |\n| SFT+RL(PRM)+DA+ICL | <ins>0.33</ins> | 0.57 | 0.52 | <ins>0.28</ins> | 0.46 | 0.36 | <ins>0.49</ins> | 0.53 | 0.443 |", "caption": "Table 1: Accuracy of different models/methods. Bold indicates the highest value, while underline indicates the highest value among the different methods based on the open-source Skywork-o1.", "description": "This table presents the accuracy results of various methods for solving scientific knowledge reasoning tasks, comparing different model approaches (vanilla, ReFT, SFT, etc.) and the impact of additional techniques like process reward modeling, data augmentation, and in-context learning.  The results are shown for eight specific reasoning tasks across four domains (biology, chemistry, physics, and materials), using 100 training samples.  The bold values represent the highest overall accuracy for each task, while the underlined values highlight the best performance achieved among methods utilizing the open-source Skywork-o1 model, providing a context for evaluating the effectiveness of different techniques relative to a baseline.", "section": "3 Experiments"}, {"content": "| Model | Biology | Chemistry T1 | Chemistry T2 | Chemistry T3 | Physics | Materials T1 | Materials T2 | Materials T3 | Avg. |\n|---|---|---|---|---|---|---|---|---|---| \n| Vanilla | 0.28 | **0.55** | **0.52** | **0.23** | **0.45** | 0.34 | 0.41 | 0.41 | 0.40 |\n| SFT | **0.33** | 0.53 | 0.49 | 0.20 | **0.45** | **0.37** | **0.43** | **0.49** | **0.41** |\n| SFT+ | 0.27 | 0.45 | 0.44 | 0.12 | 0.34 | 0.25 | 0.28 | 0.30 | 0.31 |", "caption": "Table 2: Analysis of teacher-student policy alignment. SFT and SFT+ indicate synthesizing reasoning process by the student policy itself and a stronger reasoning model QwQ-32B, respectively.", "description": "This table analyzes the impact of teacher-student policy alignment on model performance.  It compares the accuracy of two approaches: SFT (where the student model synthesizes its own reasoning process) and SFT+ (where a stronger model, QwQ-32B, synthesizes the reasoning process for the student model). The results highlight the importance of aligning the teacher and student models for optimal performance, showing that using a more powerful teacher model doesn't always improve performance if the action spaces are misaligned.", "section": "3.4 Results"}, {"content": "| Training Stages | Pre-Training |  | Fine-Tuning |  |\n|---|---|---|---|---|\n|  | Training data | Learning method | Training data | Learning method |\n| **System-1** | (Q) | Self-supervised learning | (Q,A) | SFT |\n| **System-2** | (Q,A) | RL + Self-Play | (Q,...,S<sup>j</sup>,...,A)<sup>4</sup> | RFT |\n\n<sup>4</sup>Alternatively, as configured in this paper, only providing (Q, A) pairs is feasible. For a detailed discussion, please refer to the main text.", "caption": "Table 3: System-1 v.s. System-2: relied training data and used learning method in the pre-training and fine-tuning stages", "description": "This table compares System-1 and System-2 inference methods in terms of their training data and learning methods used during pre-training and fine-tuning stages. System-1 inference directly predicts answers from questions, whereas System-2 inference involves multiple intermediate steps.  The table highlights the differences in the type of training data used (labeled vs. unlabeled) and learning methods (supervised vs. reinforcement learning) applied in each approach.", "section": "4 RELATED WORK"}, {"content": "| Methods | Reward Model | Policy Model | Target |\n|---|---|---|---| \n| RLHF | Human preference | Base model/ SFT model | Value Alignment |\n| RL-based Knowledge Distillation | Teacher model | Student model | Model Compression |\n| RFT | Domain samples | Foundation reasoning model | Specialized Reasoning |", "caption": "Table 4: Different methods of RL-based fine-tuning.", "description": "This table compares different reinforcement learning (RL)-based fine-tuning methods, highlighting key differences in their reward model source, policy model, and overall target.  It shows that while RLHF utilizes human feedback, RL-based knowledge distillation leverages a teacher model, and the paper's proposed RFT method uses domain-specific samples as the reward signal source. The policy models also vary, impacting the target outcome\u2014whether it's aligning model values, reducing model size, or improving specific reasoning capabilities.", "section": "4.3 Employing Reinforcement Learning for Fine-Tuning"}]