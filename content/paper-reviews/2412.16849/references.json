{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper introduces RLHF, a crucial technique used in reinforcement learning for aligning language models with human preferences, which is foundational to the concepts explored in the current paper."}, {"fullname_first_author": "Kehua Feng", "paper_title": "SciKnowEval: Evaluating multi-level scientific knowledge of large language models", "publication_date": "2024-01-01", "reason": "This paper introduces SciKnowEval, the benchmark dataset used for evaluating the model's performance, making it a central component of the current research."}, {"fullname_first_author": "Trung Quoc Luong", "paper_title": "Reasoning with reinforced fine-tuning", "publication_date": "2024-01-01", "reason": "This paper introduces ReFT, a key concept that directly inspires the current paper's methodology and is a major focus of the experimental comparison."}, {"fullname_first_author": "Yu Zhao", "paper_title": "Marco-01: Towards open reasoning models for open-ended solutions", "publication_date": "2024-01-01", "reason": "This paper introduces another significant model architecture that the current paper directly builds upon and uses in the experiments."}, {"fullname_first_author": "OpenAI", "paper_title": "OpenAI's reinforcement fine-tuning research program", "publication_date": "2024-01-01", "reason": "This paper presents OpenAI's RFT, the primary methodology that this paper adapts and improves upon, providing the foundation for the entire project."}]}