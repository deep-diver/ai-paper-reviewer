[{"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.1\">Model</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.2.1.1.1\">Avg. Cos.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.2.1.2.1\">Sim.</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.3.1.1.1\">Winner</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.3.1.2.1\">Cos. Sim.</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.4.1.1.1\">Judge</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.4.1.2.1\">#1</td>\n</tr>\n</table>\n</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.5.1.1.1\">Judge</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S4.T1.1.1.1.5.1.2.1\">#2</td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.1\">LLaMA 3.2 base model</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.2\">0.818</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.3\">13.97%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.4\">8.39%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.5\">26.88%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.1\">LLaMA-TOMMI-1.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.2\">0.879</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.3\">86.02%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.4\">43.44%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.5\">43.23%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.3.1\">Both models</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.3.2\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.3.3\">-</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.3.4\">2.80%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.4.3.5\">9.03%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.5.4.1\">Neither model</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.1.5.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.1.5.4.3\">-</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.5.4.4\">45.38%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S4.T1.1.5.4.5\">20.86%</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Fine-tuning effectiveness, as evaluated using cosine similarity and LLM-as-a-Judge. For cosine similarity, both the ground truth label and the model response are embedded using OpenAI\u2019s best vector embedding model (text-embedding-3-large) and used to calculate the average cosine similarity across all results (\u201cAvg. Cos. Sim.\u201d). When choosing which answer is best aligned using cosine similarity (\u201cWinner Cos. Sim.\u201d), the results show an overwhelming preference for the LLaMA-TOMMI-1.0 model. Two prompts were provided for LLM-as-a-Judge rating, see A.8.", "description": "This table presents a quantitative evaluation of the effectiveness of fine-tuning a large language model (LLM) for a specific domain.  It compares the performance of a fine-tuned model (LLaMA-TOMMI-1.0) against a base model (Llama-3.2). The evaluation uses two metrics: cosine similarity, measuring the semantic closeness of the model's response to the ground truth, and LLM-as-a-Judge, where another LLM assesses the quality of responses based on criteria like lexical matching, structural similarity, and example consistency.  Cosine similarity is calculated using OpenAI's text-embedding-3-large model.  The results demonstrate a significant improvement in performance for the fine-tuned LLaMA-TOMMI-1.0 model.", "section": "4 Experiments"}]