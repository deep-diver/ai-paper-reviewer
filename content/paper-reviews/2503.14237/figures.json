[{"figure_path": "https://arxiv.org/html/2503.14237/x1.png", "caption": "Figure 1: \nFlux (right) employs flexible sampling and token selection to achieve Token Optimization. Common methods(left) use rigid sampling(and use token reduction for applications directly).", "description": "This figure illustrates the difference between traditional video processing methods and the proposed Flux method.  Traditional methods (left) rely on rigid, fixed sampling of video frames, which leads to suboptimal accuracy-computation trade-offs due to redundancy.  Token reduction is often employed afterward to reduce computational costs, but this further limits performance. In contrast, the Flux method (right) utilizes flexible sampling and token selection to achieve \"Token Optimization.\"  This involves sampling frames at variable spatiotemporal densities and selecting a size-limited set of tokens that best represent the information within the video. This flexibility allows Flux to adapt better to varying computational budgets and achieve improved accuracy for downstream tasks, especially given limited computational resources.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14237/x2.png", "caption": "Figure 2: \nOverview of our Flux method.\nThe same-scaled FluxViT and InternVideo2\u00a0[87] series models are both pre-trained with the InternVideo2-1b model as the teacher using the same dataset. The \u201cFluxViT+\u201d refers to the results using Token Optimization at test time with the same GFLOPS.", "description": "Figure 2 presents a comparison of the performance between FluxViT and InternVideo2 models. Both models were pre-trained using the same InternVideo2-1b model as a teacher and the same dataset.  The key difference is that FluxViT incorporates the proposed Flux method, which enables flexible sampling and token selection. The chart showcases the performance of both models at different computational budgets (GFLOPs).  The \"FluxViT+\" line represents the results when using Token Optimization during the testing phase to optimize the selected input tokens within the same GFLOPs constraint, highlighting the effectiveness of Flux in optimizing video models for resource-constrained settings.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14237/x3.png", "caption": "Figure 3: \nOur proposed Flux method with UMT framework. We show that our proposed Flux training is easy to integrate with mainstream video training frameworks.", "description": "This figure illustrates how the proposed Flux method integrates with the Unmasked Teacher (UMT) framework for video training.  Flux introduces flexible sampling and token selection to address the computational redundancy in standard video models.  The diagram shows how a raw video is processed through flexible sampling with varying frame counts and resolutions, followed by token selection to produce a reduced set of tokens. These tokens are then fed into the UMT framework, enabling training of more efficient video models.  The figure highlights the ease of integrating Flux into existing video training pipelines.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14237/x4.png", "caption": "Figure 4: \nOur proposed essential modules for Flux. From the model side, Flux modules include Group-dynamic token selector, dual patch norm, and Global-Local positional embedding.", "description": "Figure 4 illustrates the core components of the Flux module, a novel data augmentation technique designed to enhance the flexibility of video models during training.  The Flux module consists of three key components: a Group-dynamic token selector which intelligently selects a subset of the most informative tokens from the input video; dual patch normalization which enhances the robustness of the patch embedding process across varying resolutions; and a Global-Local positional embedding method that incorporates both global and fine-grained positional information to handle the variable token lengths and resolutions inherent in the flexible sampling process.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14237/x5.png", "caption": "Figure 5: \nComparison between different training methods on K400 using a fixed number of 2048 tokens.\nNote the three lines and all the points share similar training and inference costs. The shaded part shows results for the AnyRes Distilled AnyRes Tuned model with spatial resolution in range (196, 252), while others use a fixed spatial resolution at 224.", "description": "This figure compares the performance of three different video model training methods on the Kinetics-400 dataset. All methods use a fixed number of 2048 tokens. The x-axis represents the number of frames, and the y-axis represents the top-1 accuracy.  The three methods are: 1) FixRes Distilled FixRes Tuned (trained and tested at a fixed spatial resolution of 224); 2) AnyRes Distilled FixRes Tuned (trained at a fixed spatial resolution of 224, but tested at resolutions between 196 and 252); and 3) AnyRes Distilled AnyRes Tuned (trained and tested with spatial resolutions between 196 and 252). The shaded region highlights the performance of the AnyRes Distilled AnyRes Tuned model, demonstrating its superior performance across different frame counts. Notably, all three methods share similar training and inference costs, making this a fair comparison of model training approaches.", "section": "3.3 Flux ViT"}, {"figure_path": "https://arxiv.org/html/2503.14237/x6.png", "caption": "Figure 6: \nOverview of Flux-Multi Tuning.", "description": "This figure illustrates the process of Flux-Multi Tuning, a method used to enhance the flexibility and robustness of video models.  It shows how multiple token counts are processed concurrently (e.g., 3072, 2048, 1024) within the same training batch.  Each token count is processed through the model, and the resulting representations are compared to representations generated from the teacher model using knowledge distillation. This process allows the model to adapt to a wider range of input sizes and computational constraints.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14237/x7.png", "caption": "Figure 7: \nGradient norms of main projector modules of Flux-Multi trained InternVideo2 on K400. We report the L2 gradient norm using bs=32.", "description": "This figure visualizes the L2 gradient norms for the main projector modules in the Flux-Multi trained InternVideo2 model, evaluated on the K400 dataset.  The batch size (bs) used for the calculation was 32.  The plot likely shows the gradient norm across various layers of the network (e.g., different ViT blocks), providing insights into training stability and potential issues like exploding or vanishing gradients.  Analyzing gradient norms helps in debugging training processes, assessing the effectiveness of regularization techniques, and understanding the impact of changes in the model architecture, such as those introduced by Flux.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14237/x8.png", "caption": "Figure 8: \nConvergence analysis of Flux-Single tuning using 3072 tokens but different frame counts directly on K400.", "description": "This figure shows the convergence curves during the fine-tuning stage of the Flux-Single method. The experiment uses a fixed number of 3072 tokens as input for all training runs. However, it varies the number of frames used to generate those tokens, ranging from 10 to 24. The y-axis represents the top-1 accuracy on the K400 dataset, while the x-axis shows the training epoch. The plot visually demonstrates how the model's performance changes based on different frame counts during training.  This helps to analyze the impact of varying the input data's temporal resolution on the final model accuracy. Different curves represent different number of frames.", "section": "3.3 FluxViT"}, {"figure_path": "https://arxiv.org/html/2503.14237/x9.png", "caption": "Figure 9: \nOverall gradient norm trend during Flux-UMT per-training. We report the overall training dynamics with our ablation setting. The FluxViT modules can lower the overall norm.", "description": "Figure 9 illustrates the overall gradient norm during the pre-training phase of the Flux-UMT model.  The graph plots the gradient norm over training epochs, comparing a standard UMT model to one augmented with the FluxViT modules (Global-Local Positional Embedding and Dual Patch Normalization). The results show that the FluxViT modules contribute to a lower overall gradient norm, indicating improved training stability and potentially better generalization.", "section": "3. Method"}]