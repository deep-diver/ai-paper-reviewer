[{"figure_path": "https://arxiv.org/html/2412.20993/extracted/6102352/figures/evaluation/inference_scaling.png", "caption": "(a)", "description": "Figure 1(a) shows the inference time scaling of the self-consistency algorithm on the GSM8K benchmark dataset.  The x-axis represents the number of output tokens, which is a proxy for the amount of computation used. The y-axis shows the overall accuracy achieved.  The curve demonstrates the typical trade-off in LLM reasoning algorithms: increasing computation (more tokens) generally leads to higher accuracy, up to a point where further increases yield diminishing returns. The figure highlights the existence of three zones: an easy zone where a small amount of computation is sufficient for high accuracy; a scaling zone where increasing computation significantly improves accuracy; and an impossible zone where no amount of computation can guarantee a correct solution.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.20993/extracted/6102352/figures/evaluation/canonical_resourece_allocation.png", "caption": "(b)", "description": "This figure illustrates a canonical resource allocation comparison between existing systems and an ideal allocation scenario.  It highlights the inefficiency of existing systems in allocating resources across tasks of varying difficulty.  Existing systems often allocate resources evenly, leading to under-allocation for harder tasks (requiring more compute to get the correct answer) and over-allocation for easier tasks (using more compute than necessary). The ideal allocation strategy dynamically adjusts resource allocation based on the difficulty of each query to optimize for accuracy, cost, and latency.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2412.20993/x1.png", "caption": "(c)", "description": "This figure shows the correlation between the certaindex values and the amount of compute required to obtain a correct answer for various questions.  Each point represents a question, and the x-axis shows the amount of compute used, while the y-axis represents the certaindex. A higher certaindex indicates a higher degree of certainty from the LLM in its response, suggesting that less compute was needed to reach the correct answer.  The plot demonstrates a strong negative correlation, meaning that higher certaindex values are associated with lower compute needs.", "section": "3 Certaindex Based Resource Allocation"}, {"figure_path": "https://arxiv.org/html/2412.20993/x2.png", "caption": "Figure 1: (a) Inference time scaling of self-consistency\u00a0[48] on GSM8K\u00a0[7]. As we uniformly increase the number of output tokens (x-axis) per question, the overall accuracy (y-axis) grows then plateaus (see \u00a72.2). (b) Canonical resource allocation in existing systems vs. ideal allocation for four queries P1-P4 with vary difficulties. (c) Correlations between certaindex and the compute required to obtain a correct answer. Each point is a question. Statistically higher certaindex indicates lower compute needed.", "description": "Figure 1 demonstrates the concept of inference-time scaling and Dynasor's adaptive resource allocation. (a) shows how accuracy on the GSM8K benchmark improves with increased compute (measured by the number of output tokens) but plateaus after a certain point due to the limitations of the model.  (b) illustrates the difference between existing systems' uniform resource allocation and Dynasor's ideal allocation which adapts to query difficulty, providing more resources to harder queries and less to easier ones. (c) shows the strong correlation between certaindex (a measure of the model's certainty) and the compute needed to get a correct answer, indicating that high-certainty queries require less compute.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.20993/x3.png", "caption": "Figure 2: Illustration of the workflow of different LLM reasoning algorithms discussed in \u00a72.1", "description": "This figure illustrates the workflow of four different LLM reasoning algorithms: Self-Consistency (SC), Rebase, Monte Carlo Tree Search (MCTS), and Internalized Chain-of-Thought (ICoT).  Each algorithm is shown as a diagram to visually represent its process.  The diagrams highlight key steps such as prompt input, trajectory expansion (representing different reasoning paths), and final answer aggregation. They also show the key control parameters (knobs) for each algorithm that affect the amount of compute used during inference, allowing a comparison of their different approaches to reasoning and computation.", "section": "2.1 LLM Reasoning Algorithms"}, {"figure_path": "https://arxiv.org/html/2412.20993/x4.png", "caption": "Figure 3: \nCorrelations between certaindex strength (y-axis) and ground truth steps to solution (x-axis) on 12 (algorithm, task dataset, LLM) settings where algorithm \u2208\\in\u2208 {SC, Rebase, MCTS, ICoT}, dataset \u2208\\in\u2208 {LiveCodeBench\u00a0[19], GSM8K, ASDiv\u00a0[34], GAME24\u00a0[54]}, and LLM \u2208\\in\u2208 {Llama\u00a0[33], Gemma\u00a0[46], Phi\u00a0[2], QWQ\u00a0[47]}. How certaindex is measured in each setting is shown in the y\ud835\udc66yitalic_y label of each plot.\nCertaindex is measured at the reasoning step marked by the black line. The orange line indicates the thresholding-based allocation. The green line illustrates a more fine-grained approach through curve fitting.\nFor all plots (except MCTS), both certaindex values and oracle steps were averaged across multiple runs to combat randomness.", "description": "Figure 3 displays the correlation between the certainty of a large language model (LLM) during reasoning and the number of steps required to reach the solution.  It presents 12 different plots, each representing a unique combination of reasoning algorithm (SC, Rebase, MCTS, ICoT), dataset (LiveCodeBench, GSM8K, ASDiv, GAME24), and LLM (Llama, Gemma, Phi, QWQ). Each plot shows the model's certainty (certaindex) measured at a specific step, compared to the actual number of steps needed for a correct solution. The different ways of measuring certaindex are indicated on the y-axis labels for each plot.  The orange and green lines represent different resource allocation strategies (threshold-based and curve fitting, respectively).  Note that for all plots except those using MCTS, the data was averaged across multiple runs to reduce the impact of randomness.", "section": "3.2 Effectiveness of Certaindex"}, {"figure_path": "https://arxiv.org/html/2412.20993/x5.png", "caption": "Figure 4: \nCertaindex Values Across Different Detection Steps in Self-Consistency Reasoning", "description": "This figure visualizes how the Certaindex values change across different reasoning steps within the Self-Consistency algorithm.  Each subplot represents a specific detection step (5, 10, 15, 20, 25, 30).  Points in the plot represent individual problems, categorized into three types: Early Stoppable Problems, Solvable Problems, and Unsolvable Problems.  The x-axis displays the number of steps, the y-axis shows the Certaindex values, a horizontal orange dashed line shows the threshold, and a vertical red dashed line shows the step at which Certaindex is collected. The consistent pattern demonstrates a strong correlation between Certaindex and the required reasoning steps, highlighting its effectiveness as a proxy for reasoning progress.", "section": "3 Certaindex Based Resource Allocation"}, {"figure_path": "https://arxiv.org/html/2412.20993/extracted/6102352/figures/gang2.jpg", "caption": "Figure 5: Correlation between certainty measurements and mean steps required to solve problems on solvable problems. We obtain the ground-truth mean steps by solving the queries using the LLM multiple times and counting the average steps.", "description": "This figure displays the correlation between different metrics used to estimate the certainty of a large language model's (LLM) answers and the actual number of steps required to obtain a correct solution.  The metrics compared include: certaindex (based on entropy), mean output length, mean normalized log probability, and linear combinations of these. The ground-truth number of steps was determined by repeatedly solving the problems with the LLM and averaging the results. The figure demonstrates that certaindex (entropy-based) shows the strongest correlation with the true number of steps required to obtain the solution.", "section": "3.3 Comparing Certaindex with Other Signals"}, {"figure_path": "https://arxiv.org/html/2412.20993/x6.png", "caption": "Figure 6: Left(a): Dynasor Architecture. Middle(b): Reasoning Program Interface. Right(c): Example Program (SC).", "description": "Figure 6 illustrates the Dynasor system architecture, which consists of three main parts: (a) Reasoning Program Abstraction, (b) Application Runtime, and (c) System Runtime.  The Reasoning Program Abstraction provides a standardized interface for various reasoning algorithms, allowing them to interact with the Dynasor system seamlessly. The Application Runtime is responsible for resource allocation based on the \"certaindex\" metric, which indicates the progress of the reasoning process. Finally, the System Runtime manages inter-program scheduling, ensuring efficient resource utilization and minimizing latency.  The figure also includes a code example (c) showing the implementation of a self-consistency (SC) reasoning program within the Dynasor framework.", "section": "5 Dynasor: System Design"}, {"figure_path": "https://arxiv.org/html/2412.20993/x7.png", "caption": "Figure 7: Illustration of Gang Scheduling", "description": "This figure illustrates the benefit of gang scheduling in reducing average latency. It compares two scenarios: one using gang scheduling and another using standard sequential scheduling.  Both scenarios involve two programs, each with two requests, that have varying processing times. In the gang scheduling scenario, requests from the same program are grouped together to minimize the impact of stragglers (longer-running requests), resulting in lower average latency. The sequential scheduling scenario processes requests one program at a time, leading to higher average latency due to delays caused by the longer-running requests from program 2.", "section": "5.3 System Runtime"}, {"figure_path": "https://arxiv.org/html/2412.20993/x8.png", "caption": "Figure 8: Token-to-accuracy metric on batch processing workloads. Mean performance and std (error bars) of 10 runs are reported.", "description": "This figure displays the results of batch processing experiments, comparing the token usage efficiency of Dynasor against baseline methods.  For various reasoning algorithms (Self-Consistency, Monte Carlo Tree Search, and Rebase) and datasets (GSM8K, LiveCodeBench, MATH), it shows the relationship between the number of tokens (x-axis) and the achieved accuracy (y-axis). Error bars represent the standard deviation across 10 runs. This allows for a direct comparison of the computational efficiency of Dynasor in achieving similar accuracy levels compared to allocating resources evenly or based on output length.", "section": "6.2 Batch Processing"}, {"figure_path": "https://arxiv.org/html/2412.20993/x9.png", "caption": "Figure 9: Evaluation on 3 online workloads on Dynasor against baselines. Rows from top to bottom: (a) Program arrival rate vs SLO attainment, (b) SLO scale vs SLO attainment, (c) Accuracy vs SLO Attainment.", "description": "Figure 9 presents a comprehensive evaluation of Dynasor's performance in online serving scenarios against two strong baselines: SGLang and ParrotServe.  The figure is divided into three subplots, each illustrating a key performance trade-off in LLM reasoning query serving: \n(a) Program arrival rate vs. SLO attainment: This plot demonstrates Dynasor's ability to sustain higher request arrival rates while meeting latency targets (SLOs). It shows the relationship between the number of incoming queries per second and the percentage of queries that meet their deadlines (SLO attainment). \n(b) SLO scale vs. SLO attainment: This subplot illustrates Dynasor's ability to handle more stringent latency requirements (tighter SLOs). Here, the SLO attainment is shown as a function of the stringency of the latency constraints. \n(c) Accuracy vs. SLO attainment: This plot highlights the trade-off between accuracy and SLO attainment, showing how the achieved accuracy changes when we adjust the resource allocation to meet different SLO requirements. It examines whether prioritizing SLO attainment affects the overall accuracy of the query results. This plot shows how the attained accuracy changes when we adjust the resource allocation to meet different SLO requirements. All three subplots showcase Dynasor's superiority over the baselines across diverse conditions, demonstrating its capacity to achieve high throughput, meet strict latency targets, and maintain high accuracy.", "section": "6.3 Online Serving"}, {"figure_path": "https://arxiv.org/html/2412.20993/x10.png", "caption": "Figure 10: Performance improvement breakdown in online self-consistency (GSM8K) workload: Impact of gang scheduling, certaindex-based resource allocation, and SJF on mean latency given a fixed request rate of 16 pps.", "description": "Figure 10 displays the impact of three key techniques in Dynasor on the mean latency of online self-consistency (SC) tasks using the GSM8K dataset.  The experiment uses a fixed request rate of 16 programs per second (pps). The three techniques analyzed are gang scheduling, certaindex-based resource allocation, and the shortest-job-first (SJF) scheduling algorithm.  Each bar represents the mean latency, allowing for a comparison of the baseline (using only Longest Prefix Matching from SGLang) against the performance gains achieved by incorporating each technique individually and in combination. This visualization helps to quantify the contribution of each component to Dynasor's overall latency reduction.", "section": "5.3 System Runtime"}, {"figure_path": "https://arxiv.org/html/2412.20993/x11.png", "caption": "Figure 11: Performance improvement breakdown of online self-consistency (MATH) under fixed P90 SLO constraints.", "description": "Figure 11 shows the performance improvements achieved by Dynasor for online self-consistency tasks on the MATH dataset, focusing on maintaining the 90th percentile (P90) service level objective (SLO).  It breaks down the contributions of three key components: Gang Scheduling, which groups requests from the same program for more efficient execution, SJF (Shortest Job First) scheduling for reducing latency, and the certaindex-based resource management. The bar chart compares the mean latency and maximum throughput (rate) of Dynasor with various combinations of these components against the baseline LPM (Longest Prefix Matching) scheduler from SGLang, demonstrating the synergistic benefits of combining these techniques.", "section": "6.5 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.20993/x12.png", "caption": "Figure 12: Performance comparison with different entropy threshold or reward score threshold.", "description": "This figure compares the performance of different thresholding strategies for resource allocation in online serving. The x-axis represents the number of tokens used, and the y-axis represents the accuracy achieved.  Different colored lines represent different methods:  'Ours' uses the proposed certaindex-based method, while 'Baseline-even' uses uniform resource allocation and 'Entropy=0.75', 'Entropy=1', 'Entropy=1.5', 'Entropy=2' represent variations on entropy-based thresholds. Similarly, 'Ours (score=0.4)', 'Score=0.05', 'Score=0.1', 'Score=0.7' show variations based on reward score thresholds, again compared to 'Baseline-even'. The plots for Self-Consistency (GSM8K) and MCTS (ASDiv) illustrate the performance difference of these approaches on different algorithms and datasets.  This shows the effectiveness and sensitivity of choosing appropriate certaindex thresholds. ", "section": "6.4 Fine-grained Resource Allocation"}, {"figure_path": "https://arxiv.org/html/2412.20993/x13.png", "caption": "Figure 13: Performance comparison with LLM activation-based predictor and output length based scheduling.", "description": "Figure 13 compares the performance of Dynasor's certaindex-based resource allocation with two alternative methods for managing inference-time compute in LLMs: using the length of generated sequences as a proxy for computational needs and using a neural network model to predict resource requirements based on LLM activations.  The results are shown across two different tasks and models. The x-axis represents the number of generated tokens (compute), while the y-axis shows the achieved accuracy.  This visualization demonstrates the superior performance of Dynasor's certaindex method, achieving higher accuracy with fewer tokens than the other two methods.", "section": "6.5 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2412.20993/x14.png", "caption": "Figure 14: Finish-Time Fairness.", "description": "This figure illustrates the fairness of Dynasor's scheduling mechanism by comparing the ratio of latency to the number of tokens used.  The baseline token count represents the number of tokens consumed when all programs utilize their maximum resources. The graph shows that Dynasor with both gang scheduling and the shortest job first (SJF) scheduling algorithm improves finish-time fairness significantly compared to the baseline, particularly for longer tasks.", "section": "6.6 Fairness Analysis"}, {"figure_path": "https://arxiv.org/html/2412.20993/x15.png", "caption": "Figure 15: Serving Qwen-QWQ using Dynasor.", "description": "This figure demonstrates the effectiveness of Dynasor in optimizing the performance of the Qwen-QWQ language model.  The original Qwen-QWQ model, due to its long chain-of-thought generation process, performs poorly under lower token limits, often failing to reach conclusions.  Dynasor addresses this by implementing two strategies:  'QWQ Guided' breaks the generation into smaller chunks with intermediate 'final answer' prompts to improve the accuracy and reduce token consumption. 'QWQ Cut' adds a certaindex-based termination condition, allowing early termination of unpromising generations. This results in significant reduction of tokens, while maintaining accuracy comparable to the original model, demonstrating that Dynasor is capable of adapting to diverse LLMs and improving their efficiency.", "section": "6.7 Case Study: Serving 01-like LLMs"}]