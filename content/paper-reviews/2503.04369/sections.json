[{"heading_title": "LLM Translationese", "details": {"summary": "The study highlights the counterintuitive presence of **translationese** in LLMs, despite their pre-training on vast natural language corpora. This suggests that supervised fine-tuning (SFT), intended to specialize LLMs for translation, may inadvertently introduce biases that prioritize literal semantic mapping over natural language generation, leading to unnatural and overly literal translations. The work systematically assesses the prevalence of **translationese** across different LLMs and languages, finding that even advanced models exhibit significant levels of this phenomenon. Interestingly, LLMs demonstrate the potential to produce more natural translations when prompted to refine their own outputs, hinting at the existence of latent knowledge and the possibility of mitigating translationese through training-aware adjustments. Addressing this issue is crucial for unlocking the full potential of LLMs in machine translation, paving the way for more fluent and target-language-consistent outputs. The effectiveness of refining golden references and filtering unnatural training instances in reducing translationese is also an essential finding."}}, {"heading_title": "SFT Bias Origins", "details": {"summary": "**Supervised Fine-Tuning (SFT) can inadvertently introduce biases** that skew LLMs away from their innate capacity for natural language generation. Despite pre-training on vast datasets of natural utterances, SFT often relies on **training data that contains translationese**. This forces the model to prioritize literal semantic mapping over stylistic fluency. SFT data are curated from existing benchmark datasets which, unfortunately, **exhibit translationese**, potentially biasing the supervised training process. By treating direct transformation from source to target as the primary objective, supervised training can overemphasize faithfulness at the expense of naturalness, leading to unexpected and unnatural translations. These biases can be hard to eliminate as the model may be inclined to continue the SFT style even during inference."}}, {"heading_title": "Refined Data SFT", "details": {"summary": "**Refining data for SFT (Supervised Fine-Tuning) focuses on enhancing the quality of training data** to improve model performance, particularly in areas like translation where subtle nuances are crucial. A key aspect is **reducing 'translationese'**, unnatural language arising from overly literal translations. This involves **techniques like polishing golden references** (high-quality, human-created translations) to make them more natural and target-language consistent. Another strategy is **filtering unnatural training instances** based on metrics such as perplexity to eliminate examples likely to introduce translationese bias. **The goal is to create a training dataset that encourages the LLM to generate fluent and natural translations, rather than simply mimicking literal correspondences between source and target languages.**"}}, {"heading_title": "PPL-TSR Relation", "details": {"summary": "Perplexity (PPL) and Translationese Span Ratio (TSR) relation is a crucial aspect of evaluating machine translation quality, particularly in the context of large language models (LLMs). **PPL, an intrinsic metric, reflects a model's confidence in its generated text; lower PPL generally indicates more natural and fluent output.** Conversely, **TSR quantifies the presence of translationese, characterized by overly literal and unnatural translations.** High TSR values suggest a stronger influence of the source language structure on the translated text, diminishing target language fluency. **Ideally, a strong negative correlation between PPL and TSR is desired, signifying that as a translation becomes more natural (lower PPL), it exhibits fewer signs of translationese (lower TSR).** However, establishing this relationship requires careful consideration, as other factors like domain specificity and text complexity can influence both metrics independently. Further research is needed to refine methods for accurately assessing and mitigating translationese using PPL and TSR in LLM-based translation systems."}}, {"heading_title": "Need Data Adjust", "details": {"summary": "Based on the broader context of mitigating translationese in machine translation, the concept of 'Need Data Adjust' would likely revolve around recognizing and rectifying biases or unnatural patterns present within the training datasets used to fine-tune large language models (LLMs). It highlights the importance of **data quality** in shaping the output of these models, suggesting that simply training on large volumes of data is insufficient. Instead, the focus shifts to **curating and refining the training data** to ensure it reflects natural and fluent language usage. This adjustment might involve techniques like **filtering out translationese-prone instances**, **polishing existing translations to remove unnatural expressions**, or **augmenting the dataset with more examples of natural language**. The goal is to reduce the model's tendency to produce overly literal or stylistically awkward translations, ultimately improving the overall naturalness and fluency of the generated text. By proactively addressing data-related issues, the translationese effect can be greatly lessened."}}]