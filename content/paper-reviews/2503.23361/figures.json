[{"figure_path": "https://arxiv.org/html/2503.23361/x1.png", "caption": "Figure 1: Overall workflow of stochastic error ascent (SEA). We search for a closed-weight model\u2019s unknown knowledge iteratively from a given knowledge base until we reach the budget. The result from SEA can be further used to analyze the model\u2019s unknown categories and error patterns.", "description": "The figure illustrates the SEA (Stochastic Error Ascent) framework.  Starting with a knowledge base and a closed-weight language model, SEA iteratively identifies knowledge gaps.  It begins by randomly sampling a small set of questions from the knowledge base to assess the model's accuracy. Subsequent iterations leverage semantic similarity to identify additional questions likely to reveal further errors, making the process efficient.  A directed acyclic graph tracks error dependencies to highlight systematic failures. The process continues until a pre-defined budget (e.g., number of queries) is exhausted.  The final output helps pinpoint the model's specific knowledge deficiencies and patterns of errors.", "section": "3 Vulnerability Discovery for Large Language Models"}, {"figure_path": "https://arxiv.org/html/2503.23361/x2.png", "caption": "Figure 2: Comparison of errors discovered by ACD, AutoBencher, and SEA. We compare ACD with SEA among the same budget while comparing AutoBencher among the same question size. For ACD, we summarized the number of failed tasks, and for SEA, we summarized the number of source errors. We let AutoBencher create 13 benchmarks, each of which takes one of the Wikipedia categories as an interesting topic. We let SEA search the same number of questions according to each model. o1-mini failed on ACD due to the violation of the prompt usage policy from OpenAI.", "description": "Figure 2 presents a comparison of the effectiveness of three different methods \u2013 Automated Capability Discovery (ACD), AutoBencher, and Stochastic Error Ascent (SEA) \u2013 in identifying knowledge deficiencies in Language Models (LLMs).  ACD and SEA are compared using the same budget, meaning the same computational resources were allocated to each. The number of errors found is represented by the number of 'failed tasks' for ACD and 'source errors' for SEA. AutoBencher is compared by keeping the number of questions generated the same as SEA, while it creates 13 benchmarks, each based on a different Wikipedia category.  The figure highlights the significant advantage of SEA in terms of both the quantity of errors detected and the cost-per-error, compared to the baseline methods. Notably, o1-mini failed in ACD due to OpenAI's prompt usage policy restrictions.", "section": "4 Comparing Stochastic Error Ascent with Baselines"}, {"figure_path": "https://arxiv.org/html/2503.23361/x3.png", "caption": "Figure 3:  Per-step error TE\u2062(fclose)subscript\ud835\udc47\ud835\udc38subscript\ud835\udc53closeT_{E}(f_{\\text{close}})italic_T start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT close end_POSTSUBSCRIPT ) and cumulative error T\ud835\udcae\u2062(fclose)subscript\ud835\udc47\ud835\udcaesubscript\ud835\udc53closeT_{{\\mathcal{S}}}(f_{\\text{close}})italic_T start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT close end_POSTSUBSCRIPT ) for each model. We observe that the errors of all models are positively related to step, indicating SEA can gradually and continually find the model\u2019s knowledge deficiencies from the knowledge base.", "description": "This figure displays the per-step error and cumulative error of different large language models (LLMs) throughout the iterative error discovery process of the SEA framework. The per-step error (TE(fclose)) represents the average error rate at each step, while the cumulative error (TS(fclose)) shows the total accumulated error up to that step.  The x-axis represents the step number in the SEA process, and the y-axis indicates the error rate. Each line in the plot represents a different LLM, illustrating how their error rates change as SEA progresses.  The positive correlation between error and step number demonstrates SEA\u2019s effectiveness in uncovering LLM knowledge deficiencies. The figure visually shows that SEA gradually identifies increasingly challenging errors.", "section": "5 Analyzing Stochastic Error Ascent"}, {"figure_path": "https://arxiv.org/html/2503.23361/x4.png", "caption": "Figure 4:  Ablation studies on the component contribution of SEA. We compare SEA with its two variants: without source pruning (i.e., pass the lines 10 and 11 in Alg.\u00a01) and random selection (i.e., pass the lines 9, 10, and 11 in Alg.\u00a01). We observe that each component contributes equally to SEA.", "description": "This figure presents an ablation study analyzing the contribution of different components within the SEA (Stochastic Error Ascent) framework.  Three versions of SEA are compared: the complete SEA model, a version without source pruning (removing low-impact error sources), and a version employing random paragraph selection instead of the error-driven selection process.  The results show that all three components (error-driven selection, source pruning, and hierarchical retrieval) contribute equally to the overall effectiveness of SEA in discovering knowledge errors in LLMs.", "section": "5 Analyzing Stochastic Error Ascent"}, {"figure_path": "https://arxiv.org/html/2503.23361/x5.png", "caption": "Figure 5: Comparison of cross-validation between each model. X-axis indicates the subset provider (i.e., \ud835\udcae^^\ud835\udcae\\hat{{\\mathcal{S}}}over^ start_ARG caligraphic_S end_ARG provider; sourced from experiments in Fig.\u00a03), and Y-axis denotes the testee. We summarize two results: (1) correlation between testee result and provider result, and (2) accuracy of testee on each provider\u2019s results. The higher the correlation, the more similar the answers of the two models are. Similarly, the higher the testee\u2019s accuracy, the more challenging the provider\u2019s question.", "description": "This figure displays a heatmap visualizing the cross-validation results between different large language models (LLMs).  The X-axis represents the LLM that generated the question subset (the 'provider'), and the Y-axis shows the LLM answering those questions (the 'testee'). Two key metrics are presented: (1) The correlation coefficient between the provider's and the testee's answers for each question, indicating the similarity of their responses; higher correlation signifies greater agreement.  (2) The testee's accuracy on the question subset provided by each provider. This represents the difficulty of the questions posed by different LLMs; higher accuracy indicates easier questions.", "section": "Comparing Stochastic Error Ascent with Baselines"}, {"figure_path": "https://arxiv.org/html/2503.23361/x6.png", "caption": "Figure 6: Error distribution of each testee model. We search with the same initial set from thirteen categories of Wikipedia. We visualize the results by t-SNE without a clustering algorithm. Each point in the figure denotes the corresponding model\u2019s source error p\u2208\ud835\udcabsource\ud835\udc5dsubscript\ud835\udcabsourcep\\in{\\mathcal{P}}_{\\text{source}}italic_p \u2208 caligraphic_P start_POSTSUBSCRIPT source end_POSTSUBSCRIPT. Different colors denote different models, and different markers denote different categories. We can observe natural clusters of each model discovered by SEA according to their knowledge omission areas.", "description": "This figure visualizes the distribution of errors discovered by the SEA method for different language models across various categories from Wikipedia's knowledge base.  Each point represents a specific error, with color indicating the language model and shape representing the Wikipedia category the error belongs to.  The visualization uses t-SNE to reduce the dimensionality of the data for better visual representation. The plot reveals distinct clusters of errors for each model, highlighting the types of knowledge each model struggles with and providing insights into their specific knowledge deficiencies.", "section": "Analyzing LLMs from the Discovery Results"}, {"figure_path": "https://arxiv.org/html/2503.23361/x7.png", "caption": "Figure 7: Error distribution of each testee model. We search with the same random initial set from Wikipedia without specifying specific topics. We visualize the results by t-SNE without a clustering algorithm. Each point in the figure denotes the corresponding model\u2019s source error p\u2208\ud835\udcabsource\ud835\udc5dsubscript\ud835\udcabsourcep\\in{\\mathcal{P}}_{\\text{source}}italic_p \u2208 caligraphic_P start_POSTSUBSCRIPT source end_POSTSUBSCRIPT. Different colors denote different models, and different markers denote different categories.", "description": "Figure 7 visualizes the distribution of errors identified by the SEA algorithm across different LLMs, without pre-selecting specific topics from the Wikipedia knowledge base.  The t-SNE algorithm is employed to reduce the high-dimensional error data into a 2D representation for visualization. Each point in the graph represents a single error identified by SEA (denoted as  p\u2208\ud835\udcabsourcep\nin\newlinemathcal{P}_{source} ), with the color indicating the LLM model that made the error and the shape representing the Wikipedia category where the error occurred.  This visualization allows for a direct comparison of the error patterns of various LLMs. ", "section": "Analyzing LLMs from the Discovery Results"}]