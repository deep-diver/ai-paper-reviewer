[{"Alex": "Hey everyone, and welcome to the podcast! Today we're diving deep into a fascinating new paper that promises to revolutionize how we 'unlearn' things in AI. Think 'Eternal Sunshine of the Spotless Mind,' but for language models! We're tackling the challenge of removing unwanted knowledge from these models without messing everything else up. Joining me is Jamie, who\u2019s going to help us unpack this exciting research.", "Jamie": "Wow, that sounds incredibly complex, Alex! Thanks for having me. So, to start, what exactly does it mean for an AI to 'unlearn' something? Why is that even necessary?"}, {"Alex": "Great question, Jamie! Imagine you've trained an AI on a massive dataset scraped from the internet. It might pick up some sensitive information, copyrighted material, or even just plain wrong facts. 'Unlearning' is the process of removing that specific knowledge without having to retrain the entire model from scratch, which would be incredibly expensive and time-consuming.", "Jamie": "Okay, that makes sense. So, it's about compliance and ethics, keeping these AI models responsible. But what\u2019s the core problem the paper addresses? What\u2019s so hard about just deleting the bad data?"}, {"Alex": "Well, here's the catch: when you try to remove something from an AI, it's easy to cause 'collateral damage.' The model\u2019s performance on other tasks can degrade because the AI\u2019s understanding is all interconnected. The paper introduces UPCORE, a new method to minimize this collateral damage during unlearning.", "Jamie": "UPCORE, got it. Hmmm, so how does UPCORE actually work? What makes it different from previous approaches?"}, {"Alex": "UPCORE stands for Utility-Preserving Coreset Selection. It\u2019s a method-agnostic data selection framework. The key idea is to identify and remove 'outliers' within the data you want to forget. These outliers, it turns out, disproportionately contribute to the damage caused by unlearning.", "Jamie": "Outliers in the forget set\u2026 interesting. How does the paper measure this 'damage,' and how does it find these outliers?"}, {"Alex": "The paper finds that model damage is correlated with the *variance* of the model's representations of the data you want to forget. Basically, if the AI's internal 'understanding' of some data points within the forget set is all over the place, those are the ones causing the most trouble when you try to remove them. UPCORE uses something called an Isolation Forest to identify these outlier data points.", "Jamie": "Isolation Forest? Umm, never heard of it. What's that, like a tiny forest that only accepts data points that get lost easily?"}, {"Alex": "Haha, kind of! It's an algorithm that identifies anomalies in a dataset. It works by randomly partitioning the data and seeing which points are easiest to isolate. The ones that are easiest to isolate are considered outliers. UPCORE prunes these outliers from the forget set, creating a smaller, more 'core' forget set.", "Jamie": "Okay, so you're not unlearning everything, just a carefully chosen subset. And the paper claims this reduces the damage, but how do you know it's actually removing the *right* information? Does it still forget what it's supposed to?"}, {"Alex": "That's the critical balance, right? The paper evaluates UPCORE across three standard unlearning methods, measuring both 'deletion efficacy' \u2013 how well it forgets the target information \u2013 and 'model preservation' \u2013 how well it retains its other abilities.", "Jamie": "And what did they find? Did UPCORE actually improve both forgetting and retaining, or was it just shifting the problem around?"}, {"Alex": "The results consistently showed that UPCORE achieved a superior balance between the two. It not only effectively removed the unwanted knowledge but also minimized the negative impact on the model's overall performance. In fact, the paper even introduces a new metric, the Area Under the Curve (AUC), to better evaluate this trade-off across different unlearning epochs.", "Jamie": "AUC, okay, that's interesting! A new metric to assess that tradeoff in unlearning\u2026 so how did UPCORE improve this AUC? Was that the key finding of the study?"}, {"Alex": "Exactly. UPCORE consistently showed the highest AUC compared to baselines, meaning it was the most effective at maximizing unlearning while minimizing damage. This suggests that UPCORE is forming a Pareto frontier \u2013 performing a better job of maximizing unlearning effectiveness while minimizing model damage. Moreover, it seems UPCORE positively leverages generalization, transferring unlearning from the core set to outlier points.", "Jamie": "Hang on, so by only unlearning a smaller, carefully curated subset of points, it somehow manages to *also* unlearn information from points that were removed? That sounds almost magical!"}, {"Alex": "It's not magic, it's clever use of collateral effects! The paper identifies two kinds of collateral effects. negative (the unintended degradation of unrelated model capabilities) and positive (transfer from the core set to the pruned, outlier points). By focusing on a lower-variance coreset, there is a potential for what we could have considered a 'detrimental collateral damage' to be turned around to our advantage when we impact the data points, untrained and originally pruned from the core set", "Jamie": "Wow this is some high level stuff - so let's dive deeper into the specific numbers/experiments they carried out. What kind of experiments were involved here?"}, {"Alex": "The researchers tested UPCORE in two settings: prompt completion and question-answering. They used a Llama-3.1-8B base model and applied three standard unlearning methods: Gradient Ascent, Refusal, and Negative Preference Optimization. They compared UPCORE against baselines of unlearning on the complete forget set and unlearning on a randomly subsampled subset.", "Jamie": "Okay, that sounds like a solid setup. So, what kind of data were they unlearning? Was it all the same type of information?"}, {"Alex": "They used factual questions across two settings. One was factual prompt completions with short answers, like 'The capital of France is Paris.' The other was a question-answering setting using TriviaQA, which has longer, more complex answers. They also created 'neighborhood' data \u2013 semantically related questions that weren't directly in the forget set \u2013 to test for collateral damage.", "Jamie": "Right, makes sense to have different types of data. And the results consistently showed UPCORE outperforming the baselines in terms of the AUC metric, right?"}, {"Alex": "Exactly! UPCORE consistently had higher AUC scores, indicating a better trade-off between deletion effectiveness and model utility. The tables in the paper are packed with numbers, but the key takeaway is that UPCORE wasn't just shifting the problem around \u2013 it was genuinely improving the balance.", "Jamie": "Hmm, does the method work on Jailbreaks as well and does it offer robust properties even then?"}, {"Alex": "Interesting thought Jamie! It is actually indeed robust to jailbreaks - it yields higher AUC across with robustness to rephrased and jailbreak variants of the forget data. And with increased Utility.", "Jamie": "Wow and what kind of other metrics did you guys measure for your research?"}, {"Alex": "In addition to measuring typical accuracy/performance after unlearning, we also introduce a key measure called Area Under the Curve(AUC) to have a better shot at comparisons of different systems. This makes comparisons easier and provides better clarity and more in depth analysis.", "Jamie": "Okay so you mentioned it's based on coreset - So what if someone just naively samples a bunch of data, is UPCORE still helpful?"}, {"Alex": "It is still indeed more helpful. UPCORE is better able to distinguish between normal data and outliers, making it the most effective method in comparison", "Jamie": "Did the research uncover some of the downsides to this methodology?"}, {"Alex": "It does as the researchers also scaled the coreset size and measured the effect - UPCORE exhibits the largest performance gain from no pruning to 10% pruning, followed by a dip at 20%. Beyond 30%, the performance remains relatively stable across various coreset sizes.", "Jamie": "And why are these models so important - what sort of use cases could we see here?"}, {"Alex": "These sorts of models are important for a lot of key use cases, like defending against extraction attacks. One example is SAFREE training free and adaptive guard for safe text to image and video generation.", "Jamie": "Wow! So what would you say is the key takeaway or next direction the researchers want to go?"}, {"Alex": "The core contribution of UPCORE is an improved method that reduces unintended performance loss while also able to combine with any data-driven unlearning method, highlighting it\u2019s potential as a principled and generalizable approach to utility-aware unlearning.", "Jamie": "Alex - what's the next steps for this kind of research?"}, {"Alex": "I think there is more exploration to be done in understanding that positive and negative transference and collateral effects during unlearning and editing. Understanding the conditions and the types of data when a method is able to maximize or improve the tradeoff is a key step", "Jamie": "Thanks so much Alex! That was a fascinating discussion! I appreciate you breaking down this complex research into something so understandable."}]