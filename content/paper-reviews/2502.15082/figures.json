[{"figure_path": "https://arxiv.org/html/2502.15082/x1.png", "caption": "Figure 1: Left: Standard unlearning methods are applied equally to all points in the forget set.\nHere, outlier points in the model\u2019s hidden space (visualized in 2D) contribute to the unintentional forgetting of points outside of the forget set (i.e. collateral damage).\nRight: By finding a lower-variance coreset within the forget set, UPCORE reduces damage while maintaining forget performance via positive transfer from the coreset to the pruned points.", "description": "This figure illustrates the core concept of the UPCORE method.  The left panel shows how standard unlearning methods impact the entire forget set uniformly. This can lead to collateral damage where information outside the forget set is unintentionally lost due to the presence of outliers (points with high variance in the model's hidden representation) within the forget set.  The right panel demonstrates how UPCORE addresses this problem. By identifying and removing outliers, UPCORE creates a lower-variance coreset. This coreset is then used for unlearning, minimizing collateral damage.  Furthermore, UPCORE leverages positive transfer learning from the coreset to the pruned points; the pruned points benefit from the learned information of the coreset and are also successfully unlearned without causing negative transfer to the other data points.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.15082/x2.png", "caption": "Figure 2: UPCORE has four stages. First, we extract hidden states from the LLM to be modified; second, we identify outliers using Isolation Forests; third, we prune outliers to select a core forget set, and fourth, we perform unlearning on the coreset.", "description": "This figure illustrates the four stages of the UPCORE algorithm.  Stage 1 involves extracting the hidden states from the large language model (LLM) being modified. These hidden states represent the LLM's internal representation of the data. Stage 2 uses Isolation Forests to identify outlier data points within the forget set (data to be removed).  These outliers are points that significantly increase the variance of the model's representation. In Stage 3, the algorithm prunes or removes these identified outliers to create a core forget set, a smaller subset of the original forget set with lower variance. Finally, Stage 4 applies an unlearning method to the LLM, using only the selected core forget set to remove the targeted information while minimizing unintended damage to the model's overall performance.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2502.15082/x3.png", "caption": "Figure 3: Trading-off between deletion effectiveness and model utility forms a Pareto frontier across epochs, shown here averaged across Counterfact topics using Gradient Ascent. Our proposed AUC metric quantifies the area under these curves, with UPCORE consistently achieving the highest AUC across all settings.", "description": "This figure demonstrates the trade-off between deletion effectiveness (how well the model forgets the unwanted information) and model utility (how well the model performs on other tasks) across different training epochs during the unlearning process.  The x-axis represents deletion effectiveness, while the y-axis represents model utility.  Each line represents a different method: complete unlearning (applying unlearning to the full forget set), random unlearning (applying unlearning to a random subset of the forget set), and UPCORE (the proposed method). The area under the curve (AUC) for each method is calculated to quantify the overall performance. UPCORE consistently achieves the highest AUC, indicating a superior balance between effective deletion and the preservation of model utility. This showcases the advantage of UPCORE over traditional unlearning methods.", "section": "4.1 Metrics and Answer Extraction"}, {"figure_path": "https://arxiv.org/html/2502.15082/x4.png", "caption": "Figure 4: AUC between forget set ROUGE and neighborhood data ROUGE averaged across topics in Counterfact. UPCORE reduces damage to neighborhood data.", "description": "This figure displays the area under the curve (AUC) for two metrics: ROUGE score on the forget set (measuring successful deletion) and ROUGE score on the neighborhood data (measuring unintended model damage).  The AUC is calculated for three different coreset selection methods: complete (using the whole forget set), random (using a random sample of the forget set), and UPCORE.  The graph shows that UPCORE consistently achieves a higher AUC across three different unlearning methods (Gradient Ascent, Refusal, and NPO), indicating a superior balance between effective deletion and preservation of model utility on related but unseen data. The results highlight that UPCORE effectively reduces unintended damage to neighborhood data by carefully selecting a coreset of the forget data.", "section": "5. Experimental Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2502.15082/x5.png", "caption": "Figure 5: Impact of scaling the coreset size on performance: AUC scores on different utility sets, averaged across Counterfact topics, for various pruning percentages.", "description": "This figure examines how changing the size of the coreset, created by pruning outliers, affects the model's performance.  The coreset size is varied by adjusting the percentage of data points pruned. The x-axis represents the different pruning percentages, while the y-axis shows the AUC (Area Under the Curve) score, calculated for various utility metrics (assessing the model's performance on data outside the unlearning scope). These metrics are averaged across multiple topics from the Counterfact dataset. The figure helps determine the optimal balance between effective data removal and preserving model utility by showing how AUC varies across different pruning rates.  It shows the tradeoff between deletion effectiveness and model utility in unlearning.", "section": "5. Experimental Results and Discussion"}, {"figure_path": "https://arxiv.org/html/2502.15082/x6.png", "caption": "Figure 6: Hidden state variance of the baseline and UPCORE forget sets across the six Counterfact forget topics. UPCORE consistently reduces variance using Isolation Forest as expected.", "description": "This figure visualizes the variance of hidden states within the forget sets generated by different methods for unlearning, namely the baseline approach and UPCORE.  It showcases the variance of hidden states for six different topics from the Counterfact dataset. Each bar represents a specific topic, and the height shows the variance. The comparison highlights that UPCORE, which employs Isolation Forests for coreset selection, significantly reduces the variance in the hidden states compared to the baseline approach. This reduction in variance is a key outcome of UPCORE, as it aims to minimize the impact of unlearning on other parts of the model by selecting a subset of forget data with lower variance.", "section": "5.5 UPCORE Lowers Forget Set Variance"}, {"figure_path": "https://arxiv.org/html/2502.15082/x7.png", "caption": "(a) Model utility and hidden state variance of the forget data show a strong negative correlation of -0.714 across data from multiple topics.", "description": "This figure displays the relationship between model utility and the variance of the model's hidden states for data points in the forget set.  The analysis is performed across multiple topics to demonstrate that this negative correlation is consistent even across different subject matters. The correlation coefficient of -0.714 indicates a strong inverse relationship: as the variance of hidden states in the forget set increases, the model's utility decreases after unlearning.", "section": "3.2. Variance as a Measure of Collateral Damage"}, {"figure_path": "https://arxiv.org/html/2502.15082/x8.png", "caption": "(b) Drop in model utility after unlearning and base model\u2019s confidence on the forget data do not show any strong correlation with a Pearson correlation value of -0.021.", "description": "This figure examines the relationship between the drop in model utility after unlearning and the model's confidence scores on the forget data.  The analysis reveals a weak negative correlation (Pearson correlation coefficient of -0.021), suggesting that the model's initial confidence in the data to be forgotten is not a strong predictor of the amount of utility loss experienced after unlearning. This implies that factors beyond simple confidence scores likely play a crucial role in determining the impact of unlearning on model performance.  The plot likely shows a scatter plot with model confidence on the x-axis and drop in model utility on the y-axis, with each point representing a data point from the forget set.", "section": "3.2. Variance as a Measure of Collateral Damage"}, {"figure_path": "https://arxiv.org/html/2502.15082/x9.png", "caption": "Figure 7: (a) Relationship between model utility and hidden state variance.\n(b) Relationship between model utility drop after unlearning and confidence on forget data.", "description": "Figure 7 displays the correlation between model performance and two factors: hidden state variance and model confidence.  Panel (a) shows a strong negative correlation between model utility (measured after unlearning) and the variance of hidden states in the model's representation of the data points designated for unlearning.  Higher variance leads to lower model utility after unlearning. Panel (b) shows that there is little to no correlation between the drop in model utility after unlearning and the model's confidence scores on those same data points. This suggests that variance of model representations is a more significant factor influencing the amount of collateral damage during unlearning than model confidence.", "section": "3.2. Variance as a Measure of Collateral Damage"}, {"figure_path": "https://arxiv.org/html/2502.15082/x10.png", "caption": "(a) Hidden state variance of the core forget set plotted against the pruning percentage across topics. The variance of the core forget data decreases nearly linearly as the pruning percentage increases.", "description": "This figure shows the relationship between the variance of the hidden states in the core forget set and the pruning percentage applied during the coreset selection process.  The x-axis represents the percentage of data points pruned from the initial forget set, while the y-axis represents the variance of the hidden states of the remaining points (the core forget set). As the pruning percentage increases (more points are removed), the variance of the hidden states in the core forget set decreases almost linearly. This demonstrates the effectiveness of the pruning strategy in reducing the variance within the forget set, a key factor in mitigating the collateral damage during the unlearning process.", "section": "3.3 UPCORE: Core Forget Set Selection"}]