[{"figure_path": "https://arxiv.org/html/2504.10449/x1.png", "caption": "Figure 1: Inference latency when using prompt length 256 and decoding length 4096.", "description": "This figure displays the inference latency, or the time taken for the model to generate outputs, across various batch sizes.  The experiment uses a fixed prompt length of 256 tokens and a decoding length of 4096 tokens. Three different language models are compared: M1, Llama-3.2-3B, and DeepSeek-R1-Distill-Qwen-1.5B. The graph illustrates the impact of batch size on inference time for each model, highlighting the relative efficiency of M1 compared to the other two models.", "section": "4.2 Speed Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.10449/x2.png", "caption": "Figure 2: Inference latency when using batch size 128.", "description": "This figure displays the inference latency, or time taken for inference, of three different language models (DeepSeek-R1-Distill-Qwen-1.5B, Llama-3.2-3B, and M1-3B) across various generation lengths while maintaining a constant batch size of 128.  The x-axis represents the generation length, showing how the latency changes as the models generate longer sequences. The y-axis represents inference time in seconds. This visualization allows for a comparison of the efficiency of the three models in terms of inference speed when generating sequences of varying lengths.", "section": "4.2 Speed Evaluation"}]