[{"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/convergence_2.jpg", "caption": "Figure 1: We propose FAST, a simple yet effective approach for tokenization of robot action trajectories via time-series compression. FAST enables training of autoregressive VLAs that solve complex dexterous manipulation tasks and generalize broadly to new scenes. We use it to train \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST, a generalist robot policy that matches the performance of the state-of-the-art \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT diffusion VLA on dexterous and long-horizon manipulation tasks, while training 5x faster (top).", "description": "This figure showcases the effectiveness of FAST, a novel action tokenization method, in training vision-language-action (VLA) models.  The top graph compares the training performance of a VLA model using FAST (\u03c0\u2080-FAST) against a state-of-the-art diffusion-based VLA (\u03c0\u2080).  The results demonstrate that \u03c0\u2080-FAST achieves comparable performance while training five times faster. The bottom part of the figure presents a series of images illustrating the diverse dexterous manipulation tasks the \u03c0\u2080-FAST model successfully performs.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.09747/x1.png", "caption": "Figure 2: Left: FAST tokenization enables training of autoregressive Transformers for dexterous robot control via simple next token prediction. Right: FAST outperforms popular binning tokenization schemes, e.g., used in OpenVLA\u00a0[39], particularly for high-frequency robot data.", "description": "Figure 2 demonstrates the effectiveness of FAST tokenization in training autoregressive transformer models for robot control.  The left panel illustrates how FAST simplifies the training process by compressing robot action sequences into more manageable tokens, allowing the model to predict the next token in the sequence efficiently, even for complex dexterous tasks.  The right panel compares FAST's performance against traditional binning methods commonly used in vision-language-action models like OpenVLA.  It shows that FAST significantly outperforms binning, especially when dealing with high-frequency robot data, highlighting its ability to handle the challenges of highly correlated actions and achieving better accuracy in next-token prediction.", "section": "IV. CASE STUDY: HOW DOES TOKENIZATION AFFECT VLA TRAINING?"}, {"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/case_study.png", "caption": "Figure 3: Effect of sampling rate on prediction performance. We train a small autoregressive transformer model on a didactic interpolation task, in which the network must predict the black dashed curve given the four circles.\nWe find that models trained with the binning tokenization approach used in prior VLAs\u00a0[10, 39] produce increasingly poor predictions as we increase the sampling frequency of the underlying signal, due to strong correlation between consecutive tokens at high frequencies. Our FAST tokenization approach, based on the discrete cosine transform (DCT), addresses the problem and leads to high-quality predictions across all sampling rates.", "description": "This figure demonstrates the impact of different action tokenization methods on the performance of autoregressive models for predicting continuous actions from high-frequency data.  A simple interpolation task is used, where the model must predict a curve given four data points.  Using a standard binning tokenization (as used in previous Vision-Language-Action models), prediction accuracy drastically decreases as the sampling rate (frequency) of the data increases.  This is because consecutive tokens become highly correlated at high frequencies, hindering the model's ability to learn meaningful patterns. In contrast, the proposed FAST tokenization method, based on the Discrete Cosine Transform (DCT), maintains high prediction accuracy across all sampling rates, demonstrating its effectiveness in handling highly correlated action data.", "section": "IV. CASE STUDY: HOW DOES TOKENIZATION AFFECT VLA TRAINING?"}, {"figure_path": "https://arxiv.org/html/2501.09747/x2.png", "caption": "Figure 4: Overview of the FAST\u00a0action tokenization pipeline. Given a normalized chunk of actions, we apply discrete cosine transform (DCT) to convert the signal to the frequency domain. We then quantize the DCT coefficients and use byte-pair encoding (BPE) to compress the flattened sequence of per-dimension DCT coefficients into the final action token sequence. See Section\u00a0V-B for a detailed description.", "description": "This figure details the FAST action tokenization pipeline, which efficiently converts continuous robot actions into a compressed sequence of discrete tokens.  The process begins with a normalized chunk of robot actions.  A Discrete Cosine Transform (DCT) converts these actions into the frequency domain, highlighting the most significant frequency components which represent the important aspects of the actions.  These DCT coefficients are then quantized, reducing their precision while preserving crucial information. Finally, Byte-Pair Encoding (BPE) compresses the flattened sequence of quantized coefficients, generating the final, compressed action token sequence. This compressed representation allows for more efficient training of vision-language-action models.", "section": "V. EFFICIENT ACTION TOKENIZATION VIA TIME-SERIES COMPRESSION"}, {"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/environments.jpg", "caption": "Figure 5: Evaluation environments. We test FAST across 7\u00a0evaluation environments: 6\u00a0real-robot tasks and 1\u00a0simulation environment. The tasks are designed to test VLA performance on highly dexterous tasks, like folding cloths from a laundry basket (\u201cLaundry Folding\u201d), and generalization, e.g., zero-shot table-top manipulation in unseen environments (\u201cDROID\u201d).", "description": "Figure 5 showcases the diverse set of seven environments used to evaluate the performance of the FAST action tokenization method.  These environments include six real-world robotic manipulation tasks and one simulated task.  The real-world tasks represent a variety of manipulation challenges, ranging from highly dexterous fine motor skills (like folding a t-shirt or arranging groceries) to more complex tasks requiring precise object placement.  The inclusion of a simulated task allows for testing generalization capabilities. The \"DROID\" task is especially notable as it evaluates the ability of the model to perform zero-shot table-top manipulations in entirely unseen environments, demonstrating the robustness and generalizability of the method.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/x3.png", "caption": "Figure 6: Comparison of policy performance using different tokenization approaches. We find that tokenization approaches that compress action targets (FAST, FSQ) lead to substantially more efficient training than the na\u00efve binning tokenization used in prior VLAs. Overall, we find that FAST\u00a0leads to more effective policy training than FSQ, particularly on dexterous real-robot tasks. Our universal tokenizer, FAST+, matches the performance of dataset-specific tokenizers. We report mean and 95% CI.", "description": "Figure 6 presents a comparison of the training efficiency and resulting policy performance achieved using different action tokenization methods for vision-language-action (VLA) models. The study compares three approaches: naive binning (a common technique in prior VLA works), frequency-space action sequence tokenization (FAST), and frequency-space quantized (FSQ). The results demonstrate that methods compressing action targets, namely FAST and FSQ, significantly enhance training efficiency compared to the naive binning approach.  Further analysis shows that FAST consistently outperforms FSQ, especially in complex, dexterous real-world robotic tasks.  The figure also validates the effectiveness of FAST+, a universal tokenizer trained on a large dataset of varied robotic actions, which exhibits performance comparable to tokenizers trained on specific datasets. Mean success rates and 95% confidence intervals are shown for each method and task.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/x4.png", "caption": "Figure 7: Evaluation environments of FAST policy trained on DROID\u00a0[38]. We find that the same policy checkpoint generalizes robustly, and performs various simple table-top tasks zero-shot across three university campuses.", "description": "This figure showcases the zero-shot generalization capabilities of a robot policy trained using the FAST action tokenization method on the DROID dataset. The same policy checkpoint, without any further fine-tuning or adaptation, successfully performs various simple tabletop manipulation tasks across three different university campuses. This demonstrates the robustness and generalizability of the FAST-trained policy, highlighting its ability to adapt to new environments and variations in object placement, lighting, and background without retraining.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/x5.png", "caption": "Figure 8: Universal tokenizer. We test the compression rate achieved by our FAST+ tokenizer vs. na\u00efve tokenization across diverse robot datasets, unseen during tokenizer training. We find that FAST is effective across a wide range of robot morphologies, action spaces and control frequencies.", "description": "Figure 8 presents a comparison of compression ratios between FAST+, the universal robot action tokenizer, and naive tokenization methods.  The comparison is made across multiple robot datasets that were *not* used during the training of FAST+.  This demonstrates the effectiveness and generalizability of FAST+ across diverse robotic setups. The results show that FAST+ consistently achieves significant compression across a wide range of robot morphologies, action spaces, and control frequencies, indicating its robustness and potential for broad applicability in various robotic tasks.", "section": "V. EFFICIENT ACTION TOKENIZATION VIA TIME-SERIES COMPRESSION"}, {"figure_path": "https://arxiv.org/html/2501.09747/x6.png", "caption": "Figure 9: Comparison of diffusion \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\u00a0[7] to our \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT model with FAST decoding on single-task training. On small datasets (Libero, T-Shirt Folding), both perform comparably. On large datasets (Table Bussing), FAST\u00a0converges faster. In DROID, we find that FAST\u00a0follows language instructions better. We report mean and 95% CI.", "description": "This figure compares the performance of a state-of-the-art diffusion-based vision-language-action (VLA) model, \u03c00, with a new autoregressive VLA model using FAST action tokenization. The comparison is made across various tasks with different dataset sizes.  The results show that on smaller datasets (Libero and T-Shirt Folding), both models perform similarly.  However, on larger datasets (Table Bussing), the model with FAST tokenization converges to a solution much faster than the diffusion-based \u03c00 model. Furthermore, when evaluated on the DROID dataset, the FAST model demonstrates superior ability to follow language instructions compared to the diffusion model. The mean and 95% confidence intervals are provided for all results.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/x7.png", "caption": "Figure 10: Rollout of \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST on the laundry folding task. FAST tokenization enables autoregressive VLAs to perform complex, long-horizon, and dexterous tasks that were impossible with previous tokenization schemes.", "description": "The figure displays a sequence of images showing the steps involved in a laundry folding task performed by a robot using the \u03c00-FAST (pi-zero-FAST) model.  The robot successfully manipulates a shirt, demonstrating complex actions like grasping, unfolding, and folding. This success highlights the effectiveness of the FAST tokenization method in enabling autoregressive Vision-Language-Action (VLA) models to handle intricate, long-duration tasks that previous methods failed to solve. The sequence shows the robot's progress, emphasizing the dexterity and planning capabilities facilitated by the improved tokenization scheme.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/x8.png", "caption": "Figure 11: Comparison of \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST and diffusion \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\u00a0[7] generalist policies. \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST matches the performance of diffusion \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT while requiring significantly less compute for training. Reported: mean and 95% CI.", "description": "Figure 11 compares the performance of two generalist robotic policies: \u03c00-FAST (an autoregressive model using the FAST tokenization method) and a diffusion-based \u03c00 model. The results show that \u03c00-FAST achieves comparable performance to the diffusion \u03c00, but with significantly less computational cost during training.  The chart displays the success rates and task progress across several complex manipulation tasks, illustrating the efficiency of the \u03c00-FAST approach. Error bars representing the 95% confidence intervals are also included.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/x9.png", "caption": "Figure 12: Comparison of compression-reconstruction tradeoff on six training datsets. Any discretization method includes some hyperparameter that controls the tradeoff between reconstruction fidelity and compression level, represented here as number of tokens in the output (vocab size is held constant across all tokenizers). We sweep this hyperparameter (FAST: rounding scale; na\u00efve tokenization: subsampling frequency; FSQ: number of latent tokens) and find that FAST performs well across a wide range of scales. In particular, although it is less efficient than VQ-based tokenizers at low fidelities, it exhibits much better scaling to higher reconstruction fidelity, making FAST much more applicable to fine-grained control problems. Specific instantiations of each tokenizer (FAST+, and na\u00efve tokenization without subsampling) are also shown.", "description": "Figure 12 illustrates the trade-off between compression and reconstruction fidelity for six different robot action tokenization methods.  The x-axis represents the number of tokens used (a measure of compression), while the y-axis shows the reconstruction error (a measure of fidelity).  Each method has a hyperparameter controlling this trade-off; for FAST, it's the rounding scale; for na\u00efve tokenization, it's the subsampling frequency; and for FSQ (Frequency-Space Quantization), it's the number of latent tokens. The figure demonstrates that FAST achieves good performance across a wide range of compression levels, significantly outperforming VQ-based methods (like FSQ) at higher fidelity levels.  This makes FAST particularly suitable for fine-grained control tasks requiring high fidelity reconstruction.", "section": "V. EFFICIENT ACTION TOKENIZATION VIA TIME-SERIES COMPRESSION"}, {"figure_path": "https://arxiv.org/html/2501.09747/x10.png", "caption": "(a) Table Bussing", "description": "This image shows a single-arm UR5e robot performing the Table Bussing task. The goal is to clear a table by picking up various objects (cups, plates, bowls, cutlery, etc.) and placing them in a trash bin or a plastic container.  The task requires precise grasping and manipulation of diverse objects. The scene is designed to be challenging, with utensils intentionally placed on top of trash and objects obstructing each other.", "section": "VI. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09747/x11.png", "caption": "(b) T-Shirt Folding", "description": "This image shows the setup for the T-Shirt Folding task.  A bimanual ARX robot is used to fold shirts. The training dataset includes approximately 150 shirts of varying sizes, colors, and styles. The evaluation scene shows five shirts in various initial configurations, which are presented one at a time. The success metric is the percentage of successfully folded shirts, as judged by a human evaluator.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_bus.jpeg", "caption": "(c) Grocery Bagging", "description": "A UR5 single-arm robot needs to pack seven objects from a table into a grocery bag, taking care not to topple or rip the bag in the process. This task requires picking a diverse set of objects and carefully inserting them into the bag.", "section": "VI. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_shirt.jpeg", "caption": "(d) Toast out of Toaster", "description": "This task requires a bi-manual Trossen ViperX robot to remove two slices of bread from a toaster and place them onto a plate.  The evaluation involves assessing the robot's ability to successfully grasp and move both slices of bread from the toaster to a plate.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_grocery.jpeg", "caption": "(e) Laundry Folding", "description": "This image shows the Laundry Folding task, one of the seven tasks used to evaluate the performance of different vision-language-action (VLA) models. The task involves a dual-arm robot that needs to take shirts and shorts from a basket, flatten them on a table, fold them, and stack the folded clothes.  This task is particularly challenging because it requires precise grasping, dynamic motions to flatten the clothes, and precise placement of the folded clothes on the existing stack.  Success is determined by a human evaluator based on the percentage of clothing items successfully folded and stacked.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_toast.jpeg", "caption": "Figure 13: Sampled initial configurations of evaluation tasks.", "description": "This figure shows example starting states for several robotic manipulation tasks used to evaluate the performance of different robotic policies.  Each subfigure displays a distinct task setup, showcasing the variety of object arrangements and robot configurations used in the experiments.  These images help illustrate the complexity of the tasks and the diversity of scenarios considered in assessing the robustness and generalizability of the various robot control methods.", "section": "VI. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09747/extracted/6136664/figures/task_laundry.jpeg", "caption": "Figure 14: Setups used for quantitative DROID evaluation.", "description": "This figure displays example setups from the quantitative evaluation of the DROID dataset.  The DROID dataset is a large-scale, in-the-wild dataset for robot manipulation. The quantitative evaluation tests the robot's ability to perform various tasks, like putting objects in specific containers, cleaning a table, and interacting with drawers.  Each image showcases a different setup, representing the diverse scenarios and objects used to assess the generalizability of the policies. The caption in the paper is short, so this provides more context for the reader.", "section": "VI. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.09747/x12.png", "caption": "Figure 15: Comparison of \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST and compute-matched diffusion \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT\u00a0[7] generalist policies. \u03c00subscript\ud835\udf0b0\\pi_{0}italic_\u03c0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT-FAST clearly outperforms the diffusion VLA when trained with the same amount of training compute, due to its faster convergence. Reported: mean and 95% CI.", "description": "Figure 15 shows a comparison of the performance of two generalist robot control policies: one is the \u03c00-FAST model (an autoregressive model using the FAST action tokenization method), and the other is a diffusion-based \u03c00 model from a prior work. Both models were trained using the same computational resources.  The results demonstrate that \u03c00-FAST significantly outperforms the diffusion \u03c00 model, achieving better task success rates across various tasks.  This superior performance is attributed to \u03c00-FAST's faster convergence during training, a key benefit enabled by the efficient FAST tokenization method. The figure displays the mean task success rates and 95% confidence intervals for each model across several tasks.", "section": "VI. Experiments"}]