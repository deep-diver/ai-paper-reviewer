[{"content": "| Baselines | En | Es | Ru | Fr | Vi | Fa | Id | Ar | Fi | Ko | Hi | Bn | Te | Sw | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Jina-embeddings-v3* [Sturua et al. (2024)](https://arxiv.org/html/2501.00874/bib.bib52) | 59.84 | 61.23 | 62.88 | 58.94 | 66.74 | 78.35 | 58.51 | 64.71 | 73.57 | 64.96 | 64.19 | 61.54 | 68.96 | 49.20 | 63.83 |\n| mGTE-base* [Zhang et al. (2024)](https://arxiv.org/html/2501.00874/bib.bib70) | 60.40 | 59.65 | 61.02 | 56.20 | 65.81 | 73.46 | 56.55 | 61.97 | 68.96 | 61.22 | 60.81 | 58.24 | 63.58 | 52.57 | 61.46 |\n| BGE-M3* [Chen et al. (2024)](https://arxiv.org/html/2501.00874/bib.bib10) | 60.09 | 60.60 | 62.37 | 57.34 | 70.69 | 78.97 | 58.78 | 64.12 | 75.60 | 64.72 | 64.61 | 65.31 | 69.85 | 54.20 | 64.80 |\n| Multilingual-E5-large* [Wang et al. (2024d)](https://arxiv.org/html/2501.00874/bib.bib61) | 61.91 | 61.97 | 62.91 | 59.40 | 71.30 | 78.08 | 55.21 | 63.41 | 76.53 | 66.55 | 63.75 | 63.67 | 67.32 | 51.55 | 64.54 |\n| UDEVER-Bloom-7B* [Zhang et al. (2023)](https://arxiv.org/html/2501.00874/bib.bib69) | 55.83 | 56.39 | 59.73 | 54.38 | 64.32 | 68.70 | 48.97 | 55.02 | 67.60 | 58.54 | 55.96 | 55.13 | 61.00 | 47.41 | 57.78 |\n| SimCSE [Gao et al. (2021b)](https://arxiv.org/html/2501.00874/bib.bib16) | 51.92 | 51.81 | 24.90 | 46.95 | 31.18 | 37.12 | 39.27 | 29.46 | 41.64 | 26.23 | 25.17 | 21.54 | 26.71 | 38.36 | 35.16 |\n| Contriever [Izacard et al. (2022)](https://arxiv.org/html/2501.00874/bib.bib19) | 49.29 | 44.26 | 26.55 | 44.05 | 33.03 | 39.66 | 38.33 | 32.36 | 45.76 | 26.47 | 23.27 | 22.61 | 22.64 | 39.26 | 34.82 |\n| GTE-large [Li et al. (2023)](https://arxiv.org/html/2501.00874/bib.bib28) | 62.29 | 51.66 | 33.49 | 50.13 | 38.88 | 44.67 | 43.07 | 30.27 | 51.98 | 27.02 | 20.38 | 22.97 | 22.75 | 41.40 | 38.64 |\n| BGE-en-1.5 [Xiao et al. (2023)](https://arxiv.org/html/2501.00874/bib.bib65) | 63.27 | 51.65 | 32.79 | 50.84 | 38.50 | 49.73 | 43.28 | 30.81 | 51.16 | 31.11 | 25.28 | 26.34 | 23.02 | 41.96 | 39.98 |\n| E5-large [Wang et al. (2024a)](https://arxiv.org/html/2501.00874/bib.bib58) | 60.12 | 52.41 | 26.81 | 51.00 | 37.99 | 39.47 | 43.86 | 31.32 | 53.59 | 28.84 | 24.57 | 23.48 | 22.03 | 43.25 | 38.48 |\n| ST5-XXL [Ni et al. (2021c)](https://arxiv.org/html/2501.00874/bib.bib43) | 58.81 | 60.35 | 44.42 | 58.50 | 41.81 | 24.66 | 53.43 | 25.30 | 52.46 | 15.43 | 18.07 | 17.10 | 21.63 | 38.81 | 37.91 |\n| GTR-XXL [Ni et al. (2021b)](https://arxiv.org/html/2501.00874/bib.bib42) | 58.12 | 54.39 | 41.94 | 53.21 | 37.96 | 24.67 | 50.08 | 25.14 | 53.88 | 15.23 | 17.35 | 15.92 | 22.12 | 40.57 | 36.47 |\n| E5-Mistral [Wang et al. (2024b)](https://arxiv.org/html/2501.00874/bib.bib59) | **66.64** | **61.84** | **61.30** | **59.65** | 58.58 | 72.55 | 58.25 | 54.43 | 66.97 | 62.82 | 56.23 | 55.10 | 47.15 | 50.61 | 59.44 |\n| LUSIFER (Ours) | 57.20 | 60.14 | 59.82 | 59.24 | **67.69** | **76.17** | **59.70** | **55.60** | **72.83** | **65.23** | **62.37** | **58.43** | **69.30** | **53.12** | **62.63** |", "caption": "Table 1: Comparative analysis of model performance across multiple languages and tasks. The table presents average metrics for each model, with the highest score for each language emphasized in bold. * denotes the models trained on extensive multilingual data.", "description": "This table presents a comprehensive comparison of various embedding models' performance across multiple languages and tasks.  The average performance metrics for each model are shown, with the best-performing model for each language highlighted in bold.  The models marked with an asterisk (*) were trained using extensive multilingual data, differentiating them from those trained primarily on English data.  This allows for a direct comparison between models trained with and without the benefit of multilingual training data. The table provides a clear picture of the relative strengths and weaknesses of different models when handling various languages and embedding tasks.", "section": "4. Main Results"}, {"content": "| Baselines | MLQARetrieval | BelebeleRetrieval | STS17 | STS22 | IndicCrosslingual | Avg. |\n|---|---|---|---|---|---|---|\n| SimCSE Gao et al. (2021b) | 7.41 | 18.35 | 39.71 | 37.95 | 0.18 | 20.72 |\n| Contriever Izacard et al. (2022) | 9.75 | 22.94 | 34.55 | 41.72 | 0.03 | 21.80 |\n| GTE-large Li et al. (2023) | 16.99 | 31.82 | 37.57 | 53.79 | 1.59 | 28.35 |\n| BGE-en-1.5 Xiao et al. (2023) | 16.64 | 31.19 | 40.40 | 50.77 | 1.11 | 28.02 |\n| E5-large Wang et al. (2024a) | 17.04 | 31.12 | 37.90 | 54.31 | 1.83 | 28.44 |\n| ST5-XXL Ni et al. (2021c) | 20.82 | 41.68 | 56.19 | 59.02 | 1.76 | 35.89 |\n| GTR-XXL Ni et al. (2021b) | 20.19 | 38.02 | 50.83 | 60.11 | 2.74 | 34.38 |\n| E5-Mistral Wang et al. (2024b) | 31.54 | 54.75 | **81.12** | **71.37** | 21.92 | 52.14 |\n| LUSIFER (Ours) | **36.68** | **57.81** | 81.09 | 70.49 | **43.40** | **57.89** |", "caption": "Table 2: Cross-lingual evaluation results. The table presents average metrics for each model over all languages of the datasets, with the highest score for each language emphasized in bold.", "description": "This table presents a comprehensive evaluation of various embedding models' performance on cross-lingual retrieval tasks.  It shows average metrics across multiple languages, highlighting the best performing model for each language in bold.  This evaluation is crucial for assessing the models' ability to generalize to languages beyond those heavily represented in their training data.", "section": "4. Experiment"}, {"content": "| Baselines | En | Es | Ru | Fr | Vi | Fa | Id | Ar | Fi | Ko | Hi | Bn | Te | Sw | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| LUSIFER (Full) | **57.20** | **60.14** | **59.82** | **59.24** | **67.69** | **76.17** | **59.70** | **55.60** | **72.83** | **65.23** | **62.37** | **58.43** | **69.30** | **53.12** | **62.63** |\n| LUSIFER (Connector Only) | 35.53 | 33.98 | 42.95 | 33.54 | 35.68 | 57.86 | 35.55 | 27.60 | 48.72 | 34.45 | 47.57 | 41.85 | 46.50 | 34.66 | 44.18 |\n| LUSIFER (Frozen Multilingual Encoder) | 50.99 | 58.77 | 58.30 | 52.73 | 62.24 | 75.88 | 58.11 | 41.66 | 70.75 | 59.53 | 62.48 | 55.53 | 66.24 | 49.12 | 58.74 |\n| LUSIFER (Alignment Only) | 43.32 | 38.94 | 45.12 | 36.75 | 41.96 | 64.60 | 38.38 | 33.07 | 52.78 | 38.08 | 53.06 | 47.84 | 48.34 | 40.03 | 44.45 |\n| LUSIFER (Representation Finetuning Only) | 49.71 | 58.76 | 58.08 | 51.01 | 62.11 | 74.01 | 57.32 | 40.95 | 68.47 | 57.81 | 59.74 | 53.53 | 63.39 | 47.03 | 57.28 |", "caption": "Table 3: Ablation study results of LUSIFER\u2019s components. The table presents average metrics for each model, with the highest score for each language emphasized in bold.", "description": "This table presents the results of an ablation study conducted on the LUSIFER model.  It systematically removes components of the model's architecture (the connector, multilingual encoder, alignment training, and representation finetuning) to determine their individual contributions to the overall performance.  The table shows average metrics across multiple languages for each configuration, with the best result for each language highlighted in bold.  This allows for a clear understanding of the relative importance of each LUSIFER component in achieving its enhanced multilingual capabilities.", "section": "4.6 Ablation Study"}, {"content": "| Hyperparameter | Alignment Training | Representation Finetuning |\n|---|---|---|\n| Batch size | 256 | 256 |\n| Learning rate | 1.5e-4 | 5e-5 |\n| Learning rate scheduler | cosine | cosine |\n| Learning rate warm-up ratio | 0.1 | 0.1 |\n| Weight decay | 0.01 | 0.01 |\n| Grad norm clipping | 1.0 | 1.0 |\n| Epochs | 2 | 1 |\n| Optimizer | AdamW | AdamW |\n| Float precision | bf16-mixed | bf16-mixed |\n| LoRA rank | 16 | 16 |\n| LoRA alpha | 32 | 32 |\n| Random mask ratio | 0.5 | - |\n| Number of hardnegatives | - | 7 |", "caption": "Table 4: Training hyperparameters for each stage.", "description": "This table details the hyperparameters used in the two-stage training process of the LUSIFER model.  The first stage focuses on aligning the multilingual encoder's representations with the target LLM's embedding space, while the second stage fine-tunes the model's representations using contrastive learning on English data.  For each stage, the table specifies hyperparameters such as batch size, learning rate, learning rate scheduler, weight decay, gradient norm clipping, epochs, optimizer, and float precision.  It also includes hyperparameters specific to the Low-Rank Adaptation (LoRA) technique used for efficient training.", "section": "3 Methodology"}, {"content": "| Stage | Dataset | Number of Samples |\n|---|---|---|\n| Alignment Training | Wikitext-103 [Merity et al. (2017)](https://arxiv.org/html/2501.00874v1#bib.bib37) | 100,000 |\n|  | MSMARCO [Bajaj et al. (2018)](https://arxiv.org/html/2501.00874v1#bib.bib5) | 100,000 |\n| Representation Finetuning | MS MARCO [Bajaj et al. (2018)](https://arxiv.org/html/2501.00874v1#bib.bib5) | 100,000 |\n|  | FEVER [Thorne et al. (2018)](https://arxiv.org/html/2501.00874v1#bib.bib54) | 100,000 |\n|  | PAQ [Lewis et al. (2021)](https://arxiv.org/html/2501.00874v1#bib.bib27) | 100,000 |\n|  | SNLI [Bowman et al. (2015)](https://arxiv.org/html/2501.00874v1#bib.bib9) | 100,000 |\n|  | HotpotQA [Yang et al. (2018)](https://arxiv.org/html/2501.00874v1#bib.bib66) | 97,800 |\n|  | SQuAD [Rajpurkar et al. (2016)](https://arxiv.org/html/2501.00874v1#bib.bib45) | 97,400 |\n|  | FiQA [Maia et al. (2018)](https://arxiv.org/html/2501.00874v1#bib.bib35) | 6,420 |\n|  | NQ [Kwiatkowski et al. (2019)](https://arxiv.org/html/2501.00874v1#bib.bib22) | 3,420 |\n|  | ArguAna [Wachsmuth et al. (2018)](https://arxiv.org/html/2501.00874v1#bib.bib56) | 1,280 |", "caption": "Table 5: Number of samples used in each dataset for training. The number of negative samples is included in the total number of samples.", "description": "This table details the number of samples used for training the LUSIFER model in each dataset.  The count includes both positive and negative samples used during the training process. This information is crucial for understanding the scale of the training data and its potential impact on model performance.", "section": "3.2 Training Pipeline"}, {"content": "| Es Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| AmazonReviewsClassification | 42.69 | 50.41 |\n| MassiveIntentClassification | 69.67 | 68.93 |\n| MassiveScenarioClassification | 74.63 | 73.41 |\n| MTOPIntentClassification | 72.16 | 80.13 |\n| MultilingualSentimentClassification | 87.91 | 91.01 |\n| TweetSentimentClassification | 49.73 | 58.55 |\n| SpanishNewsClassification | 89.5 | 87.81 |\n| PawsXPairClassification | 61.19 | 62.82 |\n| XNLI | 77.34 | 60.49 |\n| SpanishNewsClusteringP2P | 42.28 | 43.85 |\n| MLSUMClusteringP2P | 47.54 | 44.36 |\n| MLSUMClusteringS2S | 47.11 | 41.56 |\n| SIB200ClusteringS2S | 31.01 | 44.42 |\n| MultiEURLEXMultilabelClassification | 6.16 | 3.87 |\n| BelebeleRetrieval | 83.92 | 81.4 |\n| MintakaRetrieval | 48.77 | 18.17 |\n| STS17 | 87.18 | 80.84 |\n| STS22 | 71.79 | 70.66 |\n| STSBenchmarkMultilingualSTS | 84.31 | 79.89 |\n| Avg. | 61.84 | 60.14 |", "caption": "Table 6: Detailed results of E5-Mistral and LUSIFER on the Spanish benchmark datasets.", "description": "This table presents a detailed comparison of the performance of the E5-Mistral and LUSIFER models on Spanish language benchmark datasets.  It breaks down the results for each individual dataset, showing the scores achieved by each model across various tasks, allowing for a granular analysis of model performance in a specific language.", "section": "4. Experiment"}, {"content": "| En Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| AmazonCounterfactualClassification | 78.69 | 72.45 |\n| AmazonPolarityClassification | 95.91 | 94.3 |\n| AmazonReviewsClassification | 55.79 | 55.46 |\n| Banking77Classification | 88.23 | 87.33 |\n| EmotionClassification | 49.77 | 74 |\n| ImdbClassification | 94.78 | 92.52 |\n| MassiveIntentClassification | 80.57 | 75.64 |\n| MassiveScenarioClassification | 82.39 | 78 |\n| MTOPDomainClassification | 96.12 | 96.81 |\n| MTOPIntentClassification | 86.11 | 87.34 |\n| ToxicConversationsClassification | 69.59 | 82.84 |\n| TweetSentimentExtractionClassification | 63.72 | 72.74 |\n| SprintDuplicateQuestions | 95.66 | 90.99 |\n| TwitterSemEval2015 | 81.62 | 68.49 |\n| TwitterURLCorpus | 87.75 | 85.35 |\n| ArxivClusteringP2P | 50.45 | 35.6 |\n| ArxivClusteringS2S | 45.5 | 22.25 |\n| BiorxivClusteringP2P | 43.53 | 39.93 |\n| BiorxivClusteringS2S | 40.24 | 29.3 |\n| MedrxivClusteringP2P | 38.19 | 41.2 |\n| MedrxivClusteringS2S | 37.45 | 35.53 |\n| RedditClustering | 57.71 | 39.94 |\n| RedditClusteringP2P | 66.49 | 53.4 |\n| StackExchangeClustering | 73.1 | 46.41 |\n| StackExchangeClusteringP2P | 45.91 | 39.7 |\n| TwentyNewsgroupsClustering | 54.31 | 38.5 |\n| AskUbuntuDupQuestions | 66.98 | 60.56 |\n| MindSmallReranking | 32.6 | 24.55 |\n| SciDocsRR | 86.33 | 34.94 |\n| StackOverflowDupQuestions | 54.91 | 46.04 |\n| ArguAna | 61.88 | 74.15 |\n| ClimateFEVER | 38.4 | 29.24 |\n| CQADupstackTexRetrieval | 42.97 | 23.22 |\n| DBPedia | 48.9 | 17.98 |\n| FEVER | 87.8 | 82.77 |\n| FiQA2018 | 56.62 | 14.91 |\n| HotpotQA | 75.7 | 49.04 |\n| MSMARCO | 43.1 | 56.43 |\n| NFCorpus | 38.59 | 5.48 |\n| NQ | 63.5 | 42.95 |\n| QuoraRetrieval | 89.62 | 89.1 |\n| SCIDOCS | 16.27 | 5.53 |\n| SciFact | 76.41 | 66.09 |\n| Touche2020 | 26.39 | 6.33 |\n| TRECCOVID | 87.33 | 18.22 |\n| STS12 | 79.65 | 74.26 |\n| STS13 | 88.43 | 84.2 |\n| STS14 | 84.54 | 77.5 |\n| STS15 | 90.42 | 84.95 |\n| STS16 | 87.68 | 82.21 |\n| STS17 | 91.75 | 81.67 |\n| STS22 | 67.28 | 71.25 |\n| BIOSSES | 82.64 | 84.22 |\n| SICK-R | 80.76 | 78 |\n| STSBenchmark | 88.6 | 84.18 |\n| SummEval | 31.4 | 32.36 |\n| Avg. | 67.69 | 57.20 |", "caption": "Table 7: Detailed results of E5-Mistral and LUSIFER on the English benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two models, E5-Mistral and LUSIFER, on various English language benchmark datasets.  It breaks down the results for each dataset, showing the accuracy scores achieved by each model on different tasks, including classification, clustering, retrieval, reranking, and semantic textual similarity (STS). This allows for a granular analysis of the strengths and weaknesses of each model on specific tasks and datasets.", "section": "4. Experiment"}, {"content": "| Ru Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| GeoreviewClassification | 46.92 | 43.79 |\n| HeadlineClassification | 76.52 | 79.26 |\n| InappropriatenessClassification | 59.35 | 63.15 |\n| KinopoiskClassification | 60.67 | 60.57 |\n| MassiveIntentClassification | 72.06 | 71.29 |\n| MassiveScenarioClassification | 76.64 | 74.49 |\n| RuReviewsClassification | 64.10 | 67.40 |\n| RuSciBenchGRNTIClassification | 60.19 | 59.51 |\n| RuSciBenchOECDClassification | 46.30 | 46.41 |\n| GeoreviewClusteringP2P | 69.87 | 59.20 |\n| RuSciBenchGRNTIClusteringP2P | 52.96 | 55.00 |\n| RuSciBenchOECDClusteringP2P | 46.54 | 49.95 |\n| TERRA | 57.45 | 54.24 |\n| RiaNewsRetrieval | 71.39 | 49.61 |\n| RuBQRetrieval | 38.04 | 43.48 |\n| RuSTSBenchmarkSTS | 81.79 | 78.20 |\n| STS22 | 61.32 | 61.44 |\n| Avg. | 61.30 | 59.82 |", "caption": "Table 8: Detailed results of E5-Mistral and LUSIFER on the Russian benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two embedding models, E5-Mistral and LUSIFER, across various tasks and datasets within the Russian language benchmark.  For each dataset, it shows the scores achieved by both models, offering a granular view of their relative strengths and weaknesses in different aspects of embedding tasks within the Russian language.", "section": "4.1 Benchmark"}, {"content": "| Fr Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| AmazonReviewsClassification | 43.36 | 49.96 |\n| MTOPIntentClassification | 70.39 | 79.14 |\n| MassiveIntentClassification | 71.12 | 70.88 |\n| MassiveScenarioClassification | 74.68 | 73.96 |\n| TweetSentimentClassification | 50.23 | 62.62 |\n| SIB200Classification | 72.45 | 79.51 |\n| FrenchBookReviews | 46.77 | 48.07 |\n| PawsXPairClassification | 62.15 | 65.93 |\n| RTE3 | 88.45 | 87.62 |\n| XNLI | 76.60 | 62.75 |\n| MasakhaNEWSClusteringP2P | 50.96 | 48.59 |\n| MasakhaNEWSClusteringS2S | 52.08 | 63.12 |\n| MLSUMClusteringP2P | 42.69 | 42.70 |\n| MLSUMClusteringS2S | 42.60 | 41.51 |\n| HALClusteringS2S | 24.21 | 24.16 |\n| SIB200ClusteringS2S | 29.94 | 43.30 |\n| MultiEURLEXMultilabelClassification | 5.00 | 3.51 |\n| BelebeleRetrieval | 84.66 | 83.76 |\n| MintakaRetrieval | 52.60 | 18.88 |\n| OpusparcusPC | 94.58 | 90.63 |\n| STS17 | 84.66 | 82.19 |\n| SICKFr | 79.12 | 74.22 |\n| STS22 | 76.50 | 73.77 |\n| STSBenchmarkMultilingualSTS | 83.98 | 78.42 |\n| SummEvalFr | 31.38 | 31.91 |\n| Avg. | 59.65 | 59.24 |", "caption": "Table 9: Detailed results of E5-Mistral and LUSIFER on the French benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two models, E5-Mistral and LUSIFER, across various French language benchmark datasets.  It breaks down the results for each specific dataset and task, providing a granular view of each model's strengths and weaknesses in the French language.", "section": "4. Experiment"}, {"content": "| Vi Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| MassiveIntentClassification | 66.36 | 71.38 |\n| MassiveScenarioClassification | 70.69 | 74.82 |\n| MultilingualSentimentClassification | 69.30 | 81.30 |\n| SIB200Classification | 70.20 | 78.58 |\n| VieStudentFeedbackClassification | 73.02 | 77.39 |\n| XNLI | 71.32 | 61.30 |\n| SIB200ClusteringS2S | 32.93 | 46.79 |\n| BelebeleRetrieval | 79.20 | 85.51 |\n| MLQARetrieval | 32.43 | 54.61 |\n| VieQuADRetrieval | 20.35 | 45.20 |\n| Avg. | 58.58 | 67.69 |", "caption": "Table 10: Detailed results of E5-Mistral and LUSIFER on the Vietnamese benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two embedding models, E5-Mistral and LUSIFER, across various Vietnamese benchmark datasets.  It breaks down the results for each model on specific tasks, offering a granular view of their relative strengths and weaknesses in the Vietnamese language.  The metrics used likely reflect the performance on different embedding tasks (e.g., classification, clustering, retrieval, etc.).  The table allows for a precise assessment of each model's effectiveness in handling the nuances of the Vietnamese language.", "section": "4. Experiment"}, {"content": "| Fa Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| MassiveScenarioClassification | 76.37 | 77.94 |\n| MassiveIntentClassification | 71.98 | 73.32 |\n| MultilingualSentimentClassification | 80.07 | 80.54 |\n| FarsTail | 63.49 | 67.98 |\n| WikipediaRerankingMultilingual | 75.60 | 78.75 |\n| WikipediaRetrievalMultilingual | 67.77 | 78.49 |\n| Avg. | 72.55 | 76.17 |", "caption": "Table 11: Detailed results of E5-Mistral and LUSIFER on the Farsi benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two models, E5-Mistral and LUSIFER, on various benchmark datasets for the Farsi language.  It breaks down the results for each dataset, showing the scores achieved by each model on specific tasks. This allows for a granular analysis of the relative strengths and weaknesses of each model in handling the nuances of the Farsi language, contributing to a comprehensive evaluation of multilingual embedding capabilities.", "section": "4.1 Benchmark"}, {"content": "| Id Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| IndonesianMongabayConservationClassification | 24.72 | 25.27 |\n| MassiveIntentClassification | 69.51 | 71.38 |\n| MassiveScenarioClassification | 72.89 | 74.62 |\n| SIB200Classification | 80.88 | 80.44 |\n| indonli | 50.00 | 50.22 |\n| SIB200ClusteringS2S | 46.46 | 47.50 |\n| BelebeleRetrieval | 81.10 | 87.56 |\n| SemRel24STS | 40.40 | 40.57 |\n| Avg. | 58.25 | 59.70 |", "caption": "Table 12: Detailed results of E5-Mistral and LUSIFER on the Indonesian benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two embedding models, E5-Mistral and LUSIFER, across various Indonesian benchmark datasets.  It breaks down the results for each task (classification, clustering, retrieval, reranking, and semantic textual similarity) to provide a comprehensive evaluation of each model's strengths and weaknesses on Indonesian language data. The inclusion of multiple datasets allows for a more robust assessment of the models' generalizability and performance across various scenarios.", "section": "4. Experiment"}, {"content": "| Ar Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| TweetEmotionClassification | 53.74 | 49.03 |\n| ArEntail | 77.63 | 84.15 |\n| XNLI | 68.00 | 58.58 |\n| MintakaRetrieval | 17.15 | 16.59 |\n| MLQARetrieval | 28.32 | 47.90 |\n| STS17 | 75.13 | 71.44 |\n| STS22 | 61.01 | 61.54 |\n| Avg. | 54.43 | 55.60 |", "caption": "Table 13: Detailed results of E5-Mistral and LUSIFER on the Arabic benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two models, E5-Mistral and LUSIFER, on Arabic language benchmark datasets.  It breaks down the results for various tasks such as classification, clustering, retrieval, reranking, and semantic textual similarity (STS).  Each task will have associated metrics to show the numerical performance results of the models. This allows for a granular understanding of each model's strengths and weaknesses when processing Arabic text.", "section": "4. Experiment"}, {"content": "| Fi Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| FinToxicityClassification | 53.78 | 62.23 |\n| MassiveIntentClassification | 64.15 | 70.77 |\n| MassiveScenarioClassification | 67.79 | 75.02 |\n| MultilingualSentimentClassification | 72.42 | 83.59 |\n| SIB200Classification | 66.57 | 77.06 |\n| WikipediaRerankingMultilingual | 86.85 | 82.65 |\n| BelebeleRetrieval | 73.89 | 85.18 |\n| WikipediaRetrievalMultilingual | 71.90 | 82.94 |\n| OpusparcusPC | 91.41 | 91.63 |\n| FinParaSTS | 20.97 | 17.24 |\n| Avg. | 66.97 | 72.83 |", "caption": "Table 14: Detailed results of E5-Mistral and LUSIFER on the Finnish benchmark datasets.", "description": "This table presents a detailed comparison of the performance of the E5-Mistral and LUSIFER models on a range of benchmark datasets specifically designed for the Finnish language.  It provides a granular view of each model's effectiveness across various tasks, offering insights into their strengths and weaknesses when processing Finnish text data.  The results are crucial for evaluating the multilingual capabilities of the models, especially in a language with potentially limited resources.", "section": "4.1 Benchmark"}, {"content": "| Ko Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| MassiveIntentClassification | 70.42 | 69.79 |\n| MassiveScenarioClassification | 75.12 | 75.60 |\n| KorSarcasmClassification | 57.64 | 55.28 |\n| SIB200Classification | 72.70 | 77.89 |\n| KorHateSpeechMLClassification | 8.49 | 7.54 |\n| PawsXPairClassification | 53.10 | 54.97 |\n| KLUE-TC | 60.58 | 63.95 |\n| SIB200ClusteringS2S | 31.04 | 46.58 |\n| Ko-StrategyQA | 63.81 | 68.66 |\n| BelebeleRetrieval | 80.09 | 84.69 |\n| KLUE-STS | 83.48 | 84.17 |\n| KorSTS | 79.28 | 78.36 |\n| STS17 | 80.97 | 80.55 |\n| Avg. | 62.82 | 65.23 |", "caption": "Table 15: Detailed results of E5-Mistral and LUSIFER on the Korean benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two embedding models, E5-Mistral and LUSIFER, on a range of Korean benchmark datasets.  Each dataset represents a different type of natural language processing task (classification, clustering, etc.).  The table allows readers to assess the relative strengths and weaknesses of each model on various tasks, highlighting how well each model performs in the context of a specific Korean dataset.", "section": "4.1 Benchmark"}, {"content": "| Hi Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| MTOPIntentClassification | 68.84 | 79.93 |\n| SentimentAnalysisHindi | 58.98 | 73.92 |\n| MassiveIntentClassification | 64.69 | 71.01 |\n| MassiveScenarioClassification | 69.71 | 75.42 |\n| SIB200Classification | 68.43 | 75.98 |\n| TweetSentimentClassification | 37.70 | 40.78 |\n| XNLI | 65.04 | 60.26 |\n| IndicReviewsClusteringP2P | 40.04 | 42.40 |\n| SIB200ClusteringS2S | 27.32 | 45.62 |\n| WikipediaRerankingMultilingual | 85.22 | 78.17 |\n| BelebeleRetrieval | 69.73 | 66.76 |\n| MintakaRetrieval | 18.60 | 21.53 |\n| MLQARetrieval | 35.37 | 54.54 |\n| WikipediaRetrievalMultilingual | 74.62 | 75.25 |\n| IndicCrosslingualSTS | 42.30 | 58.97 |\n| SemRel24STS | 73.14 | 77.34 |\n| Avg. | 56.23 | 62.37 |", "caption": "Table 16: Detailed results of E5-Mistral and LUSIFER on the Hindi benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two embedding models, E5-Mistral and LUSIFER, across various Hindi language benchmark datasets.  The datasets cover a range of tasks including classification, clustering, reranking, retrieval, and semantic textual similarity (STS). For each dataset, the table shows the average performance score achieved by each model. This allows for a granular assessment of the relative strengths and weaknesses of both models in the context of Hindi language processing.", "section": "4. Experiment"}, {"content": "| Bn Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| BengaliDocumentClassification | 50.78 | 48.00 |\n| BengaliHateSpeechClassification | 54.67 | 51.43 |\n| MassiveIntentClassification | 59.51 | 66.65 |\n| MassiveScenarioClassification | 64.57 | 70.91 |\n| XNLIV2 | 63.66 | 60.01 |\n| IndicReviewsClusteringP2P | 38.20 | 45.68 |\n| SIB200ClusteringS2S | 23.88 | 43.96 |\n| WikipediaRerankingMultilingual | 82.66 | 76.39 |\n| BelebeleRetrieval | 60.17 | 55.77 |\n| IndicQARetrieval | 56.59 | 68.06 |\n| WikipediaRetrievalMultilingual | 71.05 | 72.47 |\n| IndicCrosslingualSTS | 35.42 | 41.86 |\n| Avg. | 55.10 | 58.43 |", "caption": "Table 17: Detailed results of E5-Mistral and LUSIFER on the Bengali benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two embedding models, E5-Mistral and LUSIFER, on a series of benchmark datasets specifically for the Bengali language.  It offers a granular view of their performance across various tasks, providing insights into their strengths and weaknesses in handling the Bengali language.", "section": "4. Experiment"}, {"content": "| Te Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| IndicNLPNewsClassification | 89.46 | 98.90 |\n| IndicSentimentClassification | 61.53 | 90.63 |\n| MassiveIntentClassification | 47.34 | 68.69 |\n| MassiveScenarioClassification | 51.67 | 74.17 |\n| SIB200Classification | 46.23 | 74.56 |\n| TeluguAndhraJyotiNewsClassification | 67.40 | 76.24 |\n| IndicReviewsClusteringP2P | 34.02 | 43.62 |\n| SIB200ClusteringS2S | 10.81 | 42.11 |\n| BelebeleRetrieval | 42.46 | 80.32 |\n| IndicQARetrieval | 33.67 | 57.61 |\n| IndicCrosslingualSTS | 8.36 | 43.76 |\n| SemRel24STS | 72.83 | 80.99 |\n| Avg. | 47.15 | 69.30 |", "caption": "Table 18: Detailed results of E5-Mistral and LUSIFER on the Telugu benchmark datasets.", "description": "This table presents a detailed comparison of the performance of the E5-Mistral and LUSIFER models on a range of Telugu language benchmark datasets.  It breaks down the results for each model across various embedding tasks, offering a granular view of their relative strengths and weaknesses on this specific language.  The results may include metrics like accuracy, precision, recall, F1-score, and others depending on the specific tasks in the benchmark.", "section": "4. Experiment"}, {"content": "| Sw Datasets | E5-Mistral | LUSIFER |\n|---|---|---|\n| AfriSentiClassification | 39.67 | 46.47 |\n| MasakhaNEWSClassification | 72.96 | 74.79 |\n| MassiveIntentClassification | 52.84 | 52.79 |\n| MassiveScenarioClassification | 61.09 | 58.59 |\n| SwahiliNewsClassification | 63.95 | 61.56 |\n| XNLI | 58.86 | 57.82 |\n| MasakhaNEWSClusteringP2P | 34.15 | 36.95 |\n| MasakhaNEWSClusteringS2S | 21.34 | 35.97 |\n| Avg. | 50.61 | 53.12 |", "caption": "Table 19: Detailed results of E5-Mistral and LUSIFER on the Swahili benchmark datasets.", "description": "This table presents a detailed comparison of the performance of two models, E5-Mistral and LUSIFER, on a series of benchmark datasets specifically designed for the Swahili language.  The datasets encompass various natural language processing tasks, allowing for a comprehensive evaluation of each model's capabilities in understanding and processing Swahili text. The results offer insights into the strengths and weaknesses of each model in handling the nuances of the Swahili language, providing valuable information for researchers and developers working with multilingual natural language processing models.", "section": "4. Experiment"}]