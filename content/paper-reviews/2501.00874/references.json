{"references": [{"fullname_first_author": "Alexis Conneau", "paper_title": "Unsupervised cross-lingual representation learning at scale", "publication_date": "2020-00-00", "reason": "This paper introduces XLM-R, a multilingual encoder model crucial to LUSIFER's architecture, providing language-agnostic representations for various languages."}, {"fullname_first_author": "Nils Reimers", "paper_title": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks", "publication_date": "2019-00-00", "reason": "This work established Sentence-BERT, a foundational model for sentence embeddings that LUSIFER builds upon and improves with its multilingual approach."}, {"fullname_first_author": "Liang Wang", "paper_title": "Improving text embeddings with large language models", "publication_date": "2024-00-00", "reason": "This paper presents E5-Mistral, a strong English-centric embedding model which LUSIFER enhances with multilingual capabilities, serving as a key comparison point for performance evaluation."}, {"fullname_first_author": "Telmo Pires", "paper_title": "How multilingual is multilingual BERT?", "publication_date": "2019-00-00", "reason": "This paper investigates the multilingual capabilities of BERT, a relevant topic given LUSIFER's focus on enhancing multilingual embeddings and zero-shot adaptation."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-00-00", "reason": "The BERT model is a fundamental model in language representation; this paper's introduction of BERT lays the groundwork for many subsequent embedding models, including those that LUSIFER builds upon."}]}