[{"figure_path": "https://arxiv.org/html/2504.07089/x3.png", "caption": "Figure 1: OmniCaptioner: the top section demonstrates its capability to process diverse visual domains. The bottom section highlights its applications in visual reasoning (associated with reasoning LLM), image generation (integrated with T2I generation models), and efficient downstream SFT tasks adaptation.", "description": "The figure showcases OmniCaptioner's versatility in handling diverse visual data types, ranging from natural images and visual text (like posters and UIs) to structured visuals (tables, charts, and diagrams).  The top part visually demonstrates this capability across various domains. The bottom part illustrates three key applications: visual reasoning (leveraging the power of LLMs), image generation (integrated with text-to-image models), and efficient supervised fine-tuning (SFT) for downstream tasks. This highlights OmniCaptioner's ability to bridge the gap between visual and textual modalities.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x4.png", "caption": "Figure 2: Performance comparison across different visual benchmarks for different LLMs/MLLMs (7B) with or without visual input. The bar with dashed borders denotes Qwen2-VL-Instruct, indicating it has pixel-level visual input, while others do not. Qwen2-VL-Ins.(NA) refers to a setting where only the question is provided as input. We divide the MME score by 100 to have the same scale as other benchmarks.", "description": "This figure showcases a performance comparison of various Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), all with 7 billion parameters, across several visual reasoning benchmarks.  The models are tested both with and without direct pixel-level visual input.  The key takeaway is the performance difference between models that receive raw visual data (Qwen2-VL-Instruct, indicated by dashed borders) and those that only receive textual descriptions of the visual data or no visual input at all (Qwen2-VL-Ins.(NA) means no visual data, only the question). The MME scores are scaled down by a factor of 100 for consistency across all benchmarks.", "section": "4 One Captioner to Rule Them All"}, {"figure_path": "https://arxiv.org/html/2504.07089/x5.png", "caption": "Figure 3: Illustration of OmniCaptioner\u2019s plug-and-play applications (Sub-figure a, b) and comparison between OmniCaptioner and LLava-OneVision-7B on non-natural image captioning (Sub-figure c). Sub-figure (a) shows that OmniCaptioner leverages LLMs\u2019 strong reasoning abilities to perform multimodal reasoning tasks. Sub-figure (b) highlights how hallucinated or inaccurate captions\u2014like those from LLava-OneVision-7B can lead to inconsistent image conversion, revealing weakened alignment capabilities in text-to-image models when captions don\u2019t faithfully represent the original content. Sub-figure (c) highlights that LLaVA-OneVision-7B, due to limited exposure to non-natural images during pretraining, struggles with perception in such domains, often leading to hallucinations, whereas OmniCaptioner provides more accurate descriptions.", "description": "Figure 3 demonstrates OmniCaptioner's versatility and effectiveness in three key areas: visual reasoning, image generation, and non-natural image captioning.  (a) showcases OmniCaptioner's enhanced visual reasoning capabilities by integrating it with LLMs (Large Language Models).  Detailed captions generated by OmniCaptioner allow LLMs to effectively solve complex visual reasoning problems. (b) illustrates the importance of accurate image captions for successful image generation.  Using inaccurate or hallucinated captions (like those from LLaVA-OneVision-7B) results in poor image generation due to weakened alignment between the text and visual modalities. (c) compares OmniCaptioner's and LLaVA-OneVision-7B's performance on non-natural images. OmniCaptioner significantly outperforms LLaVA-OneVision-7B, providing more accurate image descriptions and reducing hallucinations because OmniCaptioner was trained with a wider variety of visual domains during pretraining.", "section": "One Captioner to Rule Them All"}, {"figure_path": "https://arxiv.org/html/2504.07089/x6.png", "caption": "Figure 4: OmniCaptioner\u2019s diverse visual captioning pipeline.\nThe pipeline consists of Seed-Caption Generation to ensure precise pixel-to-word mapping, and Caption Extension to enrich caption styles to support image generation and visual reasoning tasks. OmniCaptioner utilizes a 21M-caption dataset, covering diverse domains beyond natural images, enabling more comprehensive captioning capabilities. For further details about dataset composition, please refer to Fig.\u00a07 in Appendix\u00a0A.", "description": "OmniCaptioner employs a two-stage visual captioning pipeline.  First, Seed-Caption Generation creates initial, highly accurate captions by mapping image pixels to words. This stage uses a large language model (LLM) to generate comprehensive descriptions for various visual data types, including natural images and structured images. The second stage, Caption Extension, refines and diversifies these captions, generating multiple caption styles (e.g., short, detailed, tagged, bilingual) to be suitable for a wide range of downstream tasks such as image generation and visual reasoning with LLMs. The entire process utilizes a large-scale, multi-domain dataset (21 million captions) that goes beyond typical natural image datasets. This dataset encompasses various visual data modalities including natural images, structured data (e.g., tables, charts), and visual text (e.g., posters, UI).  More detail on dataset composition is available in Figure 7 of Appendix A.", "section": "3 OMNICAPTIONER"}, {"figure_path": "https://arxiv.org/html/2504.07089/x7.png", "caption": "Figure 5: Integrate OmniCaptioner into different versions of LLMs, enabling them to handle tasks in multimodal scenarios.", "description": "Figure 5 shows the performance of integrating OmniCaptioner with different versions of LLMs on various multimodal tasks.  The graph displays how the model's performance (likely measured by accuracy or a similar metric) improves as more advanced LLMs are used. This demonstrates OmniCaptioner's ability to enhance the performance of LLMs across different model sizes and complexities, making them better suited for complex tasks that involve both visual and textual information. The x-axis likely represents different LLM versions or sizes, while the y-axis depicts the performance metric.", "section": "4 One Captioner to Rule Them All"}, {"figure_path": "https://arxiv.org/html/2504.07089/extracted/6349016/Figures/token_len_dis.jpg", "caption": "Figure 6: Token length distribution for natural images.", "description": "This figure shows the distribution of token lengths across different caption types for natural images.  The x-axis represents the token length, and the y-axis represents the frequency of captions with that length.  Separate bars are shown for different caption styles: \"Detailed Caption\", \"Medium Caption\", \"Short Caption\", and \"Tag\". This visualization helps to understand the range of caption lengths generated and the relative frequency of each caption style within the OMNICAPTIONER dataset.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x8.png", "caption": "Figure 7: Dataset composition for pretraining OmniCaptioner.", "description": "Figure 7 of the paper visualizes the composition of the OmniCaptioner dataset used for pre-training the model.  The dataset is a large-scale multimodal benchmark covering a diverse range of visual data types, including natural images, structured images (tables, charts, equations, geometric diagrams), visual text images (posters, UIs, textbooks), and videos. The figure likely shows the distribution of these data types within the dataset, perhaps illustrating the number of samples per category or the relative proportions.  The goal is to demonstrate the breadth and diversity of visual data included in the training process to support OmniCaptioner's ability to generate captions for a wide array of visual inputs.", "section": "3 OMNICAPTIONER"}, {"figure_path": "https://arxiv.org/html/2504.07089/x9.png", "caption": "Figure 8: Different system prompts used for OmniCaptioner.", "description": "Figure 8 shows different system prompts used to guide OmniCaptioner's caption generation for various image types (natural images, visual text images, and structured images).  These prompts specify the desired caption style (detailed, medium, short, tag, or to LaTeX/Markdown), and guide the model to provide specific details such as visual style, spatial relationships, or object descriptions.  The prompts are categorized by image type, providing examples of how the prompts are tailored for diverse visual inputs.  The figure is crucial in illustrating how OmniCaptioner's versatility is achieved through careful prompt engineering.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x10.png", "caption": "Figure 9: Natural image captioning.", "description": "This figure showcases examples of natural image captions generated by the OmniCaptioner model.  It demonstrates the model's ability to produce captions with varying levels of detail (short, medium, detailed, and tag captions) across diverse image subjects. Each image is accompanied by several captions illustrating different styles and levels of detail, highlighting the model's versatility and comprehensive understanding of the visual content.  These captions range from short and concise summaries to long, detailed descriptions covering multiple aspects, such as visual style, composition, and background elements, and are presented in both English and Chinese, showcasing the model's multilingual capabilities.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x11.png", "caption": "Figure 10: Table/Chart image captioning.", "description": "Figure 10 shows examples of captions generated for table and chart images.  The top example shows a caption for a table listing various racing teams, chassis types, engines, tire brands, and drivers. The caption includes a detailed breakdown of the table's columns, a description of the data, observations about trends in the data, and a summary of the key findings. The bottom example shows a caption for a line chart illustrating the accuracy of two different methods ('Few-Shot-CoT' and 'Auto-CoT') as the number of steps in an algorithm increases. The caption explains the axes labels and data series, provides observations about the trends in the data, and offers conclusions on the results.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x12.png", "caption": "Figure 11: Visual-Text image captioning.", "description": "Figure 11 presents example outputs from the OmniCaptioner model for visual-text images.  The figure shows two examples of posters, each with both English and Chinese captions generated by the OmniCaptioner in different captioning styles (detailed and OCR). These examples demonstrate OmniCaptioner's capability to handle diverse visual-text inputs and generate both detailed descriptions and textual extractions (like OCR).", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x13.png", "caption": "Figure 12: Math image captioning.", "description": "Figure 12 presents examples of math image captioning.  It shows three different mathematical expressions or diagrams and the corresponding captions generated by the OmniCaptioner system.  The first example is a complex equation incorporating trigonometric functions and integrals. The second example includes a geometric figure illustrating a triangle inscribed in a circle, and the third is another geometric illustration of two similar triangles. The captions demonstrate the system's ability to generate detailed, accurate descriptions of mathematical content in images.", "section": "3 OMNICAPTIONER"}, {"figure_path": "https://arxiv.org/html/2504.07089/x14.png", "caption": "Figure 13: UI captioning.", "description": "The figure displays a screenshot of a Hugging Face profile page for the DeepSeek organization.  The page shows details like their AI and ML interests, recent activities (model updates), a list of their model collections, and team members. The design is clean and user-friendly, using clear labels and organization to present information concisely.  Interactive elements such as buttons, links, and expandable sections allow for a deeper dive into DeepSeek's work.", "section": "3 OMNICAPTIONER"}, {"figure_path": "https://arxiv.org/html/2504.07089/x15.png", "caption": "Figure 14: PDF captioning.", "description": "Figure 14 presents example outputs from the OMNICAPTIONER model applied to PDF documents.  The figure demonstrates the model's ability to generate captions for diverse PDF content, including diagrams, text, and tables.  Each example shows a snippet of a PDF page alongside the generated caption, highlighting the model's ability to extract relevant information and create detailed and accurate descriptions.  The captions showcase both short and detailed options, illustrating the model's adaptability to different output styles.", "section": "3 OMNICAPTIONER"}, {"figure_path": "https://arxiv.org/html/2504.07089/x16.png", "caption": "Figure 15: Video captioning.", "description": "Figure 15 shows examples of video captioning results from the OMNICAPTIONER model.  The figure displays two example video clips, each with detailed captions. The captions demonstrate the model's ability to generate descriptions that encompass short, background, main object, reference, standard summary, style, and key tags and camera information.  These captions are comprehensive and detailed, capturing various aspects of the visual content, including the scene, objects, actions, and overall mood.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x17.png", "caption": "Figure 16: Natural image captioning with different system prompts.", "description": "Figure 16 showcases examples of natural image captioning using different system prompts within the OmniCaptioner framework.  It demonstrates how variations in the prompt engineering (detailed, detailed_natural, UI, OCR_textqa) result in different caption styles and levels of detail for the same image.  This highlights OmniCaptioner's ability to generate varied and contextually appropriate descriptions based on user-specified prompt characteristics.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x18.png", "caption": "Figure 17: Structured image captioning with different system prompts.", "description": "Figure 17 presents three examples of structured image captioning using different system prompts.  Each example shows a different type of structured image: an electrical circuit diagram, a mathematical function graph, and a mechanical system diagram. The captions generated by the OMNICAPTIONER model provide highly detailed descriptions of these diagrams, illustrating the model's ability to understand and translate complex visual information into rich textual descriptions.", "section": "3.2 Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.07089/x19.png", "caption": "Figure 18: Visualization of thinking process with OmniCaptioner for natural images.", "description": "Figure 18 presents three examples of how the OmniCaptioner model processes natural images and generates captions.  Each example shows the image, the generated caption by OmniCaptioner, the question posed, and the model's step-by-step reasoning process (thought bubbles) leading to the final answer.  The examples demonstrate OmniCaptioner's ability to perform visual reasoning tasks with diverse image content and question types.", "section": "4 One Captioner to Rule Them All"}, {"figure_path": "https://arxiv.org/html/2504.07089/x20.png", "caption": "Figure 19: Visualization of thinking process with OmniCaptioner.", "description": "Figure 19 visualizes the reasoning process of DeepSeek-R1-Distill-Qwen-70B and DeepSeek-R1-Distill-LLama-70B when answering questions related to images.  The figure displays the step-by-step thinking process of each LLM to solve three different problems (one natural image, one structured image, one visual-text image). For each problem, the figure presents the initial question, the detailed caption generated by OmniCaptioner, and the LLM's step-by-step reasoning process, displayed in text, leading to a final answer.  The visualization aims to demonstrate how OmniCaptioner's detailed captions enhance the performance and reasoning ability of LLMs on visual reasoning tasks.", "section": "5.1 Main Results"}, {"figure_path": "https://arxiv.org/html/2504.07089/x21.png", "caption": "Figure 20: Visualization of thinking process with OmniCaptioner for math images.", "description": "Figure 20 presents a visualization of how the OmniCaptioner model approaches math-related image understanding.  It shows the step-by-step reasoning process employed by the model when presented with a geometry problem (calculating a tree's height using similar triangles). The figure displays the original problem image and the model's detailed thought process (in English), which involves recognizing geometric properties, formulating equations, solving for the unknown, and identifying the correct option (from multiple-choice answers). This figure highlights the model's ability to break down complex problems, reason logically, and arrive at the correct solution, mimicking human problem-solving strategies.", "section": "Improved Visual Reasoning Tasks with LLMs"}, {"figure_path": "https://arxiv.org/html/2504.07089/x22.png", "caption": "Figure 21: The detailed caption from OmniCaptioner enhances the alignment capability of text-to-image generation by providing precise descriptions, ensuring that the generated image accurately reflects the intended concepts, attributes, and relationships. The generation model here is fine-tuned on images labeled by OmniCaptioner, using the SANA 1.0 model with 1.6B parameters.", "description": "This figure demonstrates the impact of detailed image captions generated by OmniCaptioner on the quality of images generated by a text-to-image model (SANA 1.0).  The top row shows example images and captions. Each image was generated from a text prompt paired with a detailed caption from OmniCaptioner.  The bottom row shows the images generated by the SANA 1.0 model when using only the original prompt without the added OmniCaptioner caption.  The comparison reveals that OmniCaptioner's detailed captions significantly improve the alignment of the generated image with the prompt's intent, providing accurate reflection of concepts, attributes, and relationships mentioned in the prompt.", "section": "5.1 Main Results"}, {"figure_path": "https://arxiv.org/html/2504.07089/x23.png", "caption": "Figure 22: Image Conversion through OmniCaptioner and SANA-1.0. The generation model, SANA-1.0, is fine-tuned on images annotated by OmniCaptioner, enabling more accurate and semantically aligned image generation.", "description": "This figure showcases examples of image generation using the SANA-1.0 model fine-tuned with captions generated by OmniCaptioner.  The left column shows the original images, the middle column displays the detailed captions generated by OmniCaptioner which are rich in detail and semantic information, and the right column shows the images generated by SANA-1.0 based on these detailed captions. The results demonstrate that OmniCaptioner's detailed captions improve the accuracy and semantic alignment of the generated images, ensuring that they closely reflect the content and style described in the captions. This highlights the effectiveness of OmniCaptioner in bridging the gap between visual and textual modalities in image generation tasks.", "section": "5.1 Main Results"}]