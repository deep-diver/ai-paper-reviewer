[{"figure_path": "https://arxiv.org/html/2501.03841/x1.png", "caption": "Figure 1: \nOverview framework.\nGiven instruction and RGB-D observation marked by VFM, VLM firstly filters task-related objects and partitions the task into stages. For each stage, VLM extracts object-centric canonical interaction primitives as spatial constraints in a closed-loop manner. For execution, the trajectory is optimized by constraints and updated in a closed loop using a 6D Pose Tracker.", "description": "The OmniManip system uses a dual closed-loop approach for robotic manipulation.  First, Vision Foundation Models (VFM) and Vision-Language Models (VLM) process RGB-D input to identify relevant objects and divide the task into stages. For each stage, the VLM extracts object-centric interaction primitives, defining them as spatial constraints within a closed loop. This involves rendering, resampling, and VLM validation to refine these constraints. Finally, during execution, the optimized trajectory is updated in another closed loop using a 6D Pose Tracker, ensuring real-time control and robustness.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.03841/x2.png", "caption": "Figure 2: Interaction points generation.", "description": "Figure 3 illustrates the process of generating interaction points for robotic manipulation tasks.  It shows how interaction points are categorized as either visible and tangible (easily seen and grasped, like a teapot handle) or invisible and intangible (not directly visible, like the center of a cup's opening). The figure visually explains how the system identifies these points, which are crucial for defining interaction primitives (combinations of points and directions that represent how the robot will interact with objects).  Different colored points represent different interaction types (e.g., 'Grasp', 'Pour', 'Place'), and their positions are shown in relation to the objects. This process is fundamental to OmniManip's object-centric approach for bridging high-level reasoning and low-level robotic actions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.03841/x3.png", "caption": "Figure 3: Interaction directions extraction.", "description": "This figure details how interaction directions are extracted within the OmniManip system.  It visually depicts how the Vision-Language Model (VLM) and Large Language Model (LLM) collaborate to determine the most relevant interaction directions for a given object and task.  The principal axes of the object are considered as candidate directions.  Each axis is described semantically by the VLM, and these descriptions are then scored by the LLM based on relevance to the task. This process results in a ranked list of directions, prioritizing those most suitable for the manipulation task.", "section": "3.2. Primitives and Constraints Extraction"}, {"figure_path": "https://arxiv.org/html/2501.03841/x4.png", "caption": "Figure 4: \nStability analysis of interaction primitives.\nVisualization of planning and corresponding execution results across different methods, demonstrated using the \u2018Pour tea\u2019 as a case study.", "description": "This figure compares the stability of interaction primitives generated by different methods across various viewpoints. It uses the 'Pour tea' task as an example.  The visualization shows the planned actions and the actual execution results for each method, allowing for a direct comparison of their robustness and reliability in handling variations in viewpoint or unexpected situations.  The differences highlight the strengths and weaknesses of each method's ability to maintain consistent and accurate manipulation even when presented with different perspectives of the scene.", "section": "4.3 Core Attributes of OmniManip"}, {"figure_path": "https://arxiv.org/html/2501.03841/x5.png", "caption": "Figure 5: Qualitative analysis of the impact of viewpoints on the performance, using \u2018Recycle the battery\u2019 as a case study.", "description": "This figure displays a qualitative comparison of how different viewpoints affect the performance of three methods (OmniManip, ReKep, and CoPa) on the 'Recycle the battery' task.  It visually demonstrates that OmniManip's performance remains consistent across various viewpoints (0\u00b0, 45\u00b0, and 90\u00b0), while ReKep's success rate is significantly impacted by the viewing angle, highlighting the robustness of OmniManip's object-centric approach.", "section": "4.3 Core Attributes of OmniManip"}, {"figure_path": "https://arxiv.org/html/2501.03841/x6.png", "caption": "Figure 6: Closed-planning. Self-correction mechanism via RRC.", "description": "This figure illustrates the closed-loop planning process within OmniManip's framework, specifically highlighting the self-correction mechanism via Resampling, Rendering, and Checking (RRC).  The process starts with initial interaction primitives and constraints.  These are rendered, checked for validity by the Vision-Language Model (VLM), and refined iteratively via resampling if necessary. This iterative refinement ensures the generated plan accurately reflects the real-world scene. The figure demonstrates this iterative refinement and eventual success, showing the steps from the initial scene, through the self-correction steps using RRC, and finally to the successful execution. This contrasts with open-loop systems, which lack this iterative refinement and are prone to failure.", "section": "3.3 Dual Closed-Loop System"}, {"figure_path": "https://arxiv.org/html/2501.03841/x7.png", "caption": "Figure 7: Two typical failure cases without closed-loop execution.", "description": "This figure showcases two common scenarios where a robotic manipulation task fails without the implementation of closed-loop execution. In the left panel, we see that during the interaction between the robot's end-effector and the object, the relative pose between them changes. This change in pose was not accounted for in the open-loop planning, resulting in failure. The right panel depicts another failure where the object's target pose is dynamic. For example, the object moves during the task, leading to an unexpected interaction and causing the task to fail.", "section": "4.3 Core Attributes of OmniManip"}]