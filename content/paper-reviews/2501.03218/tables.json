[{"content": "| Method | LLM Size | Frames | EgoSchema | MLVU | VideoMME |\n|---|---|---|---|---|---| \n| Video-LLaVA [35] | 7B | 8 | 38.4 | 47.3 | 39.9 |\n| Chat-UniVi [28] | 7B | 64 | - | - | 40.6 |\n| LLaMA-VID [34] | 7B | 1 FPS | 38.5 | 33.2 | - |\n| TimeChat [53] | 7B | 96 | 33.0 | 30.9 | 30.2 |\n| MovieChat [55] | 7B | 2048 | 53.5 | 25.8 | 38.2 |\n| Video-LLaMA2 [9] | 7B | 16 | 51.7 | 48.5 | 47.9 |\n| LLaVA-Next-Video [74] | 7B | 32 | 43.9 | - | 46.6 |\n| ShareGPT4Video [6] | 8B | 16 | - | 46.4 | 39.9 |\n| VideoChat2 [32] | 7B | 16 | 54.4 | 47.9 | 39.5 |\n| LongVA [73] | 7B | 128 | - | 56.3 | 52.6 |\n| Kangaroo [41] | 8B | 64 | - | 61.0 | 56.0 |\n| Video-CCAM [18] | 14B | 96 | - | 63.1 | 53.2 |\n| VideoXL [54] | 7B | 128 | - | 64.9 | 55.5 |\n| **Dispider (ours)** | 7B | 1 FPS | 55.6 | 61.7 | 57.2 |", "caption": "Table 1: Performance comparison on StreamingBench on Omni-source Understanding, Contextual Understanding, and Real-Time Visual Understanding. Omni-source Understanding includes Emotion Recognition (ER), Scene Understanding (SCU), Source Discrimination (SD), and Multimodal Alignment (MA). Contextual Understanding includes Misleading Context Understanding (MCU), Anomaly Context Understanding (ACU), Sequential Question Answering (SQA) and Proactive Output (PO). Real-Time Visual Understanding includes Object Perception (OP), Causal Reasoning (CR), Clips Summarization (CS), Attribute Perception (ATP), Event Understanding (EU), Text-Rich Understanding (TR), Prospective Reasoning (PR), Spatial Understanding (SU), Action Perception (ACP), and Counting (CT). Results are categorized into Human, Proprietary MLLMs, and Open-Source MLLMs for a comprehensive evaluation.", "description": "Table 1 presents a comprehensive comparison of various models' performance on the StreamingBench benchmark, which evaluates three key aspects of video understanding: Omni-source Understanding (evaluating the model's ability to understand various aspects of the video content, including emotion, scene, source, and multimodal alignment), Contextual Understanding (testing the model's capability in understanding misleading and anomalous contexts, sequential questions, and proactive outputs), and Real-Time Visual Understanding (assessing the model's proficiency in perceiving objects, causal reasoning, summarizing clips, and understanding attributes, events, text, spatial relationships, actions, and counting). The models are categorized into three groups (Human, Proprietary LLMs, Open-Source LLMs) for a complete and fair comparison.", "section": "4. Experiments"}, {"content": "| Method | Frames | TVG<sub>F1</sub> | EPM<sub>F1</sub> | TAL<sub>F1</sub> | VHD<sub>F1</sub> | DVC<sub>F1</sub> | DVC<sub>Sim</sub> | SLC<sub>F1</sub> | SLC<sub>Sim</sub> |\n|---|---|---|---|---|---|---|---|---|---| \n| *Conventional video QA inference.* |  |  |  |  |  |  |  |  |  |\n| *w/ specialized time tokens* |  |  |  |  |  |  |  |  |  |\n| VTG-LLM [24] | 96 | 15.9 | 3.7 | 14.4 | 48.2 | 40.2 | 18.6 | 20.8 | 14.4 |\n| LITA [26] | 100 | 22.2 | 4.6 | 18.0 | 23.9 | 39.7 | 17.2 | 21.0 | 12.2 |\n| ETChat [42] | 1 FPS | 38.6 | 10.2 | 30.8 | 62.5 | 38.4 | 19.7 | 24.4 | 14.6 |\n| \\hdashline \u00a0\u00a0\u00a0*w/o specialized time tokens* |  |  |  |  |  |  |  |  |  |\n| VideoChatGPT [43] | 100 | 7.0 | 1.3 | 15.1 | 28.8 | 8.8 | 11.3 | 5.7 | 10.2 |\n| Video-LLaVA [35] | 8 | 7.0 | 1.9 | 15.0 | 28.9 | 28.0 | 15.0 | 0.9 | 8.3 |\n| LLaMA-VID [34] | 1 FPS | 5.5 | 1.2 | 8.0 | 30.0 | 27.1 | 12.6 | 5.2 | 11.1 |\n| Video-LLaMA2 [9] | 8 | 0.1 | 0.0 | 0.0 | 1,5 | 0.6 | 14.5 | 0.0 | 15.2 |\n| PLLaVA [63] | 16 | 6.9 | 1.1 | 5.7 | 28.9 | 13.3 | 10.6 | 9.7 | 11.8 |\n| VTimeLLM [25] | 100 | 7.6 | 1.9 | 18.2 | 28.9 | 12.4 | 13.1 | 8.7 | 6.4 |\n| TimeChat [53] | 96 | 26.2 | 3.9 | 10.1 | 40.5 | 16.6 | 12.5 | 5.6 | 9.2 |\n| **Dispider (ours)** | 1 FPS | 43.6 | 17.2 | 29.9 | 51.5 | 31.6 | 17.8 | 14.1 | 11.7 |\n| *Streaming video QA inference.* |  |  |  |  |  |  |  |  |  |\n| VideoLLM-Online [5] | 2 FPS | 13.2 | 3.8 | 9.1 | 22.4 | 24.0 | 13.4 | 9.9 | 10.1 |\n| **Dispider (ours)** | 1 FPS | 36.1 | 15.5 | 27.3 | 54.2 | 33.8 | 18.9 | 18.8 | 12.4 |", "caption": "Table 2: Comparison on long video benchmarks. We report the accuracy on the EgoSchema full set, MLVU multiple-choice questions, and the VideoMME overall set without subtitles. For a fair comparison, we also present the model size of the LLM and the number of sampled frames.", "description": "This table presents a comparison of different video LLMs on three established long-video question answering benchmarks: EgoSchema, MLVU, and VideoMME.  The accuracy of each model is reported on the full EgoSchema dataset, MLVU's multiple-choice questions, and the overall VideoMME dataset (without subtitles).  To ensure a fair comparison across models with varying architectures and scales, the table also includes the size of the Large Language Model (LLM) used by each model and the number of video frames sampled for processing.", "section": "4.4. Conventional Video Understanding"}, {"content": "| Clip | MLVU | V-MME | TVG<sub>F1</sub> | DVC<sub>F1</sub> | DVC<sub>Sim</sub> |\n|---|---|---|---|---|---| \n| Uniform | 59.8 | 55.4 | 34.5 | 33.1 | 18.1 |\n| Scene-based | 61.7 | 57.2 | 36.1 | 33.8 | 18.9 |", "caption": "Table 3: Comparison on ETBench. We present the results for two different settings. In the conventional video QA setting, the model is required to answer the question after watching the entire video. In the streaming setting, the question is placed at the beginning of the video, and the model is expected to provide real-time responses. We report performance on six subtasks that are suitable for both evaluation settings.", "description": "Table 3 presents a comparison of the model's performance on the ETBench benchmark under two different scenarios: conventional video QA and streaming video QA. In the conventional setting, the model processes the entire video before answering a question posed at the end. In contrast, the streaming setting simulates a real-time interaction where the question is presented at the start, demanding timely and accurate responses throughout the video. The table details the model's performance across six subtasks (TVG, EPM, TAL, VHD, DVC, SLC) that assess various aspects of temporal understanding and visual reasoning, suitable for both settings. The results showcase the model's ability to handle both offline and real-time video understanding tasks effectively.", "section": "4.3. Streaming Video Understanding"}, {"content": "| \u27e8ANS\u27e9 | \u27e8TODO\u27e9 | \u27e8SILENT\u27e9 | TVG<sub>F1</sub> | DVC<sub>F1</sub> | DVC<sub>Sim</sub> |\n|---|---|---|---|---|---| \n| \u2717 | \u2717 | \u2717 | 20.1 | 19.7 | 12.3 |\n| \u2717 | \u2717 | \u2713 | 26.3 | 24.9 | 13.1 |\n| \u2713 | \u2717 | \u2713 | 35.2 | 31.0 | 17.2 |\n| \u2717 | \u2713 | \u2713 | 28.7 | 25.6 | 14.5 |\n| \u2713 | \u2713 | \u2717 | 35.5 | 30.2 | 16.8 |\n| \u2713 | \u2713 | \u2713 | 36.1 | 33.8 | 18.9 |", "caption": "Table 4: Ablation study on the clip segmentation. We compare uniform 16-frame clip segmentation and our scene-based segmentation with SigLip.", "description": "This table presents an ablation study on different clip segmentation methods used in the Dispider model.  It compares the performance of a uniform clip segmentation approach (dividing the video into fixed-length clips of 16 frames each) against the model's proposed scene-based segmentation (dynamically segmenting the video based on scene changes detected using SigLip).  The comparison is done using metrics from conventional QA tasks on the MLVU and VideoMME datasets as well as streaming metrics (temporal grounding and dense video captioning) on the ETBench dataset.  This helps analyze how the choice of segmentation strategy affects the model's accuracy in different video understanding tasks.", "section": "4.5 Ablation Study"}]