[{"figure_path": "https://arxiv.org/html/2504.07959/x2.png", "caption": "Figure 1: This paper introduces CCMNet, a framework for cross-camera color constancy. CCMNet uses pre-calibrated color correction matrices (CCMs) from camera ISP hardware to train an encoder that generates a camera fingerprint embedding (CFE), capturing the testing camera\u2019s color space. In (A), we show a raw image from a Canon 550D. In (B), we present C5 [6], which generalizes using randomly selected unlabeled images from the test camera\u2014C5\u2019s performance varies depending on the image set. In (C), we show our results, relying only on fixed CCMs in the ISP. Neither method used Canon 550D data during training. Gamma correction was applied for visualization.", "description": "Figure 1 demonstrates CCMNet, a novel framework for cross-camera color constancy.  It leverages pre-calibrated Color Correction Matrices (CCMs) from the camera's Image Signal Processor (ISP) to learn a camera's color characteristics.  An encoder is trained to create a Camera Fingerprint Embedding (CFE), enabling the system to generalize to unseen cameras.  The figure shows a comparison of CCMNet's results (C) to those of the C5 method (B).  The C5 method requires additional images from the test camera for adaptation, while CCMNet uses only the fixed CCMs from the ISP, achieving better generalization. (A) shows the input raw image.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.07959/x3.png", "caption": "Figure 2: Example of CCM calibration (A) and application (B). CCMs are calibrated to transform between CIE XYZ chromaticity and camera-specific raw-RGB values under standard illuminants with predefined color temperatures (e.g., 2856K, 6504K). For other illuminants, the calibrated CCMs are interpolated. As a result, CCMs reflect the camera\u2019s unique color characteristics, capturing how the camera perceives illuminants along the color temperature trajectory.", "description": "This figure illustrates the process of color correction matrix (CCM) calibration and application.  Panel (A) shows the calibration process where CCMs are generated by mapping the CIE XYZ color values (representing standard illuminants at specific color temperatures, such as 2856K and 6504K) to the camera's raw RGB color space. Panel (B) demonstrates how these CCMs are used.  For illuminants other than the calibration points, CCM values are interpolated. This interpolation allows for the transformation of XYZ values into the camera's raw RGB color space under various lighting conditions.  The overall effect is that the CCMs capture each camera's unique color characteristics (spectral sensitivity and response biases), showing how that particular camera responds to different illuminants.", "section": "2.2. Color Space Transfer via CCMs"}, {"figure_path": "https://arxiv.org/html/2504.07959/x4.png", "caption": "Figure 3: Overview of the CCMNet architecture. (A) Based on CCC [12] and C5 [6], CCMNet includes a network f\ud835\udc53fitalic_f that generates filters and bias from the u\u2062v\ud835\udc62\ud835\udc63uvitalic_u italic_v-histograms of the input image. To process query images from diverse camera domains with varying spectral sensitivities, CCMNet uses a camera fingerprint embedding (CFE) as guidance. (B) The CFE for three example cameras (A, B, V)\u2014two real (A, B) and one imaginary (V)\u2014is constructed by mapping predefined illuminants (2500K\u20137500K along the Planckian locus) from the CIE XYZ space to each camera\u2019s native raw RGB space using calibrated CCMs. These values are converted into a 64\u00d764646464\\times 6464 \u00d7 64 histogram and encoded into a 1D vector via a lightweight encoder.", "description": "Figure 3 illustrates the architecture of CCMNet, a novel cross-camera color constancy method.  Panel (A) shows the overall system, highlighting how CCMNet leverages a convolutional color constancy (CCC) framework and integrates a camera fingerprint embedding (CFE). The CCC framework processes input image u-v histograms to generate filters and bias for illuminant estimation. The CFE acts as a guidance mechanism, enabling adaptation to various cameras and their unique spectral sensitivities. Panel (B) details the generation of the CFE.  For three example cameras (two real, one synthetically generated), the CFE is created by transforming predefined illuminant colors (along the Planckian locus, from 2500K to 7500K) from the CIE XYZ color space into each camera's raw RGB space, using camera-specific calibrated color correction matrices (CCMs).  These transformed color values are then represented as a 64x64 histogram and encoded into a compact one-dimensional vector.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.07959/x5.png", "caption": "Figure 4: Visualization of our imaginary camera augmentation process. An image from the Sony A57 is white-balanced using the ground-truth illuminant, converted to CIE XYZ space, and mapped to the target camera\u2019s raw space. We illustrate two cases: mapping to the raw space of a real camera (Fujifilm XM1) and an imaginary camera. Brightness is adjusted for clarity.", "description": "Figure 4 demonstrates the imaginary camera augmentation technique.  A white-balanced image from a Sony A57 camera is transformed from its raw RGB color space into the device-independent CIE XYZ color space using the ground truth illuminant. This XYZ representation is then mapped back into the raw color space of another camera. The figure shows two examples: one where the target is a real camera (Fujifilm XM1) and another where the target is a synthetic, imaginary camera.  Brightness levels have been adjusted for better visualization, enhancing the differences between the transformations. This technique helps the model generalize to unseen camera types during training.", "section": "3.4 Imaginary Camera Augmentation"}, {"figure_path": "https://arxiv.org/html/2504.07959/x6.png", "caption": "Figure 5: Visual comparison of the results from C5 [6] with different additional image sets (second, third column) and CCMNet (fourth column). While C5 relies on additional images, CCMNet is optimized for fixed CFE guidance, ensuring consistent performance.", "description": "Figure 5 presents a visual comparison of color constancy results obtained using three different methods: C5 (with varying additional image sets), and CCMNet (ours). The first column shows the input raw images. The second and third columns show the C5 results, demonstrating the impact of using different sets of additional images captured from the same camera as the test image.  The fourth column presents the CCMNet results.  This comparison highlights that C5's performance heavily depends on the quality and characteristics of the additional images used during inference, whereas CCMNet consistently achieves high accuracy because it relies only on a fixed camera fingerprint embedding (CFE).", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.07959/x7.png", "caption": "Figure 6: A schematic diagram illustrating the use of ColorMatrix and ForwardMatrix. The ForwardMatrix (FM) transforms white-balanced raw data into the CIE XYZ color space, while the ColorMatrix (CM) converts CIE XYZ values of a standard light source into the camera\u2019s native raw color space. FM1 and CM1 are calibrated for standard illuminant A (2856K), and FM2 and CM2 are calibrated for the D65 illuminant (6504K).", "description": "Figure 6 illustrates the color transformations within a camera's image signal processor (ISP).  It shows how the Forward Matrix (FM) converts white-balanced raw image data from the camera's sensor into a standard color space (CIE XYZ). Conversely, the Color Matrix (CM) performs the reverse transformation, mapping CIE XYZ values (representing a standard light source like illuminant A or D65) back into the camera's specific raw color space.  The figure highlights that different matrices (FM1, CM1, FM2, CM2) are used depending on the specific illuminant (illuminant A at 2856K and D65 at 6504K) being used for calibration.", "section": "2.2. Color Space Transfer via CCMs"}, {"figure_path": "https://arxiv.org/html/2504.07959/x8.png", "caption": "Figure 7: Detailed visualization of CFE encoding process. As mentioned in the main paper, the camera\u2019s fingerprint is derived by converting the reference CIE XYZ colors (locus) along the correlated color temperature (CCT) range of 2500K\u20137500K into the corresponding RGB locus as observed by each device, followed by an encoding process. Due to this characteristic, the CFE feature inherently reflects the color characteristics induced by each camera\u2019s spectral sensitivity.", "description": "Figure 7 illustrates the process of creating a Camera Fingerprint Embedding (CFE).  First, a range of standard illuminant colors (CIE XYZ), representing color temperatures from 2500K to 7500K along the Planckian locus, are transformed into each camera's specific raw RGB color space using its calibrated color correction matrix (CCM). This transformation maps the standard illuminant colors to how each camera uniquely perceives them. The resulting RGB values for each camera are then converted into a 64x64 uv-histogram, which represents the camera's color response characteristics to the illuminant variations in the uv plane. Finally, a convolutional neural network (CNN) encoder processes this histogram data, producing an 8-dimensional CFE vector. This CFE serves as a compact representation of the camera's unique spectral sensitivity, providing device-specific guidance information for accurate color constancy across various cameras.", "section": "3.3 Camera Fingerprint Embedding"}, {"figure_path": "https://arxiv.org/html/2504.07959/x9.png", "caption": "Figure 8: Overall process of camera-to-camera mapping. In (A), subsets of images taken by different cameras from multiple datasets are white-balanced using the corresponding ground-truth illuminants, and the ForwardMatrix is used to convert them to the CIE XYZ space, creating the XYZ image pool. In (B), a reference image is sampled from the pool, and an illumination color is sampled from the augmented illumination pool of the source camera (Camera A) that originally captured the image. The sampled illumination is then mapped to the native RGB space of a randomly selected target camera (Camera B) using the ColorMatrix. Finally, in (C), the XYZ image is transformed into the white-balanced color space of Cameras A and B using the inverse of their respective ForwardMatrices, and illumination casting is applied by multiplying the images with the illumination RGB values of each camera space.", "description": "Figure 8 illustrates the camera-to-camera mapping process used for data augmentation.  It shows three key steps: (A) creating a device-independent XYZ image pool by white-balancing images from various cameras and transforming them using Forward Matrices; (B) sampling a reference image from this pool and an illumination color from the augmented illumination pool of its original camera, then mapping that illumination to a randomly chosen target camera's RGB space using a Color Matrix; and (C) transforming the XYZ image into the white-balanced RGB spaces of both the source and target cameras using inverse Forward Matrices, followed by simulating illumination by multiplying the images with the respective illumination RGB values.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.07959/x10.png", "caption": "Figure 9: Results of our imaginary camera augmentation. In each row, the leftmost and rightmost images represent the source and target camera images generated using the method described in Sec.\u00a0C, while the three middle images represent those produced by the imaginary camera, generated by interpolating between the two devices at ratios of 0.25, 0.5, and 0.75, respectively. As explained in Sec.\u00a03.4 of the main paper, the CCMs of the imaginary cameras are interpolated using the same alpha values applied during image interpolation, and the resulting CFE embeddings are generated for training. Brightness is adjusted for visibility.", "description": "This figure demonstrates the results of imaginary camera augmentation, a data augmentation technique used to improve the model's generalization capabilities.  Each row shows a pair of real camera images (leftmost and rightmost) and three images generated by interpolating between those real cameras at ratios of 0.25, 0.5, and 0.75. The interpolation process involves both the image data and the corresponding Color Correction Matrices (CCMs), ensuring consistency. These synthesized images effectively expand the model's training data, leading to better performance on unseen cameras.", "section": "3.4 Imaginary Camera Augmentation"}, {"figure_path": "https://arxiv.org/html/2504.07959/x11.png", "caption": "Figure 10: Additional results for Canon EOS 1Ds Mark III. CCMNet demonstrates superior performance on various scenes captured by unseen camera. Notably, CCMNet has never been exposed to any images or the CCM of the Canon 1Ds Mark III during training.", "description": "Figure 10 presents additional results obtained using the Canon EOS 1Ds Mark III camera.  The figure showcases CCMNet's performance on diverse scenes captured by this camera, which was never included in CCMNet's training data.  This highlights CCMNet's ability to generalize to unseen cameras and achieve superior performance without requiring any prior exposure to images or color correction matrices (CCMs) from that specific camera model.", "section": "4. Experiments"}]