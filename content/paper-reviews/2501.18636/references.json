{"references": [{"fullname_first_author": "Harsh Chaudhari", "paper_title": "Phantom: General trigger attacks on retrieval augmented language generation", "publication_date": "2024-05-20", "reason": "This paper introduces a novel attack method that targets the vulnerability of retrieval-augmented generation (RAG) systems, which is a central theme of the SafeRAG benchmark paper."}, {"fullname_first_author": "Jiawei Chen", "paper_title": "Benchmarking large language models in retrieval-augmented generation", "publication_date": "2024-00-00", "reason": "This paper provides a benchmark for RAG, setting the stage for more comprehensive security evaluation as done by SafeRAG."}, {"fullname_first_author": "Feiteng Fang", "paper_title": "Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training", "publication_date": "2024-05-20", "reason": "This paper focuses on improving RAG's robustness against noisy data, a problem that SafeRAG directly addresses and expands upon by introducing more sophisticated attack strategies."}, {"fullname_first_author": "Yi Liu", "paper_title": "RECALL: A benchmark for LLMs robustness against external counterfactual knowledge", "publication_date": "2023-00-00", "reason": "This paper introduces a benchmark for evaluating the robustness of LLMs to conflicting information, a crucial security aspect which is incorporated and expanded in SafeRAG's conflict attacks."}, {"fullname_first_author": "Kevin Wu", "paper_title": "Clasheval: Quantifying the tug-of-war between an LLM\u2019s internal prior and external evidence", "publication_date": "2024-04-00", "reason": "This paper provides a metric for evaluating the conflict between internal knowledge and external evidence in LLMs, which is relevant to evaluating the security of RAG systems, a topic directly addressed in SafeRAG."}]}