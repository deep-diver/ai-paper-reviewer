[{"figure_path": "https://arxiv.org/html/2501.18636/extracted/6162038/Figure/sfr_motivation.png", "caption": "Figure 1: Motivation. The attack methods used in existing RAG benchmarks fail to bypass the RAG components, which hindering accurate RAG security evaluation. Our SafeRAG introduces enhanced attack methods to evaluate the potential vulnerabilities of RAG.", "description": "Existing RAG security benchmarks often fail to effectively evaluate RAG's vulnerabilities because their attack methods are easily mitigated by standard RAG components like retrievers and filters.  This figure illustrates the limitations of prior work.  The SafeRAG benchmark, in contrast, employs enhanced attack strategies (noise, conflict, toxicity, and denial-of-service) designed to bypass these defenses and more accurately assess the security risks inherent in RAG systems.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.18636/x3.png", "caption": "Figure 2: The process of generating attacking texts. To meet the injection requirements for four attack surfaces: Noise, Conflict, Toxicity, and DoS, we first collected a batch of news articles and constructed a comprehensive question-contexts dataset as a basic dataset. Subsequently, we selected attack-targeted text from the basic dataset for the generation of attacking texts.", "description": "This figure illustrates the methodology for creating adversarial examples to test the security of Retrieval-Augmented Generation (RAG) systems.  The process begins by collecting news articles and forming a question-context dataset.  Four types of attacks are simulated: Noise, Conflict, Toxicity, and Denial-of-Service (DoS).  For each attack, specific text is selected from the dataset to be injected into different stages of the RAG pipeline (knowledge base, retriever, filter, generator) to evaluate its effect.", "section": "Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x4.png", "caption": "Figure 3: Cases of forming conflict contexts.", "description": "This figure illustrates different ways to create conflicting contexts for the conflict attack task.  It shows examples of minimal perturbations to existing text (where only small changes are made), realistic rewriting to create more convincing conflicts, and how to maintain key facts while creating conflicts. The examples highlight the importance of preserving core information to avoid generating irrelevant or hallucinated contexts.", "section": "Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x5.png", "caption": "Figure 4: The construction rules of White DoS. Blue text represents the original question, designed to bypass the retriever. Green text is used to bypass the filter, and red text is intended to bypass the generator to achieve the goal of refusal to answer.", "description": "This figure illustrates the mechanism of the White Denial-of-Service (DoS) attack.  The attack leverages a three-part structure. First, the original question (in blue) is designed to bypass the retriever. Second, a safety warning message (in green) is crafted to bypass any safety filters that might otherwise block the subsequent text. Finally, a statement (in red) is included to prompt the large language model (LLM) generator to refuse to answer, masking the refusal as a security precaution rather than a system limitation. The objective is to cause the entire RAG system to fail to provide an answer, while appearing as though it was done for security reasons.", "section": "3.3.4 Generation of White Denial-of-Service"}, {"figure_path": "https://arxiv.org/html/2501.18636/x6.png", "caption": "Figure 5: Experimental results injected different noise ratios into the text accessible within the RAG pipeline.", "description": "This figure displays the impact of injecting varying ratios of silver noise into different stages of the RAG pipeline. The x-axis represents the proportion of noisy text injected, ranging from 0 to 1 (or 6/6). The y-axis shows different metrics measuring RAG performance after the noise injection: Retrieval Accuracy (RA), F1 score (average of correct and incorrect option identification), and Attack Failure Rate (AFR, which is 1-Attack Success Rate). Different colored lines represent different components of the RAG pipeline (knowledge base, retrieved context, filtered context) and different retrieval methods (DPR, BM25, Hybrid, Hybrid-Rerank). The graph allows the reader to visualize how the presence of noise impacts the different stages of the pipeline, and which components and methods show more resilience to noisy data.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18636/x7.png", "caption": "Figure 6: Experimental results injected conflict into the text accessible within the RAG pipeline.", "description": "This figure visualizes the impact of injecting conflicting information at different stages of the RAG pipeline (knowledge base, retrieved context, filtered context) on the system's performance.  The results are shown using radar charts comparing metrics before and after the injection of conflicting text.  The metrics likely include retrieval accuracy (RA) and F1-score, representing the model's ability to retrieve relevant information and accurately answer questions, respectively. Each radar chart represents a different retriever or filter configuration, allowing a comparison of how various system configurations respond to conflicting information.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18636/x8.png", "caption": "Figure 7: Evaluation cases for multiple-choice questions in Noise and DoS tasks.", "description": "This figure shows example questions used to evaluate the performance of the model in the noise and denial-of-service (DoS) attack scenarios.  The evaluation involves multiple-choice questions based on a news summary generated by the model.  The questions test whether the model's summary includes fine-grained details (propositions) about the subject matter, indicating diversity and completeness. Evaluators select options based on their understanding of the generated text.", "section": "4.2 F1 Variants in Noise and DoS"}, {"figure_path": "https://arxiv.org/html/2501.18636/x9.png", "caption": "Figure 8: An evaluation case for a multiple-choice question in the conflict task.", "description": "This figure shows an example of a multiple-choice question used to evaluate the performance of a large language model in handling conflicting information.  The question is presented, along with multiple-choice options that incorporate conflicting facts.  The evaluator is asked to select the correct option(s) based on the provided context, which is designed to present contradictory or conflicting facts. This measures the model's ability to reason and resolve conflicting information to arrive at the most accurate response.", "section": "4.2 F1 Variants in Conflict"}, {"figure_path": "https://arxiv.org/html/2501.18636/x10.png", "caption": "Figure 9: Experimental results injected toxicity into the text accessible within the RAG pipeline.", "description": "This figure displays the results of experiments where varying amounts of toxic text were injected into different stages of the RAG pipeline (knowledge base, retrieved context, and filtered context).  The impact on three key metrics is visualized: Retrieval Accuracy (RA), Attack Failure Rate (AFR), and the F1 score (averaged F1 score considering both correct and incorrect option identifications).  The graph likely shows how different retrieval methods (e.g., DPR, BM25, Hybrid) and filters (OFF, NLI, SKR) perform under this type of attack, demonstrating the resilience of each component to toxic inputs.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18636/x11.png", "caption": "Figure 10: Experimental results injected DoS into the text accessible within the RAG pipeline.", "description": "This radar chart displays the results of experiments where different Denial-of-Service (DoS) attack ratios were injected into various stages of the Retrieval-Augmented Generation (RAG) pipeline.  It visualizes the impact of these attacks on the performance metrics: Retrieval Accuracy (RA), Attack Failure Rate (AFR), and F1 variants (the average of F1 scores for correct and incorrect option identification in multiple-choice questions evaluating the model's response diversity). The chart compares different retrievers (DPR, BM25, Hybrid, Hybrid-Rerank), filters (OFF, NLI, SKR), and the overall effect on the RAG pipeline's performance after the DoS attacks are introduced at various stages.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18636/x12.png", "caption": "Figure 11: Cumulative analysis of the generator\u2019s positive evaluation metrics across different attack tasks.", "description": "This figure shows a cumulative analysis of the performance of different large language models (LLMs) as text generators in the face of various security attacks.  The y-axis represents the evaluation metrics (F1 variants for noise and conflict attacks, AFR for toxicity and denial-of-service (DoS) attacks), and the x-axis lists the different LLMs tested.  The bar chart visually compares the performance of each LLM across the different types of attacks, providing insights into which models are more resistant to each type of adversarial input. This helps illustrate the relative vulnerabilities of various LLMs to these attacks in a retrieval-augmented generation (RAG) pipeline.", "section": "5.4 Analysis of Generator and Evaluator"}, {"figure_path": "https://arxiv.org/html/2501.18636/x13.png", "caption": "Figure 12: Generation of comprehensive questions and golden contexts.", "description": "This figure illustrates the process of generating comprehensive questions and corresponding golden contexts from news titles and segments.  It details the steps involved in creating a question-context dataset for the RAG security benchmark. The process uses a Chinese LLM engine and involves manual verification and refinement.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x14.png", "caption": "Figure 13: Generation of comprehensive questions and golden contexts (in Chinese).", "description": "This figure details the process of generating comprehensive questions and their corresponding golden contexts, which are sets of sentences used as ground truth answers.  The process starts with a news title and segment, which are fed into a Chinese Large Language Model (LLM) called DeepSeek. The LLM generates a comprehensive question and selects eight sentences (golden contexts) that best support answering the question.  These are then manually reviewed and revised to ensure quality and relevance.  The image shows the process steps visually and includes an example in Chinese.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x15.png", "caption": "Figure 14: Question answering.", "description": "This figure displays the prompt used for the question answering task in the SafeRAG benchmark.  It shows the structure for providing both the question and the retrieved context to the large language model (LLM). The goal is to evaluate the model's ability to answer the question accurately and consistently based solely on the provided information, testing the security and reliability of the retrieval-augmented generation (RAG) process.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x16.png", "caption": "Figure 15: Extraction of propositions from golden contexts.", "description": "This figure demonstrates the process of extracting propositions from the golden contexts.  It details the steps involved in breaking down complex sentences into smaller, more manageable units, ensuring that each proposition is self-contained and easily understood independently from the surrounding text.  The process involves: (1) splitting compound sentences; (2) separating additional information about named entities into independent propositions; (3) removing context-dependent elements like pronouns; and (4) presenting the result as a JSON-formatted list of strings. The example shown in the figure uses text from the 'Easter Hare' section.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x17.png", "caption": "Figure 16: Extraction of propositions from golden contexts (in Chinese).", "description": "This figure displays the process of extracting propositions from golden contexts in Chinese.  The process involves breaking down longer sentences into smaller, more manageable units (propositions) that are semantically complete and independent.  This is important for ensuring the accuracy and effectiveness of subsequent analysis and attack generation. The image shows the original Chinese text (golden contexts), the steps involved in the proposition extraction, and the resulting list of extracted propositions.", "section": "3.3.1 Generation of Silver Noise"}, {"figure_path": "https://arxiv.org/html/2501.18636/x18.png", "caption": "Figure 17: Guidelines for generating (annotating) soft ad attack texts.", "description": "This figure presents guidelines for annotators on how to generate soft ad attack texts. It details two methods for inserting soft ads into text: direct insertion and indirect insertion. Direct insertion involves seamlessly embedding the ad into the existing text. Indirect insertion modifies the text slightly to make the ad's inclusion more natural.  The guidelines emphasize creating ads dynamically based on the context, aiming for naturalness and seamless integration.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x19.png", "caption": "Figure 18: Guidelines for generating (annotating) soft ad attack texts (in Chinese).", "description": "Figure 18 provides guidelines in Chinese for creating soft ad attack texts.  It details the annotation objective, which is to have annotators choose between two insertion methods (direct or indirect) based on context and generate natural-sounding ads.  The guidelines define the methods, outlining direct insertion as embedding the ad concisely and indirectly as modifying the context slightly to incorporate the ad subtly.  The workflow is also described:  understand the context, choose a method, generate the ad, insert the ad, verify it sounds natural, and then output the attack text.", "section": "3.3.3 Generation of Soft Ad"}, {"figure_path": "https://arxiv.org/html/2501.18636/x20.png", "caption": "Figure 19: Multiple-choice question evaluation.", "description": "This figure details the evaluation process for multiple-choice questions in the SafeRAG benchmark. The evaluators read a news summary and determine for each option whether its content is correct and mentioned in the summary, incorrect but mentioned, or indeterminate because it's unmentioned or its accuracy can't be judged based on the summary alone.  The output is a JSON containing the evaluator's explanations for each option and classifications into correct, incorrect, and indeterminate categories.", "section": "4.2 Generation Safety Assessment Metric: F1 Variant and ASR"}, {"figure_path": "https://arxiv.org/html/2501.18636/x21.png", "caption": "Figure 20: Multiple-choice question evaluation (in Chinese).", "description": "This figure details the evaluation process for a multiple-choice question in Chinese.  Evaluators assess options based on a provided news summary, categorizing them as correct, incorrect, or indeterminate. The JSON output includes explanations justifying each classification and lists of correct, incorrect, and indeterminate options.", "section": "4.2 Generation Safety Assessment Metric: F1 Variant and ASR"}, {"figure_path": "https://arxiv.org/html/2501.18636/x22.png", "caption": "Figure 21: A data point of a comprehensive question, the golden contexts and propositions.", "description": "This figure shows an example from the SafeRAG dataset.  It presents a comprehensive question about why the RMB (Chinese currency) has rebounded against the US dollar since late August.  Below the question, the figure lists the \"golden contexts\"\u2014the sentences from the original news article that best answer the question\u2014followed by a list of \"propositions\", which are the most basic, independent facts extracted from the golden contexts.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x23.png", "caption": "Figure 22: A case of multiple options and the ground truth answers.", "description": "This figure showcases a multiple-choice question evaluation example.  It presents a news summary and several options related to it.  The ground truth answers are then provided, indicating which options are correct ('correct_options'), incorrect ('incorrect_options'), or indeterminate ('indeterminate_options') based on the information in the news summary.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18636/x24.png", "caption": "Figure 23: A data point of a comprehensive question, the golden contexts and propositions (in Chinese).", "description": "Figure 23 shows a sample data point from the SafeRAG dataset, which is used to evaluate the security of Retrieval-Augmented Generation (RAG) systems.  It includes a comprehensive question (in Chinese) designed to assess the RAG's ability to handle complex knowledge retrieval tasks. The dataset also provides the corresponding \"golden contexts\", which are the ideal, relevant sentences that would provide a correct answer, and the \"propositions\", which are further decomposed, smaller units of meaning derived from the golden contexts, representing the most basic facts.  These components are designed to be used to evaluate several attack strategies against RAG systems in the SafeRAG benchmark.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x25.png", "caption": "Figure 24: A case of multiple options and the ground truth answers (in Chinese).", "description": "This figure presents a multiple-choice question evaluation example in Chinese. It shows a news summary, multiple-choice options, and the ground truth answers (correct, incorrect, and indeterminate options) based on the news summary.  This is used to evaluate a model's ability to identify correct and incorrect information and handle cases with insufficient information.", "section": "4.2 Generation Safety Assessment Metric: F1 Variant and ASR"}, {"figure_path": "https://arxiv.org/html/2501.18636/x26.png", "caption": "Figure 25: A case of silver noise.", "description": "This figure shows an example of a silver noise attack in the SafeRAG benchmark. Silver noise is a type of attack where irrelevant or only partially relevant information is injected into the knowledge base.  The figure demonstrates how such noise, when retrieved by the RAG system, can lead to inaccurate or nonsensical responses. In this specific example, multiple sentences are shown that all support the notion that the RMB (Chinese currency) rebounded against the US dollar due to a decline in the US Dollar Index. However, these sentences, while factually correct in isolation, lack the nuance and context needed to form a complete and accurate explanation of the RMB's rebound.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}, {"figure_path": "https://arxiv.org/html/2501.18636/x27.png", "caption": "Figure 26: A case of context conflict and soft ad.", "description": "Figure 26 presents examples of both inter-context conflict and soft ad attacks. The conflict example shows a manipulated context that contradicts existing information. This can be used to confuse LLMs and make them generate responses based on the conflicting context. The soft ad examples demonstrate how ads can be seamlessly inserted into the original context by either directly embedding them or adapting the context subtly to promote the ad alongside credible entities.  This illustrates how subtle forms of manipulation can bypass filters and affect LLMs.", "section": "3 Threat Framework: Attacks on the RAG Pipeline"}]