{"references": [{"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-12-06", "reason": "This paper proposes a method for training generalized multi-query transformer models, which is highly relevant to the topic of gist token-based context compression."}, {"fullname_first_author": "William Brandon", "paper_title": "Reducing transformer key-value cache size with cross-layer attention", "publication_date": "2024-05-12", "reason": "This paper explores techniques for reducing transformer key-value cache size, a key challenge in long-context processing that is directly addressed by gist token-based methods."}, {"fullname_first_author": "Alexis Chevalier", "paper_title": "Adapting language models to compress contexts", "publication_date": "2023-12-06", "reason": "This paper is highly relevant as it directly investigates context compression techniques, providing a valuable comparison point for the gist token approach."}, {"fullname_first_author": "Suyu Ge", "paper_title": "Model tells you what to discard: Adaptive KV cache compression for LLMs", "publication_date": "2024-05-07", "reason": "This paper introduces an adaptive KV cache compression method, which is directly comparable and relevant to the gist-based approaches explored in the main paper."}, {"fullname_first_author": "Huiqiang Jiang", "paper_title": "Accelerating and enhancing LLMs in long context scenarios via prompt compression", "publication_date": "2024-08-11", "reason": "This paper explores prompt compression strategies, providing another closely related approach to compare against and contrast with the gist-based techniques."}]}