[{"figure_path": "https://arxiv.org/html/2412.17483/x1.png", "caption": "Figure 1: Overview of gist token-based context compression architectures. Long texts are segmented for compression, enabling diverse architectures through different memory locations and gist granularity.", "description": "This figure illustrates various gist token-based context compression architectures.  These architectures all begin by segmenting long input texts into smaller, more manageable chunks.  However, they differ in two key ways: (1) Memory Location \u2013 some store the compressed context as the last hidden state of the gist tokens (recurrent memory), while others store it in the key-value (KV) cache. (2) Gist Granularity \u2013 gist tokens can be inserted either coarsely (appended at the end of a segment) or finely (evenly dispersed within the segment).  The figure visually represents these different memory location and granularity combinations, showing how the gist tokens and their interactions with the original tokens (or previous outputs) contribute to the compression process within each architecture.", "section": "2 Preliminaries"}, {"figure_path": "https://arxiv.org/html/2412.17483/x2.png", "caption": "Figure 2: Comparisons of different compression methods on perplexity evaluation for language modeling.", "description": "This figure compares the perplexity scores achieved by various context compression methods against a full attention model for three language modeling datasets: PG19, ProofPile, and CodeParrot.  The x-axis represents the compression ratio, indicating the level of context compression applied. The y-axis shows the perplexity, a measure of how well the model predicts the next word in a sequence. Different colored lines represent different compression methods: fine-grained KV cache, coarse-grained KV cache, and coarse-grained recurrent memory. The full attention model serves as a baseline for comparison.  The figure helps illustrate the trade-off between compression efficiency and the resulting impact on the model's language modeling performance.", "section": "Can Gist Tokens Replace Full Attention in an Efficient and Effective Way?"}, {"figure_path": "https://arxiv.org/html/2412.17483/x3.png", "caption": "Figure 3: Average Perplexity of tokens in different positions among segments.", "description": "This figure visualizes the average perplexity scores of tokens within different segments of text.  It shows how perplexity changes across the token positions within a segment, comparing compressed models with various compression ratios (4, 8, 16, 32) against a full-attention baseline. This helps to illustrate the 'lost by the boundary' failure pattern observed in gist-token-based context compression, where the model exhibits higher perplexity near the start of segments and lower perplexity towards the end.", "section": "4.2 Failure Pattern Observations"}, {"figure_path": "https://arxiv.org/html/2412.17483/x4.png", "caption": "Figure 4: Performance on different tasks while truncating context to the last k\ud835\udc58kitalic_k tokens. When k\ud835\udc58kitalic_k is a multiple of 2048, the model will generate near the boundary.", "description": "This figure displays the performance of different models on various tasks when the context is truncated to the last k tokens.  The x-axis represents the number of tokens kept (k), and the y-axis represents the performance metric (e.g., accuracy, exact match).  Multiple lines represent different models or compression strategies. The key observation is that when k is a multiple of 2048, model performance tends to be particularly poor, indicating a boundary effect related to the way the context is segmented and compressed in those models. This suggests a failure mode in the compression methods near segment boundaries.", "section": "4.1 Compression Bottleneck Probing"}, {"figure_path": "https://arxiv.org/html/2412.17483/x5.png", "caption": "Figure 5: Performance on the 32-digit uuid recall task. We report the exact match rates of various first-k\ud835\udc58kitalic_k digits.", "description": "This figure illustrates the performance of different models on a 32-digit UUID recall task.  The x-axis represents the number of initial digits (k) used as a prompt, and the y-axis shows the percentage of exact matches achieved by the models. The results demonstrate that full-attention models maintain high accuracy regardless of the number of digits used as input, while compressed models show significantly reduced accuracy as the number of input digits increases. This highlights the difficulty that compressed models face in reconstructing the full sequence from a compressed representation. The plot visually compares the accuracy of the full attention and several gist-token based compression methods.", "section": "4.2 Failure Pattern Observations"}]