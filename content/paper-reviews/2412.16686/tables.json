[{"content": "| Method | Arena-Hard \u2191 | Alpaca-Eval V2 \u2191 | MTBench \u2191 | BBH \u2191 |\n|---|---|---|---|---|\n| Mistral-7b-v0.3 |  |  |  |  |\n| Alpaca vanilla | 3.00 | 11.73 / 7.39 | 6.37 | 34.46 |\n| Alpaca + SR | 4.20 | 11.50 / 6.52 | 6.28 | 38.40 |\n| Alpaca + NILE | **6.20** | **15.39** / **9.70** | **6.56** | **38.52** |\n| Orca vanilla | 5.30 | 12.84 / 9.54 | 5.34 | 46.37 |\n| Orca + SR | 5.70 | 18.19 / 15.24 | 6.13 | 46.01 |\n| Orca + NILE | **6.70** | **21.63** / **17.25** | **6.73** | **51.01** |\n| Meta-Llama-3.1-8B |  |  |  |  |\n| Alpaca vanilla | 2.10 | 7.58 / 5.53 | 6.31 | 58.64 |\n| Alpaca + SR | 3.30 | 9.08 / 6.84 | 6.39 | 59.91 |\n| Alpaca + NILE | **4.80** | **10.69** / **10.43** | **6.90** | **61.40** |\n| Orca vanilla | 3.60 | 10.84 / 7.52 | 7.01 | 63.02 |\n| Orca + SR | 4.20 | 12.36 / 10.46 | 7.18 | 63.77 |\n| Orca + NILE | **6.00** | **13.70** / **12.11** | **7.48** | **64.05** |", "caption": "Table 1: Prompt for generating internal knowledge demonstration i\u2062kid\ud835\udc56subscriptsuperscript\ud835\udc58\ud835\udc51\ud835\udc56ik^{d}_{i}italic_i italic_k start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT related to qidsubscriptsuperscript\ud835\udc5e\ud835\udc51\ud835\udc56q^{d}_{i}italic_q start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT.", "description": "This table describes the prompt used to generate internal knowledge demonstrations from a pre-trained large language model (LLM).  The prompt takes as input an instruction (instruction<sup>d</sup><sub>i</sub>) and its corresponding input (input<sup>d</sup><sub>i</sub>) from the instruction fine-tuning (IFT) dataset. The LLM then generates knowledge (ik<sup>d</sup><sub>i</sub>) related to the given instruction, which is used in the next step of the NILE framework to improve the consistency between the LLM's internal knowledge and the world knowledge expressed in the IFT dataset.", "section": "3.1 Internal Knowledge Extraction"}, {"content": "| Method | Arena-Hard \u2191 | Alpaca-Eval V2 \u2191 | MTBench \u2191 | BBH \u2191 |\n|---|---|---|---|---|\n| Alpaca + KSR (Mistral) | 4.00 | 9.14 / 7.29 | 6.64 | 57.67 |\n| Alpaca + KSR (Llama) | **4.80** | **10.75** / **9.38** | **6.67** | **60.73** |\n| Orca + KSR (Mistral) | 5.10 | 12.50 / 10.25 | 5.93 | 22.32 |\n| Orca + KSR (Llama) | **5.20** | **13.67** / **11.21** | **7.51** | **64.03** |", "caption": "Table 2: Prompt for knowledge extraction. Sample few-shot demonstration prompt is listed in\u00a0A.1.3.", "description": "This table presents the prompt used for extracting internal knowledge from a pre-trained large language model (LLM).  The prompt guides the LLM to generate knowledge related to a given instruction and input by providing a few-shot learning example. The few-shot examples themselves are selected based on semantic similarity to the target instruction.  Details on how these examples are selected and the full prompt engineering methodology are described in Appendix A.1.3 of the paper.", "section": "3.1 Internal Knowledge Extraction"}, {"content": "| Method | Arena-Hard \u2191 | Alpaca-Eval V2 \u2191 | MTBench \u2191 | BBH \u2191 |\n|---|---|---|---|---|\n| Alpaca + KSR w. FD | **4.80** | 10.75 / 9.38 | 6.67 | **60.73** |\n| Alpaca + KSR w. FS 1 IKE | **4.50** | **11.20** / **9.75** | **6.72** | 59.25 |\n| Alpaca + KSR w. FS 2 IKE | **4.50** | **10.82** / **10.56** | **6.76** | **61.40** |\n| Orca + KSR w. FD | **5.20** | **13.67** / **11.21** | **7.51** | **64.03** |\n| Orca + KSR w. FS 1 IKE | 4.90 | 12.46 / 10.99 | 7.40 | 63.89 |\n| Orca + KSR w. FS 2 IKE | **5.50** | **13.00** / **11.50** | **7.43** | **64.29** |", "caption": "Table 3: Prompt for Knowledge-aware Sample Revision.", "description": "This table shows the prompt template used in the Knowledge-aware Sample Revision (KSR) stage of the NILE framework.  The prompt guides a large language model (LLM) to revise an existing instruction-answer pair by incorporating internal knowledge extracted earlier in the process.  The prompt includes placeholders for the original answer, instruction, input, and extracted internal knowledge, allowing the LLM to generate a revised answer that aligns better with both the original instruction and the internal knowledge of the target LLM.", "section": "3.2 Knowledge-aware Sample Revision"}, {"content": "| Method | Arena-Hard \u2191 | Alpaca-Eval V2 \u2191 | MTBench \u2191 | BBH \u2191 |\n|---|---|---|---|---|\n| Alpaca + NILE wo. ICF | 4.50 | 10.82 / 10.56 | 6.76 | 61.40 |\n| Alpaca + NILE w. ICF (low) | 4.80 | 10.69 / 10.43 | 6.90 | 61.40 |\n| Alpaca + NILE w. ICF (high) | 4.50 | 9.92 / 9.70 | 6.79 | 61.71 |\n| Orca + NILE wo. ICF | 5.50 | 13.00 / 11.50 | 7.43 | 64.29 |\n| Orca + NILE w. ICF (low) | 6.00 | 13.70 / 12.11 | 7.48 | 64.05 |\n| Orca + NILE w. ICF (high) | 4.80 | 13.19 / 11.49 | 7.30 | 63.95 |", "caption": "Table 4: Main experiment results on Alpaca and OpenOrca datasets. The highest values are bolded, and the second highest is underlined.", "description": "This table presents the quantitative results of the main experiments conducted using the NILE framework.  It shows the performance of different models (MISTRAL-7B-v0.3 and META-LLAMA-3.1-8B) on multiple evaluation benchmarks (Arena-Hard, Alpaca-Eval V2, MTBench, and BBH) when fine-tuned with instruction-following datasets generated by the Alpaca and OpenOrca methods. For each benchmark and model combination, the vanilla score (using the original unmodified dataset), score with sample revision only (SR), and the score with the full NILE framework are compared.  The highest scores are shown in bold, and the second highest scores are underlined.  This allows for a direct comparison of the impact of each component of the NILE framework on model performance across various tasks.", "section": "4.5 Results on Orca Dataset"}, {"content": "| Method | Arena-Hard \u2191 | Alpaca-Eval V2 \u2191 | MTBench \u2191 | BBH \u2191 |\n|---|---|---|---|---|\n| Orca + NILE w. Contriver | 4.70 | 14.63 / 12.17 | 7.29 | 64.00 |\n| Orca + NILE w. BM25 | 5.50 | 13.00 / 11.50 | 7.43 | 64.29 |", "caption": "Table 5: Effects of KSR in Llama-3 finetuning with internal knowledge from different LLMs. The highest values are bolded.", "description": "This table presents the results of an ablation study evaluating the impact of using internal knowledge from different LLMs within the Knowledge-aware Sample Revision (KSR) component of the NILE framework. Specifically, it shows how Llama-3's performance on various benchmark datasets (Arena-Hard, Alpaca-Eval V2, MTBench, and BBH) changes when finetuned using datasets revised with internal knowledge extracted from either Mistral or Llama-3.  The highest performance scores for each metric are highlighted in bold, illustrating the relative effectiveness of using each LLM's knowledge for dataset revision.", "section": "4.5 Results on Orca Dataset"}, {"content": "| Instruction | Response | Internal Knowledge |\n|---|---|---|\n| *Compose a new email subject line for a following email.* <br> **Input:** The email body contains a request to renew a subscription. | **Vanilla:** \"Reminder: Renew Your Subscription Today!\" <br> **Revised:** \"<span class=\"ltx_text\" id=\"A1.T13.1.2.1.2.1.1.3\" style=\"color:#B30000;\">Action Required:</span> Time to Renew Your Subscription!\" | Email subject line: The subject line of an email is the first thing a recipient sees and can determine whether they open the email or not. It should be clear, concise, and relevant to the email content.* Subscription renewal: A subscription renewal is a request to continue receiving a service or product after the initial subscription period has ended. It is important to provide clear instructions and information about the renewal process.* Clear instructions: Providing clear instructions in the email body can help ensure that the recipient understands the request and knows what to do next. It is important to use simple language and avoid technical jargon.* Information about renewal process: Including information about the renewal process, such as the cost, payment options, and deadlines, can help the recipient make an informed decision and take the necessary steps to renew their subscription.<span class=\"ltx_text\" id=\"A1.T13.1.2.1.3.1.1.1\" style=\"color:#B30000;\">* Call-to-action: Including a clear call-to-action in the email body can help ensure that the recipient knows what to do next and takes the desired action. It is important to use a strong and direct call-to-action that is easy to understand and takes the recipient to the next step.</span> |\n| *Write a sentence containing a hyperbolic metaphor.* <br> **Input:** None | **Vanilla**: He was so hungry, he could eat an entire elephant. <br> **Revised**: He was so ravenous, <span class=\"ltx_text\" id=\"A1.T13.1.3.2.2.1.1.3\" style=\"color:#B30000;\">he could devour a mountain of hamburgers as tall as Everest.</span> | Hyperbole is a figure of speech that uses exaggeration for emphasis or effect.<span class=\"ltx_text\" id=\"A1.T13.1.3.2.3.1.1.1\" style=\"color:#B30000;\">* Hyperbolic metaphors are metaphors that use hyperbole to create a vivid and dramatic image.</span>* For example, \"The crowd was so large that it stretched from one end of the stadium to the other.\" * This metaphor uses hyperbole to create a dramatic image of the crowd\u015b size.* Hyperbolic metaphors can be used to create a strong emotional response or to make a point more effectively.<span class=\"ltx_text\" id=\"A1.T13.1.3.2.3.1.1.2\" style=\"color:#B30000;\">* They can also be used to add humor or to create a sense of exaggeration or absurdity.</span> |\n| *Create a 3-5 sentence definition for the term \"data science\".* <br> **Input:** None | **Vanilla:** Data science is an interdisciplinary field that combines techniques from statistics, mathematics, computer science, and information science to extract insights and knowledge from data. This field deals with both structured and unstructured data, using algorithms, predictive models, and machine learning to make informed decisions. Data science has broad applications, from businesses and marketing to healthcare and scientific research, where it is used to answer questions, find patterns, and make strategic decisions. <br> **Revised:** Data science is an interdisciplinary field that uses methods from statistics, mathematics, computer science, and domain-specific knowledge to extract insights and information from both structured and unstructured data. It employs techniques such as machine learning, predictive modeling, and data mining to analyze data sets, identify patterns, and make predictions. <span class=\"ltx_text\" id=\"A1.T13.1.4.3.2.1.1.3\" style=\"color:#B30000;\">The ultimate goal of data science is to help organizations make informed decisions, optimize processes, and solve complex problems by leveraging vast amounts of data. This field is critical across various industries, including healthcare, finance, and technology, where data-driven insights are essential for strategic decision-making and operational improvements</span>. | Data science is an interdisciplinary field that combines techniques from statistics, computer science, and domain knowledge to extract insights from data.* It involves the use of advanced analytics and machine learning algorithms to analyze large datasets and identify patterns, trends, and correlations.* Data scientists use various tools and techniques such as data mining, predictive modeling, and natural language processing to uncover insights and make predictions about future events.<span class=\"ltx_text\" id=\"A1.T13.1.4.3.3.1.1.1\" style=\"color:#B30000;\">* The goal of data science is to use data to solve complex problems, make better decisions, and drive business value.* Data science has become increasingly important in recent years due to the rapid growth of data and the need for organizations to make sense of it.* Data scientists play a critical role in many industries, including healthcare, finance, retail, and technology, where they help organizations gain insights from data to improve operations, optimize processes, and make better decisions.</span> |", "caption": "Table 6: Effects of IKE with different fewshot numbers\u00a0(FS) in Llama-3. The highest values are bolded, and the second highest is underlined.", "description": "This table presents the ablation study results on the impact of using different numbers of few-shot examples in the Internal Knowledge Extraction (IKE) phase of the NILE framework, specifically focusing on Llama-3 model. It compares the performance metrics across three scenarios: using a fixed 2-shot demonstration (FD), retrieving the top 1 most similar sample (FS 1 IKE), and retrieving the top 2 most similar samples (FS 2 IKE).  The results are presented for various evaluation benchmarks, allowing for a comprehensive assessment of the impact of the number of few-shot examples on model performance.  The highest values for each benchmark are highlighted in bold, and the second-highest values are underlined.", "section": "4.7 Ablation Study"}]