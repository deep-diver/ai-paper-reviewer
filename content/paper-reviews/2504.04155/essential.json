{"importance": "This paper introduces **GlotEval**, a crucial tool for evaluating LLMs across diverse languages. It encourages inclusive, transparent, and holistic evaluations, advancing robust multilingual NLP research and paving the way for more equitable language technologies.", "summary": "GlotEval: Massively multilingual LLM evaluation!", "takeaways": ["GlotEval is a lightweight framework for massively multilingual evaluation of LLMs.", "It supports consistent benchmarking, language-specific prompts and non-English-centric translation.", "GlotEval enables reliable LLM assessments in diverse linguistic contexts."], "tldr": "Large language models excel globally, but evaluation in diverse languages is challenging. Existing frameworks focus on English, overlooking multilingual scenarios. This paper introduces **GlotEval**, a lightweight framework for massively multilingual evaluation, supporting seven tasks across dozens of languages. It enables diagnosing model strengths and weaknesses in diverse linguistic contexts.\n\n**GlotEval** features consistent multilingual benchmarking by standardizing language codes. Users can configure prompts for each language, assessing instruction-following ability. It also enables non-English-centered translation evaluations. A multilingual translation case study demonstrates **GlotEval's** applicability for multilingual and language-specific evaluations.", "affiliation": "University of Helsinki", "categories": {"main_category": "Natural Language Processing", "sub_category": "Machine Translation"}, "podcast_path": "2504.04155/podcast.wav"}