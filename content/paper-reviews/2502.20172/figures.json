[{"figure_path": "https://arxiv.org/html/2502.20172/x2.png", "caption": "Figure 1: Generation examples of Dream Engine. Leveraging powerful text-to-image diffusion model and large multimodal models, Dream Engine\u00a0is capable of generating image with text-image interleaved control by merging concepts from different images.", "description": "Figure 1 showcases the capabilities of Dream Engine in generating images using a novel text-image interleaved control mechanism.  Unlike traditional methods that rely solely on text prompts, Dream Engine merges concepts from multiple source images to produce a single output image guided by both text and image input. The figure presents several example images generated by Dream Engine, demonstrating its ability to combine visual elements from diverse sources according to complex textual instructions.  This illustrates the power of Dream Engine in generating highly customized and creative images by flexibly integrating textual instructions with visual elements selected from a variety of source images.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.20172/x3.png", "caption": "Figure 2: Overview Comparison. Among all types of works connecting LMM and diffusion model, our\u00a0Dream Engine\u00a0adopts the simplest design yet achieves the best performance.", "description": "This figure compares different approaches of integrating Large Multimodal Models (LMMs) with diffusion models for image generation.  It showcases three different architectures: Emu1/Emu2 which add a regression head to the LMM's hidden states; Seed Tokenizer which expands the LMM vocabulary with visual tokens; and Dream Engine (the authors' method), which uses a simpler design involving a direct connection between the LMM and the diffusion model. The figure highlights that Dream Engine achieves superior performance despite its simpler design.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.20172/x4.png", "caption": "Figure 3: Dream Engine\u00a0architecture.", "description": "The DREAM ENGINE architecture diagram illustrates the process of image generation guided by interleaved text and image instructions. It starts with text and image inputs that are processed through a linear layer and a powerful Large Multimodal Model (LMM), producing blended visual features. These features, along with text embeddings and timestep information, are fed into a series of Multimodal Diffusion Transformer (MM-DiT) blocks.  These blocks iteratively refine the representation, culminating in a final output that can be converted into a generated image using a text-to-image model (e.g., Stable Diffusion v3.5). An adapter module aligns the output of the LMM with the MM-DiT's input requirements.  The architecture showcases the integration of a powerful LMM for effective multimodal representation and control, leading to sophisticated image generation from text-image interleaved instructions.", "section": "2. Methods"}, {"figure_path": "https://arxiv.org/html/2502.20172/x5.png", "caption": "Figure 4: Performance demonstration on Natural Object Background Merging where Dream Engine\u00a0 can understand complex text-image input. It can even set more than one object in the background (last line).", "description": "Figure 4 showcases Dream Engine's ability to seamlessly integrate multiple objects into complex scenes. Unlike previous models, Dream Engine isn't limited to simple object placement, it can intelligently merge objects from various sources into a unified background. The example in the last row demonstrates this capability, where a cat and a girl are both added to a beach scene, showcasing a high level of compositional understanding.", "section": "3. Results and Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.20172/x6.png", "caption": "Figure 5: Training stages and tasks of Dream Engine.", "description": "This figure illustrates the two-stage training process of Dream Engine. Stage 1 focuses on aligning the representation spaces of the Large Multimodal Model (LMM) and the Multimodal Diffusion Transformer (MM-DiT) using two tasks: Text-to-Image Alignment and Image-to-Image Alignment.  In Stage 2, the MM-DiT is fine-tuned to handle interleaved text and image instructions using two more tasks: Free Form Image Editing and Objects Driven Generation.  The diagrams show the inputs (text, images, instructions), the model components used, and the outputs (edited images, generated images) for each task, making it clear how the model's capabilities are built up in stages.", "section": "2. Methods"}, {"figure_path": "https://arxiv.org/html/2502.20172/x7.png", "caption": "Figure 6:  Performance demonstration on Object Driven Feature Mixing task. Dream Engine\u00a0 can understand the complex instruction while Emu2-Gen fails on the task.", "description": "This figure showcases a comparison between Dream Engine and Emu2-Gen on an image generation task involving complex instructions.  The task, termed 'Object Driven Feature Mixing,' requires the model to generate images incorporating features from two source images according to a textual description.  The example demonstrates Dream Engine's ability to successfully generate images that incorporate features from both source images as instructed. In contrast, Emu2-Gen is shown to fail this task.  The figure highlights Dream Engine's superior ability to understand and execute complex, multi-modal (text and image) instructions, which combines elements from different images based on the user's instructions.", "section": "3.3 Results and Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.20172/x8.png", "caption": "Figure 7: Performance demonstration on Free Form Image Editing task. Dream Engine\u00a0 outperforms the counterpart Emu2-Gen model in both instruction following and output image quality.", "description": "Figure 7 showcases a comparison between Dream Engine and Emu2-Gen on a free-form image editing task.  Multiple example edits are shown, highlighting Dream Engine's superior ability to precisely follow complex and nuanced instructions, resulting in higher-quality output images compared to Emu2-Gen.  Each row presents the same source image with various modifications based on different instructions, illustrating how well each model understands and implements the given edits.", "section": "3.3. Results and Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.20172/x9.png", "caption": "Figure 8: Image reconstruction performance dynamics during training. We can see that there is a concept-to-detail transition during the training period.", "description": "This figure visualizes the evolution of image reconstruction quality during the training process of the DREAM ENGINE model.  The image reconstruction task is part of Stage 1 training, focusing on aligning the representation spaces between the Large Multimodal Model (LMM) and the Multimodal Diffusion Transformer (MM-DiT).  The figure shows that initially, the model captures only high-level concepts, such as the presence of a person, an object, or a landscape. As training progresses, the reconstruction gradually incorporates more detail, showcasing a concept-to-detail transition. This visual representation demonstrates how the model learns to accurately reproduce the source image, moving from broad semantic understanding to fine-grained detail.", "section": "3.3. Results and Comparisons"}]