[{"content": "| Hidden size | Intermediate size | Attention heads | KV heads | Layers | Context Len |\n|---|---|---|---|---|---| \n| 1536 | 3840 | 24 | 8 | 48 | 4096 |", "caption": "Table 1: Model configuration for Xmodel-2.", "description": "This table details the architectural hyperparameters of the Xmodel-2 language model.  It lists the hidden size, intermediate size, number of attention heads, number of key-value heads, number of layers, and the context length. These values define the model's capacity and computational requirements.", "section": "2.1 Model Architecture"}, {"content": "| Model | ARC-c | ARC-e | Boolq | HS. | OB. | PiQA | SciQ | Wino. | Avg |\n|---|---|---|---|---|---|---|---|---|---| \n| MobiLlama-1B | 28.24 | 61.53 | 60.92 | 46.74 | 21.80 | 75.14 | 88.20 | 59.27 | 55.23 |\n| TinyLLaMA1.1-1.1B | 30.97 | 61.66 | 55.99 | 46.70 | 25.20 | 72.63 | 89.30 | 59.43 | 55.24 |\n| OLMo-1B | 28.67 | 63.34 | 61.74 | 46.97 | 25.00 | 75.03 | 87.00 | 59.98 | 55.97 |\n| OpenELM-1.1B | 28.84 | 62.37 | 63.58 | 48.36 | 25.40 | 74.76 | 90.60 | 61.72 | 56.95 |\n| Llama-3.2-1B | 31.31 | 65.36 | 63.73 | 47.78 | 26.40 | 74.48 | 91.50 | 61.01 | 57.70 |\n| MiniCPM-1.2B | 36.86 | 70.29 | 67.92 | 49.91 | 23.60 | 74.43 | 91.80 | 60.77 | 59.45 |\n| Fox-1-1.6B | 34.73 | 69.91 | 71.77 | 46.33 | 24.60 | 75.24 | 93.20 | 60.77 | 59.57 |\n| InternLM2.5-1.8B | 35.24 | 66.37 | 79.82 | 46.99 | 22.00 | 73.29 | 94.90 | 62.67 | 60.16 |\n| Qwen2-1.5B | 33.11 | 66.41 | 72.60 | 48.57 | 27.00 | 75.57 | 94.60 | 65.75 | 60.45 |\n| StableLM-2-zephyr-1.6B | 36.52 | 66.79 | 80.00 | 53.26 | 26.80 | 74.86 | 88.00 | 64.09 | 61.29 |\n| SmolLM-1.7B | 43.43 | 76.47 | 65.93 | 49.58 | 30.00 | 75.79 | 93.20 | 60.93 | 61.92 |\n| Qwen2.5-1.5B | 41.21 | 75.21 | 72.97 | 50.15 | 31.80 | 75.90 | 94.30 | 63.61 | 63.14 |\n| DCLM-1B | 41.30 | 74.79 | 71.41 | 53.59 | 32.20 | 76.93 | 94.00 | 66.22 | 63.81 |\n| Phi-1.5-1.3B | 44.80 | 76.22 | 74.95 | 47.96 | 38.60 | 76.66 | 93.30 | 72.93 | 65.68 |\n| Xmodel-2-1.2B | 39.16 | 71.55 | 74.65 | 47.45 | 29.20 | 74.81 | 93.60 | 63.93 | 61.79 |", "caption": "Table 2: Zero-shot performance on Commonsense Reasoning tasks.", "description": "This table presents the results of a zero-shot evaluation on several commonsense reasoning benchmarks.  Zero-shot means the model was not fine-tuned on these specific datasets; it used only its pre-trained knowledge. The benchmarks assess various aspects of commonsense understanding.  The table compares the performance of Xmodel-2 against other similar-sized language models (around 1-2 billion parameters), showcasing its performance relative to these baselines.  The scores reflect the accuracy of the models on each task, offering a comprehensive view of their commonsense reasoning abilities.", "section": "3.1 Commonsense Reasoning"}, {"content": "| Model | GSM8K 5-shot | MATH 4-shot | BBH 3-shot | MMLU 0-shot | HumanEval pass@1 | MBPP pass@1 | Avg |\n|---|---|---|---|---|---|---|---| \n| OpenELM-1.1B | 0.45 | 1.06 | 6.62 | 25.52 | 8.54 | 6.80 | 8.16 |\n| OLMo-1B | 2.35 | 1.46 | 25.60 | 24.46 | 5.49 | 0.20 | 9.93 |\n| TinyLLaMA1.1-1.1B | 2.50 | 1.48 | 25.57 | 25.35 | 1.83 | 3.40 | 10.02 |\n| MobiLlama-1B | 1.97 | 1.54 | 25.76 | 25.26 | 7.93 | 5.40 | 11.31 |\n| DCLM-1B | 4.93 | 2.14 | 30.70 | 46.43 | 8.54 | 6.80 | 16.59 |\n| Llama-3.2-1B | 6.60 | 1.78 | 31.44 | 36.63 | 14.63 | 22.20 | 18.88 |\n| SmolLM-1.7B | 7.51 | 3.18 | 29.21 | 27.73 | 21.34 | 31.80 | 20.13 |\n| Fox-1-1.6B | 34.34 | 7.94 | 28.75 | 39.55 | 14.02 | 9.00 | 22.27 |\n| StableLM-2-zephyr-1.6B | 41.32 | 10.12 | 32.71 | 41.30 | 25.61 | 19.40 | 28.41 |\n| Phi-1.5-1.3B | 32.15 | 3.18 | 28.81 | 41.75 | 36.59 | 35.40 | 29.65 |\n| InternLM2.5-1.8B | 27.90 | 16.68 | 41.76 | 46.30 | 27.40 | 29.60 | 31.61 |\n| MiniCPM-1.2B | 40.11 | 10.98 | 35.42 | 43.99 | 43.90 | 36.80 | 35.20 |\n| Qwen2-1.5B | 57.62 | 22.90 | 33.05 | 55.11 | 20.73 | 30.40 | 36.64 |\n| Qwen2.5-1.5B | 62.40 | 28.28 | 43.99 | 59.72 | 5.49 | 40.00 | 39.98 |\n| Xmodel-2-1.2B | 55.88 | 25.50 | 48.40 | 48.87 | 29.88 | 29.20 | 39.62 |", "caption": "Table 3: Performance on Complex Reasoning tasks.", "description": "This table presents the performance of Xmodel-2 and various other large language models on six complex reasoning benchmarks: GSM8K, MATH, BBH, MMLU, HumanEval, and MBPP.  For each model, the table shows its performance (average score) on each benchmark, and the average performance across all benchmarks.  The benchmarks assess different aspects of reasoning ability, including mathematical problem-solving, commonsense reasoning, and code generation.  The results allow for a comparison of Xmodel-2's performance against other models of similar size and architecture in complex reasoning tasks.", "section": "3 Results"}, {"content": "| Model | HotpotQA EM | FEVER EM | AlfWorld success rate | WebShop success rate | Avg |\n|---|---|---|---|---|---| \n| OLMo-1B | 2.67 | 18.58 | 0.00 | 0.00 | 4.32 |\n| Phi-1.5 1.3B | 3.54 | 17.56 | 2.24 | 0.80 | 6.04 |\n| DCLM-1B | 4.92 | 24.39 | 0.75 | 0.00 | 7.52 |\n| MobiLlama-1B | 0.00 | 30.43 | 0.00 | 0.00 | 7.61 |\n| TinyLLaMA1.1-1.1B | 2.11 | 28.77 | 0.00 | 0.20 | 7.77 |\n| OpenELM-1-1B | 2.70 | 28.37 | 0.00 | 0.40 | 7.87 |\n| StableLM-2-zephyr-1.6B | 1.44 | 20.81 | 8.96 | 2.20 | 8.35 |\n| SmolLM-1.7B | 2.28 | 31.31 | 0.00 | 0.60 | 8.55 |\n| Fox-1-1.6B | 5.37 | 30.88 | 0.00 | 0.60 | 9.21 |\n| Llama-3.2-1B | 4.87 | 27.67 | 8.21 | 3.20 | 10.99 |\n| Qwen2.5-1.5B | 13.53 | 27.58 | 5.97 | 0.60 | 11.92 |\n| MiniCPM-1.2B | 11.00 | 36.57 | 1.60 | 1.00 | 12.52 |\n| InternLM2.5-1.8B | 12.84 | 34.02 | 2.99 | 1.00 | 12.71 |\n| Xmodel-2-1.2B | 13.70 | 40.00 | 0.78 | 2.20 | 14.21 |", "caption": "Table 4: Performance on Agent tasks.", "description": "This table presents the performance of various language models on four different agent-based tasks: HotpotQA, FEVER, AlfWorld, and WebShop.  For HotpotQA and FEVER, the evaluation metric is Exact Match (EM), reflecting the percentage of questions answered perfectly.  AlfWorld and WebShop use the success rate metric, measuring the proportion of attempts that successfully completed the given tasks. The results highlight the relative strengths and weaknesses of different models in handling complex reasoning, multi-step decision making, and real-world interactions within diverse agent environments.", "section": "3.3 Agent Capabilities"}, {"content": "| Hyperparameter | Range | Options | Step Size |\n|---|---|---|---| \n| `scale_emb` | `[2, 20]` | - | 1 |\n| `dim_model_base` | - | `{32, 64, 128, 256, 512, 1024}` | - |\n| `scale_depth` | `[1, 5]` | - | 0.1 |\n| `learning_rate` | `[0.001, 0.1]` | - | 0.001 |", "caption": "Table 5: Hyperparameter search ranges for nano model.", "description": "This table details the ranges explored for each hyperparameter during the hyperparameter search for the nano model.  The search was conducted to optimize the model's performance. For each hyperparameter, the table shows the minimum and maximum values tested (Range), the specific values used in the search (Options), and the increment between values (Step Size). This information is crucial for understanding the optimization process and the range of values considered for each parameter to reach the optimal configuration for the nano model.", "section": "6.1 \u00b5P Hyperparameter Search"}, {"content": "| Name | Specific Operation |\n|---|---| \n| Embedding Output Scaling | Multiply the output of the embedding by $scale_{emb}$ |\n| Residual Connection Scaling | Scale the output tensor of a block before adding to each residual connection in each layer by $scale_{depth}/\\sqrt{num\\_layers}$ |\n| Initialization of Tensors | Set the initialization standard deviation of each two-dimensional tensor parameter to $init\\_std/\\sqrt{d_{m}/d_{base}}$, and set other parameters\u2019 initialization to 0.1 |\n| Learning Rate Scaling of Tensors | Adjust the learning rate of each two-dimensional tensor parameter to $1/(d_{m}/d_{base})$ times the learning rate of other parts (or the overall learning rate) |\n| LM Head Scaling | Adjust the output logits to $1/(d_{m}/d_{base})$ times the original value |", "caption": "Table 6: List of operations used when applying tensor program techniques.", "description": "This table details the specific operations performed when applying tensor program techniques within the Xmodel-2 architecture.  Each row represents a specific scaling or modification applied to different parts of the model, such as embedding outputs, residual connections, and tensor initializations. The 'Specific Operation' column describes precisely how each technique alters model parameters and learning rates to enhance efficiency and transferability across different model scales.", "section": "2.1 Model Architecture"}]