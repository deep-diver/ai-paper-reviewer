{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report on GPT-4, a leading proprietary multimodal large language model, which serves as a significant benchmark for the presented Baichuan-Omni-1.5 model."}, {"fullname_first_author": "Chaoyou Fu", "paper_title": "VITA-1.5: Towards gpt-40 level real-time vision and speech interaction", "publication_date": "2025-01-27", "reason": "As a leading omni-modal model, VITA-1.5 is directly compared with Baichuan-Omni-1.5 in the paper, making it crucial for establishing the latter's performance and capabilities."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "Qwen is another major open-source multimodal large language model frequently mentioned in the paper, providing essential context for understanding the relative strengths of Baichuan-Omni-1.5."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-00-00", "reason": "This paper introduces the MMLU benchmark, a key evaluation metric used to assess the language understanding capabilities of Baichuan-Omni-1.5, providing context for its performance in comparison to other models."}, {"fullname_first_author": "Yuan Liu", "paper_title": "MMBench: Is your multi-modal model an all-around player?", "publication_date": "2023-07-06", "reason": "The MMBench benchmark, detailed in this paper, is used extensively to evaluate Baichuan-Omni-1.5's image understanding capabilities, making it a crucial reference for assessing its overall performance."}]}