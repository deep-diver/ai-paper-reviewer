{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This is the seminal paper that introduced the Transformer architecture, which is the foundation of the Forgetting Transformer discussed in the paper."}, {"fullname_first_author": "Zhixuan Lin", "paper_title": "Forgetting transformer: Softmax attention with a forget gate", "publication_date": "2025-01-01", "reason": "This paper introduces the Forgetting Transformer (FoX), the central model that Adaptive Computation Pruning (ACP) is designed for."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-01-01", "reason": "This work details the LLaMA architecture, which serves as the base for the FoX (Pro) model used in the experiments, making it essential for understanding the model setup."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention-2: Faster attention with better parallelism and work partitioning", "publication_date": "2024-01-01", "reason": "This paper introduces FlashAttention, the algorithm used as the basis for the FoX implementation which the current paper modifies and optimizes with Adaptive Computation Pruning(ACP)."}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2024-01-01", "reason": "This paper introduces RoPE, a standard transformer which the Forgetting Transformer improves and is compared against."}]}