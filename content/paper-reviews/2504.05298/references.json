{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-01-01", "reason": "This paper is important as it introduces Flamingo, a visual language model, which is relevant to the current work's use of visual and language information for video generation."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-01-01", "reason": "This paper is critical because FlashAttention addresses the memory and computational inefficiencies of standard attention mechanisms, and the current work uses techniques from FlashAttention to reduce I/O latency."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2024-01-01", "reason": "This paper is important as it introduces the Mamba architecture, which provides a linear complexity alternative to self-attention, and the current work compares its TTT-layers against Mamba."}, {"fullname_first_author": "Yu Sun", "paper_title": "Learning to (learn at test time): Rnns with expressive hidden states", "publication_date": "2024-01-01", "reason": "This paper is highly relevant as it directly introduces and develops the Test-Time Training (TTT) layers, which are a core component of the current work."}, {"fullname_first_author": "Wenyi Hong", "paper_title": "Cogvideo: Large-scale pretraining for text-to-video generation via transformers", "publication_date": "2023-01-01", "reason": "This paper is crucial as it details CogVideo, the pre-trained diffusion transformer that serves as the starting point for the current work's architecture."}]}