{"importance": "This paper introduces Test-Time Training (TTT) layers to enhance long-context video generation, providing a promising approach for creating coherent, complex video stories and opening new avenues for efficient video Transformers.", "summary": "TTT layers enable minute-long video generation with coherent stories from text, overcoming long-context limitations.", "takeaways": ["Test-Time Training (TTT) layers improve video coherence in long-context generation.", "The study uses a dataset based on Tom and Jerry cartoons to demonstrate the method's effectiveness.", "TTT layers combined with Diffusion Transformers significantly outperform other methods in video quality and consistency."], "tldr": "Generating long videos with coherent stories is challenging because of the inefficiency of self-attention layers in Transformers for long contexts. Alternatives like Mamba layers have limitations in expressing complex multi-scene stories, hindering their ability to create long, coherent videos with dynamic motion. Overcoming these limitations is essential for advancing video generation technology and making it more useful.\n\nThis paper introduces **Test-Time Training (TTT) layers**, enhancing a pre-trained Diffusion Transformer's ability to generate minute-long videos from text storyboards. By integrating TTT layers, which have more expressive hidden states, the model can produce coherent scenes with dynamic motion. The approach uses a curated dataset based on Tom and Jerry cartoons to demonstrate its effectiveness, outperforming baselines in human evaluations.", "affiliation": "NVIDIA", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2504.05298/podcast.wav"}