[{"content": "| Model | Representation | FVD \u2193 | MSE \u2193 | LPIPS \u2193 | PSNR \u2191 | SSIM \u2191 |\n|---|---|---|---|---|---|---|\n| Baseline | 6-view cubemaps | 196.7 | 0.10 | 0.09 | 26.1 | 0.88 |\n| GenEx w/o SCL | panorama | 81.9 | 0.05 | 0.05 | 29.4 | 0.91 |\n| **GenEx** | panorama | **69.5** | **0.04** | **0.03** | **30.2** | **0.94** |", "caption": "Table 1: GenEx with high generation quality.", "description": "This table compares the video generation quality of GenEx, a system for generating explorable worlds from single images, with a baseline model.  The metrics used are Fr\u00e9chet Video Distance (FVD), Mean Squared Error (MSE), Learned Perceptual Image Patch Similarity (LPIPS), Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM). Lower values are better for FVD, MSE, and LPIPS, while higher values are better for PSNR and SSIM. The table shows that GenEx achieves higher quality video generation across all metrics, both with and without spherical-consistent learning (SCL). The models are evaluated using 6-view cubemap and panorama representations.", "section": "5. Applications"}, {"content": "| Method | Acc. (%) | Confidence (%) | Logic Acc. (%) |\n|---|---|---|---| \n| Random | 25.00 | 25.00 | - |\n| Human Text-only | 44.82 | 52.19 | 46.82 |\n| Human with Image | 91.50 | 80.22 | 70.93 |\n| Human with **GenEx** | **94.00** | **90.77** | **86.19** |\n| Unimodal Gemini-1.5 | 30.56 | 29.46 | 13.89 |\n| Unimodal GPT-4o | 27.71 | 26.38 | 20.22 |\n| Multimodal Gemini-1.5 | 46.73 | 36.70 | 0.0 |\n| Multimodal GPT-4o | 46.10 | 44.10 | 12.51 |\n| **GPT4-o with GenEx** | **85.22** | **77.68** | **83.88** |", "caption": "Table 2: Eval of Imagination-Augmented Policy.", "description": "This table evaluates the performance of the Imagination-Augmented Policy, a novel approach for enhancing decision-making in embodied AI. The policy allows agents to explore unseen environments through imagined observations generated by GenEx, leading to more informed actions.  The table compares the performance of various models, including unimodal (text-only) and multimodal (text and image) LLMs, with and without the GenEx augmentation. It also includes human performance as a baseline. The metrics used for evaluation are accuracy, confidence, and logical accuracy, showcasing the impact of imagined observations on decision-making quality.", "section": "5.6. Embodied Decision Making"}, {"content": "| Method | Acc. (%) | Confidence (%) | Logic Acc. (%) |\n|---|---|---|---| \n| Random | 25.00 | 25.00 | - |\n| Human Text-only | 21.21 | 11.56 | 13.50 |\n| Human with Image | 55.24 | 58.67 | 46.49 |\n| Human with **GenEx** | **77.41** | **71.54** | **72.73** |\n| Unimodal Gemini-1.5 | 26.04 | 24.37 | 5.56 |\n| Unimodal GPT-4o | 25.88 | 26.99 | 5.00 |\n| Multimodal Gemini-1.5 | 11.54 | 15.35 | 0.0 |\n| Multimodal GPT-4o | 21.88 | 21.16 | 6.25 |\n| **GPT4-o with GenEx** | **94.87** | **69.21** | **72.11** |", "caption": "Table 3: Evaluation of Multi-Agent Imagination-Augmented Policy.", "description": "This table presents the evaluation results of the Multi-Agent Imagination-Augmented Policy in a scenario involving multiple agents. It assesses the policy's effectiveness by measuring the decision-making accuracy (Acc.), confidence level (Confidence), and logical reasoning accuracy (Logic Acc.) of the agents. Different methods, including random selection, human input with and without access to GenEx, and unimodal/multimodal responses from Gemini 1.5 and GPT-40 are compared.  The results highlight the impact of imagination on decision-making in multi-agent settings.", "section": "5.6. Embodied Decision Making"}]