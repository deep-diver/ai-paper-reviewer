{"importance": "This work pioneers adaptable mobile GUI agents using demonstration-based learning. It introduces the LearnAct framework and LearnGUI benchmark, offering valuable tools and insights for creating more personalized and deployable mobile agents, impacting automation and accessibility research.", "summary": "LearnAct: A new approach for mobile GUI agents using human demonstrations and a unified benchmark.", "takeaways": ["LearnAct, a multi-agent framework, enhances mobile GUI agent adaptability through demonstration-based learning.", "LearnGUI, a new benchmark dataset, facilitates research into demonstration-based learning for mobile GUI agents.", "Experimental results show significant performance gains using LearnAct in both offline and online evaluations."], "tldr": "**Mobile GUI agents** face challenges in diverse real-world scenarios due to the vast variety of mobile applications and user interfaces. Traditional approaches that rely on pre-training or fine-tuning with massive datasets struggle to generalize effectively in unseen scenarios. This limits the widespread adoption of mobile GUI agents. There is a need for approaches that achieve both robustness and personalization by learning from a small number of user-provided examples. \n\nThe paper introduces **LearnAct, a novel multi-agent framework**, and LearnGUI benchmark that enhances mobile GUI agent capabilities through few-shot demonstration learning. LearnAct includes DemoParser, KnowSeeker and ActExecutor. Results show single demonstrations improve performance and enhance task success rates, establishing demonstration-based learning as a promising direction. ", "affiliation": "Zhejiang University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Human-AI Interaction"}, "podcast_path": "2504.13805/podcast.wav"}