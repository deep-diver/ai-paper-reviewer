[{"figure_path": "https://arxiv.org/html/2504.12369/x2.png", "caption": "Figure 1: WorldMem\u00a0enables long-term consistent world simulation with an integrated memory mechanism. (a) Previous world simulation methods typically face the problem of inconsistent world due to limited temporal context window size. (b) WorldMem\u00a0 empowers the agent to explore diverse and consistent worlds with an expansive action space, e.g., crafting environments by placing objects like pumpkin light or freely roaming around. Most importantly, after exploring for a while and glancing back, we find the objects we placed are still there, with the inspiring sight of the light melting surrounding snow, testifying to the passage of time. Red and green boxes indicate scenes that should be consistent. Project page at https://xizaoqu.github.io/worldmem.", "description": "This figure compares two scenarios of world simulation: one without memory and one with WorldMem's integrated memory mechanism.  The top row (a) shows a previous method that struggles with world consistency. The limited temporal context window leads to changes in the environment after the agent moves and returns to a previous viewpoint. The bottom row (b) demonstrates WorldMem's ability to maintain long-term consistency. By using memory, the agent can place objects (like pumpkin lights) and freely roam, returning later to find the objects still in their original location and observing temporal changes (snow melting near the light).  The red and green boxes highlight scene areas where consistency should be maintained.  WorldMem\u2019s memory mechanism enables consistent world simulation even after lengthy explorations.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.12369/x3.png", "caption": "Figure 2: Comprehensive overview of WorldMem. The framework comprises a conditional diffusion transformer integrated with memory blocks, with a dedicated memory bank storing memory units from previously generated content. By retrieving these memory units from the memory bank and incorporating the information by memory blocks to guide generation, our approach ensures long-term consistency in world simulation.", "description": "WORLDMEM's architecture consists of a conditional diffusion transformer enhanced with memory blocks and a memory bank. The memory bank stores past frames and their associated states (e.g., poses, timestamps). During generation, relevant memories are retrieved from the bank via a memory attention mechanism. This mechanism utilizes Pl\u00fccker embeddings and relative positional embeddings to effectively extract information from past frames, based on their states, enabling the accurate reconstruction of previous scenes. By incorporating timestamps into the memory, WORLDMEM models not just static scenes but also captures dynamic changes over time. This approach allows for robust long-term consistency during world simulation.", "section": "3. WORLDMEM"}, {"figure_path": "https://arxiv.org/html/2504.12369/x4.png", "caption": "Figure 3: Two-view FOV overlapping visualization", "description": "This figure visualizes how the field of view (FOV) overlap is calculated between two different viewpoints.  The Monte Carlo sampling method, used for calculating the overlap ratio, is illustrated. The overlap ratio is a key component of the memory retrieval algorithm, which helps to identify relevant memory frames to use during scene generation.", "section": "3.4 Memory Retrieve"}, {"figure_path": "https://arxiv.org/html/2504.12369/x5.png", "caption": "Figure 4: Qualitative results. We showcase WorldMem\u2019s capabilities through two sets of examples.\nTop: A comparison with Ground Truth (GT). WorldMem accurately models diverse dynamics (e.g., rain) by conditioning on 600 past frames, ensuring temporal consistency.\nBottom: Interaction with the world. Objects like hay in the desert or wheat in the plains persist over time, with wheat visibly growing. For the best experience, see the supplementary videos.", "description": "Figure 4 presents a qualitative evaluation of WorldMem's performance in generating long-term consistent world simulations. The top row compares WorldMem's output to the ground truth, demonstrating its ability to accurately capture diverse dynamic elements, such as rain, over extended sequences (600 frames). This highlights WorldMem's temporal consistency. The bottom row showcases WorldMem's ability to maintain consistency in object interactions over time. For example, objects placed in the environment (hay in a desert, wheat in a plain) persist over time, and even show dynamic changes (e.g., wheat visibly growing), demonstrating the model's ability to accurately simulate object behavior and environmental changes over long time frames. Supplementary videos are recommended for a complete understanding.", "section": "4. Results on Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.12369/x6.png", "caption": "Figure 5: Within context window evaluation examples. It illustrates an example where the motion sequence first involves turning right and then returning to the original position, demonstrating methods\u2019 ability to maintain self-contained consistency.", "description": "This figure demonstrates the ability of different methods to maintain consistency within a limited temporal window. The example shows an agent turning right and then returning to its starting position.  The image visually compares the results of different world simulation approaches, highlighting whether they successfully preserve the scene's state after the sequence of actions.  Inconsistencies in the generated environment after the agent returns to its starting point would indicate a failure to maintain short-term consistency.", "section": "4.1 Results on Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.12369/x7.png", "caption": "Figure 6: Beyond context window evaluation examples. It shows that Diffusion-Forcing suffers from inconsistency and quality degradation after generating a certain number of frames. In contrast, our method maintains high quality and faithfully reconstructs previously observed scenarios.", "description": "This figure demonstrates the long-term consistency capabilities of the proposed WORLDMEM model compared to the Diffusion-Forcing (DF) baseline.  The top row shows ground truth frames from the Minecraft environment. The middle row displays frames generated by DF, which loses visual consistency and quality after a certain number of frames, showing inconsistency in reconstructing scenes after generating a long sequence. The bottom row showcases WORLDMEM\u2019s results, demonstrating faithful reconstruction and maintained high image quality, even when the viewpoint revisits previously generated scenes.  The results highlight WORLDMEM's ability to overcome the limitations of limited context window, enabling it to build consistent long-term simulations.", "section": "4.1 Results on Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.12369/x8.png", "caption": "Figure 7: Examples on RealEstate\u00a0[51]. DFoT\u00a0[32] discards content beyond its context window, losing 360-degree consistency. In contrast, our method preserves details and accurately returns to the original location.", "description": "This figure demonstrates a comparison of the results obtained from using two different methods (DFoT and the proposed method) for generating a sequence of images from a real-world dataset. DFoT, due to its limited context window, fails to maintain consistency across a 360-degree rotation, showing significant discrepancies in the generated images as the viewpoint changes. In contrast, the proposed method, by employing a memory mechanism, successfully maintains consistency throughout the rotation, demonstrating a remarkable ability to preserve image details and accurately return to the original viewpoint.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.12369/x9.png", "caption": "Figure 8: Long-term Generation Comparison. This figure presents the PSNR of different ablation methods compared to the ground truth over a 300-frame sequence. The results show that our method without memory blocks or using random memory retrieval exhibits immediate inconsistencies with the ground truth. Additionally, the model lacking relative embeddings begins to degrade significantly beyond 100 frames. In contrast, our full method maintains strong consistency even beyond 300 frames.", "description": "This figure compares the performance of several variations of the WORLDMEM model on a 300-frame sequence generation task, evaluating the Peak Signal-to-Noise Ratio (PSNR) against the ground truth.  The variations include removing the memory blocks, using a random memory retrieval method, and removing relative embeddings. The results demonstrate that the complete WORLDMEM model (with memory blocks, strategic retrieval, and relative embeddings) maintains high consistency and accuracy throughout the entire 300-frame sequence, while the other variants show significant inconsistencies, particularly after frame 100.", "section": "4.1 Results on Generation Benchmark"}, {"figure_path": "https://arxiv.org/html/2504.12369/x10.png", "caption": "Figure 9: Results w/o and w/ time condition. Without timestamps, the model fails to differentiate memory units from the same location at different times, causing errors. With time conditioning, it aligns with the updated world state, ensuring consistency.", "description": "This figure demonstrates the importance of incorporating timestamps into the memory mechanism.  The left side shows the results without timestamps.  Because the model can't distinguish between memory units from the same location but at different times, inconsistencies arise in the generated world. The right side shows the results with timestamps included. In this case, the model correctly identifies and uses the appropriate memory units based on both location and time, leading to a consistent and accurate simulation.", "section": "4.2. Ablation"}, {"figure_path": "https://arxiv.org/html/2504.12369/x11.png", "caption": "Figure 10: Illustration of different embeddings.", "description": "This figure details the design of the different embeddings used in WORLDMEM.  It shows four subfigures: (a) Timestep Embedding, illustrating how temporal information is encoded; (b) Action Embedding, showcasing the representation of actions performed by the agent; (c) Pose Embedding, which represents the camera's position and orientation in 3D space using Pl\u00fccker coordinates; and (d) Timestamp Embedding, which encodes the time information associated with each frame.  Each subfigure shows the input dimensions, any transformations (like sinusoidal or linear embeddings, or Pl\u00fccker embedding), and the resulting embedding dimensions.", "section": "3. WORLDMEM"}, {"figure_path": "https://arxiv.org/html/2504.12369/x12.png", "caption": "Figure 11: Structure of pose predictor.", "description": "The pose predictor module predicts the pose of the next frame, taking the previous image, pose, and the upcoming action as inputs.  It uses a 3-layer CNN to process the image, followed by fully connected layers to combine image features, the previous pose, and the action embedding. Finally, a 2-layer MLP outputs the predicted next pose. This is crucial for enabling autonomous operation without needing ground truth poses during inference.", "section": "3. WORLDMEM"}, {"figure_path": "https://arxiv.org/html/2504.12369/x13.png", "caption": "Figure 12: Training Examples. Our training environments encompass diverse terrains, action spaces, and weather conditions, providing a comprehensive setting for learning.", "description": "Figure 12 showcases the diverse training environments used in the WORLDMEM model.  These environments include various terrains (e.g., plains, deserts, snowy areas), a wide range of agent actions (movement, object interaction, etc.), and different weather conditions. This variety ensures that the model is robust and generalizes well to unseen scenarios.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.12369/x14.png", "caption": "Figure 13: Visualization of Trajectory Examples in the X-Z Space. The axis scales represent distances within the Minecraft environment.", "description": "This figure visualizes example trajectories of an agent within a Minecraft environment, specifically focusing on the X and Z coordinates. The scale of the axes directly corresponds to the distances within the game world. Each trajectory line represents the path taken by the agent over a sequence of 100 frames. The seemingly random nature of these paths indicates that the agent was operating under a stochastic policy and highlights the diversity of actions and environments that the model is capable of simulating.", "section": "4. Experiments"}]