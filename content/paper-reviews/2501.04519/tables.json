[{"content": "| Task | rStar-Math (Qwen-7B) | rStar-Math (Qwen-1.5B) | rStar-Math (Phi3-mini) | OpenAI o1-preview | OpenAI o1-mini | QWQ 32B-preview | GPT-4o | DeepSeek-V3 |\n|---|---|---|---|---|---|---|---|---|\n| MATH | 90.0 | 88.6 | 86.4 | 85.5 | 90.0 | 90.6 | 76.6 | 90.2 |\n| AIME 2024 | 53.3 | 46.7 | 43.3 | 44.6 | 56.7 | 50.0 | 9.3 | 39.2 |\n| Olympiad Bench | 65.6 | 64.6 | 60.3 | - | 65.3 | 61.2 | 43.3 | 55.4 |\n| College Math | 60.5 | 59.3 | 59.1 | - | 57.8 | 55.8 | 48.5 | 58.9 |\n| Omni-Math | 50.5 | 48.5 | 46.0 | 52.5 | 60.5 | 49.6 | 30.5 | 35.9 |", "caption": "Table 1: \\sysname enables frontier math reasoning in SLMs via deep thinking over 64 trajectories.", "description": "This table presents the performance of the rStar-Math model on various math reasoning tasks.  It shows the accuracy (pass@1) achieved by rStar-Math when applied to several small language models (SLMs) of varying sizes. The results are compared against the performance of OpenAI's models and other baselines, highlighting the significant improvements in math reasoning capabilities achieved by rStar-Math through its deep thinking approach.  The table demonstrates the model's ability to achieve state-of-the-art results on standard benchmarks and even surpass the performance of larger, more powerful models.", "section": "1 Introduction"}, {"content": "| # | models in MCTS | GSM-level | MATH-level | Olympiad-level | All |\n|---|---|---|---|---|---| \n| Round 1 | DeepSeek-Coder-V2-Instruct | 96.61% | 67.36% | 20.99% | 60.17% |\n| Round 2 | policy SLM-r1 | 97.88% | 67.40% | 56.04% | 66.60% |\n| Round 3 | policy SLM-r2, PPM-r2 | 98.15% | 88.69% | 62.16% | 77.86% |\n| Round 4 | policy SLM-r3, PPM-r3 | 98.15% | 94.53% | 80.58% | 90.25% |", "caption": "Table 2: Percentage of the 747k math problems correctly solved in each round. Only problems have correct solutions are included in the training set. The first round uses DeepSeek-Coder-Instruct as the policy LLM, while later rounds use our fine-tuned 7B policy SLM.", "description": "This table shows the percentage of 747,000 math problems that were correctly solved by the model in each of four training rounds.  Only problems with verifiable correct solutions were included in the training data.  The first round used a pre-trained large language model (DeepSeek-Coder-Instruct) as the initial policy model. Subsequent rounds used a smaller, fine-tuned 7-billion parameter language model that was iteratively improved during the self-evolution process. The table provides a measure of the model's progress in solving increasingly difficult math problems across the training rounds.", "section": "3.4 Self-Evolved Deep Thinking"}, {"content": "| Round# | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| DeepSeek-Coder-V2-Instruct (bootstrap model) | 75.3 | 13.3 | 57.5 | 37.6 | 46.2 | 94.9 | 64.7 |\n| Base (Qwen2.5-Math-7B) | 58.8 | 0.0 | 22.5 | 21.8 | 41.6 | 91.6 | 51.7 |\n| policy SLM-r1 | 69.6 | 3.3 | 30.0 | 34.7 | 44.5 | 88.4 | 57.4 |\n| policy SLM-r2 | 73.6 | 10.0 | 35.0 | 39.0 | 45.7 | 89.1 | 59.7 |\n| policy SLM-r3 | 75.8 | 16.7 | 45.0 | 44.1 | 49.6 | 89.3 | 62.8 |\n| policy SLM-r4 | 78.4 | 26.7 | 47.5 | 47.1 | 52.5 | 89.7 | 65.7 |", "caption": "Table 3: Pass@1 accuracy of the resulting policy SLM in each round, showing continuous improvement until surpassing the bootstrap model.", "description": "Table 3 presents the accuracy (Pass@1) of the policy model across four rounds of training.  The initial policy model (Round 0, bootstrap model) is a pre-trained model. Each subsequent round represents an iterative improvement of the policy model, trained using data generated by the previous round. The table demonstrates consistent accuracy improvement across rounds, culminating in performance that surpasses that of the initial, pre-trained bootstrap model.", "section": "3.4 Self-Evolved Deep Thinking"}, {"content": "| Round# | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| PPM-r1 | 75.2 | 10.0 | 57.5 | 35.7 | 45.4 | 90.9 | 60.3 |\n| PPM-r2 | 84.1 | 26.7 | 75.0 | 52.7 | 54.2 | 93.3 | 73.0 |\n| PPM-r3 | 85.2 | 33.3 | 77.5 | 59.5 | 55.6 | 93.9 | 76.6 |\n| PPM-r4 | 87.0 | 43.3 | 77.5 | 61.5 | 56.8 | 94.2 | 77.8 |", "caption": "Table 4: The quality of PPM consistently improves across rounds. The policy model has been fixed with policy SLM-r1 for a fair comparison.", "description": "Table 4 presents the performance of the Process Preference Model (PPM) across four rounds of training.  The key point is that the PPM's ability to accurately evaluate the quality of reasoning steps improves with each round.  To ensure a fair comparison and isolate the PPM's progress, the policy model used remained consistent (policy SLM-r1) throughout the four rounds.  The table shows accuracy scores across various math reasoning benchmarks (MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, GaokaoEn 2023) for each round's PPM, highlighting the steady improvement in performance.", "section": "3.3 Process Preference Model"}, {"content": "| Model | Method | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | Gaokao En 2023 |\n|---|---|---|---|---|---|---|---|---|\n| *Frontier LLMs* |  |  |  |  |  |  |  |  |\n| GPT-4o | System 1 | 76.6 | 9.3 | 47.5 | 43.3 | 48.5 | 92.9 | 67.5 |\n| Claude3.5-Sonnet | System 1 | 78.3 | 16.0 | - | - | - | 96.4 | - |\n| GPT-o1-preview | - | 85.5 | 44.6 | 90.0 | - | - | - | - |\n| GPT-o1-mini | - | **90.0** | **56.7** | **95.0** | **65.3** | 57.8 | 94.8 | 78.4 |\n| *Open-Sourced Reasoning LLMs* |  |  |  |  |  |  |  |  |\n| DeepSeek-Coder-V2-Instruct | System 1 | 75.3 | 13.3 | 57.5 | 37.6 | 46.2 | 94.9 | 64.7 |\n| Mathstral-7B-v0.1 | System 1 | 57.8 | 0.0 | 37.5 | 21.5 | 33.7 | 84.9 | 46.0 |\n| NuminaMath-72B-CoT | System 1 | 64.0 | 3.3 | 70.0 | 32.6 | 39.7 | 90.8 | 58.4 |\n| LLaMA3.1-8B-Instruct | System 1 | 51.4 | 6.7 | 25.0 | 15.4 | 33.8 | 76.6 | 38.4 |\n| LLaMA3.1-70B-Instruct | System 1 | 65.4 | 23.3 | 50.0 | 27.7 | 42.5 | 94.1 | 54.0 |\n| Qwen2.5-Math-72B-Instruct | System 1 | 85.6 | 30.0 | 70.0 | 49.0 | 49.5 | 95.9 | 71.9 |\n| Qwen2.5-Math-72B-Instruct+72B ORM | System 2 | 85.8 | 36.7 | 72.5 | 54.5 | 50.6 | 96.4 | 76.9 |\n| *General Base Model: Phi3-mini-Instruct (3.8B)* |  |  |  |  |  |  |  |  |\n| Phi3-mini-Instruct (base model) | System 1 | 41.4 | 3.33 | 7.5 | 12.3 | 33.1 | 85.7 | 37.1 |\n|  \n\\sysname **(3.8B SLM+7B PPM)** | System 2 | **85.4** | **40.0** | **77.5** | **59.3** | **58.0** | **94.5** | **77.1** |\n|  \n\\sysname<sup style=\"font-size:90%;background-color:#D3E9E8;\">*64*</sup> **(3.8B SLM+7B PPM)** | System 2 | **86.4** | **43.3** | **80.0** | **60.3** | **59.1** | **94.7** | **77.7** |\n| *Math-Specialized Base Model: Qwen2.5-Math-1.5B* |  |  |  |  |  |  |  |  |\n| Qwen2.5-Math-1.5B (base model) | System 1 | 51.2 | 0.0 | 22.5 | 16.7 | 38.4 | 74.6 | 46.5 |\n| Qwen2.5-Math-1.5B-Instruct | System 1 | 60.0 | 10.0 | 60.0 | 38.1 | 47.7 | 84.8 | 65.5 |\n| Qwen2.5-Math-1.5B-Instruct+72B ORM | System 2 | 83.4 | 20.0 | 72.5 | 47.3 | 50.2 | 94.1 | 73.0 |\n|  \n\\sysname **(1.5B SLM+7B PPM)** | System 2 | **87.8** | **46.7** | **80.0** | **63.5** | **59.0** | **94.3** | **77.7** |\n|  \n\\sysname<sup style=\"font-size:90%;background-color:#D3E9E8;\">*64*</sup> **(1.5B SLM+7B PPM)** | System 2 | **88.6** | **46.7** | **85.0** | **64.6** | **59.3** | **94.8** | **79.5** |\n| *Math-Specialized Base Model: Qwen2.5-Math-7B* |  |  |  |  |  |  |  |  |\n| Qwen2.5-Math-7B (base model) | System 1 | 53.4 | 3.3 | 25.0 | 17.3 | 39.4 | 80.4 | 47.3 |\n| Qwen2.5-Math-7B-Instruct | System 1 | 73.2 | 13.3 | 62.5 | 38.2 | 45.9 | 89.9 | 62.1 |\n| Qwen2.5-Math-7B-Instruct+72B ORM | System 2 | 83.4 | 23.3 | 62.5 | 47.6 | 47.9 | **95.1** | 71.9 |\n|  \n\\sysname **(7B SLM+7B PPM)** | System 2 | **88.2** | **43.3** | **80.0** | **63.1** | **58.4** | 94.6 | **78.2** |\n|  \n\\sysname<sup style=\"font-size:90%;background-color:#D3E9E8;\">*64*</sup> **(7B SLM+7B PPM)** | System 2 | **88.6** | **46.7** | **85.0** | **63.4** | **59.3** | 94.8 | **79.2** |\n| *Math-Specialized Base Model: Qwen2.5-Math-7B* |  |  |  |  |  |  |  |  |\n| Qwen2.5-Math-7B (base model) | System 1 | 58.8 | 0.0 | 22.5 | 21.8 | 41.6 | 91.6 | 51.7 |\n| Qwen2.5-Math-7B-Instruct | System 1 | 82.6 | 6.0 | 62.5 | 41.6 | 46.8 | 95.2 | 66.8 |\n| Qwen2.5-Math-7B-Instruct+72B ORM | System 2 | 88.4 | 26.7 | 75.0 | 49.9 | 49.6 | **97.9** | 75.1 |\n|  \n\\sysname **(7B SLM+7B PPM)** | System 2 | **89.4** | **50.0** | **87.5** | **65.3** | **59.0** | 95.0 | **80.5** |\n|  \n\\sysname<sup style=\"font-size:90%;background-color:#D3E9E8;\">*64*</sup> **(7B SLM+7B PPM)** | System 2 | **90.0** | **53.3** | **87.5** | **65.6** | **60.5** | 95.2 | **81.3** |", "caption": "Table 5: The results of \\sysname and other frontier LLMs on the most challenging math benchmarks. \\sysname64 shows the Pass@1 accuracy achieved when sampling 64 trajectories.", "description": "Table 5 presents a comparison of the performance of the rStar-Math model against state-of-the-art Large Language Models (LLMs) on a variety of challenging mathematical benchmarks.  It shows the Pass@1 accuracy (the percentage of problems solved correctly in a single attempt) for each model across different datasets, including MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, and GaokaoEn 2023.  The table also includes results for rStar-Math64, which represents the performance when the model generates 64 solution attempts for each problem, showcasing the impact of increased computational resources on accuracy.  The benchmarks range in difficulty, from commonly used datasets like GSM8K to more challenging competition-level datasets such as AIME (American Invitational Mathematics Examination) and Olympiad Bench, providing a comprehensive evaluation of the models' capabilities.", "section": "4 Evaluation"}, {"content": "| Round# | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| GPT-4o | 76.6 | 9.3 | 47.5 | 43.3 | 48.5 | 92.9 | 67.5 |\n| Base 7B model | 58.8 | 0.0 | 22.5 | 21.8 | 41.6 | 91.6 | 51.7 |\n| \\sysname Round 1 | 75.2 | 10.0 | 57.5 | 35.7 | 45.4 | 90.9 | 60.3 |\n| \\sysname Round 2 | 86.6 | 43.3 | 75.0 | 59.4 | 55.6 | 94.0 | 76.4 |\n| \\sysname Round 3 | 87.0 | 46.7 | 80.0 | 61.6 | 56.5 | 94.2 | 77.1 |\n| \\sysname Round 4 | 89.4 | 50.0 | 87.5 | 65.3 | 59.0 | 95.0 | 80.5 |", "caption": "Table 6: The continuously improved math reasoning capabilities through \\sysname self-evolved deep thinking. Starting from round 2, the 7B base model powered by \\sysname surpasses GPT-4o.", "description": "This table presents the performance of the rStar-Math model across four rounds of self-evolution. It shows how the model's accuracy improves on various mathematical reasoning benchmarks (MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, GaokaoEn 2023) as it iteratively learns and refines its reasoning capabilities. Notably, starting from round 2, the 7B base model enhanced by rStar-Math surpasses the performance of GPT-4, a leading large language model, demonstrating the effectiveness of the self-evolution approach.", "section": "4 Evaluation"}, {"content": "| Dataset | MATH | AIME | AMC | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| GPT-4o | - | 76.6 | 9.3 | 47.5 | 43.3 | 48.5 | **92.9** | **67.5** |\n| GPT4-distillation (Open-sourced) | MetaMath | 55.2 | 3.33 | 32.5 | 19.1 | 39.2 | 85.1 | 43.6 |\n| NuminaMath-CoT | 69.6 | 10.0 | **50.0** | 37.2 | 43.4 | 89.8 | 59.5 |\n| Self-generation by policy SLM-r3 | Random sample | 72.4 | 10.0 | 45.0 | 41.0 | 48.0 | 87.5 | 57.1 |\n|  | Rejection sampling | 73.4 | 13.3 | 47.5 | 44.7 | 50.8 | 89.3 | 61.7 |\n| Step-by-step verified (ours) | **78.4** | **26.7** | 47.5 | **47.1** | **52.5** | 89.7 | 65.7 |", "caption": "Table 7: Ablation study on the effectiveness of our step-by-step verified reasoning trajectories as the SFT dataset. We report the SFT accuracy of Qwen2.5-Math-7B fine-tuned with different datasets.", "description": "This table presents an ablation study evaluating the effectiveness of using step-by-step verified reasoning trajectories as a training dataset for fine-tuning the Qwen2.5-Math-7B language model.  It compares the model's performance after fine-tuning on this dataset against its performance after training on several other datasets, including those generated by GPT-4 and other methods such as random sampling and rejection sampling. The results are reported as Pass@1 accuracy across various mathematical reasoning benchmarks, allowing for a comparison of the different training approaches and their impact on the final model's accuracy.", "section": "3.2 Step-by-Step Verified Reasoning Trajectory"}, {"content": "| RM | Inference | MATH | AIME | AMC | Olympiad Bench | College Math | GSM8K | GaokaoEn |\n|---|---|---|---|---|---|---|---|---|\n| o1-mini | - | 90.0 | 56.7 | 95.0 | 65.3 | 55.6 | 94.8 | 78.6 |\n| ORM | Best-of-N | 82.6 | 26.7 | 65.0 | 55.1 | 55.5 | 92.3 | 72.5 |\n| PQM | MCTS | 88.2 | 46.7 | 85.0 | 62.9 | 57.6 | 94.6 | 79.5 |\n| PPM | MCTS | 89.4 | 50.0 | 87.5 | 65.3 | 59.0 | 95.0 | 80.5 |", "caption": "Table 8: Ablation study on the reward model. Process reward models (PQM and PPM) outperform ORM, with PPM pushing the frontier of math reasoning capabilities.", "description": "This table presents an ablation study comparing three different reward model approaches for a System 2 math reasoning model: an Outcome Reward Model (ORM), a Process Reward Model using Q-values (PQM), and a Process Preference Model (PPM).  The goal is to assess the impact of the reward model on the overall accuracy of the system.  The results show that both PQM and PPM outperform the ORM, demonstrating the benefit of using fine-grained feedback on the reasoning process rather than only considering the final outcome. The PPM achieved the best performance and enabled the model to reach state-of-the-art accuracy, surpassing even the ORM used with a much larger model.", "section": "4.3 Ablation Study and Analysis"}, {"content": "| MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|\n| 5453 | 15693 | 14544 | 7889 | 4503 | 3299 | 6375 |", "caption": "Table 9: Inference costs of \\sysname. We show the average number of generated tokens required to generate a trajectory for a given question.", "description": "This table presents the average number of tokens generated by the rStar-Math model to produce a single reasoning trajectory for different types of math problems.  The token counts reflect the computational cost associated with generating solutions using Monte Carlo Tree Search (MCTS) for various problem difficulty levels (MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, GaokaoEn 2023). These costs are important for understanding the efficiency and scalability of the proposed method.", "section": "4 Evaluation"}, {"content": "| Model | MATH | AIME 2024 | AMC 2023 | Olympiad Bench | College Math | GSM8K | GaokaoEn 2023 |\n|---|---|---|---|---|---|---|---| \n| *General Base Model: Phi3-mini-Instruct (3.8B)* |  |  |  |  |  |  |  |\n| Phi3-mini-Instruct | 41.4 | 3.33 | 7.5 | 12.3 | 33.1 | 85.7 | 37.1 |\n| **Our policy model** | **68.0** | **10.0** | **37.5** | **36.6** | **48.7** | **87.9** | **53.2** |\n| *Math-Specialized Base Model: Qwen2.5-Math-1.5B* |  |  |  |  |  |  |  |\n| Qwen2.5-Math-1.5B | 51.2 | 0.0 | 22.5 | 16.7 | 38.4 | 74.6 | 46.5 |\n| Qwen2.5-Math-1.5B-Instruct | 60.0 | 10.0 | **60.0** | 38.1 | 47.7 | **84.8** | 65.5 |\n| **Our policy model** | **74.8** | **13.3** | 47.5 | **42.5** | **50.1** | 83.1 | **58.7** |\n| *Math-Specialized Base Model: Qwen2-Math-7B* |  |  |  |  |  |  |  |\n| Qwen2-Math-7B | 53.4 | 3.3 | 25.0 | 17.3 | 39.4 | 80.4 | 47.3 |\n| Qwen2-Math-7B-Instruct | 73.2 | 13.3 | **62.5** | 38.2 | 45.9 | **89.9** | 62.1 |\n| **Our policy model** | **73.8** | **16.7** | 45.0 | **43.9** | **52.0** | 88.3 | **65.2** |\n| *Math-Specialized Base Model: Qwen2.5-Math-7B* |  |  |  |  |  |  |  |\n| Qwen2.5-Math-7B | 58.8 | 0.0 | 22.5 | 21.8 | 41.6 | 91.6 | 51.7 |\n| Qwen2.5-Math-7B-Instruct | **82.6** | 6.0 | **62.5** | 41.6 | 46.8 | **95.2** | **66.8** |\n| **Our policy model** | 78.4 | **26.7** | 47.5 | **47.1** | **52.5** | 89.7 | 65.7 |", "caption": "Table 10: Pass@1 (greedy) accuracy of our fine-tuned policy models for Phi3-mini, Qwen2.5-Math-1.5B, Qwen2-Math-7B and Qwen2.5-Math-7B.", "description": "This table presents the Pass@1 accuracy, a measure of how often the model's top prediction is correct, for four different sized language models after fine-tuning.  The models are evaluated on several math reasoning benchmarks, including MATH, AIME 2024, AMC 2023, Olympiad Bench, College Math, GSM8K, and GaokaoEn 2023.  It compares the performance of the base, instruct-tuned, and rStar-Math-fine-tuned versions of each model to demonstrate the impact of the proposed method.", "section": "4 Evaluation"}]