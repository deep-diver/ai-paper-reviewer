{"references": [{"fullname_first_author": "Hunter Lightman", "paper_title": "Let's verify step by step", "publication_date": "2024", "reason": "This paper introduces the process reward model (PRM), a key component of the rStar-Math's deep thinking approach, which provides fine-grained feedback on intermediate steps during mathematical reasoning."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022", "reason": "This paper introduces the pairwise ranking loss used to train the process preference model (PPM), a crucial element in rStar-Math's self-evolution recipe."}, {"fullname_first_author": "David Silver", "paper_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm", "publication_date": "2017", "reason": "This paper introduces the Monte Carlo Tree Search (MCTS) algorithm, the core of rStar-Math's deep thinking framework, used for generating step-by-step verified reasoning trajectories."}, {"fullname_first_author": "Noah Shinn", "paper_title": "Language agents with verbal reinforcement learning", "publication_date": "2024", "reason": "This paper explores self-correction in LLMs, a concept relevant to rStar-Math's self-evolved deep thinking, which iteratively improves reasoning capabilities through self-reflection and refinement."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021", "reason": "This paper introduces the GSM8K benchmark dataset, one of the primary evaluation datasets used in rStar-Math to assess the performance of its deep thinking approach."}]}