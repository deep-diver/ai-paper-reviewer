[{"figure_path": "https://arxiv.org/html/2504.10443/x1.png", "caption": "Figure 1: Comparison of Visual and Audio Encoding in Video Modeling. (a) Existing methods encode each modality separately and then concatenate them, leading to inconsistencies and difficulties in handling long videos. (b) We propose Temporal Dynamic Context (TDC) compression, which incorporates both static visual features and dynamic video context to represent videos more effectively. This approach enables better multimodal integration and efficient compression for long videos.", "description": "Figure 1 illustrates two approaches to video encoding. (a) depicts traditional methods that process visual and audio data separately before merging them. This method is inefficient for long videos and may lead to inconsistencies. (b) introduces Temporal Dynamic Context (TDC), a novel technique which simultaneously encodes static visual features from key frames and dynamic video content.  The unified representation generated by TDC results in improved multimodal integration and efficient handling of long videos.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.10443/x2.png", "caption": "Figure 2: Architecture of Our Multimodal Video Encoder. We first extract features for each second of the video, including both visual and corresponding audio tokens. The first frame is selected as the static frame, and a Q-Former is used to perform Temporal Dynamic Context compression based on its relationship with subsequent frames, resulting in K\ud835\udc3eKitalic_K compressed tokens per frame. The final video representation consists of all static frame tokens and multimodal video context.", "description": "This figure illustrates the architecture of the multimodal video encoder used in the proposed model.  The process starts by extracting visual and audio features for each second of the input video. The first frame within each second is designated as the static frame and its features are preserved without compression. Subsequent frames within the same second are then processed using a Q-Former (a type of Transformer network) to perform Temporal Dynamic Context (TDC) compression. This compression method leverages the relationships between the static frame and subsequent frames to generate a condensed representation, resulting in K compressed tokens per frame (where K is a hyperparameter). Finally, the model combines the uncompressed static frame tokens with the compressed TDC tokens to create a comprehensive multimodal video representation.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2504.10443/x3.png", "caption": "Figure 3: Qualitative Demonstrations of Our 7B Model. (a) Our model can uniformly comprehend both audio and visual information, demonstrating strong performance in audio-visual dialogue tasks. (b) In movie description tasks, it can generate detailed descriptions of both the plot and visual elements. For extremely long videos, our LVCoT processes them segment by segment. The generated segment information, along with the timeline, serves as part of the reasoning process, enriching the final output with more details.", "description": "Figure 3 showcases the model's ability to integrate audio and visual information for improved understanding.  Part (a) demonstrates a dialogue task where the model correctly identifies both visual (number of people) and auditory (sound of a glass) details from a video clip. Part (b) shows the model's performance on a movie description task.  It provides a detailed summary that includes both plot points and specific visual elements, effectively demonstrating the model's comprehension of long videos through its LVCoT strategy, which processes videos segment-by-segment, enriching the final description by integrating information from different segments.", "section": "4.5. Qualitative Demonstrations"}]