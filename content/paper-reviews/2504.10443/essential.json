{"importance": "This work introduces a novel method for multimodal video understanding that enhances performance on various video understanding tasks and provides a strong baseline for future research.", "summary": "TDC enhances long video modeling by using temporal relationships between frames & integrates modalities for better understanding.", "takeaways": ["TDC effectively integrates visual and audio information by representing videos using static visual features and dynamic multimodal context.", "The LVCoT strategy enables MLLMs to process and reason over long videos in steps, improving performance without additional training.", "The method demonstrates strong performance on general video question answering, long video understanding, and audio-visual video comprehension benchmarks."], "tldr": "Recent advancements in Large Language Models have significantly improved language understanding. However, processing long videos remains difficult due to context length constraints and vast info. Existing methods often lose crucial data during token compression and struggle with incorporating multiple modalities like audio. Thus, this research aims to address these challenges.\n\nTo solve this, the paper introduces Temporal Dynamic Context (**TDC**), a dynamic long video encoding method using temporal frame relationships. **TDC** segments videos into consistent scenes, encodes frames using visual-audio encoders, and uses a transformer to compress tokens into temporal context tokens. This approach integrates visual, audio, and text modalities effectively. Also, it includes Long Video Chain-of-Thought (LVCoT) strategy for extremely long videos.", "affiliation": "Chinese University of Hong Kong", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Understanding"}, "podcast_path": "2504.10443/podcast.wav"}