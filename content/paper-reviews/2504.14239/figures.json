[{"figure_path": "https://arxiv.org/html/2504.14239/extracted/6373560/images/screenspot_pro_comparison.png", "caption": "Figure 1. Performance comparison of various GUI agents on the ScreenSpot-Pro benchmark. Our model, InfiGUI-R1-3B marked with a star, demonstrates competitive performance against models with larger parameter counts.", "description": "The figure showcases a performance comparison of different GUI agents on the ScreenSpot-Pro benchmark.  It plots the ScreenSpot-Pro scores (a measure of performance on GUI tasks) against the model's parameter count (representing model size).  The chart visually demonstrates that InfiGUI-R1-3B, despite having a relatively small parameter count (3 billion), achieves a competitive performance compared to significantly larger models (up to 72 billion parameters). This highlights the efficiency and effectiveness of the InfiGUI-R1-3B model architecture.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.14239/x1.png", "caption": "Figure 2. Overview of the Actor2Reasoner framework, a two-stage methodology for progressively transforming a Reactive Actor into a Deliberative Reasoner. Stage 1: Reasoning Injection uses Supervised Fine-Tuning (SFT) with Spatial Reasoning Distillation\u2014identifying reasoning bottlenecks (Pinpoint) and leveraging a teacher model (Distill)\u2014to instill foundational cross-modal reasoning and transition the agent into a Basic Reasoner (Perception \u2192\u2192\\rightarrow\u2192 Reasoning \u2192\u2192\\rightarrow\u2192 Action). Stage 2: Deliberation Enhancement applies RL to refine planning and reflection capabilities, using Sub-goal Guidance (Reward) for forward-looking task decomposition and Error Recovery Scenario Construction (Reflect) for backward-looking self-correction, culminating in a Deliberative Reasoner.", "description": "The Actor2Reasoner framework is a two-stage training process that enhances Multimodal Large Language Models (MLLMs) to improve GUI agent capabilities. Stage 1, Reasoning Injection, uses supervised fine-tuning (SFT) and spatial reasoning distillation to teach the MLLM to integrate visual and spatial information with logical reasoning before generating actions. This process involves identifying and resolving reasoning bottlenecks by leveraging a teacher model. This transitions the agent from a reactive actor to a basic reasoner. Stage 2, Deliberation Enhancement, utilizes reinforcement learning to refine the reasoner into a deliberative one. This involves two methods: sub-goal guidance, rewarding the MLLM for generating accurate intermediate goals, and error recovery scenario construction, which enhances the agent's ability to self-correct through creating failure-and-recovery training scenarios. The figure illustrates this two-stage process, highlighting key components and transitions.", "section": "3 Actor2Reasoner"}, {"figure_path": "https://arxiv.org/html/2504.14239/x2.png", "caption": "Figure 3. Reward curves during reinforcement learning training. The plot shows the overall reward and the rewards for individual task types (Low-level, High-level, Grounding) over training steps.", "description": "This figure displays the reward curves obtained during the reinforcement learning phase of training the InfiGUI-R1 agent.  The x-axis represents the training steps, while the y-axis shows the reward values.  Multiple lines are plotted, illustrating the reward progression for different aspects of the task: the overall reward, the reward specifically for low-level tasks, the reward for high-level tasks, and the reward for grounding tasks. This visualization allows for an examination of how the agent's performance improves over time on these different task aspects during the reinforcement learning process. The figure helps illustrate the efficacy of the reward design in the training process.", "section": "4.4 Visualization"}]