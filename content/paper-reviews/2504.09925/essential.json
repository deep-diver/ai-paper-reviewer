{"importance": "**FUSION** offers a new approach to cross-modal understanding, potentially impacting MLLM development. Its focus on deep integration and novel data synthesis could inspire new research directions and benchmarks for future models.", "summary": "FUSION: Fully integrates vision-language representations for enhanced cross-modal understanding with deep integration throughout the entire processing pipeline.", "takeaways": ["FUSION, a new family of MLLMs, achieves full vision-language alignment and integration.", "Text-Guided Unified Vision Encoding and Context-Aware Recursive Alignment Decoding improve integration.", "Dual-Supervised Semantic Mapping Loss and synthesized QA dataset enhance performance."], "tldr": "**Existing MLLMs** often **lack** effective vision-language integration, treating visual information as static and fusing it with language only at later stages of LLM decoding. This decoupled approach leads to limited textual interaction and **embedding misalignment**, hindering seamless integration. **FUSION**, a family of multimodal LLMs, addresses these issues by achieving **deep, dynamic integration** throughout the processing pipeline. Its **Text-Guided Unified Vision Encoding** incorporates textual info in vision encoding, achieving pixel-level integration. \n\n**Context-Aware Recursive Alignment Decoding** recursively aggregates visual features conditioned on textual context during decoding, enabling fine-grained integration. A **Dual-Supervised Semantic Mapping Loss** guides feature mapping and mitigates modality discrepancies. FUSION also uses a **Synthesized Language-Driven QA dataset**, prioritizing high-quality QA pairs to optimize text-guided integration. Results show that FUSION significantly outperforms existing methods.", "affiliation": "Peking University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.09925/podcast.wav"}