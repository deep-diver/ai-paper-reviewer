[{"figure_path": "https://arxiv.org/html/2412.14161/x2.png", "caption": "Figure 1: An overview of TheAgentCompany benchmark. It features a reproducible and self-hosted environment, simulated colleagues to test agent communication capabilities, checkpoint and execution-based evaluation, and a set of 175 diverse, realistic and professional tasks in a software engineering company setting.", "description": "The figure provides a high-level overview of TheAgentCompany, a benchmark designed for evaluating AI agents in realistic work environments. It highlights key features such as:\n- Reproducible and Self-hosted Environment: Ensuring consistent and comparable evaluation across different methods and over time.\n- Simulated Colleagues: Testing agent communication capabilities.\n- Checkpoint and Execution-based Evaluation: Assessing agent progress and overall task completion.\n- Diverse and Realistic Tasks: Focusing on a set of 175 professional tasks commonly encountered in a software engineering company, making the benchmark relevant to real-world work scenarios.", "section": "3 THEAGENTCOMPANY ENVIRONMENT SETUP"}, {"figure_path": "https://arxiv.org/html/2412.14161/x3.png", "caption": "Figure 2:  Example TheAgentCompany workflow illustrating an agent managing a sprint for the RisingWave project. The task involves identifying and moving unfinished issues to next sprint cycle, notifying assignees of those issues, running a code coverage script, uploading summarized report to OwnCloud, and incorporating feedback on report from a simulated project manager.", "description": "This figure illustrates a workflow of an agent completing a project management task within TheAgentCompany environment.  The agent uses various tools and interacts with simulated colleagues to manage a sprint for the RisingWave project. Key steps shown in the workflow include:\n- Accessing and updating sprint issues in Plane.\n- Notifying issue assignees via Rocket.Chat.\n- Cloning the project repository from GitLab.\n- Running a code coverage script.\n- Uploading a summarized report to OwnCloud.\n- Incorporating feedback from a simulated project manager. Each step has associated checkpoints and scores, demonstrating the agent\u2019s progress and performance on the task.", "section": "4.2 WORKFLOW"}, {"figure_path": "https://arxiv.org/html/2412.14161/x4.png", "caption": "Figure 3:  Overview of OpenHands\u2019 default CodeAct + Browsing agent architecture, the baseline agent used throughout the experiments.", "description": "This figure provides a schematic overview of the agent architecture employed in the study.  The agent interacts with a simulated environment through three key interfaces: a browser, a bash shell, and an IPython server.  The core of the agent's operation involves receiving observations from the environment and using these, along with a history of past actions and observations, to determine the next action to take.  This action is then relayed back to the environment, and the cycle continues.  The diagram illustrates this flow, showing an example of an LLM prompt and the subsequent actions generated by the agent.", "section": "6 BASELINE AGENT"}, {"figure_path": "https://arxiv.org/html/2412.14161/x5.png", "caption": "(a) Success rate across platforms", "description": "This bar chart compares the success rates of two large language models, Claude-3.5-sonnet and Llama-3.1-405B, across four different platforms: GitLab, Plane, RocketChat, and ownCloud.  Success rate is defined as the percentage of tasks completed successfully on each platform. Claude-3.5-sonnet consistently outperforms Llama-3.1-405B on all platforms. Both models exhibit the highest success rates on GitLab and Plane, and struggle the most on ownCloud and RocketChat. This suggests that tasks involving coding and project management are easier for LLMs compared to those involving file management and communication.", "section": "7.2 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2412.14161/x6.png", "caption": "(b) Success rate across task categories", "description": "This bar chart compares the success rates of two large language models, Claude-3.5-Sonnet and Llama-3.1-405B, across seven different task categories: Software Development Engineering (SDE), Project Management (PM), Data Science (DS), Administration (Admin), Human Resources (HR), Finance, and Other. The x-axis represents the task category, and the y-axis represents the success rate, expressed as a percentage. Each bar represents the success rate of a specific LLM on a specific task category.", "section": "7.2 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2412.14161/x7.png", "caption": "Figure 4: Comparing agent success rate across platforms (left) and task categories (right).", "description": "This figure presents two bar charts comparing the success rates of different large language models (LLMs) on tasks within TheAgentCompany benchmark. The left chart (a) breaks down success rates by *platform*, indicating how well the models perform on tasks involving GitLab, Plane, RocketChat, and ownCloud. It highlights performance disparities across these platforms, suggesting areas where LLMs excel or struggle. The right chart (b) compares success rates across different *task categories* related to job roles within a software company, including Software Development Engineer (SDE), Project Management (PM), Data Science (DS), Administrative (Admin), Human Resources (HR), Finance, and Other. This analysis reveals how well LLMs perform on tasks typically associated with different professions within a simulated work environment.  The specific models compared in both graphs are Claude-3.5-Sonnet and Llama-3.1-405B.", "section": "7.2 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2412.14161/x8.png", "caption": "Figure 5: Simulated Colleague Communication Example 1 \u2013 The agent is tasked with collecting required equipment while adhering to the department\u2019s budget. After calculating that the requested items exceed the budget, the agent negotiates with the simulated colleague to reduce the request, showcasing its ability of effective communication.", "description": "The agent communicates with a simulated colleague (Zhang Wei) through RocketChat to request equipment (three HP Workstations and three wireless mice). Zhang Wei informs the agent that the request exceeds the department's budget. The agent then revises the request to two mice and two desktops, demonstrating its ability to negotiate and adhere to budget constraints.", "section": "7.2 ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2412.14161/x9.png", "caption": "Figure 6: Simulated Colleague Communication Example 2 \u2013 The agent is tasked with writing a job description for a new graduate software engineering position. To fulfill the task, the agent communicates with simulated Project Manager to gather requirements. The agent requests the job description template, minimum and preferred qualifications, and the ideal salary range. This interaction evaluates the agent\u2019s ability to gather information systematically and clarify task-related requirements through effective communication.", "description": "The agent interacts with a simulated project manager (Li Ming) through a chat interface to gather requirements for a new graduate software engineering job description.  The agent asks for the job description template, minimum and preferred qualifications, and the ideal salary range. Li Ming provides the requested information.  This tests the agent's ability to systematically collect information and clarify requirements via professional communication.", "section": "7.2 ANALYSIS"}]