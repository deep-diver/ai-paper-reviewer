[{"figure_path": "https://arxiv.org/html/2503.00784/x1.png", "caption": "Figure 1: Wall time for draft model autoregressive generation and target model parallel verification of 8 tokens with varying input lengths. The draft phase has become a comparable bottleneck to the verification phase, and executing the lightweight draft model on CPU does not compromise generation efficiency.", "description": "This figure displays the time taken for autoregressive generation using the draft model and parallel verification using the target model, both processing 8 tokens.  The input sequence length varies, allowing for observation of how this affects the time for each phase.  The key takeaway is that the draft model's processing time, even when run on the CPU, becomes comparable to the target model's verification time (which runs on the GPU), highlighting that it is a significant bottleneck. The results demonstrate that offloading the draft model to the CPU does not negatively impact overall generation efficiency.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.00784/x2.png", "caption": "Figure 2: Dynamic multi-sequence drafting. pi,jsubscript\ud835\udc5d\ud835\udc56\ud835\udc57p_{i,j}italic_p start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT represents the probability of the j\ud835\udc57jitalic_j-th ranked token at the i\ud835\udc56iitalic_i-th position in the generated sequence. \u03b8=p1,1\u00d7p2,1\ud835\udf03subscript\ud835\udc5d11subscript\ud835\udc5d21\\theta=p_{1,1}\\times p_{2,1}italic_\u03b8 = italic_p start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT \u00d7 italic_p start_POSTSUBSCRIPT 2 , 1 end_POSTSUBSCRIPT serves as the threshold.\nTokens with probabilities p1,ksubscript\ud835\udc5d1\ud835\udc58p_{1,k}italic_p start_POSTSUBSCRIPT 1 , italic_k end_POSTSUBSCRIPT exceeding the threshold \u03b8\ud835\udf03\\thetaitalic_\u03b8 will continue to be predicted sequentially, forming a independent draft sequence.", "description": "Figure 2 illustrates the dynamic multi-sequence drafting method used in DuoDecoding.  The probability of each token (p<sub>i,j</sub>) is calculated, where 'i' is the token's position in the sequence, and 'j' represents its rank among the generated candidates. A threshold (\u03b8) is set, calculated as the product of the probabilities of the top two tokens in the first position (p<sub>1,1</sub> * p<sub>2,1</sub>). Tokens with probabilities (p<sub>1,k</sub>) greater than \u03b8 will form a new draft sequence, improving overall acceptance rate of draft tokens.  This multi-sequence approach is in contrast to single-sequence drafting where only the tokens with probability above the threshold in the first sequence would be accepted.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.00784/x3.png", "caption": "Figure 3: Comparison of Time to First Token (TTFT) across different tasks and models. The y-axis shows the relative TTFT normalized by vanilla autoregressive generation. Lower values indicate better latency performance.", "description": "Figure 3 presents a comparison of the time to generate the first token (TTFT) across various tasks and language models, specifically Vicuna and Llama.  The y-axis displays the relative TTFT, normalized against the TTFT achieved by a standard autoregressive generation method, providing a clear representation of latency improvements. Lower values on the y-axis represent a faster TTFT, indicating superior latency performance.", "section": "4.3 Time to First Token"}, {"figure_path": "https://arxiv.org/html/2503.00784/x4.png", "caption": "Figure 4: Profiling of time and number of processed token in one generation iteration for different decoding strategies.", "description": "This figure presents a detailed performance breakdown for different decoding methods during a single generation iteration.  It compares autoregressive generation, speculative decoding, and the proposed DuoDecoding approach. The figure shows the time spent and the number of tokens processed within each iteration.  This allows for a direct comparison of computational efficiency and throughput between the methods. The visual comparison clearly highlights the trade-offs between speed and the number of tokens processed per iteration for each method.", "section": "4.4.2 Profiling"}, {"figure_path": "https://arxiv.org/html/2503.00784/x5.png", "caption": "Figure 5: Distribution of sequence numbers during generation process in DuoDecoding.", "description": "This figure shows the distribution of the number of draft sequences used during the text generation process in the DuoDecoding model.  The x-axis represents the number of sequences, and the y-axis shows the percentage of occurrences. The data is separated into two tasks: Math and Translation, highlighting the differences in sequence number usage across different tasks.  This distribution reveals insights into the model's adaptability to varying degrees of uncertainty in different text generation scenarios.", "section": "4.4.4 Sequence Number Distribution"}]