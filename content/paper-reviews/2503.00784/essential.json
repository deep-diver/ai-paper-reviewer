{"importance": "This paper is important as it addresses the **critical challenge of inference speed in LLMs**, offering a practical solution that balances performance and efficiency. By creatively leveraging heterogeneous computing, DuoDecoding **opens new avenues for optimizing LLM deployment** and can inspire future research.", "summary": "DuoDecoding: Accelerating LLM inference by strategically deploying draft & target models on CPU & GPU for parallel decoding and dynamic drafting.", "takeaways": ["DuoDecoding achieves significant speedups in LLM generation latency by using both CPU and GPU.", "A hardware-aware optimal draft budget minimizes idle times and maximizes hardware utilization, enhancing efficiency.", "Dynamic multi-sequence drafting improves draft quality and overall performance by adapting to the uncertainty of draft outputs."], "tldr": "Large language models are powerful, but their slow generation speed limits their practicality. Speculative decoding improves this by using a smaller \"draft\" model to predict multiple tokens at once, which are then verified by the larger \"target\" model. However, the draft model introduces overhead, becoming a bottleneck. Prior solutions often compromise the quality of the draft model to reduce this overhead.\n\nDuoDecoding tackles this challenge by running the **draft model on the CPU and the target model on the GPU**, enabling parallel processing. It introduces a **hardware-aware draft budget** to balance CPU and GPU usage and uses **dynamic multi-sequence drafting** to improve draft quality. Experiments show DuoDecoding significantly speeds up generation while maintaining performance.", "affiliation": "Fudan University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.00784/podcast.wav"}