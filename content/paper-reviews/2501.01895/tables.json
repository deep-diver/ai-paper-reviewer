[{"content": "| Method | PSNR\u2191 | FVD\u2193 | Quality\u2191 | Seman\u2191 | Consist\u2191 | Continuity\u2191 | Long Task | \n|---|---|---|---|---|---|---|---| \n| **DC-FN** | 25.42 | 445.94 | 54 | 97 | **92** | 80 | \u00d7 | \n| **EnerVerse** | **26.1** | **404.65** | **59** | 97 | 89 | **90** | \u2713 |", "caption": "Table 1: Performance comparison between DynamiCrafter (FN) and our proposed approach across Atomic Task metrics (Quantitative Comparison and User Study) and Long Task ability. The proposed method outperforms DynamiCrafter (FN) in most metrics, demonstrating its effectiveness in video generation and task performance.", "description": "This table presents a quantitative and qualitative comparison of video generation performance between DynamicCrafter (with FreeNoise, denoted as DC-FN), a state-of-the-art baseline video generation model, and the proposed EnerVerse model.  The comparison is done across several metrics related to video quality (PSNR and FVD), and task performance (Semantic Alignment, Consistency, Continuity, and Long Task Ability).  Atomic Task metrics assess performance on short, simple tasks, while Long Task Ability evaluates performance on longer, more complex tasks.  A user study involving robotics experts contributed to the qualitative assessment. The results demonstrate that EnerVerse significantly outperforms the baseline in most metrics, highlighting its improved performance in both video generation and robotic manipulation tasks.", "section": "4.2 Comparison Results"}, {"content": "| Model | Visual Input | Spatial | Object | Goal | Long | Avg. |\n|---|---|---|---|---|---|---|\n| **Diffusion Policy** | One Third View | 78.3 | 92.5 | 68.3 | 50.5 | 72.4 |\n| **Octo** | One Third View | 78.9 | 85.7 | **84.6** | 51.1 | 75.1 |\n| **OpenVLA** | One Third View | 84.7 | 88.4 | 79.2 | 53.7 | 76.5 |\n| **MDT** | One Third & One Wrist View | 78.5 | 87.5 | 73.5 | 64.8 | 76.1 |\n| **MAIL** | One Third & One Wrist View | 74.3 | 90.1 | 81.8 | 78.6 | 83.5 |\n| **EnerVerse** | One FAV | **92.1** | **93.2** | 78.1 | **73.0** | **84.1** |\n| **EnerVerse** | Three FAVs | **91.2** | **97.7** | **85.0** | **80.0** | **88.5** |", "caption": "Table 2: Evaluation results on the LIBERO benchmark across four task suites. Our method achieves superior performance in both single and multi-visual input settings.", "description": "This table presents a quantitative comparison of different robotic policy models on the LIBERO benchmark, a standard test suite for evaluating robotic manipulation skills.  The benchmark consists of four sub-suites (Spatial, Object, Goal, and Long), each assessing different aspects of robotic capabilities. The table shows the success rate of each model across these sub-suites under two different input scenarios: a single camera view and three camera views (representing multiple perspectives).  ENERVerse, the proposed model in the paper, is compared against several other state-of-the-art models. The results highlight ENERVerse's superior performance, especially when utilizing multiple visual inputs, demonstrating its ability to effectively integrate information from multiple perspectives and improve task completion success.", "section": "4 Experiments"}, {"content": "| Setup | w/o Sparse Memory | w Sparse Memory |\n|---|---|---|\n| LIBERO-Long-SV | 30.8 | 73 |", "caption": "Table 3: Performance comparison on the LIBERO-Long task with and without Sparse Memory.", "description": "This table presents a comparison of the performance on the LIBERO-Long task, a subset of the LIBERO benchmark focusing on long-range manipulation tasks, with and without the use of a sparse memory mechanism within the ENERVERSE model.  It highlights how the sparse memory mechanism improves model robustness and efficiency by reducing redundancy and mitigating the risk of model collapse, particularly during the handling of long sequences. The table likely shows success rates or other relevant metrics to quantify this performance difference.", "section": "4.3 Further Studies"}, {"content": "| Strategy | All-Scratch | With DC Pretrain. | One-Stage Co-Train | Two-Stage Finetune |\n|---|---|---|---|---|\n| LIBERO-Spatial | Failed | 79 | 86.3 | **92.1** |", "caption": "Table 4: Performance comparison of different training strategies on the LIBERO-Spatial task suite. The metrics are the task success rates.", "description": "This table presents a comparison of four different training strategies used for the robotic policy in the LIBERO-Spatial task suite.  The strategies are: training from scratch, training with pretrained weights from a general video generator, one-stage co-training (optimizing both the policy and video generation simultaneously), and a two-stage approach (pretraining the video generator and then fine-tuning with only the policy loss). The metric used to evaluate these strategies is the task success rate, representing the percentage of successful task completions.", "section": "4. Experiments"}]