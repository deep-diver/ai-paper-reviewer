[{"figure_path": "https://arxiv.org/html/2501.14249/x1.png", "caption": "Figure 1: Compared against the saturation of some existing benchmarks, Humanity\u2019s Last Exam accuracy remains low across several frontier models, demonstrating its effectiveness for measuring advanced, closed-ended, academic capabilities. The sources for our evaluation metrics are detailed in Section\u00a0C.5. We further evaluate more frontier models on HLE in Table\u00a01.", "description": "Humanity's Last Exam (HLE) is a new benchmark designed to assess the capabilities of large language models (LLMs) on challenging, closed-ended academic questions.  This figure compares the accuracy of several state-of-the-art LLMs on HLE against their performance on other established benchmarks like MMLU.  The results show that while LLMs achieve near-perfect scores on older benchmarks, their accuracy on HLE remains low. This demonstrates that HLE is indeed a more challenging and effective benchmark for measuring the true capabilities of advanced LLMs in the realm of complex academic reasoning.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.14249/x2.png", "caption": "Figure 2: Samples of the diverse and challenging questions submitted to Humanity\u2019s Last Exam.", "description": "This figure showcases a small sample of the diverse and challenging questions included in the Humanity's Last Exam benchmark.  The questions span a wide range of academic disciplines, including Classics, Ecology, Mathematics, Computer Science, and Chemistry, and vary in their format, including multiple-choice and short-answer questions, some accompanied by images.  The questions are designed to assess advanced reasoning capabilities and resist quick solutions found through internet searches.", "section": "3 Dataset"}, {"figure_path": "https://arxiv.org/html/2501.14249/x3.png", "caption": "Figure 3: HLE consists of 3,00030003{,}0003 , 000 exam questions in over a hundred subjects, grouped into high level categories here. We provide a more detailed list of subjects in Section\u00a0B.3.", "description": "Humanity's Last Exam (HLE) dataset contains 3,000 questions spanning over 100 subjects.  The figure displays a high-level breakdown of these subjects categorized into broader groups (e.g., Math, Biology, Humanities).  A more detailed list of all subjects is available in Section B.3 of the paper. The visualization helps to show the diversity of topics covered in the HLE benchmark.", "section": "3 Dataset"}, {"figure_path": "https://arxiv.org/html/2501.14249/x4.png", "caption": "Figure 4: Dataset creation pipeline. We accept questions that make frontier LLMs fail, then iteratively refine them with the help of expert peer reviewers. Each question is then manually approved by organizers or expert reviewers trained by organizers. A private held-out set is kept in addition to the public set to assess model overfitting and gaming on the public benchmark.", "description": "The figure illustrates the multi-stage process of creating the HUMANITY'S LAST EXAM dataset.  It begins with a large number of initial question attempts, many of which are filtered out because state-of-the-art LLMs are able to answer them.  The remaining questions then undergo iterative refinement through expert review, with both initial feedback rounds and subsequent organizer/expert approval. Finally, the dataset is split into a publicly released set and a private held-out set. The private set is reserved for assessing model overfitting and potential gaming strategies.", "section": "3 Dataset"}, {"figure_path": "https://arxiv.org/html/2501.14249/x5.png", "caption": "Figure 5: Average completion token counts of reasoning models tested, including both reasoning and output tokens. We also plot average token counts for non-reasoning models in Section\u00a0C.3.", "description": "Figure 5 presents a bar chart comparing the average number of tokens generated by various large language models (LLMs) while answering questions from the HUMANITY'S LAST EXAM (HLE) benchmark.  The chart separates models into two categories: reasoning models and non-reasoning models.  The token counts include both the tokens used for reasoning (intermediate steps) and those in the final answer.  For a more detailed breakdown of token counts for non-reasoning models, refer to Section C.3 of the paper.  The chart visually demonstrates the significant difference in computational cost (measured by token usage) between reasoning and non-reasoning LLMs, implying that the reasoning process adds substantial computational overhead.", "section": "4 Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.14249/x6.png", "caption": "Figure 6: Average output token counts of non-reasoning models.", "description": "This figure displays the average number of tokens generated by several non-reasoning language models across different subject categories within the HUMANITY'S LAST EXAM (HLE) benchmark.  The categories shown include Mathematics, Biology/Medicine, Physics, Computer Science/AI, Humanities/Social Sciences, Chemistry, Engineering, and Other.  It provides a visual comparison of the computational cost associated with each model's responses for different subject areas, illustrating variations in complexity and potentially revealing trends in token generation efficiency across various subjects and models.", "section": "C.3 Non-Reasoning Model Token Counts"}]