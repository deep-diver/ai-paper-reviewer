{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which forms the foundation for many modern large language models and is a key component being unified with Mamba in the TransMamba framework."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "This paper introduces the Mamba architecture, a state space model with linear complexity that offers efficiency gains in long-sequence processing, and it's a core element unified with Transformers in the TransMamba framework."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-01-01", "reason": "Introduces FlashAttention, a technique to improve the efficiency of attention mechanisms, which can improve the speedup of a Transformer and is a core component in TransMamba."}, {"fullname_first_author": "Wayne Xin Zhao", "paper_title": "A survey of large language models", "publication_date": "2023-03-01", "reason": "Provides a broad overview of large language models, placing TransMamba in the context of the broader field."}, {"fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "publication_date": "2024-05-21", "reason": "This paper explores the relationship between Transformers and SSMs, revealing that they are both structured with duality that is key to understand how they can be unified into TransMamba."}]}