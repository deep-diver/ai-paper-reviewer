[{"figure_path": "https://arxiv.org/html/2503.24067/x1.png", "caption": "Figure 1: TransMamba has shared parameters to flexibly switch between Attention and SSM, and TransPoints decide which parts of token sequence use Attention or SSM.", "description": "TransMamba uses shared parameters for both the Transformer's attention mechanism and Mamba's state space model (SSM).  The choice between using attention or SSM for processing tokens is determined dynamically by TransPoints, which are strategically placed within the token sequence. TransPoints act as switching points, allowing the model to adapt its computational approach based on the length and characteristics of the input sequence.  This flexible approach combines the strengths of both Transformer and Mamba, leveraging attention for shorter sequences and SSM for longer ones to optimize efficiency and performance.", "section": "2.2 Overall Framework of TransMamba"}, {"figure_path": "https://arxiv.org/html/2503.24067/x2.png", "caption": "Figure 2: TransMamba generally shows better efficiency and performance with different sizes.", "description": "The figure shows a comparison of the training efficiency and performance of TransMamba against other models (Transformer, Mamba2, and Hybrid) across different model sizes. It demonstrates that TransMamba consistently achieves superior efficiency and performance, highlighting its scalability and effectiveness in large language modeling.", "section": "3.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.24067/x3.png", "caption": "Figure 3: (a) Structure of TransMamba. Attention and SSM have shared parameters \ud835\udc16\ud835\udc10\ud835\udc0a\ud835\udc15subscript\ud835\udc16\ud835\udc10\ud835\udc0a\ud835\udc15\\mathbf{W_{QKV}}bold_W start_POSTSUBSCRIPT bold_QKV end_POSTSUBSCRIPT and \ud835\udc16\ud835\udc02\ud835\udc01\ud835\udc31subscript\ud835\udc16\ud835\udc02\ud835\udc01\ud835\udc31\\mathbf{W_{CBx}}bold_W start_POSTSUBSCRIPT bold_CBx end_POSTSUBSCRIPT. Tokens are either processed via the green path (SSM mode) or the blue path (Attention mode). (b) Memory Converter. (c) The TransPoint Scheduling of TransMamba.", "description": "Figure 3 illustrates the architecture of TransMamba, a novel model that combines Transformer and Mamba.  Panel (a) shows the overall structure, highlighting how shared parameters (WQKV and WCBx) allow flexible switching between attention (Transformer) and SSM (Mamba) mechanisms.  The green path represents the SSM mode, while the blue path represents the attention mode, showing how tokens are processed differently based on which mode is selected.  Panel (b) details the Memory Converter, a crucial component for seamlessly transitioning information between the attention and SSM modules. Finally, Panel (c) depicts the TransPoint scheduling strategy, which determines when the model switches between attention and SSM modes during sequence processing.", "section": "2.2 Overall Framework of TransMamba"}, {"figure_path": "https://arxiv.org/html/2503.24067/x4.png", "caption": "Table 1: Compare the matrix form of SSM and Attention. The core mechanisms of Attention and SSM show consistency in dual form, which is the mathematical basis that enables us to unify Transformer and Mamba.", "description": "This table compares the mathematical formulas of the self-attention mechanism used in Transformers and the state space model (SSM) used in Mamba.  It highlights the structural similarities between the two models by showing how their core operations (QKV in attention and CBx in SSM) can be represented in a dual form. This mathematical equivalence forms the foundation for unifying the Transformer and Mamba architectures within the TransMamba framework.", "section": "2.1 Preliminary"}, {"figure_path": "https://arxiv.org/html/2503.24067/x5.png", "caption": "Table 2: Compare the training FLOPs of Transformer, Mamba and optimal TransMamba. The FLOPs of TransMamba is a quadratic function of the TransPoint, and its specific value is related to the speed optimization coefficients of Transformer and Mamba respectively.", "description": "This table compares the computational cost (floating-point operations, or FLOPs) per layer for training three different sequence modeling architectures: the Transformer, Mamba, and the proposed TransMamba.  The Transformer's FLOPs are quadratic in sequence length (T), while Mamba's are linear.  TransMamba's FLOPs depend on the location of the 'TransPoint,' which determines the switch between Transformer and Mamba mechanisms.  The formula shows that TransMamba's efficiency is a function of the TransPoint's position and is influenced by the relative efficiency of the Transformer and Mamba implementations.", "section": "2.1 Preliminary"}]