[{"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.2.1.1.1.1\">domain</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.1.1.1.2\">sub-domains</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.1.1.1.3\">examples</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.2.1.2.2.1\">attribute</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.2.2.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.1.1\">gender, age, race, profession, posture,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.2.1\">appearance, clothing and accessories, action</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.2.2.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.2.2.3.1.1.1.1\">male, female, white man, the police officer,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.2.2.3.1.2.1.1\">person with a shocked expression, person wearing</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.2.2.3.1.3.1.1\">a mask, person standing</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.2.1.3.3.1\">position</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.3.3.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.3.3.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.2.1.1.1\">inner position (human to human),</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.2.1.2.1\">outer position (human to environment)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.3.3.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.3.3.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.3.3.3.1.1.1.1\">the second person from left to right, person at</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.3.3.3.1.2.1.1\">the right, person closest to the microphone,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.3.3.3.1.3.1.1\">person sitting in the chair</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.2.1.4.4.1\">interaction</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.4.4.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.4.4.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.2.1.1.1\">inner interaction (human with human),</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.2.1.2.1\">outer interaction (human with environment)</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.4.4.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.4.4.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.4.4.3.1.1.1.1\">two people holding hands, people locked in</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.4.4.3.1.2.1.1\">each other\u2019s gaze, the person holding a gun,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.4.4.3.1.3.1.1\">person holding the certificate in hand</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.2.1.5.5.1\">reasoning</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.5.5.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.5.5.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.2.1.1.1\">inner position reasoning,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.2.1.2.1\">outer position reasoning,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.2.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.2.1.3.1\">attribute reasoning</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.5.5.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.5.5.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.5.5.3.1.1.1.1\">all the people to the right of the person closest</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.5.5.3.1.2.1.1\">to the glass, person wearing a lab coat but not putting</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.5.5.3.1.3.1.1\">their hand on the board</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S3.T1.2.1.6.6.1\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.6.6.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.2.1.6.6.1.1.1.1\">celebrity</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.1.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.2.1.6.6.1.1.2.1\">recognition</td>\n</tr>\n</table>\n</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.6.6.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.6.6.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.2.1.1.1\">actor, character, athlete, entrepreneur,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.2.1.2.1\">scientist, politician, singer</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.2.1.6.6.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.6.6.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.6.6.3.1.1.1.1\">Brad Pitt, Bruce Wayne, Cristiano Ronaldo,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.6.6.3.1.2.1.1\">Rihanna, Elon Musk, Albert Einstein, Donald Trump</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.7.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"S3.T1.2.1.7.7.1\">rejection</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"S3.T1.2.1.7.7.2\">attribute, position, interaction, reasoning</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"S3.T1.2.1.7.7.3\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.7.7.3.1\">a man in red hat, three women in a circle</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: \nThe primary annotation domains and their corresponding sub-domains within HumanRef.", "description": "Table 1 provides a detailed breakdown of the annotation domains and sub-domains used in the HumanRef dataset.  It shows how different aspects of human appearance, spatial relations, actions, and identities are categorized and annotated for more comprehensive understanding.  These annotations are fundamental to the task of referring to any person, ensuring that the dataset accurately reflects the complexity of real-world scenarios.", "section": "3. HumanRef Dataset"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.1.1\">gender, age, race, profession, posture,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.2.1\">appearance, clothing and accessories, action</td>\n</tr>\n</table>", "caption": "Table 2: \nMain statistics of the HumanRef dataset, including the number of images, the number of referring expressions, the average word count per referring expression, and the average number of instances associated with each referring expression.", "description": "Table 2 presents a comprehensive statistical overview of the HumanRef dataset, a crucial component of the research on referring to any person.  It details the dataset's size and composition, providing key metrics for understanding its scale and complexity.  These metrics include: the total number of images in the dataset, the total count of referring expressions used to describe individuals within those images, the average length (word count) of each referring expression, and the average number of individuals referenced by a single expression. This information is essential for evaluating the size and scope of the dataset, gauging the complexity of the referring expressions, and comprehending the scale of the multi-instance referring challenges that the dataset addresses.", "section": "3. HumanRef Dataset"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.2.2.3.1.1.1.1\">male, female, white man, the police officer,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.2.2.3.1.2.1.1\">person with a shocked expression, person wearing</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.2.2.3.1.3.1.1\">a mask, person standing</span></td>\n</tr>\n</table>", "caption": "Table 3: \nComparison of the HumanRef Benchmark with RefCOCO/+/g. For a fair comparison, we present only the statistics related to human referring in RefCOCO/+/g.", "description": "Table 3 compares the HumanRef benchmark dataset with the RefCOCO+/g datasets, focusing solely on statistics related to human references for a fair comparison. It highlights key differences in the number of images, referring expressions, average word count per expression, and average number of persons per image and per referring expression.", "section": "3. HumanRef Dataset"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.3.3.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.2.1.1.1\">inner position (human to human),</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.2.1.2.1\">outer position (human to environment)</td>\n</tr>\n</table>", "caption": "Table 4: Benchmarking multimodal models on HumanRef Benchmark. R, P, and DF1 represent Recall, Precision, and DensityF1, respectively. \u2020\u2020\\dagger\u2020 A simple baseline that uses the bounding boxes of all persons in the image as results, simulating a person detection model that does not follow the referring description. \u2217Molmo-7B-D predicts point coordinates as output and use point-in-mask evaluation criteria.", "description": "This table presents a benchmark comparison of various multimodal models on the HumanRef dataset, a novel dataset designed for evaluating human referring tasks.  The performance of each model is assessed across five sub-tasks: Attribute, Position, Interaction, Reasoning, and Celebrity. The evaluation metrics include Recall (R), Precision (P), Density F1 (DF1), and a rejection score.  A simple baseline is included that simulates a person detection model without understanding the language description, simply using all detected person bounding boxes as predictions. Note that one model, Molmo-7B-D, uses a different evaluation metric (point-in-mask), which differs from the IoU-based metric used for other models. This difference in metric is noted in the caption. The results highlight the challenges of existing models in handling the complexities of multi-instance referring and the importance of appropriate dataset design and model training strategies.", "section": "5. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.3.3.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.3.3.3.1.1.1.1\">the second person from left to right, person at</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.3.3.3.1.2.1.1\">the right, person closest to the microphone,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.3.3.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.3.3.3.1.3.1.1\">person sitting in the chair</span></td>\n</tr>\n</table>", "caption": "Table 5: \nData, task, and trainable modules for each stage.", "description": "This table details the four-stage training process for the RexSeek model. Each stage uses different datasets, training tasks, and sets of trainable model modules to progressively enhance the model's capabilities.  Stage 1 focuses on aligning visual and textual modalities. Stage 2 concentrates on visual perception tasks. Stage 3 improves the model's general understanding abilities. Finally, Stage 4 fine-tunes the model on the HumanRef dataset for human referring.", "section": "4. RexSeek Model"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.4.4.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.2.1.1.1\">inner interaction (human with human),</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.2.1.2.1\">outer interaction (human with environment)</td>\n</tr>\n</table>", "caption": "Table 6: \nRejection score comparison under different model scales with and without rejection data during training.", "description": "This table compares the rejection scores achieved by different sized models (indicated by the number after the model name) when trained both with and without rejection data.  The rejection score reflects how often the model correctly identifies when the referenced person is *not* in the image, avoiding the generation of incorrect bounding boxes (hallucination).  A higher rejection score indicates better performance in this crucial aspect of the referring task.", "section": "3.5 HumanRef Benchmark"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.4.4.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.4.4.3.1.1.1.1\">two people holding hands, people locked in</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.4.4.3.1.2.1.1\">each other\u2019s gaze, the person holding a gun,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.4.4.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.4.4.3.1.3.1.1\">person holding the certificate in hand</span></td>\n</tr>\n</table>", "caption": "Table 7: \nAblation experiments on multi-stage training by loading models from different training stages and fine-tuning them on the HumanRef dataset. We Qwen2.5-3B as the base LLM.", "description": "This table presents ablation study results on the impact of multi-stage training for the RexSeek model.  Different versions of the RexSeek model, each pre-trained to varying degrees of completion (stage1, stage2, stage3), were fine-tuned on the HumanRef dataset. The results, using Recall (R), Precision (P), and DensityF1 (DF1), demonstrate how each pre-training stage affects the final model's performance on the HumanRef benchmark.  The base Large Language Model (LLM) used was Qwen2.5-3B.", "section": "5. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.5.5.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.2.1.1.1\">inner position reasoning,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.2.1.2.1\">outer position reasoning,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.2.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.2.1.3.1\">attribute reasoning</td>\n</tr>\n</table>", "caption": "Table 8: \nZero-shot evaluation of RexSeek on RefCOCO/+/g. We use the open-set detector DINOX to detect the subject object in the image and use the detected bounding box as input to RexSeek.", "description": "This table presents the results of a zero-shot evaluation of the RexSeek model on the RefCOCO+/g benchmark.  Instead of training RexSeek on RefCOCO+/g data, the researchers used a different, pre-trained object detector (DINOX) to locate objects within images. The bounding boxes identified by DINOX were then fed as input to RexSeek, which made its predictions without ever having seen the RefCOCO+/g data during training. This demonstrates RexSeek's ability to generalize to unseen datasets and tasks.", "section": "5. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.5.5.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.5.5.3.1.1.1.1\">all the people to the right of the person closest</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.5.5.3.1.2.1.1\">to the glass, person wearing a lab coat but not putting</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.5.3.1.3\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.5.5.3.1.3.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.5.5.3.1.3.1.1\">their hand on the board</span></td>\n</tr>\n</table>", "caption": "Table 9: Examples of Inner and Outer Position References.", "description": "This table presents examples illustrating how spatial relationships between individuals are described in the HumanRef dataset.  It showcases two categories of positional descriptions: inner position (relative to other people in the image) and outer position (relative to environmental elements). The examples demonstrate the variety and nuance in specifying locations using natural language.", "section": "3. HumanRef Dataset"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.6.6.1.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.1.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.2.1.6.6.1.1.1.1\">celebrity</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.1.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S3.T1.2.1.6.6.1.1.2.1\">recognition</td>\n</tr>\n</table>", "caption": "Table 10: Examples of Inner and Outer Interaction References.", "description": "This table presents examples of how human interactions are categorized in the HumanRef dataset.  It shows examples of both \"inner interactions\" (interactions between people) and \"outer interactions\" (interactions between people and objects or the environment). Each example provides a natural language description of the interaction.  This illustrates the variety of ways human interaction is represented within the dataset, crucial for accurate referring expression comprehension.", "section": "3. HumanRef Dataset"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.6.6.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.2.1.1.1\">actor, character, athlete, entrepreneur,</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.2.1.2.1\">scientist, politician, singer</td>\n</tr>\n</table>", "caption": "Table 11: Examples of Reasoning-Based Referring Expressions.", "description": "This table presents examples of referring expressions that require reasoning to resolve.  The expressions demonstrate three types of reasoning: inner position reasoning (relating individuals to each other), outer position reasoning (relating individuals to environment landmarks), and attribute reasoning (combining attributes with negation). Each example highlights the multi-step inference process necessary to identify the correct individual(s).", "section": "3. HumanRef Dataset"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.6.6.3.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.3.1.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.6.6.3.1.1.1.1\">Brad Pitt, Bruce Wayne, Cristiano Ronaldo,</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.6.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.6.6.3.1.2.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S3.T1.2.1.6.6.3.1.2.1.1\">Rihanna, Elon Musk, Albert Einstein, Donald Trump</span></td>\n</tr>\n</table>", "caption": "Table 12: Names for each sub-domain of the celebrity recognition subset.", "description": "This table lists the names of celebrities included in the HumanRef dataset's celebrity recognition subset, categorized into six sub-domains: Character, Singer, Actor, Athlete, Entrepreneur, and Politician.  Each sub-domain contains a list of specific individuals representing a variety of well-known people across different fields. This detailed breakdown demonstrates the diversity of celebrity representation within the HumanRef dataset.", "section": "3.4. Automatic Annotation"}]