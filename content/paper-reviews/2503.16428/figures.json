[{"figure_path": "https://arxiv.org/html/2503.16428/x1.png", "caption": "Figure 1: Illustration of XAttention: XAttention optimizes attention through a three-step process: (Left) Strided Antidiagonal Scoring: Each block (8\u00d7\\times\u00d78 in this example) is scored by summing values along its strided antidiagonals (stride = 4), with red lines generally indicating higher summed values and blue lines lower values. (Middle) Block Selection: High-scoring blocks are selected based on these evaluations. (Right) Block Sparse Attention: Attention is computed only on the selected blocks (red blocks on the right), achieving substantial computational savings. This example uses a sequence length of 24.", "description": "XAttention employs a three-step process to optimize attention computation.  First, strided antidiagonal scoring sums values within 8x8 blocks of the attention matrix using a stride of 4.  Blocks with higher sums (red) are considered more important than those with lower sums (blue). Second, block selection identifies the most important blocks based on their antidiagonal scores.  Finally, block sparse attention performs computations only on these selected blocks (red blocks), significantly reducing the computational cost. This figure illustrates the process using a sequence length of 24.", "section": "2. Method"}, {"figure_path": "https://arxiv.org/html/2503.16428/x2.png", "caption": "Figure 2: XAttention\u2019s antidiagonal pattern intersects both vertical and slash patterns within a block, enabling efficient detection of these patterns and guiding effective sparse attention computation.", "description": "The figure illustrates how XAttention's novel antidiagonal scoring method effectively captures important information within attention blocks.  By summing values along antidiagonals (with a specified stride), XAttention identifies blocks containing both vertical and diagonal patterns \u2013 crucial indicators of significant relationships between tokens.  This method is superior to simple pooling because it directly addresses and avoids missing these key attention patterns, leading to more precise block selection and higher efficiency in sparse attention computation.", "section": "2.1 Importance Prediction"}, {"figure_path": "https://arxiv.org/html/2503.16428/x3.png", "caption": "Figure 3: Qualitative comparison of video generation results on the VBench benchmark using the first prompt in the VBench dataset. Rows show frames from videos generated using: (1) Full Attention (baseline), (2) XAttention with no warmup and (\u03c4\ud835\udf0f\\tauitalic_\u03c4 = 0.95), (3) XAttention with 5 warmup steps and (\u03c4\ud835\udf0f\\tauitalic_\u03c4 = 0.9), and (4) XAttention with 5 warmup steps and (\u03c4\ud835\udf0f\\tauitalic_\u03c4 = 0.95). XAttention with warmup achieves high visual fidelity to the full attention baseline.", "description": "This figure presents a qualitative comparison of video generation results obtained from four different methods using the first prompt from the VBench dataset.  The four methods are: (1) Full Attention (used as a baseline for comparison), (2) XAttention without any warmup period (with \u03c4 = 0.95), (3) XAttention with a 5-step warmup period (\u03c4 = 0.9), and (4) XAttention with a 5-step warmup period (\u03c4 = 0.95).  Each row displays selected frames from a video generated by one of the four methods, allowing for a visual comparison of the quality and fidelity of the generated videos. The key takeaway is that XAttention, especially when using a warmup period, generates videos with high visual fidelity, closely matching the quality of those produced using the full attention baseline.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16428/x4.png", "caption": "Figure 4: Speedup comparison of attention methods across context lengths, relative to FlashInfer\u2019s implementation of FlashAttention. XAttention consistently outperforms other sparse attention methods, achieving up to 13.5x speedup at 256K tokens.", "description": "This figure compares the speedup achieved by various attention mechanisms against FlashAttention (as implemented in FlashInfer) across different context lengths.  The x-axis represents the sequence length (in tokens), and the y-axis displays the speedup factor relative to FlashAttention.  The results demonstrate that XAttention consistently outperforms other sparse attention methods (MInference, SeerAttention, FlexPrefill), achieving the highest speedup, reaching up to 13.5x at a context length of 256K tokens.  This highlights XAttention's efficiency in handling very long sequences.", "section": "3.3 Efficiency Results"}, {"figure_path": "https://arxiv.org/html/2503.16428/x5.png", "caption": "Figure 5: Breakdown of prefill attention time. Xattention significantly reduces pattern selection time while maintaining density, achieving substantial acceleration compared to existing methods.", "description": "Figure 5 is a bar chart comparing the time spent on pattern search and attention computation during the prefill stage of different sparse attention methods. XAttention significantly reduces the time required for pattern search while maintaining a similar attention density compared to other methods, resulting in substantial speedup in overall attention computation.", "section": "3.3 Efficiency Results"}]