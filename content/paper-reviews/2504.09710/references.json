{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper laid the foundation for aligning language models using reward signals derived from human preferences, which is a key component of RL-based post-training."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper simplifies the RLHF pipeline by replacing RL rollouts with a classification-style loss derived from a KL-constrained reward maximization objective, becoming a fundamental method."}, {"fullname_first_author": "Zhihong Shao", "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models", "publication_date": "2024-02-01", "reason": "This paper introduces Group Relative Policy Optimization (GRPO) and is important as the method used in experiments."}, {"fullname_first_author": "Yoshua Bengio", "paper_title": "Curriculum learning", "publication_date": "2009-01-01", "reason": "This paper provides the foundation for curriculum learning in machine learning and details the importance of using a meaningful progression over training samples."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "This paper details the Proximal Policy Optimization (PPO) algorithm, which is a widely used and strong baseline in many LLM alignment settings."}]}