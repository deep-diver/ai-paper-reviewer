[{"figure_path": "https://arxiv.org/html/2502.13142/x1.png", "caption": "Figure 1: Overview of ARM4R. We introduce an Auto-regressive Robotic Model that leverages low-level 4D Representations (3D point tracks across time) learned from human videos to yield a better pre-trained robotic model.", "description": "This figure illustrates the architecture of ARM4R, an Auto-regressive Robotic Model.  It shows a three-stage training process. First, low-level 4D representations (3D point tracks over time) are learned from massive unlabeled human video data.  This pre-training step captures fundamental properties of the physical world without relying on expensive robotic data. Second, this knowledge is transferred and refined by fine-tuning on robotic videos, adapting the model to the specific characteristics of robotic environments. Finally, the model is fine-tuned for robotic control by using robot proprioceptive data (the robot's own internal sensory information).  The entire process allows for efficient transfer learning from abundant human video data to the more limited domain of robotic data, resulting in a better pre-trained robotic model that can generalize across different robots and environments.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13142/x2.png", "caption": "Figure 2:  ARM4R is trained in three stages. Top Grey Box: The first two stages focus on learning a scene-wide 4D representation by predicting 3D points across time, where Stage 1 pre-trains on a large egocentric human dataset (Epic-Kitchens100), and Stage 2 fine-tunes on a smaller dataset (1-2K demonstrations) of robotic scenes, adapting the point tracking to robotic scene and camera. Bottom Grey Box: Finally, the model is fine-tuned to predict robot proprioceptive states rather than 3D points to enable robotic control.", "description": "ARM4R training is divided into three stages.  The first stage uses a large-scale egocentric human video dataset (Epic-Kitchens100) to pre-train the model to predict 3D point tracks over time, establishing a strong understanding of scene-wide 4D representations (3D points plus time). The second stage fine-tunes this pre-trained model on a smaller dataset (1-2K demonstrations) of robotic scenes. This adaptation step refines the point tracking to account for the differences in camera views and the robotic environment. Finally, the third stage fine-tunes the model to predict robot proprioceptive states instead of 3D points.  This allows the model to transfer knowledge from human video data and use it effectively for low-level robotic control.", "section": "3. Auto-regressive Robotic Models"}, {"figure_path": "https://arxiv.org/html/2502.13142/x3.png", "caption": "Figure 3: Ablation Study for Stages 1 and 2. We train ARM4R on three real tasks in the Kinova setting, ablating Stages 1 and 2. The results indicate that while both stages improve performance, Stage 1 has a more significant impact.", "description": "This ablation study investigates the individual contributions of the two pre-training stages (Stage 1: human video pre-training, Stage 2: robot video fine-tuning) in ARM4R's performance on three real-world robotic tasks using a Kinova robot.  By systematically removing one stage at a time, the experiment isolates the impact of each pre-training phase. The results demonstrate that while both stages contribute to improved performance, the human video pre-training (Stage 1) has a considerably more significant positive effect than the robot video fine-tuning (Stage 2). This suggests that learning from diverse human actions is more crucial for generalizing to new robotic tasks than fine-tuning on limited robot-specific data.", "section": "4.4. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13142/extracted/6193432/Figures/supp_human.png", "caption": "Figure 4: Visualization of ARM4R\u2019s 3D Point Track results on randomly chosen Epic-Kitchens (in-domain) and Ego-4D (out-of-domain) human videos.", "description": "This figure visualizes the performance of the ARM4R model on 3D point tracking tasks.  It shows examples of the model's predictions overlaid on frames from both in-domain (Epic-Kitchens) and out-of-domain (Ego4D) human videos. Each row displays a different video sequence, demonstrating how well the model can track 3D points across time even in videos it has not been trained on directly. The visual comparison highlights the model's generalization capabilities.", "section": "3. Auto-regressive Robotic Models"}, {"figure_path": "https://arxiv.org/html/2502.13142/extracted/6193432/Figures/supp_robot.png", "caption": "Figure 5: Visualization of ARM4R\u2019s 3D Point Track results on randomly chosen Kinova (in-domain) and Open X-Embodiment (out-of-domain) robot videos.", "description": "Figure 5 presents a qualitative assessment of ARM4R's 3D point tracking capabilities.  It showcases the model's performance on both in-domain (Kinova robot) and out-of-domain (OpenX Embodiment) robot videos.  The figure visually demonstrates the accuracy and robustness of the 3D point tracks generated by the model across diverse robotic scenarios and datasets, highlighting its ability to generalize beyond the specific data it was trained on.", "section": "3. Auto-regressive Robotic Models"}, {"figure_path": "https://arxiv.org/html/2502.13142/extracted/6193432/Figures/kinova_setup.jpg", "caption": "Figure 6: The real-world experiment setup of Kinova robot.", "description": "The image shows the real-world experimental setup used for the Kinova robot.  The robot arm is mounted on a base designed to mimic the height and orientation of a human shoulder, providing a more naturalistic setting for robotic manipulation tasks.  Two Logitech BRIO 4K cameras are strategically positioned to capture the manipulation actions: one from an ego-centric view (as if from the robot's perspective) and one from the side. This setup allows for comprehensive data collection, including visual information from multiple angles.", "section": "4.3 Real Robot Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.13142/extracted/6193432/Figures/kinova_tasks.jpg", "caption": "Figure 7: Task building of real-world Kinova setup.", "description": "This figure shows the experimental setup and tasks conducted using a Kinova Gen3 robot. It details the specific configurations for tasks such as picking up a cube, stacking cubes, destacking cubes, picking and placing toys or a basketball, and pushing buttons. Each task involves distinct actions, object placements, and movement patterns, showcasing the variety of robotic manipulation scenarios tested in the study.  The image displays the robot's arm performing each task in various stages and positions.", "section": "4.3 Real Robot Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.13142/extracted/6193432/Figures/franka_setup.jpg", "caption": "Figure 8: The real-world experiment setup of Franka robot.", "description": "The image shows the real-world experimental setup for the Franka robot.  A Franka Emika Panda robot arm is equipped with a Franka gripper and is positioned on a table. Two Logitech BRIO 4K cameras are set up on either side of the table to capture RGB images of the robot's actions, providing multi-view visual data for the experiments. Autofocus is disabled on the cameras, and they capture images at a resolution of 640x480.", "section": "4. Experiments and Results"}]