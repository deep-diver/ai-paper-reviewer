{"references": [{"fullname_first_author": "Aditya Agrawal", "paper_title": "exmy: A data type and technique for arbitrary bit precision quantization", "publication_date": "2024-05-01", "reason": "This paper introduces a novel data type and technique for efficient quantization, which is crucial for optimizing communication bandwidth in distributed training."}, {"fullname_first_author": "Zachary Charles", "paper_title": "Towards federated foundation models: Scalable dataset pipelines for group-structured learning", "publication_date": "2024-07-01", "reason": "This work explores the scalability challenges of training large language models in a federated setting, directly relevant to the context of distributed optimization."}, {"fullname_first_author": "Arthur Douillard", "paper_title": "DiLoCo: Distributed low-communication training of language models", "publication_date": "2024-01-01", "reason": "This paper introduces DiLoCo, a key method in distributed optimization that is directly improved upon in the current work, hence its high importance."}, {"fullname_first_author": "Arthur Douillard", "paper_title": "Streaming DiLoCo with overlapping communication: Towards a distributed free lunch", "publication_date": "2025-01-01", "reason": "This paper proposes Streaming DiLoCo, a variant of DiLoCo that is compared with the new method in the current work, making it highly relevant."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-12-01", "reason": "This paper provides insights into compute-optimal training of large language models, which is crucial for optimizing the efficiency of distributed training algorithms."}]}