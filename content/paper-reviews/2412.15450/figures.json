[{"figure_path": "https://arxiv.org/html/2412.15450/x1.png", "caption": "(a) All models", "description": "This figure shows the relationship between model size and median performance across all the benchmark tasks considered in the paper.  The x-axis represents the model size in billions of parameters, and the y-axis represents the median F1 score across all benchmark tasks.  The plot allows for a visual comparison of the performance of different models with varying sizes, indicating whether larger models consistently outperform smaller models in the context of this study.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x2.png", "caption": "(b) Without modified models", "description": "This figure shows the relationship between model size and median performance across multiple benchmark tasks, but only includes models that were not specifically adapted for the Dutch language.  It helps to visualize if larger models inherently perform better, without the confounding factor of language-specific tuning. The graph allows for comparison of performance across different model sizes, focusing on general multilingual capabilities rather than adaptations for a particular language.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x3.png", "caption": "Figure 1: Model size vs. median performance", "description": "This figure compares the median performance of various large language models (LLMs) across multiple benchmark tasks against their model size (in billions of parameters).  It visualizes the relationship between model size and performance, showing whether larger models necessarily lead to better results. The figure is split into two subfigures: (a) includes all models evaluated; (b) limits the comparison to models that were not specifically adapted for Dutch, offering a clearer view of size's effect without the confounding factor of language-specific adaptation.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x4.png", "caption": "(a) All models", "description": "This figure shows the relationship between model size (in billions of parameters) and the median performance across all benchmark tasks for all the models considered in the paper.  The x-axis represents model size, while the y-axis shows the weighted F1 score, a common metric used to evaluate the performance of models on classification tasks.  The plot helps to visualize whether larger models generally perform better and whether there are clear trends based on model size.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x5.png", "caption": "(b) Without modified models", "description": "This figure is a bar chart visualizing the relationship between model size (in billions of parameters) and median performance across multiple benchmarks.  However, unlike Figure 1(a), which includes all models, Figure 1(b) focuses only on those models that were not specifically fine-tuned or adapted for the Dutch language.  This allows for a more direct comparison of the inherent capabilities of different base models, without the influence of targeted Dutch language adaptations.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x6.png", "caption": "Figure 2: Model release date vs. median performance", "description": "This figure illustrates the relationship between the release date of the language models and their median performance across various benchmark tasks.  It shows that more recently released models generally outperform older models, even when controlling for model size. This suggests that ongoing advancements in the field are leading to improvements in model capabilities.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x7.png", "caption": "(a) ARC", "description": "This figure shows the performance of various language models on the AI2 Reasoning Challenge (ARC) benchmark.  The x-axis represents the different language models tested, while the y-axis shows the weighted F1 score achieved by each model.  The plot visually compares the performance of different models\u2014including those specifically adapted for Dutch and those which are multilingual\u2014on this reasoning task.  The error bars indicate the 95% confidence intervals, which reflect the uncertainty in the model's performance.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x8.png", "caption": "(b) DBRD", "description": "This figure shows the performance of various language models on the Dutch Book Review Dataset (DBRD) sentiment analysis benchmark.  The x-axis represents the different language models evaluated, and the y-axis shows their weighted F1 scores. The models are ranked based on their performance on this specific sentiment analysis task, showcasing the relative strengths and weaknesses of each model in classifying Dutch book reviews as positive or negative.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x9.png", "caption": "(c) Dutch CoLA", "description": "This bar chart displays the performance of various language models on the Dutch CoLA benchmark, which evaluates grammatical acceptability in Dutch.  The models are ranked on the weighted F1 score, with higher scores indicating better performance.  The chart allows comparison of different models based on their size (number of parameters) and release date to understand how model architecture, size, training data, and release timing affect performance on this specific task.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x10.png", "caption": "(d) Global-MMLU", "description": "This figure shows the performance of various language models on the Global MMLU (Massive Multitask Language Understanding) benchmark.  The Global MMLU benchmark assesses a model's ability to solve a wide range of problems across various academic and professional fields, reflecting the breadth of knowledge and reasoning abilities of the model.  The x-axis represents the different language models, and the y-axis represents their performance scores on the Global MMLU benchmark. Higher scores indicate better performance.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x11.png", "caption": "(e) XLWIC", "description": "Figure 3(e) displays the results of the XLWIC (Cross-lingual Word-in-Context) benchmark.  XLWIC assesses a model's ability to perform word sense disambiguation. The chart shows the performance (weighted F1 score) of various models on this task, highlighting the relative strengths and weaknesses of different models in distinguishing between different meanings of a word within context.  The models are compared on this task. ", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x12.png", "caption": "Figure 3: Results per benchmark", "description": "This figure presents a bar chart visualization for each benchmark used in the paper to evaluate the performance of various language models.  The benchmarks cover different aspects of LLM capabilities, including reasoning (ARC), sentiment analysis (DBRD), grammatical acceptability (Dutch CoLA),  world knowledge and language understanding (Global MMLU), and word sense disambiguation (XLWIC). Each bar in the chart represents the weighted F1 score achieved by a particular model on a specific benchmark.  The figure allows for a comparison of model performance across the different benchmarks and helps in understanding the strengths and weaknesses of each model in various language tasks.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x13.png", "caption": "(a) ARC", "description": "This figure shows the performance of various language models on the ARC (AI2 Reasoning Challenge) benchmark.  The x-axis represents the different language models, and the y-axis shows their weighted F1 scores.  The models are ranked by their performance on this specific benchmark, providing a visual comparison of their reasoning abilities.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x14.png", "caption": "(b) DBRD", "description": "This bar chart displays the weighted F1 scores achieved by various language models on the Dutch Book Review Dataset (DBRD) sentiment analysis benchmark.  The models are ranked by their performance, illustrating the relative strengths and weaknesses of different approaches to Dutch natural language processing. The chart helps to visualize the differences in accuracy across different models, particularly in identifying positive and negative sentiment in Dutch book reviews.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x15.png", "caption": "(c) Dutch CoLA", "description": "This figure shows the performance of various language models on the Dutch CoLA benchmark.  The Dutch CoLA benchmark evaluates a model's ability to assess the grammatical acceptability of Dutch sentences.  The x-axis represents the different language models, and the y-axis represents the weighted F1 score, a metric measuring the accuracy of the model's grammaticality judgments.  Higher scores indicate better performance.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x16.png", "caption": "(d) Global-MMLU", "description": "This bar chart displays the weighted F1 scores achieved by various language models on the Global MMLU (Massive Multitask Language Understanding) benchmark.  The x-axis represents the different language models tested, while the y-axis shows their corresponding weighted F1 scores.  This benchmark assesses a model's ability to solve a range of tasks spanning different academic and professional fields.  The chart visually compares the performance of different models, highlighting their strengths and weaknesses on this comprehensive evaluation.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x17.png", "caption": "(e) XLWIC", "description": "Figure 3(e) displays the results of the XLWIC benchmark.  The XLWIC (Cross-lingual Word-in-Context) benchmark evaluates the ability of language models to perform word sense disambiguation in Dutch. The y-axis represents the weighted F1 score, indicating the performance of different models on this task. The x-axis shows the different models tested, including both Dutch-specific models and multilingual models. The graph helps to compare the performance of these models on word sense disambiguation within the context of the study.", "section": "4.4 Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x18.png", "caption": "Figure 4: Performance vs. size across all benchmarks", "description": "This figure displays the performance of various language models across five different benchmark tasks: ARC (reasoning), DBRD (sentiment analysis), Dutch CoLA (grammaticality), Global MMLU (world knowledge), and XLWIC-NL (word sense disambiguation).  The x-axis represents the model size in billions of parameters, and the y-axis represents the weighted F1 score, a metric reflecting the model's performance. Each data point represents a specific model's performance on a particular benchmark, allowing for comparison of model performance relative to both task and size.  The figure helps to visualize the relationships between model size, task type, and overall performance.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2412.15450/x19.png", "caption": "(a) ARC", "description": "This bar chart displays the performance of various language models on the ARC (AI2 Reasoning Challenge) benchmark.  The x-axis represents the different language models, while the y-axis shows their weighted F1 scores.  The chart visually compares the performance of Fietje and its variants against other models, both Dutch-specific and multilingual, highlighting relative strengths and weaknesses in reasoning tasks.", "section": "4.4 Results"}]