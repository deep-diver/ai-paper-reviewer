[{"Alex": "Hey everyone, and welcome to the podcast! Today, we're diving into some seriously cool AI research that could change how we evaluate those super-smart multimodal AI models. Think of it as AI judging AI, unsupervised! I'm Alex, your host, and I'm thrilled to have Jamie with us, ready to unpack this with me.", "Jamie": "Hey Alex, thanks for having me! AI judging AI? Sounds a bit like Skynet, but hopefully in a productive way. I\u2019m excited to learn more."}, {"Alex": "Exactly! So, Jamie, the research paper we're looking at today is titled 'UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation.' In a nutshell, it's a new way to assess these AI models that doesn't rely on tons of human-created questions and answers.", "Jamie": "Okay, so less human input. That's interesting because usually, evaluation is very human-intensive. Ummm, what\u2019s UPME stand for, and why is reducing human involvement a big deal?"}, {"Alex": "UPME stands for Unsupervised Peer review MLLM Evaluation framework. The big deal about reducing human involvement is scale. Existing methods are limited because designing quality Q&A pairs for images takes a HUGE amount of time and effort. UPME aims to automate this, allowing for much broader and faster evaluations.", "Jamie": "Gotcha, so it's about making the evaluation process more efficient and scalable. But, hmm, if humans aren't creating the questions, where do they come from? And how do we make sure they're actually any good?"}, {"Alex": "Great question! That\u2019s where the \u2018peer review\u2019 aspect comes in. UPME uses the AI models themselves to generate questions. It pulls two candidate models and then a review model from a pool. The review model crafts questions about a given image, and the candidate models answer them.", "Jamie": "Whoa, so the AI models are basically interviewing each other? That's wild! But isn't there a risk of bias if one AI is judging another? Like, would they favor responses similar to their own?"}, {"Alex": "Precisely! That's a critical concern the researchers addressed. They implemented a 'vision-language scoring system' to mitigate bias. It looks at three main aspects: response correctness, visual understanding and reasoning, and image-text correlation. It aims to score the responses by comparing the responses to visual contents and making sure the answers are relevant and correct in terms of visual understanding", "Jamie": "Okay, that sounds more objective. But how does it measure visual understanding and reasoning? That seems like a pretty subjective thing to quantify, even for a human."}, {"Alex": "They use a combination of techniques. The review model assesses the responses based on its own interpretative and analytical capabilities related to visual content. For the image-text correlation part, they use something called CLIP scores, which measures how well the text aligns with the visual content of the image.", "Jamie": "CLIP scores, got it. So it\u2019s trying to ensure the AI isn't just making stuff up that doesn't relate to the picture at all. Ummm, how do they make sure the models aren't just favoring longer or more complex answers?"}, {"Alex": "That's verbosity bias, and it's a real issue with LLMs. The vision-language scoring system helps to reduce this bias because it focuses on accuracy, relevance and visual understanding. The researchers also use dynamic weight optimization, so iteratively updating the models' scores to refine weights through consistency. This approach helps UPME reduce verbosity in the peer review system.", "Jamie": "Dynamic weight optimization... sounds complicated! But, okay, the system adjusts over time to become less biased. So, after all this AI-on-AI action, what kind of results did they actually get? Does UPME actually work?"}, {"Alex": "That\u2019s the exciting part! They tested UPME on two datasets, MMStar and ScienceQA. On MMStar, it achieved a Pearson correlation of 0.944 with human evaluations, and 0.814 on ScienceQA. That means UPME aligned remarkably well with human judgments, even though it's completely unsupervised.", "Jamie": "Wow, those are some pretty high correlation scores. So, it's not just faster, it's also accurate? That's impressive. What are MMStar and ScienceQA? Why those datasets in particular?"}, {"Alex": "MMStar focuses on creating a benchmark requiring visual content for reasoning and rigorously selecting visual-dependent samples. ScienceQA underscores the importance of chain-of-thought in scientific reasoning tasks. They chose them because they cover a range of visual content reasoning and multimodal content across different disciplines.", "Jamie": "Okay, a good mix of reasoning and visual understanding. So, if UPME is so great, what are the limitations? What are the researchers still working on?"}, {"Alex": "Well, like any research, it's not a perfect solution. While UPME reduces human workload, it still requires a pool of MLLMs to act as judges. The researchers think about making this more flexible. Also, there might be a room for enhancement of visual scoring methods, especially in edge cases.", "Jamie": "That makes sense. Always more to explore! Alex, thanks so much for walking me through this. It's fascinating to see AI being used to evaluate AI, and it sounds like UPME is a really promising step in that direction."}, {"Alex": "You're very welcome, Jamie! It's been a pleasure to discuss it with you. Shall we dive deeper?", "Jamie": "Yeah, I'd love to. Alex, you mentioned some optimization of scores and the weights of the models. How is the optimization exactly done and how many epochs did they run for the model to converge, I am just curious about a bit more implementation details?"}, {"Alex": "That's a super insightful question, Jamie! The researchers employed dynamic weight optimization using Mean Squared Error (MSE) as the loss function. They iteratively update the confidence weights of each model to minimize the discrepancy between the model's estimated score and its performance. The optimization framework typically reliably converges within 30 epochs, and in general 64 different initial settings are experimented", "Jamie": "I see, so it's an iterative process of refining the weights based on performance feedback. By the way, you mentioned CLIP to correlate vision and texts, How is image-text relation exactly done and which architecture did they choose? I believe there are different CLIP versions and I am so curious about the details."}, {"Alex": "Of course, happy to elaborate on that! To compute the image-text correlation, they employ CLIP model, measuring cosine similarity between image and text embeddings. If the text goes over CLIP's token input limit they segment. The average cosine similarity is then calculated to evaluate image-text alignment, penalizing content of segments that aren't properly aligned with the visual information", "Jamie": "Wow, it's very details! Thanks a lot! Then, this might be a bit of a naive question, but How is judge correctness determined? Is it just based on the overall score from the visual-language scoring system, or is there some other metric involved?"}, {"Alex": "Not naive at all, it's a crucial point! Judge correctness is assessed by evaluating the responses based on the review model's judgment for candidates! They use a pairwise scoring approach, where candidates are evaluated against one another, determining the model's score for response correctness in this process. But, hmm, how about different types of images, and How does it work in complex images that aren't simple? ", "Jamie": "Oh, that's cool. So, It's making peer comparisons rather than one-off assessments! Ummm, regarding the image complexity, I wonder if the results hold in more abstract images such as sketches?"}, {"Alex": "That's a great point. I don't believe the current study explicitly tested abstract images or sketches, the datasets were chosen as general and multi-disciplinary enough. However, the framework is flexible so the team are probably thinking of extending the datasets and test cases.", "Jamie": "That is exciting to think about the future! When you mentioned the datasets earlier, I remember MMStar has been used, why do you think they used this? Are there some unique aspects to this dataset?"}, {"Alex": "Yeah, MMStar dataset is super great! MMStar is unique in terms of requiring visual content for reasoning, rigorous selection of visual-dependent samples, and detection of data leakage. It evaluates multimodal capabilities of MLLMs, with the samples exhibiting visual dependency and minimal data leakage, that is why UPME also tries to mitigate data leakage risks as well!", "Jamie": "That makes a lot of sense given the overall context. Talking about models again, Is there some specific architecture they recommend to use with UPME and achieve best peer review results?"}, {"Alex": "Good point. While the study doesn't explicitly recommend a specific architecture, the results indicate that GPT-4o exhibited really great reviewing skills and had the best performance, achieving high correlation scores! But I guess you are also thinking about this, How can we guarantee fairness if the initial set of models is biased?", "Jamie": "Yeah! Precisely that! Is there a feedback loop to correct for the reviewers models mistakes to ensure fairness and keep the system robust?"}, {"Alex": "That's a really deep question. UPME uses a dynamic weight optimization technique! Also they tested with multiple initial settings to ensure the framework reliably converges! So, it is trying to correct and reduce reviewers bias.", "Jamie": "Thanks! With those initial settings, I believe it should be enough! So given the ability to effectively enhance evaluation, I assume there may be a large number of applications! So, can you list some potential applications in the real world? What problems might UPME help solve outside of just research?"}, {"Alex": "Absolutely! I think UPME has huge potential. For example, you could be applied to improve the training process of the models! Another great application is automatic evaluation of AI-driven educational tools, ensuring their accuracy and effectiveness and minimizing educators workloads. It also enables quick and objective assessment of AI agents or visual assistants.", "Jamie": "That's a great number of examples! Thanks for answering all my questions! I can see this is a truly great work and really innovative thinking! I do look forward to further research on this!"}, {"Alex": "The pleasure was all mine, Jamie! And that's all the time we have for today. In short, UPME offers a promising, unsupervised approach to evaluating multimodal AI models, reducing human workload and biases. It's a great step towards more efficient and reliable AI assessment. I believe UPME will truly open the horizon for testing new visual models in the future! Thanks again for joining me, and until next time!", "Jamie": "Thanks Alex! It was a great discussion!"}]