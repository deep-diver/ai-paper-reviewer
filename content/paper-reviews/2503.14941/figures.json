[{"figure_path": "https://arxiv.org/html/2503.14941/x1.png", "caption": "Figure 1: Existing methods for evaluating MLLMs face various challenges. Our proposed UPME framework addresses these limitations by leveraging a peer review mechanism, reducing annotation costs, and aligning closely with human judgment.", "description": "The figure illustrates the challenges associated with existing Multimodal Large Language Model (MLLM) evaluation methods. These methods often involve significant human annotation effort for creating question-answer pairs, leading to high costs and limited evaluation scale.  Additionally, existing MLLM-as-Judge approaches, while aiming to reduce human workload, can introduce biases. The figure contrasts these existing approaches with the proposed Unsupervised Peer review MLLM Evaluation (UPME) framework.  UPME tackles these limitations by using an unsupervised peer-review mechanism in which MLLMs automatically generate questions and evaluate each other's answers.  This reduces the dependence on human annotation, minimizes costs, and achieves evaluation results that closely align with human judgment.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14941/x2.png", "caption": "Figure 2: The UPME framework consists of three main components: (i)\ud835\udc56(i)( italic_i ) Peer Review Mechanism, where two candidate models and one review model are randomly selected from the MLLM pool. The review model generates questions based on a selected image, and candidate models provide responses. (i\u2062i)\ud835\udc56\ud835\udc56(ii)( italic_i italic_i ) Vision-Language Judgment Scoring System, which evaluates answers based on textual correctness, visual understanding and reasoning, and image-text correlation. (i\u2062i\u2062i)\ud835\udc56\ud835\udc56\ud835\udc56(iii)( italic_i italic_i italic_i ) Dynamic Weight Optimization, ensuring consistency between confidence weights and estimated scores through iterative optimization cycles.", "description": "The figure illustrates the UPME framework's architecture, detailing its three core components.  The Peer Review Mechanism randomly selects two candidate language models and a review model from a pool. The review model generates a question about a given image, and the candidate models provide answers. The Vision-Language Judgment Scoring System then assesses these answers based on three criteria: response correctness, visual understanding and reasoning, and image-text correlation (using CLIP scores). Finally, the Dynamic Weight Optimization component refines the confidence weights associated with each model through iterative optimization cycles, ensuring consistent scores across the evaluation process. This iterative process improves the accuracy and reduces bias in the evaluation.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.14941/x3.png", "caption": "Figure 3: Convergence experiments.", "description": "The figure displays the convergence behavior of the UPME framework's dynamic weight optimization process during its training. It shows how the average loss decreases and the average Pearson correlation increases over epochs (training iterations), indicating that the model's predicted scores are becoming increasingly similar to human-provided scores.  The graph visually demonstrates that the UPME framework's optimization process effectively converges towards a solution that aligns with human judgment, as reflected by the rising Pearson correlation coefficient.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14941/x4.png", "caption": "Figure 4: The performance of UPME in different sample size.", "description": "This figure showcases the impact of varying sample sizes on the performance of the Unsupervised Peer Review MLLM Evaluation (UPME) framework.  The x-axis represents different sample sizes used for evaluation (25, 50, 75, and 100). The y-axis displays three key metrics: Pearson correlation, Spearman correlation, and Permutation Entropy.  Pearson and Spearman correlations measure the similarity between the UPME-generated scores and human-annotated scores, while Permutation Entropy assesses the complexity and unpredictability of the scores.  The figure likely contains separate lines for both MMSTAR and ScienceQA datasets, illustrating how the performance of UPME in terms of these metrics changes as the sample size increases. It demonstrates the point at which increasing the sample size yields diminishing returns, suggesting an optimal sample size for the UPME framework.", "section": "4.2.3. Impact of Sample Size"}, {"figure_path": "https://arxiv.org/html/2503.14941/x5.png", "caption": "Figure 5: Model accuracy comparison in peer review framework w/ and w/o UPME, where Peer Review_Cor. represents the correctness of the original peer review, and UPME_Cor. and UPME_Vis. correspond to the two judgment dimensions of response correctness and visual understanding, introduced in subsection\u00a03.2.", "description": "Figure 5 presents a comparison of model accuracy in peer review settings, both with and without the UPME framework.  The x-axis shows different models being evaluated. The y-axis represents the accuracy percentage. Three bars are shown for each model:  'Peer Review_Cor' represents the accuracy of the original peer review method considering only response correctness; 'UPME_Cor' indicates the accuracy of UPME focusing on response correctness; 'UPME_Vis' shows UPME's accuracy considering both response correctness and visual understanding (as defined in Section 3.2). This figure visually demonstrates the improvement in accuracy that UPME achieves by incorporating visual understanding into its evaluation.", "section": "5.2. Performance w/ and w/o UPME"}, {"figure_path": "https://arxiv.org/html/2503.14941/x6.png", "caption": "(a) Verbosity Bias", "description": "The figure (a) visualizes the verbosity bias in the peer review process.  It shows a comparison of the number of correct and incorrect responses given by the peer review models. The x-axis represents the verbosity of the response (Non-Verbose vs Verbose), and the y-axis represents the number of responses. The figure demonstrates that verbose responses were more likely to be incorrectly judged in the peer review setting compared to non-verbose responses, indicating a bias towards longer responses.", "section": "5. Human Preference Alignment Analysis"}, {"figure_path": "https://arxiv.org/html/2503.14941/x7.png", "caption": "(b) Self Preference", "description": "Figure 6(b) shows the heatmap and statistical analysis results for self-preference bias. The heatmap visually represents the distribution of correct and incorrect responses within two groups: those favored by the model itself and those that are not. The statistical analysis, including the Chi-square value, p-value, and Phi coefficient, quantifies the significance of the self-preference bias. This figure provides a visual and quantitative assessment of the degree to which the models show a bias towards selecting their own responses as superior, even when other responses are more accurate or appropriate.", "section": "5. Human Preference Alignment Analysis"}, {"figure_path": "https://arxiv.org/html/2503.14941/x8.png", "caption": "Figure 6: Heatmap and Table for Peer Review and UPME on Self Preference and Verbosity Bias.", "description": "Figure 6 presents a comparative analysis of Peer Review and UPME methods in terms of mitigating self-preference and verbosity biases in multimodal large language model (MLLM) evaluation.  It uses a heatmap visualization to illustrate the distribution of correct and incorrect responses, categorized by verbosity (verbose vs. non-verbose) and self-preference (self vs. non-self). The accompanying table provides a quantitative summary using statistical measures (Chi-square, p-value, and Phi coefficient) to formally test the significance of these biases within both methods, showcasing how UPME effectively reduces the impact of these biases compared to the traditional Peer Review approach.", "section": "5. Human Preference Alignment Analysis"}, {"figure_path": "https://arxiv.org/html/2503.14941/extracted/6270370/imgs/screenshot.png", "caption": "Figure 7: Case study illustration of UPME. We provide the original human-designed questions and the UPME-generated questions, along with the answer analysis. The upper case presents the Disability of review model, where the review model can not answer the original question itself. The middle case demonstrates cases exhibiting verbosity bias. The bottom case shows self-preference bias.", "description": "Figure 7 showcases three examples illustrating how the UPME framework addresses issues in traditional MLLM-as-judge methods. The top example demonstrates a scenario where the review model is unable to answer the original question, highlighting limitations of previous methods. The middle example illustrates the presence of verbosity bias in the original peer-review approach, where the length of responses influences evaluation. The bottom example demonstrates self-preference bias, showing how models favor their own outputs over superior alternatives.  UPME is shown to mitigate these issues by generating new questions that are suitable for the review model's capabilities and by employing a vision-language scoring system that focuses on visual understanding and reasoning, rather than solely on response length.", "section": "5. Case Study"}]