{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 Technical Report", "publication_date": "2023-03-01", "reason": "This report is important as it details the capabilities of GPT-4, a foundational LLM upon which multimodal models are built."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "publication_date": "2020-10-01", "reason": "This paper introduces Vision Transformers (ViT), a visual encoder crucial to many MLLMs."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Improved Baselines with Visual Instruction Tuning", "publication_date": "2024-01-01", "reason": "This paper details LLaVA, a prominent MLLM used as a baseline in the ShortV paper and is thus an important reference."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-01-01", "reason": "This reference paper introduces CLIP, a method for visual pre-training that is widely used in MLLMs to encode visual information."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open Foundation and Fine-tuned Chat Models", "publication_date": "2023-07-01", "reason": "This paper presents Llama 2, a powerful open-source language model used as the backbone for many MLLMs, including the ones studied in this paper."}]}