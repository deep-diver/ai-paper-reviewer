[{"figure_path": "https://arxiv.org/html/2504.00502/x1.png", "caption": "(a) ShortV", "description": "This figure illustrates the ShortV method.  It shows a comparison of a dense layer in a standard Multimodal Large Language Model (MLLM) with a ShortV layer. In the standard dense layer, both text and visual tokens are processed. In the ShortV layer, visual tokens are frozen, meaning their updates are not computed, leading to computational savings.  This freezing occurs in layers identified as ineffective by a metric called Layer Contribution (LC). The ShortV layer only processes text tokens through the feed-forward network (FFN) and the attention mechanism, significantly reducing the computational cost for the visual tokens.", "section": "3. ShortV"}, {"figure_path": "https://arxiv.org/html/2504.00502/x2.png", "caption": "(b) Performance vs. the Number of ShortV Layers", "description": "This figure shows the relationship between the number of ShortV layers used and the average performance across multiple vision-language benchmarks.  ShortV layers are modified layers where visual token updates are frozen, aiming to improve efficiency without significant performance loss. The x-axis represents the number of ShortV layers (the number of ineffective layers replaced), and the y-axis shows the average performance, which likely represents a normalized score across various benchmarks. The plot demonstrates how performance changes as more layers are replaced with ShortV layers, allowing readers to assess the trade-off between efficiency (fewer computations) and performance.", "section": "3. ShortV"}, {"figure_path": "https://arxiv.org/html/2504.00502/x3.png", "caption": "Figure 1: \n(a) Illustration of ShortV.\nWe identify ineffective layers for visual tokens and replace these layers with sparse ShortV layers.\nIn ShortV layers, we freeze visual tokens, and eliminate computations related to updating them.\nShortV improves MLLM efficiency in a training-free manner and involves no parameter updates.\nNotably, ShortV is compatible with token pruning methods, e.g. FastV.\n(b) Performance vs. the number of ShortV layers. Average Performance means a normalized average score on multiple benchmarks.\nShortV can freeze visual tokens in approximately 60% of the MLLM layers with nearly no performance degradation.", "description": "Figure 1(a) illustrates ShortV, a method to enhance the efficiency of Multimodal Large Language Models (MLLMs). ShortV identifies ineffective layers for visual tokens (layers whose transformations minimally affect the model's output). These layers are replaced with sparse ShortV layers. In these new layers, updates to visual tokens are frozen, reducing computational costs without retraining. Importantly, ShortV is compatible with token pruning methods like FastV. Figure 1(b) shows the performance of ShortV across multiple benchmarks as the number of ShortV layers increases.  It demonstrates that ShortV can freeze visual token updates in roughly 60% of MLLM layers with negligible performance impact.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.00502/x4.png", "caption": "(a) Sparse layer where visual tokens are frozen, used to investigate layer redundancy for visual tokens.\nOnly text tokens function as queries and are passed through the FFN.\nWe also denote this layer as ShortV layer.", "description": "This figure illustrates a sparse layer where visual tokens are frozen, preventing them from being updated or participating in calculations within the layer.  This technique is used to investigate layer-wise redundancy in Multimodal Large Language Models (MLLMs) by isolating the impact of visual tokens. Only text tokens are allowed to function as queries and pass through the feed-forward network (FFN) in this modified layer, which is also referred to as a ShortV layer. The purpose is to measure the effect of removing the visual tokens' transformations on the model's output, thus quantifying the layer's contribution to visual token processing. This helps determine if layers are redundant for processing visual tokens and can be replaced with the more efficient ShortV layer, thus improving model efficiency.", "section": "2. Layer Redundancy in MLLMs"}, {"figure_path": "https://arxiv.org/html/2504.00502/x5.png", "caption": "(b) Sparse layer where text tokens are frozen, used to investigate layer redundancy for text tokens.\nOnly visual tokens function as queries and are passed through the FFN.", "description": "This figure shows a sparse Transformer layer where text tokens are frozen, meaning their hidden states remain unchanged during the layer's processing.  Only visual tokens serve as queries in the self-attention mechanism and are passed through the feed-forward network (FFN). This setup is used to measure the impact of a layer's transformations specifically on text tokens, allowing the researchers to determine the layer-wise redundancy concerning text information in the model. By comparing the model's output with and without these transformations, the layer's contribution to processing text tokens is quantified.", "section": "2.2. Layer Contribution Metric"}, {"figure_path": "https://arxiv.org/html/2504.00502/x6.png", "caption": "Figure 2: Sparse layers used to investigate layer redundancy for different tokens.\nTo investigate layer redundancy for certain tokens, we freeze these tokens within the layer, i.e. keep hidden states of these tokens unchanged, and measure the divergence between the model\u2019s output logits and those of the original model.\nWe gray out the attention that does not need calculation.", "description": "This figure illustrates the \"sparse layers\" used in a method to identify layer-wise redundancy in multimodal large language models (MLLMs).  Two types of sparse layers are shown: one where visual tokens are frozen (their hidden states remain unchanged during processing), and one where text tokens are frozen. By comparing the model's output logits with and without freezing tokens in each layer, the researchers quantify each layer's contribution to the model's performance, specifically for visual or text tokens.  The grayed-out portions of the attention mechanisms highlight the computational savings achieved by freezing specific tokens, thereby determining which layers are less important for each type of token.", "section": "Layer Redundancy in MLLMs"}, {"figure_path": "https://arxiv.org/html/2504.00502/x7.png", "caption": "(a) LLaVA-1.5-7B", "description": "The figure shows the Layer Contribution (LC) scores for LLaVA-1.5-7B across different layers. LC quantifies a layer's impact on model output by measuring the divergence when transformations on specific tokens (visual or text) are frozen within that layer. The plot displays LC scores for visual tokens (blue) and text tokens (orange) across each layer. Lower LC scores indicate less contribution, suggesting inefficiency for the corresponding tokens within that layer.  The graph visualizes how much each layer impacts visual and text tokens differently. This is used to illustrate the layer-wise redundancy which motivates the ShortV technique. ", "section": "2. Layer Redundancy in MLLMs"}]