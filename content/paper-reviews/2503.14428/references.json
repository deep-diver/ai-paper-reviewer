{"references": [{"fullname_first_author": "Kaiyue Sun", "paper_title": "T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-Video Generation", "publication_date": "2024-07-14", "reason": "This benchmark paper is a primary evaluation source in the main paper, as indicated in its abstract and experiments section."}, {"fullname_first_author": "Zhuoyi Yang", "paper_title": "CogVideoX: Text-to-Video Diffusion Models with an Expert Transformer", "publication_date": "2024-08-06", "reason": "CogVideoX is the main model used as the backbone, and the authors conduct extensive experiments based on it."}, {"fullname_first_author": "Haoxin Chen", "paper_title": "VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models", "publication_date": "2024-01-09", "reason": "The main paper performs experiments using VideoCrafter2; therefore, this reference is crucial for understanding the experimental setup."}, {"fullname_first_author": "Ye Tian", "paper_title": "VideoTetris: Towards Compositional Text-to-Video Generation", "publication_date": "2024-01-01", "reason": "VideoTetris is one of the most important baseline models that performs comparisons with in the main paper."}, {"fullname_first_author": "Jiuniu Wang", "paper_title": "ModelScope Text-To-Video Technical Report", "publication_date": "2023-08-06", "reason": "ModelScope is a fundamental text-to-video generation model, serving as a key baseline for performance comparisons."}]}