{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-15", "reason": "This paper introduces CLIP, which is fundamental in aligning vision and language modalities for transfer learning, a key aspect addressed in the current paper."}, {"fullname_first_author": "Maxime Oquab", "paper_title": "Dinov2: Learning robust visual features without supervision", "publication_date": "2024-01-01", "reason": "This paper presents DINOv2, a vision-only pre-training method that the current paper builds upon and aims to enhance, particularly in multimodal understanding."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-11", "reason": "This paper introduces LLaVA, a visual instruction tuning method that serves as a baseline and is used for fine-tuning and evaluation in the current work."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2021-05-17", "reason": "This paper presents the Vision Transformer (ViT) architecture, which is the backbone for many vision foundation models and is used in the current paper."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-06", "reason": "This paper introduces the Transformer architecture, which is fundamental to both vision and language models and is a key component in many of the models used in the current paper."}]}