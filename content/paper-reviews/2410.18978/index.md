---
title: "Framer: Interactive Frame Interpolation"
summary: "Framer: an interactive frame interpolation tool lets users customize video transitions by adjusting keypoints, yielding smooth, creative resultsâ€”even handling complex scenarios with an 'autopilot' mod..."
categories: ["AI Generated"]
tags: ["ðŸ”– 24-10-24", ]
showSummary: true
date: 2024-10-24
draft: false
---

{{< keyword >}} 2410.18978 {{< /keyword >}}

### TL;DR


{{< lead >}}

Framer is a new approach to video frame interpolation that lets users interactively create smooth transitions between two images. Unlike traditional methods that automatically generate transitions, Framer allows users to customize the process by dragging and manipulating keypoints on the images. This gives users more control over the final result.  The paper also introduces an "autopilot" mode that automatically estimates the keypoints and their trajectories, making it easier to use for users who don't want to manually adjust the keypoints. Experiments show Framer produces high-quality results in various applications, including image morphing, time-lapse video generation, and cartoon animation, often outperforming existing methods.  The core innovation is the blend of human interaction and a generative model, allowing for both fine-grained control and ease of use.

{{< /lead >}}


{{< button href="https://arxiv.org/abs/2410.18978" target="_self" >}}
{{< icon "link" >}} &nbsp; read the paper on arXiv
{{< /button >}}
<br><br>
{{< button href="https://huggingface.co/papers/2410.18978" target="_self" >}}
{{< icon "hf-logo" >}} &nbsp; on Hugging Face
{{< /button >}}

#### Why does it matter?
This paper is important for researchers working on video frame interpolation, generative models, and human-computer interaction.  It introduces a novel interactive approach that bridges the gap between fully automated and entirely manual methods, offering a more intuitive and controllable way to generate high-quality results. The "autopilot" mode also simplifies the process for users who prefer less interaction.  Furthermore, the integration of user-guided controls and the use of a generative model offer promising new avenues for research and development in related fields.
#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Framer allows users to customize video transitions by interactively adjusting keypoints, enabling finer control over local motions. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The "autopilot" mode automates keypoint trajectory estimation, simplifying usage and producing high-quality results. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Framer demonstrates superior performance on various applications (image morphing, time-lapse generation) compared to existing methods. {{< /typeit >}}
{{< /alert >}}

------
#### Visual Insights



![](https://ai-paper-reviewer.com/2410.18978/figures_1_0.png)

> ðŸ”¼ Figure 1 shows examples of interactive frame interpolation results generated by the Framer model, highlighting its ability to customize local motions and handle challenging cases.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>





![](https://ai-paper-reviewer.com/2410.18978/charts_15_0.png)

> ðŸ”¼ The chart shows the FID and FVD scores on DAVIS-7 dataset for different 3D-UNet decoder feature indices used for point tracking.
> <details>
> <summary>read the caption</summary>
> Figure S1: Ablations on diffusion feature for point tracking at test time, experiments conducted on DAVIS-7 (left) and UCF101-7 (right).
> </details>





{{< table-caption >}}
<table id='1' style='font-size:14px'><tr><td></td><td colspan="4">DAVIS-7</td><td colspan="6">UCF101-7</td></tr><tr><td></td><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td><td>FVDâ†“</td><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td><td>FVDâ†“</td></tr><tr><td>AMT (Li et al., 2023)</td><td>21.66</td><td>0.7229</td><td>0.2860</td><td>39.17</td><td>245.25</td><td>26.64</td><td>0.9000</td><td>0.1878</td><td>37.80</td><td>270.98</td></tr><tr><td>RIFE (Huang et al., 2020)</td><td>22.00</td><td>0.7216</td><td>0.2663</td><td>39.16</td><td>319.79</td><td>27.04</td><td>0.9020</td><td>0.1575</td><td>27.96</td><td>300.40</td></tr><tr><td>FLAVR Kalluri et al. (2023)</td><td>20.94</td><td>0.6880</td><td>0.3305</td><td>52.23</td><td>296.37</td><td>26.50</td><td>0.8982</td><td>0.1836</td><td>37.79</td><td>279.58</td></tr><tr><td>FILM (Reda et al., 2022)</td><td>21.67</td><td>0.7121</td><td>0.2191</td><td>17.20</td><td>162.86</td><td>26.74</td><td>0.8983</td><td>0.1378</td><td>16.22</td><td>239.48</td></tr><tr><td>LDMVFI (Danier et al., 2024)</td><td>21.11</td><td>0.6900</td><td>0.2535</td><td>21.96</td><td>269.72</td><td>26.68</td><td>0.8955</td><td>0.1446</td><td>17.55</td><td>270.33</td></tr><tr><td>DynamicCrafter (Xing et al., 2023)</td><td>15.48</td><td>0.4668</td><td>0.4628</td><td>35.95</td><td>468.78</td><td>17.62</td><td>0.7082</td><td>0.3361</td><td>61.71</td><td>646.91</td></tr><tr><td>SVDKFI (Wang et al., 2024a)</td><td>16.71</td><td>0.5274</td><td>0.3440</td><td>26.59</td><td>382.19</td><td>21.04</td><td>0.7991</td><td>0.2146</td><td>44.81</td><td>301.33</td></tr><tr><td>Framer (Ours)</td><td>21.23</td><td>0.7218</td><td>0.2525</td><td>27.13</td><td>115.65</td><td>25.04</td><td>0.8806</td><td>0.1714</td><td>31.69</td><td>181.55</td></tr><tr><td>Framer with Co-Tracker (Ours)</td><td>22.75</td><td>0.7931</td><td>0.2199</td><td>27.43</td><td>102.31</td><td>27.08</td><td>0.9024</td><td>0.1714</td><td>32.37</td><td>159.87</td></tr></table>{{< /table-caption >}}

> ðŸ”¼ Table 1 quantitatively compares Framer with other video interpolation methods using reconstruction and generative metrics across all seven generated frames.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames.
> </details>



### More visual insights

<details>
<summary>More on figures
</summary>


![](https://ai-paper-reviewer.com/2410.18978/figures_4_0.png)

> ðŸ”¼ Figure 2 illustrates the architecture of Framer, showing its interactive mode, autopilot mode, trajectory controlling branch, and video frame interpolation fine-tuning process.
> <details>
> <summary>read the caption</summary>
> Figure 2: Framer supports (a) a user-interactive mode for customized point trajectories and (b) an 'autopilot' mode for video frame interpolation without trajectory inputs. During training, (d) we fine-tune the 3D-UNet of a pre-trained video diffusion model for video frame interpolation. Afterward, (c) we introduce point trajectory control by freezing the 3D-UNet and fine-tuning the controlling branch.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_5_0.png)

> ðŸ”¼ Figure 3 illustrates the bi-directional point tracking method used in Framer's 'autopilot' mode for estimating point trajectories.
> <details>
> <summary>read the caption</summary>
> Figure 3: Point trajectory estimation. The point trajectory is initialized by interpolating the coordinates of matched keypoints. In each de-noising step, we perform point tracking by finding the nearest neighbor of keypoints in the start and end frames, respectively. Lastly, We check the bi-directional tracking consistency before updating the point coordinate.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_6_0.png)

> ðŸ”¼ Figure 4 shows a qualitative comparison of the middle frame of seven interpolated frames generated by different methods, including the ground truth.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparison. 'GTâ€™ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_6_1.png)

> ðŸ”¼ The figure is a pie chart showing the results of a human preference test comparing Framer with several other video interpolation methods, indicating that Framer is overwhelmingly preferred.
> <details>
> <summary>read the caption</summary>
> Figure 5: Reults on human preference.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_7_0.png)

> ðŸ”¼ The figure shows three sets of video frame interpolation results, each generated with different drag controls, demonstrating the fine-grained customization offered by the Framer model.
> <details>
> <summary>read the caption</summary>
> Figure 6: Results on user interaction. The first row is generated without drag input, while the other two are generated with different drag controls. Customized trajectories are overlaid on frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_7_1.png)

> ðŸ”¼ The figure showcases various frame interpolation results generated by the Framer model, highlighting its ability to handle fine-grained customization and challenging scenarios.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_8_0.png)

> ðŸ”¼ Figure 1 showcases examples of frame interpolation results generated by the Framer model, highlighting its ability to customize local motions and handle challenging interpolation scenarios.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_8_1.png)

> ðŸ”¼ Figure 1 showcases examples of frame interpolation results generated by the Framer model, highlighting its ability to customize local motions and handle challenging cases.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_9_0.png)

> ðŸ”¼ Figure 1 showcases the results of Framer, demonstrating its ability to customize local motions and generate varying interpolation results from the same input frames, including handling challenging cases.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_9_1.png)

> ðŸ”¼ The figure shows ablation studies on each component of the Framer model, demonstrating the impact of trajectory guidance, trajectory updates, and bi-directional consistency verification on the final video interpolation results.
> <details>
> <summary>read the caption</summary>
> Figure 12: Ablations on each component. 'w/o trajectory' denotes inference without guidance from point trajectory, 'w/o traj. update' indicates inference without trajectory updates, and 'w/o bi' suggests trajectory updating without bi-directional consistency verification.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_17_0.png)

> ðŸ”¼ Figure 4 presents a qualitative comparison of the proposed Framer model with other state-of-the-art video frame interpolation methods, showing the middle frame of seven generated frames for each method.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparison. 'GTâ€ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_18_0.png)

> ðŸ”¼ Figure 4 shows a qualitative comparison of the proposed Framer method against several state-of-the-art video frame interpolation methods, showcasing the superior visual quality of Framer's results.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparison. 'GTâ€ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_19_0.png)

> ðŸ”¼ Figure 4 showcases a qualitative comparison of the proposed Framer model with existing methods for video frame interpolation on various sequences, highlighting the superior visual quality and detail preservation of Framer.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparison. â€œGTâ€ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_20_0.png)

> ðŸ”¼ Figure 4 shows a qualitative comparison of the proposed Framer model against several state-of-the-art video frame interpolation methods on various application scenarios.
> <details>
> <summary>read the caption</summary>
> Figure 4: Qualitative comparison. 'GTâ€™ strands for ground truth. For each method, we only present the middle frame of 7 interpolated frames. The full results can be seen in Fig. S4 and Fig. S5 in the Appendix.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_21_0.png)

> ðŸ”¼ Figure 1 showcases examples of frame interpolation results produced by the Framer model, highlighting its ability to handle various levels of motion complexity and user customization.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_21_1.png)

> ðŸ”¼ The figure showcases the results of interactive frame interpolation using Framer, demonstrating fine-grained control over local motions and smooth transitions between frames, even in challenging scenarios.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_22_0.png)

> ðŸ”¼ Figure 1 showcases examples of frame interpolation results generated by the Framer model, highlighting its ability to customize local motions and handle challenging cases.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_22_1.png)

> ðŸ”¼ Figure 1 showcases the interactive frame interpolation results produced by Framer, demonstrating its ability to customize local motions and handle challenging cases.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



![](https://ai-paper-reviewer.com/2410.18978/figures_22_2.png)

> ðŸ”¼ Figure 1 showcases examples of frame interpolation results generated by the Framer model, highlighting its ability to customize local motions and handle challenging cases.
> <details>
> <summary>read the caption</summary>
> Figure 1: Showcases produced by our Framer. It facilitates fine-grained customization of local motions and generates varying interpolation results given the same input start and end frame pair (first 3 rows). Moreover, Framer handles challenging cases and can realize smooth image morphing (last 2 rows). The input trajectories are overlayed on the frames.
> </details>



</details>



<details>
<summary>More on charts
</summary>


![](https://ai-paper-reviewer.com/2410.18978/charts_15_1.png)

> ðŸ”¼ The chart shows the FID and FVD scores on DAVIS-7 and UCF101-7 datasets for different 3D-UNet decoder feature indices used for point tracking in the Framer model.
> <details>
> <summary>read the caption</summary>
> Figure S1: Ablations on diffusion feature for point tracking at test time, experiments conducted on DAVIS-7 (left) and UCF101-7 (right).
> </details>


![](https://ai-paper-reviewer.com/2410.18978/charts_15_2.png)

> ðŸ”¼ The chart displays the FID and FVD scores on the DAVIS-7 dataset, varying the start and end steps used for correspondence guidance during diffusion sampling.
> <details>
> <summary>read the caption</summary>
> Figure S2: Ablations on the start and end diffusion steps for correspondence guidance, experiments conducted on DAVIS-7 (left) and UCF101-7 (right). We use a total sampling step of 30.
> </details>


![](https://ai-paper-reviewer.com/2410.18978/charts_15_3.png)

> ðŸ”¼ The chart displays the FID and FVD scores on DAVIS-7 and UCF101-7 datasets for different ranges of diffusion steps used for correspondence guidance.
> <details>
> <summary>read the caption</summary>
> Figure S2: Ablations on the start and end diffusion steps for correspondence guidance, experiments conducted on DAVIS-7 (left) and UCF101-7 (right). We use a total sampling step of 30.
> </details>


![](https://ai-paper-reviewer.com/2410.18978/charts_16_0.png)

> ðŸ”¼ The chart shows the impact of varying the number of trajectories used for guidance on FID and FVD scores for video frame interpolation on the DAVIS-7 and UCF101-7 datasets.
> <details>
> <summary>read the caption</summary>
> Figure S3: Ablations on the number of trajectories for guidance during sampling, experiments conducted on DAVIS-7 (left) and UCF101-7 (right).
> </details>


![](https://ai-paper-reviewer.com/2410.18978/charts_16_1.png)

> ðŸ”¼ The chart displays the FID and FVD scores on DAVIS-7 and UCF101-7 datasets for different numbers of trajectories used for guidance during sampling.
> <details>
> <summary>read the caption</summary>
> Figure S3: Ablations on the number of trajectories for guidance during sampling, experiments conducted on DAVIS-7 (left) and UCF101-7 (right).
> </details>


</details>



<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table id='1' style='font-size:16px'><tr><td>Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov. CDFI: compression-driven network design for frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.</td></tr><tr><td>Jiong Dong, Kaoru Ota, and Mianxiong Dong. Video frame interpolation: A comprehensive survey. ACM Trans. Multim. Comput. Commun. Appl., 2023.</td></tr><tr><td>Haiwen Feng, Zheng Ding, Zhihao Xia, Simon Niklaus, Victoria Fernandez Abrevaya, Michael J. Black, and Xuaner Zhang. Explorative inbetweening of time and space. arXiv: Computing Research Repo., abs/2403.14611, 2024.</td></tr><tr><td>Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In Int. Conf. Comput. Vis., 2023.</td></tr><tr><td>Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao. Featureflow: Robust video interpolation via structure-to-texture generation. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.</td></tr><tr><td>Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. arXiv: Computing Research Repo., abs/2311.16933, 2023.</td></tr><tr><td>Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv: Computing Research Repo., abs/2404.02101, 2024.</td></tr><tr><td>Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. RIFE: real-time intermediate flow estimation for video frame interpolation. arXiv: Computing Research Repo., abs/2011.06294, 2020.</td></tr><tr><td>Siddhant Jain, Daniel Watson, Eric Tabellion, Aleksander Holynski, Ben Poole, and Janne Kontkanen. Video interpolation with diffusion models. arXiv: Computing Research Repo., abs/2404.01203, 2024.</td></tr><tr><td>Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan Yang, Erik G. Learned-Miller, and Jan Kautz. Super slomo: High quality estimation of multiple intermediate frames for video interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., 2018.</td></tr><tr><td>Xin Jin, Longhai Wu, Guotao Shen, Youxin Chen, Jie Chen, Jayoon Koo, and Cheul-Hee Hahm. Enhanced bi-directional motion estimation for video frame interpolation. In IEEE Winter Conf. Appl. Comput. Vis., 2023.</td></tr><tr><td>Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, and Du Tran. FLAVR: flow-agnostic video representations for fast frame interpolation. In IEEE Winter Conf. Appl. Comput. Vis., 2023.</td></tr><tr><td>Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker: It is better to track together. arXiv: Computing Research Repo., abs/2307.07635, 2023.</td></tr><tr><td>Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu, Xiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang. Ifrnet: Intermediate feature refine network for efficient frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.</td></tr><tr><td>Hyeongmin Lee, Taeoh Kim, Tae-Young Chung, Daehyun Pak, Yuseok Ban, and Sangyoun Lee. Adacof: Adaptive collaboration of flows for video frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., 2020.</td></tr><tr><td>Changlin Li, Guangyang Wu, Yanan Sun, Xin Tao, Chi-Keung Tang, and Yu- Wing Tai. H-VFI: hierarchical frame interpolation for videos with large motions. arXiv: Computing Research Repo., abs/2211.11309, 2022.</td></tr><tr><td>Zhen Li, Zuo-Liang Zhu, Linghao Han, Qibin Hou, Chun-Le Guo, and Ming-Ming Cheng. AMT: all-pairs multi-field transforms for efficient frame interpolation. In IEEE Conf. Comput. Vis. Pattern Recog., 2023.</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ Table 1 quantitatively compares Framer's performance against other video interpolation methods using reconstruction and generative metrics across 7 generated frames.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames.
> </details>

{{< table-caption >}}
<table id='6' style='font-size:14px'><tr><td></td><td colspan="5">DAVIS-7</td><td colspan="5">UCF101-7</td></tr><tr><td></td><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td><td>FVDâ†“</td><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td><td>FVDâ†“</td></tr><tr><td>w/o trajectory</td><td>20.19</td><td>0.6831</td><td>0.2787</td><td>28.25</td><td>128.71</td><td>24.16</td><td>0.8677</td><td>0.1798</td><td>32.64</td><td>195.54</td></tr><tr><td>w/o traj. updating</td><td>20.82</td><td>0.7054</td><td>0.2621</td><td>27.33</td><td>120.73</td><td>24.69</td><td>0.8748</td><td>0.1842</td><td>31.95</td><td>187.37</td></tr><tr><td>w/o bi-directional</td><td>20.94</td><td>0.7102</td><td>0.2602</td><td>27.23</td><td>116.81</td><td>24.73</td><td>0.8746</td><td>0.1845</td><td>31.66</td><td>183.74</td></tr><tr><td>Framer (Ours)</td><td>21.23</td><td>0.7218</td><td>0.2525</td><td>27.13</td><td>115.65</td><td>25.04</td><td>0.8806</td><td>0.1714</td><td>31.69</td><td>181.55</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ Table 1 quantitatively compares Framer with other video interpolation methods using reconstruction and generative metrics across 7 generated frames.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames.
> </details>

{{< table-caption >}}
<table id='8' style='font-size:18px'><tr><td rowspan="2"></td><td colspan="4">DAVIS-7 (mid-frame)</td><td colspan="4">UCF101-7 (mid-frame)</td></tr><tr><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td></tr><tr><td>w/o trajectory</td><td>19.30</td><td>0.6504</td><td>0.3093</td><td>57.10</td><td>23.14</td><td>0.8523</td><td>0.1967</td><td>54.98</td></tr><tr><td>w/o traj. updating</td><td>19.84</td><td>0.6700</td><td>0.2935</td><td>55.37</td><td>23.60</td><td>0.8590</td><td>0.2009</td><td>53.83</td></tr><tr><td>w/o bi-directional</td><td>19.95</td><td>0.6739</td><td>0.2919</td><td>54.75</td><td>23.65</td><td>0.8586</td><td>0.2016</td><td>53.54</td></tr><tr><td>Framer (Ours)</td><td>20.18</td><td>0.6850</td><td>0.2845</td><td>55.13</td><td>23.92</td><td>0.8646</td><td>0.1889</td><td>53.33</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ Table S2 presents ablation study results on the performance of Framer by removing or disabling each of its components, focusing on the middle frame of a seven-frame interpolation sequence.
> <details>
> <summary>read the caption</summary>
> Table S2: Ablations on each component, evaluating only the middle frame out of all 7 generated frames. 'w/o trajectory' denotes inference without guidance from point trajectory, 'w/o traj. updating' indicates inference without trajectory updates, and 'w/o bi' suggests trajectory updating without bi-directional consistency verification.
> </details>

{{< table-caption >}}
<table id='4' style='font-size:16px'><tr><td></td><td colspan="4">DAVIS-7 (mid-frame)</td><td colspan="4">UCF101-7 (mid-frame)</td></tr><tr><td></td><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td><td>PSNRâ†‘</td><td>SSIMâ†‘</td><td>LPIPSâ†“</td><td>FIDâ†“</td></tr><tr><td>AMT (Li et alâŒ€, 2023)</td><td>20.59</td><td>0.6834</td><td>0.3564</td><td>100.36</td><td>25.24</td><td>0.8837</td><td>0.2237</td><td>75.97</td></tr><tr><td>RIFE (Huang et al., 2020)</td><td>20.74</td><td>0.6813</td><td>0.3102</td><td>80.78</td><td>25.68</td><td>0.8842</td><td>0.1835</td><td>59.33</td></tr><tr><td>FLAVR Kalluri et al. (2023)</td><td>19.93</td><td>0.6514</td><td>0.4074</td><td>118.45</td><td>24.93</td><td>0.8796</td><td>0.2164</td><td>79.86</td></tr><tr><td>FILM (Reda et al., 2022)</td><td>20.28</td><td>0.6671</td><td>0.2620</td><td>48.70</td><td>25.31</td><td>0.8818</td><td>0.1623</td><td>41.23</td></tr><tr><td>LDMVFI (Danier et al., 2024)</td><td>19.87</td><td>0.6435</td><td>0.2985</td><td>56.46</td><td>25.16</td><td>0.8789</td><td>0.1695</td><td>43.01</td></tr><tr><td>DynamicCrafter (Xing et al., 2023)</td><td>14.61</td><td>0.4280</td><td>0.5082</td><td>77.65</td><td>17.05</td><td>0.6935</td><td>0.3502</td><td>97.01</td></tr><tr><td>SVDKFI (Wang et al., 2024a)</td><td>16.06</td><td>0.4974</td><td>0.3719</td><td>53.49</td><td>20.03</td><td>0.7775</td><td>0.2326</td><td>69.26</td></tr><tr><td>Framer (Ours)</td><td>20.18</td><td>0.6850</td><td>0.2845</td><td>55.13</td><td>23.92</td><td>0.8646</td><td>0.1889</td><td>53.33</td></tr><tr><td>Framer with Co-Tracker (Ours)</td><td>21.94</td><td>0.7693</td><td>0.2437</td><td>55.77</td><td>25.86</td><td>0.8868</td><td>0.1873</td><td>54.64</td></tr></table>{{< /table-caption >}}
> ðŸ”¼ Table 1 quantitatively compares Framer with other video interpolation methods across reconstruction and generative metrics, evaluating all 7 generated frames.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparison with existing video interpolation methods on reconstruction and generative metrics, evaluated on all 7 generated frames.
> </details>

</details>


### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2410.18978/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/21.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2410.18978/22.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}