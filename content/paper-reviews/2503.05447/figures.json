[{"figure_path": "https://arxiv.org/html/2503.05447/x1.png", "caption": "Figure 1: Linear-MoE Architecture. In each Linear-MoE block, there is both an LSM layer and an MoE layer, with each layer preceded by its own normalization layer. The LSM layer is designed as a flexible abstraction of LSM methods, including: linear attention, SSM, and linear RNN, which follows a unified recurrence framework.", "description": "The Linear-MoE architecture consists of stacked Linear-MoE blocks. Each block contains a normalization layer followed by an LSM layer and an MoE layer.  The LSM layer is a flexible module that unifies different linear sequence modeling methods such as linear attention, state space models, and linear RNNs under a common recurrence framework.  The MoE layer implements the standard mixture-of-experts mechanism for sparse activation.", "section": "2.1 Linear-MoE Architecture"}, {"figure_path": "https://arxiv.org/html/2503.05447/x2.png", "caption": "Figure 2: Sequence Parallelism Approach on Hybrid Linear-MoE models. We exemplify the parallelism on the hybrid layers of LSM and standard attention with both TP and SP (both have a dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reduce-scatter in backward, RS/AG: reduce-scatter in forward and all-gather in backward, AG/No: all-gather in forward and no-op in backward, No/AG: no-op in forward and all-gather in backward. Note that the SP communication operations for linear attention operate on the memory state \ud835\udc0cs\u2208\u211dd\u00d7dsubscript\ud835\udc0c\ud835\udc60superscript\u211d\ud835\udc51\ud835\udc51\\mathbf{M}_{s}\\in\\mathbb{R}^{d\\times d}bold_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d end_POSTSUPERSCRIPT, while for standard attention, they operate on states \ud835\udc0as,\ud835\udc15s\u2208\u211dC\u00d7dsubscript\ud835\udc0a\ud835\udc60subscript\ud835\udc15\ud835\udc60superscript\u211d\ud835\udc36\ud835\udc51\\mathbf{K}_{s},\\mathbf{V}_{s}\\in\\mathbb{R}^{C\\times d}bold_K start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_C \u00d7 italic_d end_POSTSUPERSCRIPT.", "description": "Figure 2 illustrates the sequence parallelism approach used in hybrid Linear-MoE models, which combine linear sequence modeling (LSM) layers and standard attention layers.  The diagram shows how the computation is distributed across multiple GPUs (GPU0, GPU1, GPU2, GPU3) for both LSM and standard attention layers, using both tensor parallelism (TP) and sequence parallelism (SP), each with a dimension of 2.  The colors yellow and green represent communication operations for TP and SP, respectively.  Abbreviations AG, RS, and No represent all-gather, reduce-scatter, and no-op operations in the forward and backward passes.  A key distinction is highlighted: sequence parallelism for linear attention operates on the memory state  (a matrix of size d x d), whereas sequence parallelism for standard attention operates on the key (K) and value (V) matrices (matrices of size C x d). This difference reflects the distinct computational characteristics of the two types of layers.", "section": "2.2 Training"}, {"figure_path": "https://arxiv.org/html/2503.05447/x3.png", "caption": "Figure 3: Linear-MoE System Implementation. The Linear-MoE system is composed of two main subsystems: Modeling and Training. It is developed in a non-intrusive manner, utilizing the latest version of Megatron-Core. All components within the system are designed with extensibility in mind, encompassing the LSM modules, base models, examples, and training technologies. This design allows for future enhancements and extensions of the system.", "description": "The Linear-MoE system is composed of two main subsystems: Modeling and Training.  The Modeling subsystem provides a unified framework for various linear sequence modeling (LSM) methods, including linear attention, state space models, and linear RNNs.  These LSM modules can be integrated with Mixture-of-Experts (MoE) layers. The Training subsystem facilitates efficient training by incorporating advanced parallelism technologies, particularly Sequence Parallelism.  The figure illustrates the architecture, highlighting the modular and extensible design that allows for easy integration of new LSM methods, base models, and training techniques in the future. It leverages Megatron-Core for core functionalities.", "section": "2 Linear-MoE System"}, {"figure_path": "https://arxiv.org/html/2503.05447/x4.png", "caption": "Figure 4: Training Throughput (Tokens/s). As sequence length increases, the throughput of Baseline declines significantly, whereas LSM models maintain stable training efficiency.", "description": "Figure 4 illustrates the training throughput, measured in tokens per second, for various models across different sequence lengths and batch sizes. The 'Baseline' model, which represents a standard Transformer model with softmax attention, shows a significant decrease in throughput as the sequence length increases.  This illustrates the quadratic complexity of softmax attention. In contrast, Linear Sequence Modeling (LSM) methods demonstrate much more stable training throughput, even with longer sequence lengths. This highlights the advantage of LSM in maintaining efficient training, regardless of input sequence size.", "section": "3.2 Training and Inference Efficiency"}, {"figure_path": "https://arxiv.org/html/2503.05447/x5.png", "caption": "Figure 5: Inference Efficiency of A0.3B-2B Model Instances. We variate the decoding length from 1K to 128K with fixed batch size of 16 on single A800 80GB GPU to evaluate the Baseline w/ FlashAttention-2 and the Linear-MoE w/ Basic Linear Attention in terms of inference latency time and GPU memory usage.", "description": "This figure compares the inference efficiency of two A0.3B-2B models: a baseline model using FlashAttention-2 and a Linear-MoE model using basic linear attention.  Both models were tested on a single A800 80GB GPU with a fixed batch size of 16, while varying the decoding length from 1K to 128K tokens. The graph illustrates the trade-off between inference latency (time) and GPU memory usage for both models across different decoding lengths. This allows for a direct comparison of the performance and resource consumption of the two approaches for long sequence inference.", "section": "3.2 Training and Inference Efficiency"}, {"figure_path": "https://arxiv.org/html/2503.05447/x8.png", "caption": "Figure 6: Training Loss Curves of A0.3B-2B Model Instances. Left: pure Linear-MoE models; Right: hybrid Linear-MoE models. Linear-MoE shows competitive training convergence performance compared to the standard attention Baseline.", "description": "This figure displays the training loss curves for the A0.3B-2B Linear-MoE model.  The left panel shows curves for models using only Linear-MoE layers, while the right panel presents curves for hybrid models which incorporate both Linear-MoE and standard Transformer-MoE layers.  The comparison highlights that the Linear-MoE models demonstrate competitive training convergence performance when compared to the baseline model which utilizes standard attention mechanisms.", "section": "3.3 Training Loss and Evaluation"}]