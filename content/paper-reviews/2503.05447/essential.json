{"importance": "This paper is important for researchers, because it addresses the growing need for **efficient and scalable training** of large language models. By integrating LSMs with MoE, it opens new avenues for **architectural innovation** and efficient handling of long sequences. The exploration of hybrid models also provides valuable insights for future research.", "summary": "Linear-MoE: Integrates Linear Sequence Modeling with Mixture-of-Experts, achieving efficiency gains and competitive performance in large language models.", "takeaways": ["Linear-MoE, a novel system, integrates LSM and MoE for efficient large-scale model training.", "The system offers a unified framework for LSM and facilitates training via sequence parallelism.", "Evaluations show Linear-MoE achieves efficiency while maintaining performance."], "tldr": "Large language models require scalable training approaches. **Linear Sequence Modeling (LSM)** and **Mixture-of-Experts (MoE)** are promising architectural improvements. However, attention layers rely on softmax, leading to quadratic complexity with input sequence length. LSM has emerged to achieve impressive efficiency with linear training and constant memory inference, and can be expressed with matrix-valued hidden states, similar to RNN. \n\nThis paper introduces **Linear-MoE**, production-level system that combines LSM with MoE for large-scale models. Linear-MoE has modeling and training subsystems with linear attention, state space models, and linear RNN. Incorporating sequence parallelism enhances training. Evaluations show that Linear-MoE achieves efficiency while maintaining competitive performance.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.05447/podcast.wav"}