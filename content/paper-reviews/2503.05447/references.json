{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which is the foundation of many modern large language models and the attention mechanism crucial for sequence modeling."}, {"fullname_first_author": "Robert A. Jacobs", "paper_title": "Adaptive mixtures of local experts", "publication_date": "1991-01-01", "reason": "This paper is a foundational work on Mixture-of-Experts (MoE), a key architectural component for scaling models by sparsely activating different experts for different inputs."}, {"fullname_first_author": "Angelos Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-01-01", "reason": "This work introduced linear attention, a crucial technique to reduce the computational complexity of the original transformer attention mechanism."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-01-01", "reason": "This paper presents Mamba, a state space model (SSM) that achieves linear time complexity for sequence modeling and has become a prominent alternative to Transformers."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-01-01", "reason": "This paper introduced the Adam optimization algorithm, which is widely used in deep learning for training large models efficiently."}]}