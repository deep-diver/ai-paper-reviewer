[{"figure_path": "https://arxiv.org/html/2501.04001/x2.png", "caption": "Figure 1: Illustration of capabilities of our proposed Sa2VA. (a). Given a video, Sa2VA is able to segment the referred object and understand the whole scene. (b).Sa2VA supports image conversation, video conversation, image referring segmentation, video referring segmentation, and grounded caption generation with single-shot instruction-tuning. (c).Sa2VA achieves strong results on multiple images, video referring segmentation, and chat benchmarks compared with existing MLLMs, such as GLaMM\u00a0[66] and OMG-LLaVA\u00a0[99].", "description": "Figure 1 demonstrates the capabilities of Sa2VA, a novel model for dense grounded understanding of images and videos.  Panel (a) showcases Sa2VA's ability to segment a specified object within a video scene while also comprehending the broader context. Panel (b) highlights Sa2VA's versatility in handling various tasks, including image and video conversations, image and video referring segmentation, and generating grounded captions using a single-shot instruction-tuning method. Panel (c) presents a comparative analysis of Sa2VA's performance against other state-of-the-art Multimodal Large Language Models (MLLMs) across multiple image and video benchmarks, demonstrating Sa2VA's superior capabilities in tasks such as video referring segmentation and conversational question answering.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.04001/x3.png", "caption": "Figure 2: Our proposed Sa2VA model. The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through a large language model (LLM). The output text tokens are used to generate the [SEG] token and associated language outputs. The SAM-2 decoder receives the image and video features from the SAM-2 encoder, along with the [SEG] token, to generate corresponding image and video masks.", "description": "The Sa2VA model processes input text, visual prompts, images, and videos.  First, these inputs are converted into token embeddings. These embeddings are then fed into a Large Language Model (LLM) which processes the information and outputs text tokens.  Importantly, these text tokens are used to generate a special '[SEG]' token which acts as an instruction for the next stage.  The LLM also generates other associated language outputs, such as answers to questions. Finally, the image and video features, along with the '[SEG]' token, are passed to the SAM-2 decoder, which generates the corresponding image and video masks.", "section": "3.2 Sa2VA Framework"}, {"figure_path": "https://arxiv.org/html/2501.04001/x4.png", "caption": "Figure 3: Data annotation pipeline. Our proposed automatic data annotation pipeline consists of three stages: object/part-level, scene-level, and video-level text expression annotation. We use different colors in the final expression to highlight the information derived from each stage. Best view on screen and zoom out.", "description": "This figure illustrates the three-stage automatic data annotation pipeline used to create the Ref-SAV dataset.  The first stage, object/part-level annotation, involves cropping objects from video frames and using a large language model (InternVL2-76B) to generate detailed descriptions.  These descriptions are checked for consistency by another model (Qwen2-72B). The second stage, scene-level annotation, uses the object/part-level descriptions and the image (with the object highlighted) to create a more comprehensive scene description. Finally, the video-level annotation stage combines the scene-level descriptions with uniformly sampled video frames to generate a final video-level description.  Different colors in the final description highlight information from each stage.", "section": "3.3 Ref-SAV Dataset and Benchmark"}, {"figure_path": "https://arxiv.org/html/2501.04001/x5.png", "caption": "Figure 4: The samples of our Ref-SAV benchmark. Our proposed benchmark features multi-granularity, complex occlusion and reappearing, and both short and long-format text expressions.", "description": "Figure 4 showcases examples from the Ref-SAV benchmark dataset, highlighting its key characteristics:  It includes a variety of scenarios with different levels of complexity, including cases with multiple objects (multi-granularity), significant occlusions where objects are partially hidden, objects that disappear and reappear (reappearing), and descriptions that range from short and concise to long and detailed (short and long-format text expressions). These diverse examples are designed to thoroughly evaluate the robustness and accuracy of models in referring video object segmentation tasks.", "section": "3.3 Ref-SAV Dataset and Benchmark"}, {"figure_path": "https://arxiv.org/html/2501.04001/x6.png", "caption": "Figure 5: Visualization results on image referring segmentation task.", "description": "This figure showcases the effectiveness of the Sa2VA model in performing image referring segmentation.  Each row displays a prompt (instructing the model to segment a specific object) followed by the corresponding image and the model's generated segmentation mask.  The masks accurately highlight the requested objects (cooker, pasta, cameraman, person on a bicycle, bicycle, car, motorcycle, and trash can), demonstrating Sa2VA's capacity for precise object localization and segmentation based on natural language instructions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04001/x7.png", "caption": "Figure 6: Visualization results on video referring segmentation.", "description": "This figure showcases the model's video referring segmentation capabilities.  Multiple video clips are presented, each with a corresponding textual instruction for object segmentation. The model successfully identifies and segments the requested object (e.g., a backpack, a dog) within the complex video frames, accurately outlining the object's shape and boundaries throughout the video's duration.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04001/x8.png", "caption": "Figure 7: Visualization results on visual prompt understanding task. We use the masks predicted by our model under the GCG task as visual prompts, and generated region-level descriptions for these masks. The object masks and their captions for the corresponding region are highlighted in the same color.", "description": "This figure showcases the model's visual prompt understanding capabilities.  The masks generated by the model for the grounded caption generation (GCG) task are used as input visual prompts. The model then provides detailed region-level descriptions corresponding to the input masks. The figure displays multiple examples, with each object mask and its accompanying description shown in the same color for easy visual comparison and understanding.  This demonstrates the model's ability to interpret and describe image regions based on fine-grained segmentation and contextual understanding.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.04001/x9.png", "caption": "Figure 8: Visualization results on GCG tasks. Top: our method. Bottom: OMG-LLaVA\u00a0[99]. Note that, our method has stronger and fined-grained grounding ability and text alignment than OMG-LLaVA\u00a0[99], previous strong baseline.", "description": "Figure 8 presents a comparison of Grounded Caption Generation (GCG) results between the proposed Sa2VA model and the previous state-of-the-art model, OMG-LLaVA. The figure visually demonstrates Sa2VA's superior performance in generating more accurate and detailed captions aligned with the segmented object regions in images and videos. Sa2VA exhibits stronger grounding ability and more precise text-mask alignment compared to OMG-LLaVA.", "section": "Experiments"}]