{"references": [{"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational to the field of large language models, introducing the concept of few-shot learning, which is critical to the Sa2VA model's approach."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-12-01", "reason": "This paper introduces the crucial concept of visual instruction tuning, a core technique used by Sa2VA to enhance its multi-modal capabilities."}, {"fullname_first_author": "Nikhila Ravi", "paper_title": "SAM2: Segment anything in images and videos", "publication_date": "2024-08-01", "reason": "Sa2VA leverages SAM2's image and video segmentation capabilities, making this a central component of Sa2VA's architecture and functionality."}, {"fullname_first_author": "Zhe Chen", "paper_title": "InternVL 2.0", "publication_date": "2024-01-01", "reason": "Sa2VA uses InternVL 2.0 as its pre-trained large language model foundation, thus InternVL 2.0 serves as a significant building block of Sa2VA's capabilities."}, {"fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision", "publication_date": "2024-08-01", "reason": "Sa2VA draws on LLaVA-OneVision's approach of unifying image and video understanding, and the paper's methods are integral to Sa2VA's multi-modal design."}]}