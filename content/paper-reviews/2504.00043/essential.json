{"importance": "This paper is important for researchers because it **introduces a new benchmark for evaluating LLMs and LVLMs**. It provides a more realistic, multimodal evaluation approach that can **better assess models' reasoning capabilities** for complex tasks. It also **opens avenues for future research** in multimodal RL training. ", "summary": "CrossWordBench: Reasoning of LLMs/LVLMs via Puzzles", "takeaways": ["Reasoning LLMs outperform non-reasoning models by effectively leveraging crossing-letter constraints.", "LVLMs struggle with crossword puzzles, with performance strongly correlated to grid-parsing accuracy.", "Even puzzles from saturated benchmarks remain challenging, highlighting the importance of structural constraints in reasoning evaluation."], "tldr": "Current reasoning evaluation frameworks often fail to capture the dynamic interplay between textual and visual constraints, which are crucial for real-world problem-solving. To address this, the paper introduces **CrossWordBench**, a new benchmark designed to evaluate the reasoning capabilities of both Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) through the medium of crossword puzzles, which require adherence to semantic constraints from text-based clues and intersectional constraints from visual grid structures. \n\nThe **CrossWordBench** offers a controllable puzzle generation framework that produces puzzles in multiple formats and evaluation strategies.  The paper's extensive evaluation of over 20 models reveals that reasoning LLMs outperform non-reasoning models substantially by effectively leveraging crossing-letter constraints. It further demonstrates that LVLMs struggle with the task, showing a strong correlation between their puzzle-solving performance and grid-parsing accuracy.", "affiliation": "Carnegie Mellon University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2504.00043/podcast.wav"}