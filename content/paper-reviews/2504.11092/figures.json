[{"figure_path": "https://arxiv.org/html/2504.11092/extracted/6362965/figure/diffusion_illustration/illustration.png", "caption": "Figure 1: \n\nVivid4D. We improve dynamic scene reconstruction from casually captured monocular videos by synthesizing augmented views. Our approach integrates both geometric and generative priors to reformulate the video augmentation as a video inpainting task.\nThis enables our method to effectively complete invisible regions in the scene and enhance reconstruction quality.", "description": "The figure illustrates the Vivid4D method, which enhances 4D reconstruction from monocular videos.  Casually captured videos are augmented by synthesizing additional views. This is achieved by combining geometric and generative priors, treating the view augmentation as a video inpainting problem. The inpainting process fills in missing parts of the scene, leading to more complete and higher-quality 4D reconstructions.  The figure visually demonstrates this process, showing a dynamic scene, the monocular video input, and the augmented videos produced by Vivid4D.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.11092/extracted/6362965/figure/pipeline/pipeline.png", "caption": "Figure 2: Video Inpainting for 4D Reconstruction. To train the video inpainting model, we use 2D tracking to generate masked training pairs from unposed web videos. During 4D reconstruction, we warp the monocular video to novel viewpoints, creating masked videos that our inpainting diffusion model then completes.", "description": "This figure illustrates the training and application of a video inpainting model for 4D reconstruction.  The training process uses unposed web videos and 2D tracking to identify moving objects.  Masks are generated based on this tracking data, creating incomplete video sequences. The model is trained to fill in the masked areas, learning to generate temporally and spatially consistent video frames.  During the 4D reconstruction stage, the monocular video is warped to generate new viewpoints, resulting in masked areas.  These masked videos are fed into the trained model, which inpaints the missing regions to create complete augmented videos, providing enhanced input for 4D reconstruction.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.11092/extracted/6362965/figure/view_augmentation/view_augmentation.png", "caption": "Figure 3: 4D reconstruction based on view augmentation. Given an input monocular video, we first perform sparse reconstruction to obtain camera poses and align monocular depth to metric scale, forming an initial data buffer \ud835\udc9f0subscript\ud835\udc9f0\\mathcal{D}_{0}caligraphic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. In each iterative view augmentation step, we select frames at each timestamp from the previous buffer \ud835\udc9fj\u22121subscript\ud835\udc9f\ud835\udc571\\mathcal{D}_{j-1}caligraphic_D start_POSTSUBSCRIPT italic_j - 1 end_POSTSUBSCRIPT and warp them to novel viewpoints using pre-defined camera poses \ud835\udc13\ud835\udc13\\mathbf{T}bold_T, creating new perspective videos with continuous invisible region masks. These masked videos, along with binary masks and an anchor video, are fed into our pre-trained anchor-conditioned video inpainting diffusion model to produce completed novel-view videos. We update the buffer \ud835\udc9fjsubscript\ud835\udc9f\ud835\udc57\\mathcal{D}_{j}caligraphic_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT with these enhanced videos, their metric depths and poses. Finally, both the original monocular video and all synthesized multi-view videos are used to supervise 4D scene reconstruction.", "description": "This figure illustrates the iterative view augmentation process used in the Vivid4D model for 4D reconstruction from monocular video.  It begins with sparse reconstruction of an input video to estimate camera poses and obtain metric depth. This information forms an initial data buffer (D0).  In each iteration, frames are selected from the previous buffer (Dj-1), warped to novel viewpoints using predefined camera poses (T), and inpainted using a video inpainting diffusion model. The inpainted videos, along with their depth and poses, are added to the current buffer (Dj), expanding the set of views over iterations. Finally, both the original and the synthesized multi-view videos are used as supervision for 4D scene reconstruction.", "section": "3. Method"}]