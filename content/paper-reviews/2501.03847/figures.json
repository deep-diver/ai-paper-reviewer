[{"figure_path": "https://arxiv.org/html/2501.03847/x1.png", "caption": "Figure 1. Diffusion as Shader (DaS) is (a) a 3D-aware video diffusion method enabling versatile video control tasks including (b) animating meshes to video generation, (c) motion transfer, (d) camera control, and (e) object manipulation.", "description": "This figure illustrates the core concept and capabilities of the Diffusion as Shader (DaS) model.  Panel (a) provides a high-level overview of the DaS architecture, highlighting its 3D-awareness. This 3D awareness is key to the model's ability to perform various video manipulation tasks. The subsequent panels (b-e) showcase examples of these capabilities: (b) demonstrates the ability to animate 3D meshes and generate realistic videos from them, (c) shows motion transfer, allowing the model to apply the motion from one video to another, (d) illustrates camera control where the viewpoint is modified to create different perspectives on the same scene, and finally, (e) exhibits object manipulation, permitting changes to the location and appearance of objects within a video. These examples collectively highlight the versatile control DaS offers over video generation.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.03847/x2.png", "caption": "Figure 2. Architecture of DaS. (a) We colorize dynamic 3D points according to their coordinates to get (b) a 3D tracking video. (c) The input image and the 3D tracking video are processed by (d) a transformer-based latent diffusion with a variational autoencoder (VAE). The 3D tracking video is processed by a trainable copy of the denoising DiT and zero linear layers are used to inject the condition features from 3D tracking videos into the denoising process.", "description": "This figure illustrates the architecture of the Diffusion as Shader (DaS) model.  It begins by explaining how 3D points are color-coded based on their coordinates (a), forming a 3D tracking video (b) which acts as a control signal.  The input image (c) and the 3D tracking video are then fed into a transformer-based latent diffusion model using a Variational Autoencoder (VAE). A crucial step is the use of a 'trainable copy' of the denoising Diffusion Transformer (DiT) to process the 3D tracking video.  Zero linear layers help integrate the condition features from this video into the denoising process, making the model 3D-aware.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.03847/x3.png", "caption": "Figure 3. 3D tracking video generation in (a) object manipulation, (b) animating mesh to video generation, (c) camera control, and (d) motion transfer.", "description": "This figure illustrates how 3D tracking videos are generated for four different video control tasks using the Diffusion as Shader (DaS) method. (a) Object manipulation:  Depth maps and object segmentation are used to identify and manipulate the 3D points of an object, creating a 3D tracking video that guides the object's movement in the generated video. (b) Animating meshes to videos: Animated 3D meshes are converted into 3D tracking videos, providing control over mesh animation. (c) Camera control: Depth maps and camera trajectories are used to generate a 3D tracking video that directs camera movements in the output video. (d) Motion transfer: A 3D tracker processes a source video to generate a 3D tracking video which captures and transfers the motion to a new video with potentially altered styling or content.", "section": "3.3 Finetuning with 3D tracking videos"}, {"figure_path": "https://arxiv.org/html/2501.03847/x4.png", "caption": "Figure 4. Qualitative results of DaS on the camera control task. We show 4 trajectories (left, right, up, down) with large movements.", "description": "This figure showcases the qualitative results of the Diffusion as Shader (DaS) model on camera control tasks.  Four distinct camera trajectories are depicted (left, right, up, and down movements) across multiple frames. The large-scale movements demonstrate the model's ability to generate videos with diverse and complex camera paths accurately. Each row shows a sequence of frames from a video generated with DaS, demonstrating the smoothness and realism of the camera motions.", "section": "4.1 Camera control"}, {"figure_path": "https://arxiv.org/html/2501.03847/x5.png", "caption": "Figure 5. Qualitative comparison on motion transfer between our method, CCEdit\u00a0(Feng et\u00a0al., 2024b), and TokenFlow\u00a0(Geyer et\u00a0al., 2023b).", "description": "Figure 5 presents a qualitative comparison of motion transfer results obtained using three different methods: the authors' proposed method, CCEdit (Feng et al., 2024b), and TokenFlow (Geyer et al., 2023b).  For each method, two example video generation results are displayed, each accompanied by the corresponding text prompt. This allows for a visual assessment of each method's ability to accurately translate the motion described in the text prompt into the generated video, considering factors like fidelity, coherence, and overall quality of the video generated. The comparison highlights the relative strengths and weaknesses of each approach in achieving accurate motion transfer.", "section": "4.2 Motion transfer"}, {"figure_path": "https://arxiv.org/html/2501.03847/x6.png", "caption": "Figure 6. Qualitative results on motion transfer of our method.", "description": "Figure 6 showcases the qualitative results of the motion transfer task using the proposed Diffusion as Shader (DaS) method.  It displays several pairs of source and transferred videos generated by the DaS model. The source video is shown on the top row of each pair, while the DaS-generated transferred video is presented in the bottom row. Each pair demonstrates the ability of DaS to transfer motion while maintaining good quality and visual coherence.", "section": "4.2 Motion transfer"}, {"figure_path": "https://arxiv.org/html/2501.03847/x7.png", "caption": "Figure 7. More results of the animating mesh to video generation task. Our method enables the generation of different styles from the same mesh.", "description": "Figure 7 showcases the versatility of the proposed Diffusion as Shader (DaS) method in generating videos from animated 3D meshes.  It demonstrates that, using the same underlying mesh as input, DaS can produce videos with vastly different artistic styles.  This highlights the method's ability to decouple animation from visual style, allowing for greater control and creative freedom in video generation.", "section": "3.4 Animating meshes to videos"}, {"figure_path": "https://arxiv.org/html/2501.03847/x8.png", "caption": "Figure 8. Qualitative comparison on the animating mesh to video task between our method and CHAMP\u00a0(Zhu et\u00a0al., 2024).", "description": "Figure 8 presents a qualitative comparison of video generation results from two different methods: the proposed 'Diffusion as Shader' (DaS) approach and the CHAMP method (Zhu et al., 2024). Both methods tackle the task of animating 3D meshes into realistic videos. The figure visually demonstrates the capabilities of each approach by showing generated video frames corresponding to various animation sequences and stylistic choices (different styles), allowing for a direct comparison of their respective visual quality, animation smoothness, and overall realism.  The comparison highlights the strengths and weaknesses of each method in terms of producing high-fidelity, visually appealing videos from input 3D mesh animations.", "section": "4.3 Animating meshes to videos"}, {"figure_path": "https://arxiv.org/html/2501.03847/x9.png", "caption": "Figure 9. Qualitative results of our method on the object manipulation task. The top part shows the results of translation while the bottom part shows the results of rotating the object.", "description": "Figure 9 presents qualitative results demonstrating the object manipulation capabilities of the Diffusion as Shader (DaS) model. The top half showcases examples of object translation, where the object's position changes within the scene.  The bottom half displays object rotation, showing how DaS can modify the object's orientation. Each example includes the input image and the result generated by DaS, illustrating the model's ability to seamlessly integrate object manipulation into video generation.", "section": "4.4 Object manipulation"}, {"figure_path": "https://arxiv.org/html/2501.03847/x10.png", "caption": "Figure 10. Generated videos using depth maps or 3D tracking videos as control signals. Our 3D tracking videos provide better quality on the cross-frame consistency for video generation than depth maps.", "description": "This figure compares the results of video generation using two different types of 3D control signals: depth maps and 3D tracking videos.  The goal was to assess the impact of each method on the temporal consistency of generated videos, meaning how well the generated frames flow together smoothly over time.  The image showcases several generated video sequences side-by-side, allowing for a visual comparison of the quality of each approach.  The results demonstrate that videos created using 3D tracking videos exhibit substantially better cross-frame consistency than those produced with depth maps.", "section": "4.5 Analysis"}, {"figure_path": "https://arxiv.org/html/2501.03847/x11.png", "caption": "Figure 11. Failure cases. (Top) Incompatible tracking video. When a tracking video that does not correspond to the structures of the input image is provided, DaS will generate a video with a scene transition to a compatible new scene.\n(Bottom) Out of tracking range. For regions without 3D tracking points, the tracking video fails to constrain these regions and DaS may generate some uncontrolled content.", "description": "Figure 11 demonstrates two failure scenarios of the DaS model. The top panel shows a case where an incompatible tracking video (i.e., a video whose 3D points don't correspond to the input image's content) is used.  As a result, the model generates a video that shifts to a visually similar but different scene.  The bottom panel illustrates another failure: when the 3D tracking video lacks data for certain regions of the input image (out-of-tracking range).  In this case, the model generates video content for those regions that's not constrained by the 3D tracking information, resulting in uncontrolled or unrealistic visual elements.", "section": "Limitations and conclusions"}]