[{"figure_path": "https://arxiv.org/html/2503.09566/x1.png", "caption": "Figure 1: Overview of our method.\nOur method employs progressive frame rates, which utilizes full frame rate only in the final stage as shown in (a) and (b), thereby largely optimizing computational efficiency in both training and inference shown in (c).", "description": "This figure illustrates the core concept of the Temporal Pyramid Video Diffusion Model (TPDiff).  Panel (a) shows a traditional video diffusion model using a constant frame rate throughout the entire diffusion process, while panel (b) depicts the TPDiff approach. TPDiff progressively increases the frame rate across different stages of the diffusion process. Only the final stage operates at the full frame rate. This strategy significantly reduces computational costs during both training and inference.  Panel (c) provides a quantitative comparison of training and inference efficiency between the vanilla video diffusion model and TPDiff, demonstrating the improved efficiency of the proposed method.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09566/x2.png", "caption": "Figure 2: Methodology. a) Pipeline of temporal pyramid video diffusion model. We divide diffusion process into multiple stages with increasing frame rate. In each stage, new frames are initially temporally interpolated from existing frames. b) Our training strategy: stage-wise diffusion. In vanilla diffusion models, the noise direction along the ODE path points toward the real data distribution. In stage-wise diffusion, the noise direction is oriented to the end point of the current stage.", "description": "Figure 2 illustrates the methodology of the Temporal Pyramid Video Diffusion model (TPDiff). Part (a) shows the model's pipeline, where the diffusion process is divided into multiple stages with progressively increasing frame rates.  Within each stage, temporal interpolation is used to generate initial frames, enhancing efficiency.  Part (b) contrasts the training strategies of TPDiff and traditional vanilla diffusion models. Vanilla diffusion models directly guide the noise direction along the ordinary differential equation (ODE) path toward the true data distribution.  In contrast, TPDiff employs a stage-wise diffusion strategy where the noise direction in each stage is oriented towards the endpoint of that specific stage, improving training efficiency and making the training more stable and effective. ", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.09566/x3.png", "caption": "Figure 3: Data-Noise Alignment. For every training sample, (a) vanilla diffusion training randomly samples noises across the entire noise distribution, resulting in stochastic ODE path during training. (b) In contrast, our method samples noises in the closest range, making the ODE path approximately deterministic during training.", "description": "This figure illustrates the core concept behind data-noise alignment, a crucial aspect of the proposed training strategy.  Panel (a) depicts the stochastic nature of vanilla diffusion training, where noise is sampled randomly across the entire noise distribution, leading to an ODE path that varies significantly across training iterations. Panel (b) shows that, in contrast, the proposed method uses data-noise alignment to select noise from a much smaller range that's closest to the data. This ensures that the ODE path is far more deterministic and stable across training steps, resulting in improved training efficiency.", "section": "3.3. Training strategy"}, {"figure_path": "https://arxiv.org/html/2503.09566/x4.png", "caption": "Figure 4: Qualitative comparison.\nIn each pair of videos, the first row presents the results of models trained using vanilla diffusion and the second row shows the results of our method. The first two video pairs are generated by MiniFlux-vid and the remaining are generated by animatediff.", "description": "This figure shows a qualitative comparison of video generation results between models trained with vanilla diffusion and the proposed TPDiff method. Each pair of videos presents a comparison; the top row displays the output from the vanilla diffusion model and the bottom row shows the output from TPDiff. The first two pairs were generated using the MiniFlux-vid model, while the remaining two pairs were generated using the AnimateDiff model.  The figure demonstrates the visual differences in video quality, highlighting how TPDiff enhances the results.", "section": "4.3. Qualitative result"}, {"figure_path": "https://arxiv.org/html/2503.09566/x5.png", "caption": "Figure 5: Convergence curve of vanilla diffusion models and our method on (a) DDIM, (b) Flow Matching. We illustrate the FVD of two methods with different GPU hours consumed. Our method achieves higher training efficiency compared to vanilla approachs.", "description": "This figure displays the training efficiency of the proposed TPDiff model and vanilla diffusion models using two different diffusion methods: DDIM (denoising diffusion implicit models) and Flow Matching.  The x-axis represents the GPU hours consumed during training, and the y-axis represents the Fr\u00e9chet Video Distance (FVD), a metric that measures the quality of generated videos.  Lower FVD values indicate better video quality.  The plots show that for the same level of video quality (FVD), the TPDiff model requires significantly fewer GPU hours to train compared to vanilla diffusion methods for both DDIM and Flow Matching.  This demonstrates the efficiency improvements achieved by the proposed TPDiff model.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09566/x6.png", "caption": "Figure 6: Ablation study of inference strategy. Our method generates smooth, high-quality videos, whereas the baseline without inference renoising exhibits significant flickers", "description": "This ablation study compares the video generation quality of the proposed method with and without the inference denoising step. The results show that the proposed method, with the inference denoising, generates smoother, higher-quality videos while the baseline without this step produces videos with significant flickering artifacts.", "section": "4.4. Ablation study"}, {"figure_path": "https://arxiv.org/html/2503.09566/x7.png", "caption": "Figure 7: Ablation study of data-noise alignment. Our method can produce clearer videos compared to the baseline.", "description": "This ablation study compares video generation results with and without data-noise alignment. The figure shows that using data-noise alignment (our method) produces clearer videos compared to the baseline which does not use data-noise alignment, highlighting the importance of this technique in enhancing video quality.", "section": "4.4. Ablation study"}, {"figure_path": "https://arxiv.org/html/2503.09566/x8.png", "caption": "Figure 8: Comparison between vanilla diffusion and our method after 5000 training steps. Our method can generate temporally stable videos even at very early training steps while vanilla method cannot. The prompt is \u201dA serene scene of a sunflower field.\u201d", "description": "This figure compares video generation results from a vanilla diffusion model and the proposed TPDiff model after only 5000 training steps.  The target video is a serene scene of a sunflower field. The key observation is that TPDiff produces temporally stable and coherent videos even in the early stages of training, whereas the vanilla diffusion model struggles to generate consistent video frames at this point.", "section": "4.3. Qualitative result"}]