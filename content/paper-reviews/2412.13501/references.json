{"references": [{"fullname_first_author": "Rishi Bommasani", "paper_title": "On the opportunities and risks of foundation models", "publication_date": "2022-01-01", "reason": "This paper provides a comprehensive overview of foundation models, discussing their capabilities, limitations, and potential societal impact, serving as a crucial foundation for understanding the context of GUI agents."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2018-10-01", "reason": "BERT's introduction of pre-trained transformers revolutionized natural language processing, and its influence is evident in many GUI agent architectures that leverage LLMs for language understanding and generation."}, {"fullname_first_author": "Alexey Dosovitskiy", "paper_title": "An image is worth 16x16 words: Transformers for image recognition at scale", "publication_date": "2020-10-29", "reason": "This work introduces the Vision Transformer (ViT), a pivotal architecture for processing visual information in GUI agents, enabling them to perceive and interact with on-screen elements."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2020-07-22", "reason": "T5's unified text-to-text framework has significantly influenced the design of GUI agents by simplifying the processing and generation of diverse action representations, such as natural language, code, or structured commands."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-18", "reason": "Llama 2's open-source nature and strong performance make it a readily available and powerful LLM for researchers, facilitating experimentation and development of GUI agents with varying capabilities."}]}