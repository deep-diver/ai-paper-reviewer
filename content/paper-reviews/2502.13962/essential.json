{"importance": "This paper is important for researchers because it addresses the limitations of existing test-time scaling evaluations by considering **confidence in model responses**. It introduces a method for evaluating models in settings with **non-zero response risk**, offering a more realistic assessment of performance. This opens new avenues for investigating confidence-aware question answering and its applications in practical scenarios.", "summary": "Test-time scaling + confidence = better QA!", "takeaways": ["Increasing compute budget improves both accuracy and confidence in correct answers.", "Evaluating models with non-zero response risk is crucial for real-world applications.", "Test-time scaling can be improved by incorporating confidence thresholds."], "tldr": "Large language models (LLMs) have shown great performance in reasoning tasks, and increasing compute during testing can further improve accuracy. However, typical evaluations assume the system always answers, neglecting confidence and real-world scenarios where incorrect answers have costs. This paper addresses whether a model is confident in its answer and whether it is appropriate to always provide a response.\n\nThis paper introduces confidence scores for **selective question answering**, enabling models to abstain when uncertain. They find that increased compute not only improves accuracy but also confidence in correct answers. By evaluating models with **varying response risks**, the authors show the benefits of confidence-aware test-time scaling. The paper suggests reporting evaluations under these settings.", "affiliation": "Johns Hopkins University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Question Answering"}, "podcast_path": "2502.13962/podcast.wav"}