[{"figure_path": "https://arxiv.org/html/2501.03006/x2.png", "caption": "Figure 1: RGBA Video Generation with TransPixar. By introducing LoRA layers into DiT-based text-to-video model with a novel alpha channel adaptive attention mechanism, our method enables RGBA video generation from text while preserving Text-to-Video quality.", "description": "Figure 1 showcases the capabilities of TransPixar in generating RGBA videos from textual descriptions.  TransPixar integrates Low-Rank Adaptation (LoRA) layers into a Diffusion Transformer (DiT)-based text-to-video model.  A key innovation is the inclusion of a novel alpha channel adaptive attention mechanism which allows for the simultaneous and consistent generation of both RGB (red, green, blue) and alpha (transparency) video channels. The result is high-quality RGBA videos that maintain the visual fidelity of the original text-to-video model, enabling the seamless integration of transparent elements into generated video scenes.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2501.03006/x3.png", "caption": "Figure 2: Comparison between Generation-Then-Prediction and our Joint Generation approach. Given the generated RGB in (a), (b) and (c) show the predicted alpha (top) and the composited result (bottom). In (d), the top shows the jointly generated alpha.", "description": "This figure compares three different approaches to generating RGBA (Red, Green, Blue, Alpha) videos.  The first method (b) uses a generation-then-prediction approach. It first generates an RGB video (a), then uses a video matting technique to predict the alpha channel (top of (b)) representing transparency. The result of combining the RGB and the predicted alpha channel is shown in the bottom of (b). The second method (c) also uses a generation-then-prediction approach, but instead uses a generative model that includes a prior for transparency (MariGold or Lotus model). Its alpha channel and composite are shown in the top and bottom of (c). The third method (d) demonstrates the authors' proposed joint generation approach, where RGB and alpha channels are generated simultaneously. This produces the jointly generated alpha channel and the final composite result shown in the top and bottom of (d). The figure visually demonstrates that the joint generation approach (d) achieves better consistency and alignment between the RGB and alpha channels than the generation-then-prediction approaches (b, c).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.03006/x4.png", "caption": "Figure 3: Pipeline of TransPixar. Our method is organized as follows: (1) Left: we extend the input of DiT to include new alpha tokens; (2) Top Center: we initialize alpha tokens with our positional encoding; (3) Bottom Center: we insert a partial LoRA and adjust attention computation during training and inference.", "description": "TransPixar's architecture modifies a pre-trained Diffusion Transformer (DiT) model for generating RGBA videos.  The figure details three key steps: (1) extending the input sequence to include alpha tokens alongside the text and RGB tokens; (2) initializing these alpha tokens with a custom positional encoding; and (3) incorporating a Low-Rank Adaptation (LoRA) layer and adjusting the attention mechanism during training to ensure the model effectively learns to generate consistent RGB and alpha channels.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.03006/x5.png", "caption": "Figure 4: Positional Encoding Design for RGBA Generation. Assigning alpha tokens the same positional encoding as RGB yields similar results, resulting in faster convergence after 1000 iterations compared to standard encoding strategies.", "description": "This figure compares different positional encoding strategies for generating RGBA videos.  It shows that using the same positional encoding for both RGB and alpha channels leads to faster model convergence during training.  Specifically, after 1000 training iterations, the model trained with the shared positional encoding shows improved performance compared to models trained with other encoding strategies. This suggests that this approach helps align the RGB and alpha channels effectively, leading to more efficient and potentially higher-quality RGBA video generation.", "section": "3.2 Our Approach"}, {"figure_path": "https://arxiv.org/html/2501.03006/x6.png", "caption": "Figure 5: Attention Rectification. (a) Eliminating all attention from alpha as a key preserves 100% RGB generation but leads to poor alignment. (b) Retaining all attention significantly degrades quality, causing a lack of motion in bicycles. (c) Our method achieves an effective balance.", "description": "This figure demonstrates the impact of different attention mechanisms on the quality and alignment of generated RGBA videos.  Panel (a) shows that removing all attention from the alpha channel as a key input maintains the quality of RGB generation but results in a misalignment between the RGB and alpha channels. Panel (b) illustrates that including all possible attention paths significantly reduces the overall video quality and causes a loss of motion. Panel (c) displays the results of the proposed method that achieves a balance between preserving the original RGB video quality and generating a well-aligned alpha channel.", "section": "3.3 Analysis"}, {"figure_path": "https://arxiv.org/html/2501.03006/x7.png", "caption": "Figure 6: Applications. Top: Text-to-Video with Transparency. Bottom: Image-to-Video generation with transparency.\n.", "description": "This figure showcases the applications of TransPixar in two scenarios: text-to-video generation and image-to-video generation, both incorporating transparency. The top row displays examples of text prompts used to generate videos, demonstrating the model's capability to produce videos with transparent elements. The bottom row illustrates the generation of videos from input images, also highlighting the ability to produce transparent effects in these scenarios.", "section": "4.1. Applications"}, {"figure_path": "https://arxiv.org/html/2501.03006/x8.png", "caption": "Figure 7: Comparison with Generation-then-Prediction Pipelines. Our method demonstrates superior alignment.", "description": "Figure 7 compares the results of TransPixar with two other approaches for generating RGBA videos: a generation-then-prediction pipeline using video matting (Lotus + RGBA and SAM-2). TransPixar demonstrates better alignment between the generated RGB and alpha channels, resulting in more realistic and seamlessly integrated transparent elements.", "section": "4. Comparisons"}, {"figure_path": "https://arxiv.org/html/2501.03006/x9.png", "caption": "Figure 8: Comparison with Joint Generation Pipelines. Top: LayerDiffusion + AnimateDiff; Bottom: Ours. Our method achieves better alignment and generates corresponding motion described by prompts.", "description": "Figure 8 compares the results of two different approaches to generating RGBA videos: LayerDiffusion combined with AnimateDiff (top) and the authors' proposed TransPixar method (bottom).  The comparison highlights the superior alignment between RGB and alpha channels achieved by TransPixar, as well as its ability to more accurately generate motion consistent with the text prompts used to generate the videos.  The image shows several example videos created with each method, illustrating the differences in alpha channel quality and motion consistency.", "section": "4. Comparisons"}, {"figure_path": "https://arxiv.org/html/2501.03006/x10.png", "caption": "Figure 9: Alternative Designs for Joint Generation with DiT. Sequence extension (b) represents our method.", "description": "Figure 9 explores different architectural approaches for jointly generating RGB and alpha video channels within a Diffusion Transformer (DiT) framework.  It compares three variations: (a) batch extension, (b) sequence extension (the authors' proposed method), and (c) latent dimension extension. Each approach modifies the input to the DiT model differently to handle both RGB and alpha information, aiming to achieve consistent and high-quality joint generation.  The figure highlights how the authors' chosen sequence extension method (b) integrates alpha information by extending the input sequence length, facilitating the generation of both RGB and alpha videos simultaneously within the same DiT architecture.", "section": "3.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2501.03006/x11.png", "caption": "Figure 10: Ablation Study. (a) Ours; (b) Ours without RGB-attend-to-Alpha; (c) Ours with Text-attend-to-alpha; (d) Batch Extension Strategy; (e) Latent Dimension Extension Strategy. Our method maintains high-quality motion generation (e.g., butterflies waving their wings) while achieving good alignment.", "description": "This ablation study analyzes the impact of different design choices and attention mechanisms on the performance of the TransPixar model for generating RGBA videos.  The results demonstrate the importance of the RGB-attend-to-Alpha attention mechanism for achieving good alignment between the RGB and Alpha channels, while maintaining high quality motion generation. Removing this mechanism leads to misalignment, while adding the Text-attend-to-Alpha mechanism introduces interference and negatively impacts quality.  Alternative strategies for handling batch size and latent dimension are also evaluated, showcasing the effectiveness of the proposed sequence extension approach.  Specific examples include the generation of butterflies, where the quality of wing movement is used to illustrate successful or unsuccessful alignment between the generated RGB and Alpha components.", "section": "4.3. Ablation Study"}]