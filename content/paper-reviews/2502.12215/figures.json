[{"figure_path": "https://arxiv.org/html/2502.12215/x1.png", "caption": "Figure 1: The average length of correct solutions versus incorrect solutions evaluated on the same questions. For each question, solution lengths were averaged separately for correct and incorrect responses, then averaged across all questions.", "description": "This figure compares the average lengths of correct and incorrect solutions generated by different language models for the same set of questions.  The data is broken down by model, showing the average length of solutions that were correct versus those that were incorrect.  The comparison reveals whether longer reasoning chains (longer solutions) are more likely to lead to correct answers or if there is a difference in solution length between correct and incorrect answers. This helps illustrate whether increasing reasoning chain length during inference consistently leads to improved accuracy.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x2.png", "caption": "(a) Evaluation for Solution length.", "description": "This figure shows the average lengths of solutions categorized into five groups based on their rank (from shortest to longest) for four different models (QwQ, R1-Distill-32b, R1-Distill-14b, R1-Distill-1.5b and LIMO) across four benchmarks (MATH, AIME, Omini-MATH, and GPQA).  The x-axis represents the group number (1 being shortest, 5 being longest), while the y-axis represents the average solution length in tokens for each group.  The figure helps visualize the diversity of solution lengths generated by each model and across the different benchmarks.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x3.png", "caption": "(b) Evaluation for accuracy.", "description": "This figure shows the accuracy of different groups of solutions for four benchmark datasets (MATH, AIME, Omini-MATH, and GPQA). The solutions are categorized into five groups based on their length.  Group 1 contains the shortest solutions and Group 5 contains the longest solutions. The graph displays the accuracy for each group in each dataset.  This visualization helps to understand the relationship between solution length and accuracy for different models.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x4.png", "caption": "Figure 2: Solutions of QwQ and R1 were categorized into different groups according to their length and evaluated in terms of solution length (a) and accuracy (b). The categorization of solutions is progressed for each question independently, i.e., all groups of solutions are corresponding to the same questions.", "description": "Figure 2 shows the results of an experiment to investigate the relationship between solution length and accuracy in the QwQ and R1 models. The models generated multiple solutions for the same questions, and these solutions were categorized into five groups based on their lengths: shortest, short, medium, long, and longest. The figure contains two subfigures: (a) shows the average length of solutions in each group, demonstrating that the average length increases with each successive group; (b) shows the average accuracy of solutions in each group, illustrating that there is no clear trend between solution length and accuracy.  This indicates that longer solutions do not necessarily lead to higher accuracy.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x5.png", "caption": "(a) Max Token Limitation", "description": "This figure shows the relationship between the model's accuracy and the maximum number of tokens it is allowed to generate (Max Token Limitation).  It helps to understand whether limiting the maximum number of tokens affects the model's performance, particularly when generating long solutions, and whether increasing the token limit beyond a certain point provides diminishing returns.", "section": "4.2 Explaining Invalid Scaling: The Key Factor is the Failure of Self-Revision"}, {"figure_path": "https://arxiv.org/html/2502.12215/x6.png", "caption": "(b) Frequence of \u201cWait\u201d", "description": "The figure shows the relationship between the average number of occurrences of the word \"wait\" in a solution and the length of that solution. The word \"wait\" is an indicator of self-revision during the solution generation process. The figure demonstrates that longer solutions tend to contain more instances of self-revision. This suggests that longer reasoning chains do not always lead to better performance, as self-revisions may introduce errors or cause the model to deviate from the correct solution path.", "section": "4.2 Explaining Invalid Scaling: The Key Factor is the Failure of Self-Revision"}, {"figure_path": "https://arxiv.org/html/2502.12215/x7.png", "caption": "Figure 3: (a): The relationship between model accuracy and the generation parameter Max Token Limitation. (b): The relationship between solution length and the average number of \u201cwait\u201d occur in a solution.", "description": "Figure 3 demonstrates the impact of the maximum token limit and self-revision on model performance.  Panel (a) shows that there's an optimal token limit beyond which accuracy does not consistently improve, suggesting the models might not benefit from excessively long generations. Panel (b) illustrates a positive correlation between solution length and the frequency of 'wait' markers (indicating self-revision). This suggests that longer solutions often involve more self-correction attempts.", "section": "Explaining Invalid Scaling: The Key Factor is the Failure of Self-Revision"}, {"figure_path": "https://arxiv.org/html/2502.12215/x8.png", "caption": "(a) Acc of R1-Distill-32b, 14b and LIMO", "description": "The figure shows the accuracy of three different models (R1-Distill-32b, R1-Distill-14b, and LIMO) during a sequential revision process.  The x-axis represents the iteration number of the revision process, indicating how many times the model refined its answer. The y-axis displays the accuracy of the model's response at each iteration.  The plot illustrates how accuracy changes as the model iteratively revises its answer. This is related to the concept of 'self-revision' in large language models, where the model refines its initial response based on its own assessment.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x9.png", "caption": "(b) Acc of R1-Distill-1.5b, QwQ", "description": "This figure (4b) presents the accuracy of the R1-Distill-1.5b and QwQ models during sequential revisions.  It shows how the accuracy of these models changes as they undergo multiple self-revision iterations. The x-axis represents the number of revision iterations, and the y-axis represents the model's accuracy.  The plot allows for a comparison of the accuracy trends of these two models in response to iterative self-refinement, illustrating how their performance evolves with increasing numbers of revisions.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x10.png", "caption": "(c) Solution lengths during revisions.", "description": "This figure shows how the length of solutions changes as the model undergoes iterative revisions during the sequential scaling process.  Specifically, it visualizes the trend of solution length across multiple revision steps to illustrate whether the solution length consistently increases or shows other patterns.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x11.png", "caption": "Figure 4: (a): Accuracy of R1-Distill-32b, R1-Distill-14b and LIMO during sequential revisions. (b): Accuracy of R1-Distill-1.5b and QwQ during sequential revisions. (c) Solution length increased with the more revision steps.", "description": "This figure displays the impact of iterative refinement on model accuracy and solution length.  Subfigure (a) and (b) show accuracy changes across multiple revision steps for several large language models (LLMs), highlighting variations in how effectively these models improve their answers through iterative self-correction. Subfigure (c) demonstrates a consistent increase in solution length as the number of revision steps increases, irrespective of accuracy improvements. This suggests that longer reasoning chains are not always correlated with accuracy gains in these models.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x12.png", "caption": "Figure 5: The ratio of turning an initial correct answer to incorrect one (correct to wrong) and an initial incorrect answer to a correct one (wrong to correct) during sequential scaling.", "description": "This figure shows the results of an experiment investigating the effectiveness of self-revision during sequential scaling. It plots the ratio of times an initially correct answer was changed to an incorrect one (correct to wrong) against the ratio of initially incorrect answers that were changed to correct ones (wrong to correct). The x-axis represents the number of sequential revision steps, and the y-axis shows the ratio.  The graph displays how the model's self-correction ability changes as the process iterates. Different colored lines represent different language models (QwQ, R1-Distill-1.5b, R1-Distill-14b, R1-Distill-32b, and LIMO), allowing for a comparison of their performance.", "section": "4.2 Explaining Invalid Scaling: The Key Factor is the Failure of Self-Revision"}, {"figure_path": "https://arxiv.org/html/2502.12215/x13.png", "caption": "(a) Evaluation on Coverage.", "description": "This figure shows the relationship between the number of tokens used and the coverage achieved by different test-time scaling strategies (sequential and parallel) on the AIME benchmark. The coverage is defined as the proportion of questions where at least one of the generated solutions is correct. This figure helps illustrate the effectiveness of parallel scaling, compared to sequential scaling, in improving the model's ability to find correct answers.", "section": "5 Sequential Scaling vs. Parallel Scaling"}, {"figure_path": "https://arxiv.org/html/2502.12215/x14.png", "caption": "(b) Evaluation on Accuracy", "description": "This figure displays the accuracy of different groups of solutions categorized by their length for the QwQ and R1 models across four benchmarks (MATH, AIME, Omini-MATH, and GPQA). Each benchmark's accuracy is shown in a separate subplot, with groups of solutions (1 to 5) arranged along the x-axis. Group 1 represents the shortest solutions, while Group 5 represents the longest solutions. The y-axis represents the accuracy of the solutions in each group. The figure illustrates whether longer solutions consistently lead to better accuracy.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}, {"figure_path": "https://arxiv.org/html/2502.12215/x15.png", "caption": "Figure 6: (a): the coverage of sequential scaling and parallel scaling on AIME. (b): the accuracy of squential revision and majority vote on AIME.", "description": "Figure 6 presents a comparative analysis of sequential and parallel scaling methods on the AIME benchmark.  Subfigure (a) shows the coverage (pass@k score), illustrating the proportion of times at least one of the multiple generated solutions contains the correct answer, as a function of the number of tokens used for both sequential scaling (iterative prompting for self-revision) and parallel scaling (sampling multiple solutions). Subfigure (b) displays the accuracy of these two methods under the same conditions, indicating the percentage of times the correct answer was selected.", "section": "5 Sequential Scaling vs. Parallel Scaling"}, {"figure_path": "https://arxiv.org/html/2502.12215/x16.png", "caption": "Figure 7: Parallel-scaling performance of Majority Vote, Shortest and Shortest Majority Vote on AIME.", "description": "This figure (Figure 7) showcases the performance comparison of three parallel scaling methods: Majority Vote, Shortest, and Shortest Majority Vote, all evaluated on the AIME benchmark.  The x-axis represents the number of tokens used during parallel solution generation, and the y-axis indicates the accuracy achieved by each method. The graph displays how the accuracy of each method changes with the increase of allocated tokens. This allows for a direct comparison of the effectiveness of these three approaches in enhancing the model's reasoning capability through parallel scaling.", "section": "5 Sequential Scaling vs. Parallel Scaling"}, {"figure_path": "https://arxiv.org/html/2502.12215/x17.png", "caption": "Figure 8: The number of correct solutions and tokens distributed across groups of different lengths.", "description": "Figure 8 presents a detailed analysis of the distribution of correct solutions and their corresponding token counts across different solution length categories.  The data is visualized to show how the number of correct solutions and total tokens varies as solution length increases. This helps in understanding the relationship between solution length and accuracy, revealing whether longer solutions consistently lead to more accurate answers or if there's a different trend.", "section": "4.1 Invalid Scaling of CoT Length: Longer CoTs Do not Improve Performance"}]