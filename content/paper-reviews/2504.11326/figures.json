[{"figure_path": "https://arxiv.org/html/2504.11326/extracted/6362130/figures/m1_2.jpg", "caption": "Figure 1: Overview of the PGMR Framework. Inference and Pseudo-Label-Based Model Selection: Employing five models to conduct inference operations, and the model with optimal performance for different video contents is intelligently selected.", "description": "Figure 1 illustrates the Adaptive Pseudo-labels Guided Model Refinement Pipeline (PGMR), a key component of the BrainyBots' solution for the MOSE track.  The pipeline begins with multi-model inference, where five separate models (SAM2, TMO, Cutie, XMem, and LiVOS) independently process the video frames. Each model generates segmentation masks, tracking IDs, and confidence scores. These individual results are then fused into a comprehensive pseudo-label using a weighted voting scheme that considers both the consistency of model predictions and their associated confidence. This pseudo-label acts as a baseline for further refinement. A model recommendation mechanism dynamically selects the optimal model for each video frame, based on features extracted from the frame and the historical performance of each model. The selection is guided by the pseudo-label and aims to leverage the unique strengths of each model for optimal performance in different video content scenarios.  The resulting segmentation masks represent the final output of the PGMR pipeline.", "section": "3. MOSE Track Top Solution"}, {"figure_path": "https://arxiv.org/html/2504.11326/extracted/6362130/figures/m2.png", "caption": "Figure 2: Overview of Team DeepSegMa\u2019s method.", "description": "Team DeepSegMa's method for video object segmentation involves several key components: a transformer-based baseline model enhanced with a multi-task loss function; a targeted data augmentation strategy to handle real-world variations; a mask confidence control mechanism and temporal fusion for inference; and finally, the construction of a tailored training dataset, MOSE+, to improve model generalization.  The figure visually depicts the workflow, highlighting the integration of these components.", "section": "3.2. 2nd Team in MOSE Track: DeepSegMa"}, {"figure_path": "https://arxiv.org/html/2504.11326/x1.png", "caption": "Figure 3: Network Architecture of FVOS.", "description": "The figure shows the network architecture of FVOS (Fast Video Object Segmentation), which consists of three main components: model fine-tuning training, morphological post-processing, and multi-scale segmentation result fusion.  The architecture primarily uses Transformers for feature extraction and attention computation.  The process begins with fine-tuning a pre-trained model on the MOSE dataset.  Then morphological post-processing addresses gaps between adjacent objects by using dilation operations.  Finally, multi-scale segmentation results are fused to improve overall accuracy.", "section": "3.3. 3rd Team in MOSE Track: JIO"}, {"figure_path": "https://arxiv.org/html/2504.11326/extracted/6362130/figures/m3_3.png", "caption": "Figure 4: Test time data augmentation and multi-scale magnification operations. (a) original image. (b) clockwise by 90\u2218. (c) clockwise by 180\u2218. (d) clockwise by 270\u2218. (e) horizontal flipping. (f) multi-scale magnification.", "description": "This figure demonstrates the test-time data augmentation techniques used to enhance the robustness and generalization of the model.  The original image (a) undergoes several transformations: rotation by 90\u00b0 (b), 180\u00b0 (c), and 270\u00b0 (d); horizontal flipping (e); and multi-scale magnification (f). These augmentations help the model to better handle variations in viewpoint and scale during inference, ultimately improving its performance.", "section": "3.3 3rd Team in MOSE Track: JIO"}, {"figure_path": "https://arxiv.org/html/2504.11326/x2.png", "caption": "Figure 5: The architecture of Sa2VA\u00a0[46]. The model first encodes the input texts, visual prompts, images, and videos into token embeddings. These tokens are then processed through a large language model (LLM). The output text tokens are used to generate the \u201c[SEG]\u201d token and associated language outputs. The SAM 2 decoder receives the image and video features from the SAM 2 encoder, along with the \u201c[SEG]\u201d token, to generate corresponding image and video masks.", "description": "Figure 5 illustrates the Sa2VA model's architecture [46], which processes input text, visual prompts, images, and videos.  First, these inputs are encoded into token embeddings.  These embeddings are then fed into a large language model (LLM). The LLM processes these tokens and generates output text tokens, which are used to create a special '[SEG]' token along with associated language outputs.  These outputs, along with image and video features from the SAM 2 encoder, are sent to the SAM 2 decoder.  Finally, the SAM 2 decoder generates the corresponding image and video masks.", "section": "4. MeViS Track Top Solution"}, {"figure_path": "https://arxiv.org/html/2504.11326/x3.png", "caption": "Figure 6: Overview of ReferDINO-Plus. For each video-description pair, we input it into ReferDINO to derive the object masks Mrsubscript\ud835\udc40\ud835\udc5fM_{r}italic_M start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and the corresponding scores Srsubscript\ud835\udc46\ud835\udc5fS_{r}italic_S start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT across the frames. Then, we select the mask with the highest score as the prompt for SAM2, producing refined masks Mssubscript\ud835\udc40\ud835\udc60M_{s}italic_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT. Finally, we fuse the two series of masks through the conditional mask fusion strategy.", "description": "ReferDINO-Plus refines object segmentation in videos.  It starts by using ReferDINO to generate initial masks (Mr) and their corresponding confidence scores (Sr) for each frame given a video and its textual description. The mask with the highest score is selected and used as input to SAM2 to produce refined masks (Ms). Finally, these refined masks (Ms) and the original masks (Mr) are combined using a conditional fusion strategy to generate the final segmentation masks.", "section": "4.2. 2nd Team in MOSE Track: ReferDINO-iSEE"}]