{"references": [{"fullname_first_author": "D. Hafner", "paper_title": "Mastering diverse domains through world models", "publication_date": "2023-01-04", "reason": "This paper introduces DreamerV3, a state-of-the-art model-based reinforcement learning algorithm that serves as a strong baseline and is directly compared against in the current work."}, {"fullname_first_author": "V. Micheli", "paper_title": "Transformers are sample-efficient world models", "publication_date": "2022-09-00", "reason": "This paper introduces IRIS, a model-based reinforcement learning algorithm using transformer world models, which is heavily built upon and improved in the current work."}, {"fullname_first_author": "R. S. Sutton", "paper_title": "Reinforcement learning: An introduction", "publication_date": "2018-00-00", "reason": "This is a foundational textbook in reinforcement learning, providing the theoretical background for many concepts used in the paper."}, {"fullname_first_author": "J. Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-06", "reason": "This paper introduces the Proximal Policy Optimization (PPO) algorithm, a crucial component of both the model-free and model-based reinforcement learning agents discussed in the paper."}, {"fullname_first_author": "M. Matthews", "paper_title": "Craftax: A lightning-fast benchmark for open-ended reinforcement learning", "publication_date": "2024-02-16", "reason": "This paper introduces the Craftax benchmark, the primary environment used for evaluating the proposed methods, making it a central reference for the experimental results."}]}