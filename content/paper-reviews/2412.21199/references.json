{"references": [{"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-07", "reason": "This paper introduced HumanEval, a benchmark heavily used in the field and extended in the current work."}, {"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-06", "reason": "This paper introduced MBPP, another benchmark heavily used in the field and extended in the current work."}, {"fullname_first_author": "Alex Gu", "paper_title": "Cruxeval: A benchmark for code reasoning, understanding and execution", "publication_date": "2024-00-00", "reason": "This paper is cited as a relevant work that evaluates code reasoning, which is related to self-invoking code generation."}, {"fullname_first_author": "Terry Yue Zhuo", "paper_title": "Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions", "publication_date": "2024-06-20", "reason": "This paper introduced BigCodeBench, a benchmark that is relevant to evaluating LLMs' capabilities to handle complex problems involving multiple function calls from external libraries."}, {"fullname_first_author": "Ziyang Luo", "paper_title": "WizardCoder: Empowering code large language models with evolution-instruct", "publication_date": "2023-06-06", "reason": "This paper is cited as an example of instruction-tuning approaches, a topic discussed and compared in the current work."}]}