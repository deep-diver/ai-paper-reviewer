[{"figure_path": "https://arxiv.org/html/2503.05244/x1.png", "caption": "Figure 1: WritingBench query example with color-coded requirements. The three black-bordered categories highlight essential requirements analyzed in follow-up assessments. Red phrases correlate with gray-shaded writing support materials.", "description": "This figure shows an example of a query from the WritingBench benchmark.  The query is for a video script for a film review, written in the style of past commentary videos. The query's requirements are color-coded to indicate different categories such as Personalization, Stylistic Adjustments, Format Specifications, Content Specificity, and Length Constraints.  Three main requirement categories are highlighted with black borders, signifying their importance in the evaluation process. Red phrases indicate the additional materials provided to support the writing task.  The color-coding and materials highlight the complexity WritingBench aims to address, moving beyond simple single-sentence prompts to simulate real-world writing scenarios.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.05244/x2.png", "caption": "Figure 2: Construction pipeline of WritingBench queries. The refinement pool contains five writing requirements (three core competencies with black borders) and one expression type (purple). Checked strategies refine initial queries into multi-requirement prompts (color-coded text) with red phrases referencing gray materials. Implementation details in Section\u00a03.1.", "description": "This figure illustrates the four-stage query construction pipeline used to create WritingBench queries.  It starts with initial query generation using LLMs, then uses a refinement pool containing five writing requirements (three core competencies\u2014personalization, stylistic adjustments, and format specifications\u2014highlighted with black borders) and an expression type (purple).  Checked strategies, refining the initial queries, are applied to produce multi-requirement prompts (color-coded text), with red phrases referencing gray-shaded writing support materials provided in the initial query.  The process ensures diverse writing tasks covering various domains and includes integrating heterogeneous sources of materials. The implementation details are described in Section 3.1 of the paper.", "section": "3.1 Benchmark Construction"}, {"figure_path": "https://arxiv.org/html/2503.05244/x3.png", "caption": "Figure 3: Domain categories in WritingBench. Inner represents the 6 primary domains and outer depicts 100 secondary subdomains (Sub = subdomains per category, Num = dataset entries).", "description": "This figure shows a donut chart illustrating the distribution of WritingBench's 1239 queries across six primary domains and their corresponding 100 secondary subdomains. Each primary domain is represented by a segment of the donut chart and its size corresponds to the number of queries in that domain. The secondary subdomains are further broken down within each primary domain, and the number of queries (Num) and the number of subdomains (Sub) are indicated for each primary domain.  The chart visually represents the breadth and depth of WritingBench's coverage of various writing tasks.", "section": "3 WritingBench"}, {"figure_path": "https://arxiv.org/html/2503.05244/x4.png", "caption": "Figure 4: Example of dynamically generating criteria for a writing query in WritingBench. Different background colors represent various types of requirements.", "description": "This figure showcases the dynamic criteria generation process within WritingBench. A writing query is provided as input, and the system automatically generates five evaluation criteria, each with a detailed description and a 10-point scoring rubric. The diverse background colors highlight the different types of requirements (e.g., formatting, style, content). This illustrates how WritingBench adapts its evaluation to each writing task, providing a more nuanced and comprehensive assessment compared to traditional static evaluation methods.", "section": "3.2 Query-Dependent Evaluation Framework"}, {"figure_path": "https://arxiv.org/html/2503.05244/x5.png", "caption": "Figure 5: Scores of different models across different subdomains in WritingBench. Red indicates higher score and blue refers to lower score. The figures are the average score of each subdomain for different models.", "description": "Figure 5 is a heatmap visualization showing the performance of various LLMs (large language models) across 100 subdomains within six primary domains of the WritingBench benchmark. Each subdomain represents a specific type of writing task (e.g., writing a scientific paper, a legal document, a poem etc.). The color intensity represents the average score achieved by each model on each subdomain, with red indicating higher scores and blue representing lower scores.  This figure allows for a detailed comparison of the strengths and weaknesses of different LLMs in various writing scenarios.", "section": "3 WritingBench"}, {"figure_path": "https://arxiv.org/html/2503.05244/x6.png", "caption": "Figure 6: Scores of different models across various input lengths on the WritingBench.", "description": "This figure presents a performance comparison of various large language models (LLMs) on the WritingBench benchmark across different input lengths.  The x-axis represents the range of input lengths (in tokens), and the y-axis shows the corresponding average scores achieved by each LLM.  The different colored lines represent different LLMs, allowing for a visual comparison of how their performance varies with input length. This illustrates the impact of input length on the ability of each model to generate high-quality writing.", "section": "4 Experiment"}, {"figure_path": "https://arxiv.org/html/2503.05244/x7.png", "caption": "Figure 7: Scores of different models across various output lengths on the WritingBench.", "description": "This figure shows the performance of various LLMs (large language models) on the WritingBench benchmark across different output lengths.  The x-axis represents output length in tokens, and the y-axis represents the average score achieved by each model.  Each line represents a different LLM, illustrating how well each model performs at generating text of varying lengths. The plot helps to identify strengths and weaknesses of the LLMs in producing longer or shorter responses, highlighting models better suited for generating longer-form content. The scores likely reflect a composite of quality metrics such as fluency, coherence, and relevance.", "section": "4.5 Ablation of Length"}]