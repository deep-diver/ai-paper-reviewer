{"references": [{"fullname_first_author": "Jiahui Yu", "paper_title": "CoCa: Contrastive Captioners are Image-Text Foundation Models", "publication_date": "2022-05-01", "reason": "This paper introduces the CoCa framework, which is foundational to the approach of the current paper, inspiring the use of contrastive learning for vision-language tasks."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-03-01", "reason": "This paper introduces CLIP, the contrastive vision-language pre-training model used as a backbone in the current paper, providing strong visual and linguistic representations."}, {"fullname_first_author": "DaveZhenyu Chen", "paper_title": "Scan2Cap: Context-aware Dense Captioning in RGB-D Scans", "publication_date": "2021-01-01", "reason": "As a seminal work in 3D captioning, it establishes the task of context-aware dense captioning in RGB-D scans and serves as a baseline for comparison in the current paper."}, {"fullname_first_author": "Dave Zhenyu Chen", "paper_title": "ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language", "publication_date": "2020-01-01", "reason": "It presents the ScanRefer dataset, a key benchmark used in the current paper to evaluate 3D object localization and captioning performance."}, {"fullname_first_author": "Sijin Chen", "paper_title": "End-to-End 3D Dense Captioning with Vote2Cap-DETR", "publication_date": "2023-01-01", "reason": "As a strong baseline method for 3D dense captioning, this paper proposes an end-to-end approach using Vote2Cap-DETR, which is compared against the proposed 3D CoCa."}]}