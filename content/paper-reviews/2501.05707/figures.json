[{"figure_path": "https://arxiv.org/html/2501.05707/x1.png", "caption": "Figure 1: Multiagent finetuning improves reasoning performance over multiple rounds of finetuning.\nOur multiagent finetuning procedure enables models to improve across multiple iterations of finetuing. Results reported on the MATH dataset.", "description": "The figure displays the performance of single-agent and multi-agent fine-tuning on the MATH reasoning dataset across multiple rounds of fine-tuning.  It showcases how multi-agent fine-tuning consistently improves the model's performance with each round, unlike single-agent fine-tuning, which plateaus or even declines after a certain number of rounds.  This demonstrates the advantage of the proposed multi-agent approach for iterative model improvement.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05707/x2.png", "caption": "Figure 2: Overview of Multiagent Finetuning.We first use multiagent debate and majority voting to create the finetuning datasets (left). These datasets are then used to finetune the generation and critic agents (right). When finetuning generation models, we use the majority voted result (\u201dcorrect\u201d output) to select first-round responses from each agent. We then finetune critic models using responses from the final round based on whether responses match the majority voted result (mix of \u201dcorrect and incorrect\u201d outputs). The finetuned models are combined through multiagent debate to generate more accurate answers. In this figure, we illustrate a single finetuning iteration. Applying multiple rounds of finetuning iterations can significantly boost performance.", "description": "This figure illustrates the multiagent finetuning process.  First, a multiagent debate is conducted, generating multiple responses to an input. Majority voting determines the 'correct' answer.  The left side shows this debate and voting process, generating the finetuning datasets. Then, these datasets are used to finetune two sets of models: generation models (trained on 'correct' first-round responses) and critic models (trained on a mix of 'correct' and 'incorrect' final-round responses).  The right side displays this finetuning process. Finally, the finetuned generation and critic models are combined via another multiagent debate to produce a more accurate final answer. The figure depicts a single finetuning iteration; repeating this iterative process multiple times significantly improves performance.", "section": "Multiagent Finetuning of Language Models"}, {"figure_path": "https://arxiv.org/html/2501.05707/x3.png", "caption": "Figure 3: Diversity is preserved and can improve across iterations of finetuning. We measure the response diversity of our method and the single-agent finetuning method on the MATH dataset using two diversity measures. The diversity of our method remains consistent over finetuning iterations for one metric and improves for another metric, whereas the diversity of the single-agent method drops significantly.", "description": "This figure displays the results of two diversity metrics (likelihood and embedding dissimilarity) across multiple iterations of finetuning using both a multi-agent and a single-agent approach.  The y-axis represents the diversity metric values, and the x-axis shows the number of finetuning iterations.  The multi-agent method demonstrates consistent or improved diversity across the iterations. In contrast, the single-agent method shows a significant decrease in diversity as finetuning progresses. This visualization highlights the benefits of the multi-agent approach in maintaining or even enhancing diversity during the self-improvement process, which is crucial for avoiding performance plateaus.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05707/x4.png", "caption": "Figure 4: Relationship between accuracy and diversity. We visualize the relationship between embedding dissimilarity and MATH accuracy across rounds of finetuning. Our multiagent finetuning preserves diversity across rounds of finetuning while improving accuracy.", "description": "This figure examines the correlation between model accuracy on the MATH dataset and the diversity of generated responses across multiple rounds of finetuning.  The x-axis represents the embedding dissimilarity, a measure of how different the generated responses are from each other. A higher value indicates greater diversity. The y-axis shows the accuracy of the model on the MATH dataset. The plot shows two lines: one for the multi-agent finetuning method proposed in the paper, and one for a single-agent finetuning baseline.  The multi-agent line demonstrates that as the number of finetuning rounds increases, the diversity of responses remains relatively stable while the accuracy steadily increases. In contrast, the single-agent line shows that accuracy plateaus while the diversity of responses decreases. This suggests that the multi-agent finetuning method is more effective at maintaining response diversity over multiple rounds, which potentially contributes to its improved accuracy.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05707/x5.png", "caption": "Figure 5: Zero-shot generalization of the proposed method. Our method demonstrates zero-shot generalization capabilities. When trained on the MATH dataset, it can effectively generalize to the GSM dataset. It outperforms all the baselines that are trained on the GSM dataset.", "description": "Figure 5 illustrates the zero-shot generalization capabilities of the proposed multiagent finetuning method.  The model was trained exclusively on the MATH dataset (mathematics problems), and then evaluated on the GSM dataset (grade school math word problems), a dataset it had never encountered during training. Despite this, the multiagent finetuned model significantly outperformed all baseline models which *were* trained on the GSM dataset. This demonstrates the model's ability to transfer knowledge acquired from one task to a related but distinct task without any specific training on the target dataset.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.05707/x6.png", "caption": "Figure 6: Consensus: Response diversity across finetuning iterations. We measure the response diversity based on agent consensus of our method and the single-agent finetuning method on the MATH dataset. The diversity of our method remains consistent over finetuning iterations, whereas the diversity of the single-agent method drops significantly.", "description": "This figure displays the response diversity across multiple rounds of fine-tuning for both multi-agent and single-agent methods. Response diversity is measured using agent consensus, which is calculated as the proportion of agents that provide the same final answer for each problem.  The graph shows that for the multi-agent finetuning method, the response diversity remains relatively stable across multiple iterations. In contrast, the single-agent method exhibits a significant drop in response diversity as the number of fine-tuning iterations increases. This indicates that the single-agent approach loses its ability to generate diverse responses after a certain number of fine-tuning cycles.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05707/x7.png", "caption": "Figure 7: KL-Divergence: Response diversity across finetuning iterations. We measure diversity based on the KL-divergence between the probabilities of the output tokens between agents. Similar to our likelihood measurement, we find that diversity is preserved across rounds of finetuning.", "description": "Figure 7 presents a graph showing how response diversity changes across multiple iterations of finetuning.  The diversity metric used is KL-divergence, which quantifies the difference in the probability distributions of output tokens generated by different agents within a multi-agent system.  The results indicate that response diversity, as measured by KL-divergence, remains relatively stable or even increases across multiple rounds of finetuning, suggesting that the model's ability to generate diverse responses is not diminished by repeated training.", "section": "3. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.05707/x8.png", "caption": "Figure 8: KL Diversity between finetuned and unfinetuned LLM. We measure the KL-divergence between likelihoods of responses from finetuned agents and base LLM agents for single-agent finetuning and generation/critic agents from multiagent finetuning. Likelihoods are calculated using Gemma-2 (2B). We find that our method diverges from the base LLM probabilities and furthermore, critic agents have better divergence in responses and our method has better diversity metrics than single-agent FT.", "description": "This figure shows the KL divergence between the probability distributions of responses generated by finetuned language models and an unfinetuned base model.  The KL divergence measures how different the probability distributions are.  The figure compares single-agent finetuning with multi-agent finetuning, specifically highlighting the performance of generation and critic agents in the multi-agent approach.  The results show that the multi-agent finetuning method leads to a larger KL divergence compared to single-agent finetuning, indicating that the multi-agent models generate more diverse responses.  Furthermore, critic agents in the multi-agent setup show even greater divergence, suggesting that the specialized roles of the agents contribute to response diversity.", "section": "C DIVERSITY METRICS"}, {"figure_path": "https://arxiv.org/html/2501.05707/x9.png", "caption": "Figure 9: Embedding Dissimilarity: Response diversity across finetuning iterations. We measure the response diversity based on the embedding dissimilarity between the responses of different agents, where embeddings are computed using the T5-3B encoder. We notice that similar to likelihood measurement, we find that diversity is preserved across rounds of finetuning.", "description": "Figure 9 displays the results of measuring response diversity using embedding dissimilarity across multiple rounds of finetuning.  Two different language models (Phi-3 and Mistral) are analyzed.  Embeddings for responses generated by different agents within the multi-agent system are calculated using the T5-3B encoder.  The dissimilarity between these embeddings is then computed and averaged across all agent pairs, providing a quantitative measure of diversity.  The key observation is that the diversity, as measured by this embedding dissimilarity, remains relatively stable across multiple rounds of finetuning for both language models. This contrasts with other results in the paper where single-agent approaches showed a decrease in diversity over the finetuning iterations, highlighting the benefit of the proposed multi-agent system.", "section": "4.2 Agent Response Diversity"}, {"figure_path": "https://arxiv.org/html/2501.05707/x10.png", "caption": "Figure 10: Inducing diversity through increasing temperature. We introduce an additional baseline where we apply the Single-Agent FT baselin with a temperature of 2. By increasing the sampling temperature, we allow the model to generate more diverse responses. We observe that our method out-performs higher temperature settings, which demonstrates that temperature does not increase diversity in a way that is useful for accuracy.", "description": "This figure compares the performance of the proposed multiagent finetuning method with a single-agent finetuning baseline.  The single-agent method is tested with two different sampling temperatures (1.0 and 2.0).  Increasing the temperature in the single-agent approach is a common technique to increase the diversity of generated responses. The results show that the multiagent approach significantly outperforms the single-agent method at both temperature settings, demonstrating that simply increasing response diversity is not sufficient to achieve high accuracy. The improvement of the multiagent method is consistent across multiple iterations of finetuning, while the single-agent methods show less consistent gains.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05707/x11.png", "caption": "Figure 11: Multiple iterations of finetuning over all levels of MATH. We apply multiple iterations of finetuning over 500 examples of MATH sampled from all levels. Even over a more difficult domain, we see significant improvements from multiagent finetuning that continue to self-improve.", "description": "This figure shows how the multiagent finetuning method continues to improve performance on the MATH dataset even after multiple rounds of finetuning.  The x-axis represents the number of finetuning iterations, and the y-axis shows the accuracy achieved on the MATH dataset.  The accuracy of the multiagent finetuning method steadily increases with each iteration, while the single-agent finetuning method plateaus after a few iterations. This demonstrates that the multiagent approach is more effective at continuous self-improvement, especially on more challenging tasks like those in the MATH dataset which includes problems from all difficulty levels.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.05707/x12.png", "caption": "Figure 12: Testing zero-shot generalization across 1000 GSM problems We test the zero-shot capabilities of our method using models trained on the MATH dataset. We find that over 1000 problems of GSM, our method performs better than all baselines.", "description": "This figure displays the results of a zero-shot generalization experiment.  Models were trained on the MATH dataset (a challenging math word problem dataset) and then evaluated on the GSM dataset (a grade-school math dataset), without any prior exposure to GSM problems. The bar chart compares the accuracy of the proposed multi-agent finetuning method against several baseline methods: Base, Majority, Debate, STaR, and Majority FT.  The results clearly show that the multi-agent finetuning method significantly outperforms all other baselines, achieving the highest accuracy on unseen GSM problems. This demonstrates the strong generalization capability of the proposed model, even when transferring knowledge between distinct but related math problem datasets.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.05707/x13.png", "caption": "Figure 13: Zero-shot generalization after arithmetic finetuning. We evaluate the ability of our method to generalize after finetuning Mistral on the arithmetic task and evaluating on GSM. We find that this aids in GSM performance, even more than finetuning with MATH.", "description": "This figure demonstrates the zero-shot generalization capabilities of the multiagent finetuning method.  The model was fine-tuned on a dataset of arithmetic problems, then evaluated on the GSM dataset (a different task). The results show that the performance on GSM is even better than when the model was fine-tuned directly on the MATH dataset. This highlights the model's ability to transfer knowledge learned from one task to another.", "section": "3 EXPERIMENTS"}]