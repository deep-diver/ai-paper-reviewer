{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces the foundation for denoising diffusion probabilistic models, a key technique used in the proposed LatentLM model for autoregressive generation of continuous data."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-06-01", "reason": "This paper is highly relevant as it details a key component of LatentLM, utilizing transformers for high-resolution image synthesis, showcasing the effectiveness of transformers in multimodal data generation."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Zero-shot text-to-image generation", "publication_date": "2021-02-12", "reason": "This paper is important as it demonstrates zero-shot text-to-image generation which showcases the potential of a unified multimodal generation model akin to LatentLM."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This work is crucial because it details the training of transferable visual models from natural language, providing insight into the multimodal learning that LatentLM aims to achieve."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper is significant as it introduces the Llama model, which serves as the foundation for the causal transformer architecture implemented in LatentLM, a core component for multimodal understanding and generation."}]}