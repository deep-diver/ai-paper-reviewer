{"importance": "This paper is important for researchers by **addressing limitations in VDC models related to fine-grained alignment and human preference**. The Cockatiel framework provides a new approach and opens avenues for **improving video understanding and generation tasks**.", "summary": "Cockatiel: Ensembling synthetic & human-preferred training boosts detailed video captioning, setting new SOTA on VDCSCORE.", "takeaways": ["Existing VDC models suffer from imbalanced video-caption alignment and misalignment with human preferences.", "Cockatiel, a new framework, ensembles synthetic and human-aligned training to improve VDC.", "The method achieves state-of-the-art performance on VDCSCORE with better human preference alignment."], "tldr": "Video Detailed Captioning (VDC) is important for vision-language tasks but faces challenges in fine-grained alignment and human preference. Current models struggle to provide comprehensive captions, often favoring specific aspects and failing to align with human preferences. Existing VDC models are trained on synthetic captions, lacking real-world guidance, creating a crucial need for improved methods.\n\nTo fix the above issues, this paper introduces Cockatiel, a novel three-stage training pipeline that ensembles synthetic and human-aligned training data. First, a human-aligned caption quality scorer is used to select high-performing synthetic captions. Cockatiel-13B is then trained on this curated data, and finally, Cockatiel-8B is distilled for easier use. **This approach achieves new state-of-the-art results on VDCSCORE**, demonstrating improved dimension-balanced performance and better alignment with human preferences.", "affiliation": "Shanghai Academy of Artificial Intelligence for Science", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.09279/podcast.wav"}