[{"content": "| Captioning Methods | 3DVAE<sub>*score*</sub>\u2193 | CLIP<sub>*SenbySen*</sub>\u2191 | Avg. Length |\n|---|---|---|---| \n| Panda-70M | 140.25 | 0.1956 | 13 words |\n| ShareGPT4Video | 141.00 | 0.2132 | 191 words |\n| LLaVA-Video-72B | 139.88 | 0.2060 | 102 words |\n| MiraData(GPT-4o) | **137.50** | **0.2156** | 263 words |\n| **InstanceCap**(Ours) | **134.25** | **0.2133** | 157 words |", "caption": "Table 1: Quantitative comparisons on reconstruction-via-recaption results. The best results are marked in bold, and the second-best are underscored.\nAs a reference, CogVideoX-5b accepts 226226226226 text tokens, with any excess being truncated.", "description": "This table presents a quantitative comparison of various video captioning methods, including Panda-70M, ShareGPT4Video, LLaVA-Video-72B, MiraData (GPT-40), and the proposed InstanceCap, based on their performance in video reconstruction tasks.  The metrics used for evaluation include 3DVAEscore, CLIP SenbySen, and average caption length. The table highlights the best and second-best performing methods for each metric.  Additionally, it notes that CogVideoX-5b, the video generation model used in the evaluation, has a maximum input token limit of 226, which means that longer captions were truncated. This information helps contextualize the performance results of the different captioning methods, especially those generating longer captions.", "section": "4. Experimental setup"}, {"content": "| T2V Model | Single\u2191 | | | | | Multiple\u2191 | | | Average\u2191 |\n|---|---|---|---|---|---|---|---|---|---| \n| | Action | Color | Shape | Texture | Detail | Action | Color | Texture | | \n| CogVideoX-5B [30] | 64% | 60% | 44% | 60% | 20% | 8% | 48% | 40% | 43.00% | \n| Pyramid-Flow-2B [8] | 44% | 68% | 32% | 32% | 7% | 4% | 24% | 16% | 28.38% | \n| Open-Sora Plan v1.3-2.7B [11] | 64% | 44% | 36% | 32% | 27% | 20% | 32% | 12% | 33.38% | \n| Open-Sora v1.2-1.1B [35] | 40% | **56%** | **36%** | **40%** | 13% | 12% | 16% | 16% | 28.63% | \n| + InstanceCap (Ours) | **56%** | **60%** | **40%** | **48%** | **27%** | **16%** | **32%** | **24%** | **37.88%** | \n| + Panda-captioner [4] | 40% | 48% | 28% | 40% | 20% | 8% | 20% | 12% | 27.00% | \n| + ShareGPT4Video [3] | 40% | 44% | 32% | 24% | 13% | **16%** | 8% | **20%** | 24.63% | \n| + LLaVA [16] | **52%** | 52% | 28% | 28% | **20%** | 12% | **28%** | 16% | **29.50%** |", "caption": "Table 2: Quantitative comparison between\u00a0\ud835\ude78\ud835\ude97\ud835\ude9c\ud835\ude9d\ud835\ude8a\ud835\ude97\ud835\ude8c\ud835\ude8e\ud835\ude72\ud835\ude8a\ud835\ude99\ud835\ude78\ud835\ude97\ud835\ude9c\ud835\ude9d\ud835\ude8a\ud835\ude97\ud835\ude8c\ud835\ude8e\ud835\ude72\ud835\ude8a\ud835\ude99\\mathtt{InstanceCap}typewriter_InstanceCap\u00a0and SOTA video captioning models, all based on the popular T2V model Open-Sora. Additionally, we also compare three powerful T2V models, including CogVideoX-5B, Pyramid-Flow, and Open-Sora Plan. The best results of video captioning methods and Open-Sora are marked in bold, and the second-best are underscored.", "description": "This table presents a quantitative comparison of InstanceCap and other state-of-the-art video captioning models.  All methods are evaluated based on their performance on the popular open-source T2V model, Open-Sora [35]. The table also includes results for CogVideoX-5B, Pyramid-Flow, and Open-Sora Plan, which are three other high-performing T2V generation models.  The comparison focuses on video generation quality achieved when these models are used with different captioning methods. The metrics used for evaluating video generation quality are based on instance level attributes including 'Single Action', 'Color', 'Shape', 'Texture', 'Detail' as well as 'Multiple Action', 'Color', 'Texture' for single-object and multi-object scenarios respectively.  The highest scores achieved by the captioning models and by Open-Sora across all metrics are highlighted in bold. The second-highest score is underscored.", "section": "4.3. Text-to-video generation"}, {"content": "| Distortion type | 3DVAE score\u2193 | Setting | \n|---|---|---| \n| Blurring | 7.71 | GaussianBlur(kernel=(5, 5), sigma=0) | \n| Compression artifacts | 11.19 | JPEG compression (quality 5-30) | \n| Corruptions | 39.80 | Random pixel masking (binary mask) | \n| Random noise | 49.70 | Gaussian noise (mean=0, stddev=25) | \n| Brightness distortion | 63.25 | Scaling (factor 0.5-1.5) | \n| Spatial shifts | 78.94 | Random affine shifts (\u00b110 pixels) | \n| T2V models Avg. | 134 ~ 145 | - | \n| Broken video | 149.50 | - |", "caption": "Table S1: 3DVAE scores for various distortions and video models, showcasing its effectiveness in capturing perceptual similarities and reconstruction accuracy. The setting column provides details of the experimental setup for each distortion type.", "description": "This table presents 3DVAE scores, a metric evaluating video reconstruction quality by calculating the distance between latent representations of original and reconstructed videos. It showcases the effectiveness of this metric by applying it to videos distorted in various ways (blurring, compression, etc.) and comparing it to typical scores from T2V models. The \"Setting\" column provides specific parameters for each distortion type.", "section": "5. Evaluation metrics for video reconstruction"}, {"content": "| Instance Detail | Instance Detail | Hallucination Scores | Hallucination Scores |\n|---|---|---|---| \n| **1** | Descriptions are extremely vague, imprecise, or largely inaccurate. Almost no specific details from the video are captured correctly. | **1** | Severe hallucination - Describes many nonexistent details, significantly misrepresents what is shown, or introduces extensive irrelevant content with many unrelated topics or external information. |\n| **2** | Descriptions have major inaccuracies or omit many important details. Only a few basic elements are described correctly. | **2** | Frequent hallucination - Multiple instances of fabricated or misrepresented details and significant extra content introducing information beyond the video scope. |\n| **3** | Descriptions are moderately accurate but lack precision in some areas. Core details are present but some secondary details are missing or incorrect. | **3** | Occasional hallucination - A few minor instances of fabricated details, misrepresentations, or the addition of extra content not covered in the video. |\n| **4** | Descriptions are largely accurate and detailed. Most key elements and nuances from the video are captured correctly, with only minor omissions or imprecisions. | **4** | Minimal hallucination - One or two very minor discrepancies or limited introduction of external information. |\n| **5** | Descriptions are highly precise and comprehensive. All important details from the video are captured accurately, including subtle elements and specific examples. | **5** | No hallucination - All described details accurately reflect what is shown in the video, with no external content added. |", "caption": "Table S2: This table outlines scoring criteria for Instance Detail and Hallucination Scores, integrating intrinsic and extrinsic hallucinations into a unified framework for evaluation.", "description": "This table provides a detailed breakdown of the scoring criteria employed for evaluating both instance detail and hallucination aspects, combining intrinsic and extrinsic hallucinations within a unified evaluation framework. The \"Instance Detail\" section assesses the accuracy and completeness of descriptions, ranging from extremely vague to highly precise. The \"Hallucination Scores\" section evaluates the presence and severity of hallucinated details, categorizing them from severe (many fabricated details) to minimal (very few discrepancies).", "section": "6. Inseval"}]