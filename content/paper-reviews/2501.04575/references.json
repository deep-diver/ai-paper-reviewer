{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-11-28", "reason": "This paper introduces Flamingo, a visual language model that significantly advanced the potential of GUI agents by enabling powerful visual understanding capabilities and reasoning based on visual information."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduced the CLIP model, a significant foundation for multimodal large language models (MLLMs) that are crucial for GUI agents' visual understanding and grounding capabilities."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "publication_date": "2023-08-01", "reason": "This paper introduced Qwen-VL, a high-performing MLLM used in the InfiGUIAgent, facilitating the agent's enhanced GUI understanding and reasoning."}, {"fullname_first_author": "Boyu Gou", "paper_title": "Navigating the digital world as humans do: Universal visual grounding for GUI agents", "publication_date": "2024-10-01", "reason": "This paper proposed UGround, a strong baseline model for GUI grounding that is compared against InfiGUIAgent's performance, highlighting the impact of InfiGUIAgent's advanced reasoning."}, {"fullname_first_author": "Kevin Qinghong Lin", "paper_title": "ShowUI: One vision-language-action model for generalist GUI agent", "publication_date": "2024-12-01", "reason": "This paper introduced ShowUI, another strong baseline model for GUI interaction that is compared to InfiGUIAgent, showcasing InfiGUIAgent's superior accuracy."}]}