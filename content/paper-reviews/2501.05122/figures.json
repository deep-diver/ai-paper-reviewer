[{"figure_path": "https://arxiv.org/html/2501.05122/x1.png", "caption": "Figure 1: \nExploring drivers of multilingual ability:\n(1) Languages in the training data; (2) Distribution of languages in the training data;\n(3) Incorporating multilingual OCR samples to understand non-English text in images.", "description": "This figure illustrates the key factors investigated in the paper to understand multilingual capabilities in large vision-language models (LVLMs).  It's broken down into three parts:\n\n(1) **Training Data Languages:** Shows a tiered structure of languages included in the training data, categorized from high to low-resource languages. This helps to visualize the different language mixes used in experiments.\n\n(2) **Language Data Distribution:**  Illustrates different proportions of English versus multilingual data in the training.  It demonstrates how altering this ratio affects the model\u2019s performance.\n\n(3) **Multilingual Text in Images:** Presents examples of how multilingual text is incorporated into images in the training data. This element specifically focuses on improving the model's ability to understand OCR data from various languages.\n\nThe figure is designed to show the various ways in which language is introduced in the data for LVLMs, aiming to improve multilingual performance.", "section": "2 Drivers of Multilingual Ability"}, {"figure_path": "https://arxiv.org/html/2501.05122/extracted/6121021/img/smpqa/bar_plot_1_en.png", "caption": "Figure 2: Prompts used for the different datasets of our test suite. For M3Exam and xMMMU, the questions contain images at individual positions, and also the options can consist of images. In total, a sample of M3Exam can contain up to 8 images and 8 options, and a sample of xMMMU can contain up to 4 images and 4 options.", "description": "This figure details the prompts used for each dataset in the paper's evaluation suite.  It highlights the diversity of input types across different tasks. For instance, some tasks involve single images, while others require multiple images as inputs.  Furthermore, the options within certain multiple-choice questions (e.g., M3Exam and xMMMU) can also include images.  The figure provides a concise visualization of the variations in question design and the complexity of the visual and textual elements required for each task.", "section": "2.1 Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2501.05122/extracted/6121021/img/smpqa/bar_plot_1_id.png", "caption": "(a) Example of a bar plot in SMPQA for English. \nQuestions for Grounding:\n\"Is the bar with label \u2019reward\u2019 the biggest?\",\n\"Is the bar with label \u2019incredible\u2019 the biggest?\",\n\"Is the bar with label \u2019reverse\u2019 the smallest?\",\n\"Is the bar with label \u2019sunset\u2019 the smallest?\",\n\"Is the bar with label \u2019closed\u2019 colored in yellow?\",\n\"Is the bar with label \u2019closed\u2019 colored in purple?\",\n\"Is the bar with label \u2019twitter\u2019 colored in purple?\",\n\"Is the bar with label \u2019twitter\u2019 colored in red?\" \nQuestions for Reading:\n\"What is the label of the biggest bar?\",\n\"What is the label of the smallest bar?\",\n\"What is the label of the yellow bar?\",\n\"What is the label of the red bar?\",\n\"What is the label of the purple bar?\"", "description": "The figure shows two types of questions for a bar chart. Grounding questions verify the understanding of the chart's structure, for example, identifying the tallest bar or the color of a specific bar. Reading questions test the ability to extract information from the chart, such as identifying the label of the tallest bar or the color of a specific bar.  The example uses an English bar chart, but the paper states that the SMPQA dataset includes various languages and scripts.", "section": "RQ4: Improving on Multilingual Text-in-Image Tasks"}]