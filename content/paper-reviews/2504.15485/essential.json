{"importance": "This paper is important because it exposes **weaknesses in VLMs' spatial reasoning under occlusion**, highlighting the need for improved world modeling capabilities. This work opens new avenues for future research to enhance VLMs' ability to reason about partially observed scenes.", "summary": "CAPTURE: VLMs struggle with spatial reasoning in occluded environments; a novel benchmark exposes their limitations.", "takeaways": ["VLMs struggle with spatial reasoning and object counting when objects are occluded, performing worse than humans.", "Providing VLMs with auxiliary information (object coordinates or inpainted images) significantly improves performance, suggesting limitations in visual integration and world modeling.", "Current VLMs lack the robust spatial understanding and reasoning abilities necessary for effectively handling occlusions in complex visual scenes."], "tldr": "Visual reasoning about occluded objects is crucial, but current Vision-Language Models (VLMs) struggle with it. Occlusions, where objects are partially or fully hidden, challenge spatial comprehension. To address this, the researchers introduce **Counting Amodally for Patterns Through Unseen REgions (CAPTURE)**, a novel benchmark. CAPTURE tests a model's ability to count objects arranged in a pattern by inferring how the pattern continues behind an occluder. This requires both recognizing visual patterns and spatial reasoning. The study evaluates four strong VLMs (GPT-4o, Intern-VL2, Molmo, and Qwen2-VL) on CAPTURE. \n\nThe results show that VLMs struggle to count objects in patterns, especially when occluded. Crucially, models perform worse with occlusion, indicating a deficiency in inferring unseen spatial relationships. Even the strongest VLMs fail to count accurately with occlusion. Humans, in contrast, exhibit very little error on CAPTURE. The study finds that providing auxiliary information about occluded object locations increases performance, underscoring that the model error stems from an inability to handle occlusion and difficulty counting in images. This benchmark highlights the need for improved world modeling capabilities in VLMs.", "affiliation": "UNC Chapel Hill", "categories": {"main_category": "Computer Vision", "sub_category": "Visual Question Answering"}, "podcast_path": "2504.15485/podcast.wav"}