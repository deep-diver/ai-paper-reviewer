[{"figure_path": "https://arxiv.org/html/2412.21139/x1.png", "caption": "Figure 1: SWE-Gym enables scalable improvements for software engineering agents.\nTop: Training time scaling shows consistent performance improvements as we obtain more training trajectories, with no signs of saturation at 491 trajectories. We use temperature t=0\ud835\udc610t=0italic_t = 0.\nBottom: For inference time scaling, we generate a number of candidate trajectories per task and select the best using a verifier trained on SWE-Gym. This approach demonstrates roughly logarithmic gains with the number of sampled solutions.\nt=0\ud835\udc610t=0italic_t = 0 (excluded from regression) is used as the first hypothesis to be consistent with the top figure; later rollouts use t=0.5\ud835\udc610.5t=0.5italic_t = 0.5.", "description": "This figure demonstrates the scalability of improvements achieved using SWE-Gym for software engineering agents. The top panel shows training time scaling, illustrating consistent performance gains with an increasing number of training trajectories, even up to 491 trajectories without any sign of saturation.  A temperature of 0 was used during training. The bottom panel displays inference time scaling, showcasing approximately logarithmic performance increases as the number of candidate trajectories per task increases. A verifier, trained using SWE-Gym, selects the best trajectory.  While a temperature of 0 is used for the initial hypothesis to maintain consistency with the top panel, subsequent rollouts utilize a temperature of 0.5.", "section": "4. Scaling Agent Improvements with SWE-Gym"}, {"figure_path": "https://arxiv.org/html/2412.21139/x2.png", "caption": "Figure 2: Repository distribution of SWE-Gym instances.", "description": "This figure shows the distribution of the 2,438 Python software engineering tasks in SWE-Gym across 11 different open-source repositories.  The size of each repository's representation in the chart visually corresponds to the number of tasks sourced from it.  This illustrates the diversity of projects represented within SWE-Gym and highlights the prevalence of certain repositories (such as pandas) compared to others.", "section": "2. SWE-Gym Environment"}, {"figure_path": "https://arxiv.org/html/2412.21139/x3.png", "caption": "Figure 3: Success distribution over 30 rounds on SWE-Gym Lite with 7B model in zero-shot. The distribution is naturally biased toward easy tasks. Per instance capping reduces this bias but lowers the total trajectory count for training. We set temperature t=1\ud835\udc611t=1italic_t = 1 during sampling.", "description": "This figure shows the distribution of successful trajectories over 30 rounds of sampling for a 7B model on the SWE-Gym Lite dataset.  The zero-shot setting reveals a long-tail distribution, indicating a bias towards easier tasks.  The x-axis shows the number of successful trajectories for each task, while the y-axis represents the number of tasks with that many successful trajectories.  The authors highlight how 'per-instance capping' (limiting the number of successful attempts per task) mitigates the bias towards easy tasks. However, this method also decreases the total number of trajectories available for training.", "section": "2.3. Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2412.21139/x4.png", "caption": "Figure 4: Scaling inference-time compute improves performance on SWE-Bench Verified using a fine-tuned verifier.\nBoth the agent and the verifier are Qwen2.5-Coder-Instruct-32B model fine-tuned on the corresponding dataset (\u00a74.1.1). OpenHands (Wang et\u00a0al., 2024c) is used as the agent scaffold.\nThe first rollout was performed with temperature t=0\ud835\udc610t=0italic_t = 0, and t=0.5\ud835\udc610.5t=0.5italic_t = 0.5 was used for the rest.", "description": "This figure demonstrates how increasing the computational resources allocated to inference improves the performance of software engineering agents on the SWE-Bench Verified dataset.  The experiment uses a fine-tuned Qwen-2.5-Coder-Instruct-32B language model as both the agent and verifier. The agent utilizes the OpenHands framework for its interactions.  The initial inference uses a temperature of 0, while subsequent inferences use a temperature of 0.5. The graph illustrates the relationship between the number of agent rollouts (attempts to solve the problem) and the success rate (resolve rate).  The improvement in performance is directly tied to increased compute, enabling the exploration of multiple solution candidates.", "section": "4. Scaling Agent Improvements with SWE-Gym"}, {"figure_path": "https://arxiv.org/html/2412.21139/x5.png", "caption": "Figure 5: Abaltion study for verifier training (\u00a74.1.1). Performances are evaluated on SWE-Bench Verified.\nBoth the agent and the verifier are Qwen2.5-Coder-Instruct-32B model fine-tuned on the corresponding dataset (\u00a74.1.1). OpenHands (Wang et\u00a0al., 2024c) is used as the agent scaffold.", "description": "This ablation study analyzes the impact of different training data on verifier performance.  The experiment evaluates performance on the SWE-Bench Verified dataset using a 32B Qwen2.5-Coder-Instruct model for both the agent and the verifier.  The agent utilizes the OpenHands scaffold. The study compares the performance of a verifier trained with: a mixture of on-policy and off-policy data (the default setting), only on-policy data from the fine-tuned model, only off-policy data, and off-policy data with twice the number of negative examples.  The results show the effectiveness of combining on-policy and off-policy data for optimal verifier performance.", "section": "4. Scaling Agent Improvements with SWE-Gym"}, {"figure_path": "https://arxiv.org/html/2412.21139/x6.png", "caption": "Figure 6: Scaling inference-time compute for MoatlessTools Agents with learned verifiers. We set temperature t=0.5\ud835\udc610.5t=0.5italic_t = 0.5 during sampling.", "description": "This figure demonstrates how inference-time performance scales with increased compute for software engineering agents using the MoatlessTools framework.  The experiment employs learned verifiers to evaluate multiple agent-generated solutions for a given task. The graph shows the improvement in the resolution rate (success rate) of the MoatlessTools agents on the SWE-Bench Lite benchmark as the number of agent rollouts (and consequently, the amount of compute) increases.  A temperature of 0.5 was used during the sampling process for generating candidate solutions.", "section": "4. Scaling Agent Improvements with SWE-Gym"}, {"figure_path": "https://arxiv.org/html/2412.21139/x7.png", "caption": "Figure 7: \nModel performance scaling with training data size.\nThe x-axis shows the percentage of training data used in log base 2 scale.", "description": "This figure illustrates how the performance of two different sized language models (7B and 32B parameters) scales with the amount of training data used.  The x-axis uses a logarithmic scale (base 2) to represent the percentage of the total training data used, ranging from 25% to 100%. The y-axis represents the model's performance, specifically the resolve rate (percentage of successfully solved problems) on the SWE-Bench Verified dataset. The graph shows an upward trend for both models, indicating that increased training data leads to improved performance. The 32B model consistently outperforms the 7B model across all data sizes, demonstrating the benefit of larger model capacity.", "section": "4.2. Training-Time Scaling with Data"}, {"figure_path": "https://arxiv.org/html/2412.21139/x8.png", "caption": "Figure 8: Comparison of three data sampling approaches: without deduplication, repository-based sampling, and random sampling (\u00a74.2). All variants use the 32B model evaluated on SWE-Bench Verified.", "description": "Figure 8 illustrates the impact of different data sampling methods on the performance of a 32B language model evaluated on the SWE-Bench Verified dataset.  Three approaches are compared: random sampling (without deduplication), repository-based sampling (with deduplication), and random sampling (with deduplication).  The x-axis represents the percentage of training data used, and the y-axis represents the model's resolve rate. This figure shows how different strategies for selecting training data affect the model's performance and whether the size or diversity of the training data is a limiting factor.", "section": "4. Scaling Agent Improvements with SWE-Gym"}]