{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-01-03", "reason": "This paper introduces LoRA, a parameter-efficient finetuning technique that is crucial to the approach of this submission."}, {"fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "publication_date": "2020-01-17", "reason": "This paper explores the scaling laws of language models, explaining how performance improves with increased data and model size."}, {"fullname_first_author": "Alex Warstadt", "paper_title": "BLiMP: The benchmark of linguistic minimal pairs for English", "publication_date": "2020-01-01", "reason": "This paper introduces BLiMP, a benchmark used to evaluate the linguistic capabilities of language models, and that is directly employed in this submission."}, {"fullname_first_author": "Geoffrey Hinton", "paper_title": "Distilling the knowledge in a neural network", "publication_date": "2015-03-09", "reason": "This paper introduces knowledge distillation, a technique that enables training smaller student models from larger teacher models and that is directly employed in this submission."}, {"fullname_first_author": "Leshem Choshen", "paper_title": "[call for papers] the 2nd BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus", "publication_date": "2024-04-10", "reason": "This paper introduces the BabyLM challenge, which provides techniques and recipes to pretrain on small data set size which is what the submission employs to contrast the data and aid lingustic queries."}]}