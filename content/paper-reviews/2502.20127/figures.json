[{"figure_path": "https://arxiv.org/html/2502.20127/x1.png", "caption": "Figure 1: Rule-based reward example for file localization subtask. LLM generates CoT data for a given issue, the reward for the sampled CoT is then calculated by the F\u03b2subscript\ud835\udc39\ud835\udefdF_{\\beta}italic_F start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT score based on the extracted answer and the ground-truth answer.", "description": "This figure illustrates the reward calculation mechanism in the rule-based reinforcement learning stage of SoRFT for the file localization subtask.  The process begins with an LLM generating Chain-of-Thought (CoT) data in response to a given software issue.  This CoT data represents the LLM's reasoning process toward identifying the relevant file(s). A reward score, specifically the F-beta score (F\u03b2), is then computed. This score compares the files identified by the LLM (extracted answer) against the actual files modified in the ground truth solution (ground-truth answer). A higher F-beta score indicates better accuracy in the LLM's file localization. This reward signal is crucial in guiding the LLM's learning during the reinforcement learning phase to improve its file localization capabilities.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.20127/x2.png", "caption": "Figure 2: SoRFT consists three parts: (1) decompose issue resolving into four subtasks: file localization, function localization, line localization and code edit generation; (2) fine-tune LLMs with rejection-sampled CoT data to enable it follow the task format and reasoning methods for each subtask; (3) employ rule-based reinforcement learning to further enhance the issue resolving ability of LLMs.", "description": "SoRFT is composed of three stages: (1) Issue decomposition into four subtasks (file, function, line localization and code edit generation); (2) Supervised fine-tuning of LLMs using rejection-sampled Chain-of-Thought (CoT) data to align model reasoning with task format and methods; (3) Rule-based reinforcement learning using Proximal Policy Optimization (PPO) with ground-truth rewards to further enhance issue-resolving performance.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.20127/x3.png", "caption": "(a)", "description": "Figure 3 visualizes the SoRFT framework's architecture. It is composed of three main stages: subtask decomposition, rejection-sampled supervised fine-tuning (SFT), and rule-based reinforcement learning (RL).  The subtask decomposition breaks down the issue-resolving task into four subtasks: file localization, function localization, line localization, and code edit generation. The SFT stage uses a teacher LLM to generate Chain-of-Thought (CoT) data and filters out negative samples based on ground truth answers before fine-tuning the LLM. Finally, the RL stage employs rule-based reinforcement learning with proximal policy optimization (PPO) and reward mechanisms based on ground truth answers for each subtask, further enhancing the LLM's issue-resolving abilities.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.20127/x4.png", "caption": "(b)", "description": "Figure 2(b) shows the second stage of the SoRFT framework, which is rejection-sampled supervised fine-tuning.  In this stage, a teacher LLM generates Chain-of-Thought (CoT) data for each subtask (file, function, line localization, and code edit generation).  Negative samples are filtered out based on ground truth answers. Then, supervised fine-tuning is performed on the remaining positive samples to enable the model to understand the format and reasoning mechanisms of each subtask.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.20127/x5.png", "caption": "(c)", "description": "This figure shows the performance comparison of different rule-based reward strategies on the SWE-bench Verified dataset. The x-axis represents the training steps, and the y-axis represents the average length of responses and thoughts generated by the model during training.  The left chart displays the response length, and the right chart displays the thought length for different reward strategies. The results reveal that a robust reward rule using FB score leads to more stable and better performance compared to the simpler hit score.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.20127/x6.png", "caption": "Figure 3: Comparison of rule-based reward strategy: hit score v.s. F\u03b2subscript\ud835\udc39\ud835\udefdF_{\\beta}italic_F start_POSTSUBSCRIPT italic_\u03b2 end_POSTSUBSCRIPT score.", "description": "This figure compares the performance of two different reward strategies used in reinforcement learning for an issue-resolving task.  The first strategy, 'hit score', simply rewards the model if any part of its generated response matches the ground truth. The second strategy, 'F\u03b2 score', uses a more nuanced evaluation metric (F-beta score) which balances precision and recall, giving more weight to recall (\u03b2 > 1). The plots in Figure 3 show the change in response length and the total number of answers generated over training steps using each of these reward strategies. The comparison demonstrates that the F\u03b2 score leads to more stable and effective model learning compared to the simpler hit score which is vulnerable to reward hacking.", "section": "3 Approach"}, {"figure_path": "https://arxiv.org/html/2502.20127/x7.png", "caption": "Figure 4: Reward over PPO training steps.", "description": "This figure shows the reward trend during the Proximal Policy Optimization (PPO) training process. The reward, which reflects the performance of the model on the issue-resolving subtasks, steadily increases as the training progresses, indicating successful learning and improvement of the model.", "section": "3.3 Rule-based Reinforcement Learning"}]