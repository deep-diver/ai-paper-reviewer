[{"content": "| Prompt method | Content |\n|---|---| \n| Vanilla CoT | Let\u2019s think step by step: | \n| CoT with Token Budget | Let\u2019s think step by step and use less than budget tokens: | \n| Example | Let\u2019s think step by step and use less than 50 tokens: |", "caption": "Table 1: Illustrations of the vanilla CoT prompt and the token-budget-aware prompt.", "description": "This table illustrates the difference between the standard Chain-of-Thought (CoT) prompting and a modified version that incorporates a token budget. The vanilla CoT prompt simply instructs the LLM to think step by step, leading to potentially lengthy and verbose reasoning.  The token-budget-aware prompt adds a constraint, specifying a maximum number of tokens the LLM should use in its response. This encourages more concise reasoning, which can have implications for cost and efficiency.", "section": "3 Token Redundancy in LLM Reasoning"}, {"content": "| Dataset | Directly Answering |  |  | Vanilla CoT |  |  | TALE (Ours) |  |  |\n|---|---|---|---|---|---|---|---|---|---| \n|  | ACC \u2191 | Output Tokens \u2193 | Expense \u2193 | ACC \u2191 | Output Tokens \u2193 | Expense \u2193 | ACC \u2191 | Output Tokens \u2193 | Expense \u2193 |\n|---|---|---|---|---|---|---|---|---|---| \n| GSM8K | 28.29% | 12.46 | 39.43 | 81.35% | 318.10 | 541.09 | 84.46% | 77.26 | 279.84 |\n| GSM8K-Zero | 97.21% | 18.85 | 91.69 | 99.50% | 252.96 | 886.79 | 98.72% | 22.67 | 276.12 |\n| MathBench-Arithmetic | 59.67% | 41.10 | 9.78 | 75.00% | 313.51 | 78.58 | 73.67% | 39.60 | 18.62 |\n| MathBench-Middle | 33.33% | 5.00 | 3.58 | 84.67% | 553.93 | 68.22 | 79.33% | 238.14 | 42.95 |\n| MathBench-High | 51.33% | 5.00 | 4.07 | 84.00% | 653.24 | 82.44 | 80.00% | 254.82 | 47.61 |\n| MathBench-College | 44.00% | 5.00 | 3.68 | 78.00% | 675.78 | 81.56 | 70.00% | 259.85 | 45.60 |\n| Average | 52.31% | 14.57 | 25.37 | 83.75% | 461.25 | 289.78 | 81.03% | 148.72 | 118.46 |", "caption": "Table 2: Comparison of TALE (Zero-shot Estimator Version) and other prompt engineering methods.\n\u201cDirectly Answering\u201d means prompting LLM without any reasoning process.\n\u201cVanilla CoT\u201d means the vanilla CoT prompting with budget.\nThe model used in our evaluation is GPT-4o-mini\u00a0OpenAI (2024a).\nObserve that TALE achieves an average accuracy (ACC) of 80.22%, with an average output token cost of 138.53 and an average expense of 118.46.\nTALE reduces output token costs by 67%, lowers expenses by 59%, and maintains competitive performance compared to the vanilla CoT approach.\nACC \u2191\u2191\\uparrow\u2191, Output Tokens \u2193\u2193\\downarrow\u2193, Expense (10\u22125\u2062$superscript105currency-dollar10^{-5}\\$10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT $ / sample) \u2193\u2193\\downarrow\u2193.", "description": "Table 2 presents a comparative analysis of three different prompt engineering methods: Directly Answering (no reasoning), Vanilla CoT (Chain-of-Thought with no budget constraint), and TALE (Token-Budget-Aware LLM Reasoning using a zero-shot estimator).  The evaluation uses the GPT-40-mini language model.  Key metrics include accuracy (ACC), average output token cost, and average expense per query. TALE demonstrates a balance between accuracy and efficiency, maintaining competitive accuracy with significantly reduced token costs and expenses compared to Vanilla CoT, while outperforming Directly Answering.", "section": "6.2 RQ1. Effectiveness of TALE"}, {"content": "| LLM | Directly Answering |  |  | Vanilla CoT |  |  | TALE (Ours) |  |  |\n|---|---|---|---|---|---|---|---|---|---| \n|  | ACC \u2191 | Output Tokens \u2193 | Expense \u2193 | ACC \u2191 | Output Tokens \u2193 | Expense \u2193 | ACC \u2191 | Output Tokens \u2193 | Expense \u2193 |\n| Yi-lightning | 66.67% | 80.01 | 3.09 | 79.33% | 998.10 | 21.55 | 76.67% | 373.52 | 17.25 |\n| GPT-4o-mini | 44.00% | 5.00 | 3.68 | 78.00% | 675.78 | 81.56 | 70.00% | 259.85 | 45.60 |\n| GPT-4o | 57.33% | 5.00 | 61.34 | 84.00% | 602.29 | 1359.42 | 80.00% | 181.61 | 759.95 |", "caption": "Table 3: The generalization of TALE (Zero-shot Estimator Version) across different LLMs. Yi-lightning\u00a0Wake et\u00a0al. (2024), GPT-4o-mini\u00a0OpenAI (2024a) and GPT-4o\u00a0OpenAI (2024b) are taken into consideration. We conduct the evaluation on MathBench-College. ACC \u2191\u2191\\uparrow\u2191, Output Tokens \u2193\u2193\\downarrow\u2193, Expense (10\u22125\u2062$superscript105currency-dollar10^{-5}\\$10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT $ / sample) \u2193\u2193\\downarrow\u2193.", "description": "Table 3 presents a comparison of the performance of the TALE model (using the zero-shot estimator) across three different large language models (LLMs): Yi-lightning, GPT-40-mini, and GPT-40.  The evaluation focuses on the MathBench-College dataset, measuring accuracy (ACC), the number of output tokens generated, and the computational expense (cost) of using each LLM.  The table highlights the model's ability to generalize well across different LLMs while maintaining efficiency by reducing the number of tokens and computational cost.", "section": "6.4 RQ3. Generalization of TALE"}]