[{"figure_path": "https://arxiv.org/html/2503.02357/x1.png", "caption": "Figure 1: Illustration of the unified evaluation dimensions of Q-Eval-100K. We focus on visual quality (including all factors that may impact the viewing experience) and alignment level, which measures the accuracy of the generated content to the prompt.", "description": "The figure illustrates the two key aspects of evaluating text-to-vision content using the Q-Eval-100K dataset: visual quality and alignment.  Visual quality encompasses all factors influencing the viewing experience, such as clarity, color balance, aesthetic appeal, and overall visual fidelity. Alignment, on the other hand, assesses how accurately the generated image or video matches the given text prompt.  The figure uses examples to visually represent different levels of visual quality (poor, fair, good, excellent) and alignment, demonstrating the range captured by the Q-Eval-100K dataset.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.02357/x2.png", "caption": "Figure 2: Illustration of the Sample and Scrutinize quality control strategy for annotations in Q-Eval-100K. We randomly select a sample of 5K instances from the full dataset, which are then reviewed by experts to establish golden scores. A batch of annotations is approved only if the scores of the sampled instances show a high correlation with these expert-assigned golden scores.", "description": "The figure illustrates a quality control process for ensuring the accuracy of annotations in the Q-Eval-100K dataset.  A random sample of 5,000 instances from the complete dataset is selected and evaluated by experts to establish 'golden scores' \u2013 the benchmark for quality.  Any subsequent batch of annotations is only accepted if the scores given by annotators for the sampled instances exhibit a high correlation with these expert-determined golden scores. This process helps to maintain a consistent and reliable standard of annotation quality across the entire dataset.", "section": "3. Q-Eval-100K Construction"}, {"figure_path": "https://arxiv.org/html/2503.02357/x3.png", "caption": "Figure 3: Overview of the Q-Eval-100K construction process. We design a wide range of prompts and employ various text-to-vision models to generate diverse content. Subjective evaluations are then conducted to rate the visual quality and alignment of these generated instances. The resulting scores form the SFT dataset, which can help inject corresponding knowledge into LMMs.", "description": "The figure illustrates the Q-Eval-100K dataset creation process.  It starts with prompt design, encompassing diverse prompts created manually and sourced from other datasets.  These prompts are fed into various text-to-vision models to generate a wide range of images and videos.  Human annotators then subjectively evaluate each generated instance, assessing both its visual quality and alignment with the original prompt. These evaluations are represented by Mean Opinion Scores (MOS).  The resulting MOS data then forms a Supervised Fine-Tuning (SFT) dataset. This SFT dataset is specifically designed to train Large Multimodal Models (LMMs), effectively injecting human judgment on visual quality and alignment into the model's training process.", "section": "3. Q-Eval-100K Construction"}, {"figure_path": "https://arxiv.org/html/2503.02357/x4.png", "caption": "(a) Image Alignment", "description": "This figure shows the distribution of Mean Opinion Scores (MOS) for image alignment across different text-to-image generative models.  It uses violin plots to represent the distribution of scores for each model, visualizing both the central tendency and the spread of scores. This allows for comparison of the alignment capabilities of various models, showing which models tend to produce more consistent alignments between the generated images and their corresponding text prompts.", "section": "4. Q-Eval-Score"}, {"figure_path": "https://arxiv.org/html/2503.02357/x5.png", "caption": "(b) Video Alignment", "description": "This figure shows the distribution of Mean Opinion Scores (MOS) for video alignment across various text-to-video generation models. The MOS represents human judgment on how well the generated video aligns with the corresponding text prompt.  The box plots visualize the range, median, and quartiles of MOS scores for each model. The variations in the distributions reveal the differences in the alignment capabilities of different generation models.  Models closer to a score of 5 demonstrate a higher level of alignment, indicating a better correspondence between the visual content of the video and the text prompt.", "section": "4. Q-Eval-Score"}, {"figure_path": "https://arxiv.org/html/2503.02357/x6.png", "caption": "(c) Image Visual Quality", "description": "This figure is a violin plot showing the distribution of Mean Opinion Scores (MOS) for the visual quality of generated images across various generative models. Each violin represents a different model, and the width of the violin reflects the density of scores at different MOS values. The plot provides a visualization of the performance of various models on the image visual quality, allowing for a comparison of their visual quality capabilities.", "section": "3.4. Statistical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02357/x7.png", "caption": "(d) Video Visual Quality", "description": "This figure (d) shows the distribution of Mean Opinion Scores (MOS) for the visual quality of generated videos in the Q-Eval-100K dataset.  It uses violin plots to visualize the MOS distribution across different video generation models, showing the variation in visual quality for each model.  The x-axis represents different models, and the y-axis represents the MOS values for visual quality. The violin plot shows both the probability density and the quantiles of the data. The wider the violin, the higher the data density for that model and the higher the variability of the visual quality.", "section": "3.4. Statistical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02357/x8.png", "caption": "Figure 4: MOS distributions for the visual quality and alignment of generated images and videos in the Q-Eval-100K dataset respectively.", "description": "This figure displays violin plots showing the distributions of Mean Opinion Scores (MOS) for visual quality and alignment of generated images and videos from the Q-Eval-100K dataset.  Each plot represents a different model's performance on image alignment, video alignment, image visual quality, and video visual quality, allowing for a comparison of model performance across these categories and media types.", "section": "3.4. Statistical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02357/x9.png", "caption": "Figure 5: The pipeline of the proposed Q-Eval-Score model involves multiple stages. First, the Q-Eval-100K SFT dataset is used to train the LMM on visual quality and alignment knowledge. Then, context prompts are applied to guide the LMM towards generating more detailed and accurate outputs. Finally, the rating token probabilities are converted into predicted scores. Additionally, long prompt alignment is achieved through a Vague-to-Specific strategy to further refine the model\u2019s responses.", "description": "The Q-Eval-Score model uses a multi-stage process to evaluate visual quality and alignment in text-to-vision content.  First, a large language model (LMM) is trained on the Q-Eval-100K dataset, learning to assess both visual quality and alignment. Next, context prompts are used to guide the LMM, prompting more detailed and accurate responses. The model outputs probabilities for different rating tokens (e.g., Excellent, Good, Fair, Poor, Bad), which are then converted into final scores.  A special \"Vague-to-Specific\" strategy further refines the alignment scores, particularly for long prompts, by breaking them down into more manageable parts for evaluation.", "section": "4. Q-Eval-Score"}, {"figure_path": "https://arxiv.org/html/2503.02357/x10.png", "caption": "Figure 6: An example of the Vague-to-Specific strategy. The original long prompt is divided by the LLM (QwenLM\u00a0[5]) into a Vague Prompt and several Specific Prompts. The alignment score is first calculated separately for each part, then combined using weighted averaging to form the final score.", "description": "The figure illustrates the Vague-to-Specific strategy used to evaluate alignment in long prompts.  A long prompt, describing a complex scene, is fed into a Large Language Model (LLM). The LLM processes this prompt and outputs two things: 1) a concise summary (the 'Vague Prompt') capturing the essence of the original prompt, and 2) several shorter, more specific prompts (the 'Specific Prompts') that break down the original prompt's details.  The alignment score is then calculated separately for the Vague Prompt and each Specific Prompt.  These individual scores are finally combined using weighted averaging to get a final alignment score that more accurately reflects the overall consistency between the generated image and the original complex prompt.", "section": "4.2.1 Context Prompt"}, {"figure_path": "https://arxiv.org/html/2503.02357/x11.png", "caption": "(a)", "description": "This radar chart visualizes the overall performance of various models on visual quality assessment.  Each axis represents a specific evaluation metric (Instance-level SRCC, Instance-level PLCC, Model-level SRCC, Model-level PLCC for both image and video), and each model's performance is plotted as a point on the chart.  The radial distance of the point from the center indicates the model's performance score, allowing for a direct comparison across different models and evaluation metrics.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.02357/x12.png", "caption": "(b)", "description": "This radar chart visualizes the overall performance of various models on the alignment aspect of the Q-Eval-100K dataset.  The chart presents the Instance-level and Model-level performance of each model, measured using SRCC (Spearman rank correlation coefficient) and PLCC (Pearson linear correlation coefficient).  Specifically, it compares the performance of CLIPScore, BLIP2Score, VQAScore, and Q-Eval-Score across different evaluation metrics on both images and videos.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.02357/x13.png", "caption": "Figure 7: Radar charts of the overall performance on the Visual Quality and Alignment aspects on Q-Eval-100K, where IR, IP, MR, MP indicate Instance-level SRCC, Instance-level PLCC, Model-level SRCC, Model-level PLCC, and -i, -v represents image and video respectively.", "description": "This figure presents a comparative analysis of visual quality and alignment performance across multiple evaluation methods on the Q-Eval-100K dataset.  Two radar charts display the results: one for visual quality and one for alignment. Each chart shows the performance of different methods (indicated by abbreviations such as CLIPScore, BLIP2Score, and Q-Eval-Score) across four evaluation metrics: instance-level Spearman's rank correlation coefficient (SRCC), instance-level Pearson linear correlation coefficient (PLCC), model-level SRCC, and model-level PLCC. The results are further broken down by image ('-i') and video ('-v') modalities. This allows for a comprehensive comparison of the methods' performance across different evaluation aspects and data types.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.02357/x14.png", "caption": "Figure 8: Visualization comparison results.", "description": "This figure presents a visual comparison of the results from different evaluation methods, including CLIPScore, BLIP2Score, and Q-Eval-Score.  For each of six example image-text pairs, the figure shows the Mean Opinion Score (MOS) from human evaluations alongside the scores assigned by each of the evaluation methods. This comparison allows for a visual assessment of how well each evaluation metric correlates with human judgments of visual quality and alignment.", "section": "Visualization comparison results"}]