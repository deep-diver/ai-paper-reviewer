[{"figure_path": "https://arxiv.org/html/2412.11449/extracted/6065548/paperwhisper.png", "caption": "Fig.\u00a01:  (Left) Whisper Architecture proposed by OpenAI [26] which treats ASR as a sequence to sequence which takes in mel-spectrogram slices and decodes it token by token. It has a Transformer Encoder stack on the spectrogram followed by a Transformer decoder, trained for the shift-by-one token prediction, and the cross-attention module on learned spectrogram representation. (Right) Our generative model combines both continuous and discrete representations. We align the spectrogram and ENCODEC coarse tokens. Instead of a Transformer encoder, we pass spectrogram slices through lightweight decoder blocks. The learned representation per-token slice is concatenated with discrete tokens corresponding to the spectrogram slice to have a decoder Transformer stack, trained on shift by one next token prediction, similar to a typical LLM pre-training.", "description": "The figure illustrates two architectures: Whisper (left) and the proposed Generative Whisper-GPT (right). Whisper, an ASR model, uses a Transformer Encoder on mel-spectrogram slices, followed by a Transformer Decoder for next-token prediction.  The Generative Whisper-GPT model combines continuous (mel-spectrogram) and discrete (ENCODEC coarse tokens) representations.  Spectrogram slices are passed through lightweight decoder blocks, and the learned representation is concatenated with corresponding discrete tokens. This combined input feeds a decoder Transformer stack for next-token prediction, similar to LLM pre-training.", "section": "3. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2412.11449/extracted/6065548/fig_music.png", "caption": "Fig.\u00a02: Comparison of GPT on coarse acoustic tokens with i) GPT-L ii) Our hybrid continuous-discrete representation.", "description": "This figure compares the performance of three different models on a music dataset, measured by validation loss over the number of training epochs. The models are:\n1. **GPT on coarse acoustic tokens (Baseline GPT-S):** A standard GPT model trained on discrete audio tokens representing coarse acoustic features.  This model serves as the baseline.\n2. **LARGE-GPT on acoustic tokens (GPT-L):** A larger GPT model, also trained on the coarse acoustic tokens.  This model tests whether simply increasing the size of the GPT model improves performance.\n3. **Whisper-GPT on hybrid representation:** This is the proposed model, which uses both continuous audio representations (mel-spectrogram) and discrete acoustic tokens. The hypothesis is that this hybrid approach will leverage the strengths of both representations.\n\nThe plot shows that increasing the size of the GPT model from baseline to large GPT offers marginal improvements. Whisper GPT model significantly outperforms both baseline and large GPT model showing the effectiveness of leveraging the continuous audio representation along with discrete tokens for next token prediction in a generative setup.", "section": "4. RESULTS AND DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2412.11449/extracted/6065548/whisper-GPT.png", "caption": "Fig.\u00a03: Comparison of GPT on coarse acoustic tokens with i) GPT-L ii) Our hybrid continuous-discrete representation.", "description": "This figure presents a comparison of performance on the LibriSpeech dataset using different model architectures for predicting coarse acoustic tokens. The plot shows the validation loss over the number of training epochs. Three models are compared:\n1. GPT-S (Small): A baseline GPT model with 3.7 million parameters.\n2. GPT-L (Large): A scaled-up GPT model with 40 million parameters.\n3. Whisper-GPT: The proposed hybrid model combining continuous (mel-spectrogram) and discrete (acoustic token) representations, having 4 million parameters. The graph demonstrates that the smaller hybrid model (Whisper-GPT) achieves comparable or better performance than the much larger GPT-L model, and significantly outperforms the smaller GPT-S model.", "section": "4. Results and Discussion"}]