[{"content": "| Model | # of Param | NLL | Accuracy | PPL |\n|---|---|---|---|---| \n| Baseline GPT-S | 3.7 million | 2.02 | 34.18% | 7.54 |\n| GPT-L | 40 million | 1.94 | 34.82% | 6.96 |\n| **Hyrbid LLM** | **4 million** | **1.93** | **35.05%** | **6.96** |", "caption": "Table 1: Negative-log likelihood (NLL) and Perplexity (PPL) scores for our proposed hybrid architecture, baseline GPT-Small and a GPT-Large 10 times larger than GPT-Small for LibriSpeech.", "description": "This table compares the performance of three different models on the LibriSpeech dataset using negative log-likelihood (NLL), accuracy, and perplexity (PPL) as metrics. The models compared are a baseline small GPT, a larger GPT (10x the size of small GPT), and the proposed hybrid model. The hybrid model combines continuous audio representation with discrete tokens and is shown to outperform the larger GPT model despite having fewer parameters.", "section": "4. Results and Discussion"}, {"content": "| Model | # of Param | NLL | Accuracy | PPL |\n|---|---|---|---|---| \n| Baseline GPT-S | 3.7 million | 2.78 | 34.96% | 16.12 |\n| GPT-L | 40 million | 2.77 | 35.72% | 15.96 |\n| **Hyrbid LLM** | **4 million** | **2.52** | **38.47%** | **12.43** |", "caption": "Table 2: Negative-log likelihood (NLL) and Perplexity (PPL) scores for our proposed hybrid architecture, baseline GPT-Small and a GPT-Large 10 times larger than GPT-Small for Music Dataset.", "description": "This table compares the performance of three different models on a music dataset using negative log-likelihood (NLL), accuracy, and perplexity (PPL) as metrics. The models being compared are a baseline small GPT model, a larger GPT model, and the proposed hybrid model. The hybrid model combines continuous audio representation with discrete tokens.", "section": "4. Results and Discussion"}]