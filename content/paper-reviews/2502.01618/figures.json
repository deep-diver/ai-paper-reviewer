[{"figure_path": "https://arxiv.org/html/2502.01618/x1.png", "caption": "Figure 1: State-space model for inference-time scaling. c\ud835\udc50citalic_c is a prompt, x1,\u2026,xTsubscript\ud835\udc651\u2026subscript\ud835\udc65\ud835\udc47x_{1},\\dots,x_{T}italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT are sequence of partial LLM outputs and o1,\u2026,oTsubscript\ud835\udc5c1\u2026subscript\ud835\udc5c\ud835\udc47o_{1},\\dots,o_{T}italic_o start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 , italic_o start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT are the \u201cobserved\u201d acceptance. We cast inference-time scaling as to estimate the latent states conditioned on ot=1subscript\ud835\udc5c\ud835\udc611o_{t}=1italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 for t=1,2,\u2026,T\ud835\udc6112\u2026\ud835\udc47t=1,2,\\dots,Titalic_t = 1 , 2 , \u2026 , italic_T, i.e.\u00a0all being accepted.", "description": "The figure illustrates a state-space model used for inference-time scaling in large language models (LLMs).  The model is a probabilistic graphical model where: 'c' represents the input prompt; x\u2081, x\u2082, ..., x\u209c are sequences of partial LLM outputs at different time steps; and o\u2081, o\u2082, ..., o\u209c are binary observations indicating whether each partial output was accepted or not. The goal of inference-time scaling is to estimate the latent states (x\u2081, x\u2082, ..., x\u209c) given that all observations are 1 (all partial outputs were accepted). This approach frames the scaling problem as probabilistic inference rather than a search problem, offering a more robust method less prone to reward hacking.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.01618/x2.png", "caption": "(a) Llama-3.2-1B-Instruct", "description": "This figure (Figure 2a) presents the performance comparison of different inference-time scaling methods on the Llama-3.2-1B-Instruct model.  It shows how accuracy varies as a function of the computation budget (number of model generations).  The comparison includes several baselines: Pass@1 (a single greedy generation), Weighted Best-of-N (WBoN), Dynamic Variable-Time Search (DVTS), and the proposed Particle Filtering (PF) method.  The plot visually demonstrates the superior scaling performance of PF compared to the baselines, showing its ability to achieve higher accuracy with a smaller computation budget.", "section": "5. Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.01618/x3.png", "caption": "(b) Llama-3.1-8B-Instruct", "description": "This figure (Figure 2b) presents the performance comparison of different inference-time scaling methods on the Llama-3.1-8B-Instruct model.  It shows how accuracy changes as the computational budget (number of model generations) increases.  The methods compared include the proposed Particle Filtering (PF) approach, Weighted Best-of-N (WBON), Dynamic Variable-Time Search (DVTS), and a 0-shot Chain-of-Thought (CoT) baseline. The graph allows for a visual assessment of the scaling efficiency and relative performance of each method on this specific language model.", "section": "5. Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.01618/x4.png", "caption": "(c) Qwen2.5-Math-1.5B-Instruct", "description": "This figure shows the performance of the particle filtering (PF) method compared to other inference-time scaling methods (Weighted Best-of-N, DVTS, and 0-shot CoT) on the Qwen2.5-Math-1.5B-Instruct model.  The x-axis represents the computational budget (number of model generations), and the y-axis represents the accuracy achieved on a challenging mathematical reasoning task. The graph demonstrates that PF achieves significantly better scaling, surpassing other methods and approaching the accuracy of larger models within a smaller budget.", "section": "5. Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.01618/x5.png", "caption": "(d) Qwen2.5-Math-7B-Instruct", "description": "This figure (2d) presents the performance comparison of different inference-time scaling methods on the Qwen2.5-Math-7B-Instruct model.  It shows how accuracy changes as the computation budget (number of model generations) increases.  The comparison includes the proposed particle filtering method (PF) and baselines such as weighted best-of-N (WBON), dynamic variable-time search (DVTS), and a simple greedy 0-shot chain of thought approach. The plot illustrates the superior scaling efficiency of PF, highlighting its ability to achieve higher accuracy with significantly fewer model generations compared to the baselines. It also compares the performance against GPT-40 and the o1-preview model.", "section": "5. Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.01618/x6.png", "caption": "Figure 2: \nPerformance of PF compared to other inference-time scaling methods across different model families. Figure\u00a02(a) and Figure\u00a02(b) demonstrate results for the Llama-3 family, where PF outperforms WBoN and DVTS in both cases and approaches the performance of much larger models like Llama-3.1-70B and even GPT-4o . Figure\u00a02(c) and Figure\u00a02(d) show results for the Qwen family, where PF achieves superior scaling against baslines, enabling the smaller model Qwen2.5-Math-1.5B-Instruct to surpass GPT-4o in performance within a limited compute budget. Larger Qwen2.5-Math-7B-Instruct model efficiently scale to match o1-preview performance on MATH500.", "description": "This figure compares the performance of the proposed Particle Filtering (PF) method against other inference-time scaling techniques (Weighted Best-of-N, DVTS) across different model families (Llama-3 and Qwen).  The plots show that PF consistently outperforms the baselines in terms of scaling. Notably, PF enables smaller models to match or even exceed the performance of significantly larger models (like Llama-3.1-70B and GPT-4) and achieves this with a much lower computational budget.", "section": "5. Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.01618/x7.png", "caption": "(a) Particle filtering uses the rewards to produce a softmax distribution and does stochastic expansion of N\ud835\udc41Nitalic_N based sampling.", "description": "Figure 3(a) illustrates the core idea of the particle filtering method for inference-time scaling.  It contrasts particle filtering with the beam search method. In particle filtering, a reward model generates scores for each partial answer. These scores are converted into weights (using a softmax function) that determine the probability of selecting each partial answer for expansion in the next step.  This probabilistic selection allows for exploration of multiple potential solutions, unlike beam search, which deterministically expands only the top-N highest-scoring partial answers. The stochastic nature of particle filtering makes it more robust to imperfections or noise in the reward model.", "section": "4.2. Particle filtering for inference-time scaling"}, {"figure_path": "https://arxiv.org/html/2502.01618/x8.png", "caption": "(b) Beam search treats the rewards as exact and performs deterministic expansion based on beam size N\ud835\udc41Nitalic_N and beam width M\ud835\udc40Mitalic_M.", "description": "This figure illustrates beam search, a deterministic approach to inference-time scaling.  Unlike probabilistic methods that incorporate uncertainty, beam search treats the reward signals from a process reward model (PRM) as completely accurate.  It operates by expanding the search space based on a fixed beam size (N) and width (M).  The beam size limits the number of candidate sequences explored simultaneously, while the beam width determines how many token options are considered at each step.  The algorithm keeps track of the N best sequences according to the PRM's reward, extending them deterministically until a final output is produced. This contrasts with probabilistic methods, which incorporate stochasticity and uncertainty in the exploration of the search space.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2502.01618/x9.png", "caption": "Figure 3: A side-by-side comparison between particle filtering and its closet search-based counterpart, beam search.\nCompared with beam search in Figure\u00a03(b) where the selection and expansion is deterministic (implicitly assumes the rewards are correct), particle filtering in Figure\u00a03(a) trust the rewards with uncertainty and propagate the expansion via sampling.\nA more detailed, step-by-step version of particle filtering can be found in Figure\u00a09 of Appendix\u00a0A.1.", "description": "Figure 3 illustrates the core difference between particle filtering and beam search, two approaches to inference-time scaling.  Beam search, shown in 3(b), is deterministic.  It uses the reward model's scores as completely accurate and selects the top-scoring options for expansion.  In contrast, particle filtering (3(a)) acknowledges that the reward model's scores are uncertain. It uses these scores to create a probability distribution and samples from it, making expansion probabilistic.  This allows particle filtering to explore a wider range of possibilities and be less sensitive to inaccuracies in the reward model, as opposed to the deterministic and potentially myopic beam search.  Further details of particle filtering are found in Figure 9 in Appendix A.1.", "section": "Method"}]