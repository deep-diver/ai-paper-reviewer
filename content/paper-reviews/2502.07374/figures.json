[{"figure_path": "https://arxiv.org/html/2502.07374/x1.png", "caption": "(a) Responses of the base model, with Long CoT SFT, and with Long CoT LoRA.", "description": "This figure shows the comparison of three different models' responses to the same question.  The first is the base model, which does not utilize Long Chain of Thought (Long CoT) reasoning. The second model uses Long CoT reasoning trained via supervised fine-tuning (SFT), and the third model uses Long CoT reasoning fine-tuned with low-rank adaptation (LoRA).  The different response styles and quality highlight the impact of Long CoT training on the model's reasoning ability. ", "section": "Learning to reason is data- and parameter-efficient"}, {"figure_path": "https://arxiv.org/html/2502.07374/x2.png", "caption": "(b) Performance of different models on five difference reasoning benchmarks.", "description": "The bar chart compares the performance of different LLMs (Large Language Models) on five reasoning benchmarks: Math-500, AIME 2024, AMC 2023, OlympiadBench, and LiveCodeBench.  It shows the accuracy achieved by different models, including Qwen2.5-32B-Instruct with and without fine-tuning using Long Chain of Thought (Long CoT) samples, and also includes the performance of the OpenAI o1-preview model as a reference point.  This visual representation allows for easy comparison of the different models across various reasoning tasks, highlighting the impact of Long CoT fine-tuning on model performance.", "section": "Learning to reason is data- and parameter-efficient"}, {"figure_path": "https://arxiv.org/html/2502.07374/x3.png", "caption": "Figure 1: Learning to reason is data- and parameter-efficient. When fine-tuned on a small amount (17k) of Long CoT samples distilled and reject-sampled from DeepSeek-R1 with either LoRA or full-parameter tuning, the model easily learns to perform reflection and backtracking by using keywords such as \u201cHowever\u201d and \u201cAlternatively\u201d (Top). Consequently, the fine-tuned models improve significantly across five popular math and coding benchmarks (Bottom). For fine-tuning, the base model is Qwen2.5-32B-Instruct.", "description": "This figure demonstrates the data and parameter efficiency of fine-tuning a large language model (LLM) to perform Long Chain-of-Thought (Long CoT) reasoning.  The top panel shows example Long CoT reasoning outputs, highlighting the use of keywords like \"However\" and \"Alternatively\" which indicate reflection and backtracking. The bottom panel displays a significant performance improvement across five popular math and coding benchmarks after fine-tuning with only 17,000 Long CoT samples using either Low-Rank Adaptation (LoRA) or full parameter-tuning. The base model used for fine-tuning was Qwen2.5-32B-Instruct.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.07374/x4.png", "caption": "Figure 2: Model accuracy with different data sizes, and comparison to DeepSeek R1. The teacher model is DeepSeek R1, and the student model is Qwen-32B-Instruct trained with full parameter fine-tuning. While the student model continues to benefits from more SFT data from DeepSeek R1, a small amount of data, e.g., 16k is sufficient to significantly boost the average performance by 15.2%.", "description": "This figure shows the impact of different training data sizes on the accuracy of a Qwen-32B-Instruct language model fine-tuned using the DeepSeek R1 model as a teacher.  The x-axis represents the number of training samples from the DeepSeek R1 model, and the y-axis shows the average accuracy across five reasoning benchmarks. The figure demonstrates that even a relatively small amount of training data (e.g., 16,000 samples) leads to a significant improvement in the student model's performance, achieving a 15.2% increase in average accuracy. The plot also includes the performance of the DeepSeek R1 model for comparison.", "section": "3. Simple distillation is effective"}, {"figure_path": "https://arxiv.org/html/2502.07374/x5.png", "caption": "Figure 3: Reasoning step modifications. To evaluate perturbations to global structure across reasoning steps, we perform three modifications: deletion, insertion, and shuffling. These modifications break logical consistency across steps and degrade model accuracy far more than changes to local content within reasoning steps.", "description": "This figure demonstrates the impact of modifying the structure of reasoning steps in a Long Chain of Thought (Long CoT) on model performance.  Three types of structural modifications were applied: deleting steps, inserting steps, and shuffling steps.  The results show that altering the structure significantly reduces model accuracy, far more than changing the content within individual steps. This highlights the importance of maintaining logical flow and coherence in the reasoning process for effective performance.  The figure visually represents the impact of these structural changes on the model's ability to reason correctly.", "section": "4 Long CoT: Structure Is The Key"}, {"figure_path": "https://arxiv.org/html/2502.07374/x6.png", "caption": "Figure 4: \nGeneralization to other models. Accuracy for models of different sizes and architectures without SFT (green) and with SFT (blue). Most models show significant improvements when fine-tuned with 17k samples from R1-Preview, showing that the Long CoT fine-tuning is beneficial across models.", "description": "This figure demonstrates the effectiveness of Long Chain of Thought (Long CoT) fine-tuning across various language models.  It compares the accuracy of several models (of different sizes and architectures) before and after fine-tuning using 17,000 samples from the R1-Preview dataset. The results show that fine-tuning with Long CoT data significantly improves the reasoning capabilities of most models, indicating the generalizability and benefits of this training approach.", "section": "5. Ablation Study"}]