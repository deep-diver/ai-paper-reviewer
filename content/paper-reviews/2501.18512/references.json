{"references": [{"fullname_first_author": "H. Brendan McMahan", "paper_title": "Communication-efficient learning of deep networks from decentralized data", "publication_date": "2017-00-00", "reason": "This paper introduces Federated Averaging (FedAvg), a foundational algorithm for federated learning which is the basis for the DiLoCo algorithm discussed in the current paper."}, {"fullname_first_author": "Sashank Reddi", "paper_title": "Adaptive federated optimization", "publication_date": "2021-00-00", "reason": "This paper introduces FedOpt, a generalization of FedAvg that uses a bi-level optimization approach, which is directly relevant to and improves upon the DiLoCo algorithm."}, {"fullname_first_author": "Arthur Douillard", "paper_title": "DiLoCo: Distributed low-communication training of language models", "publication_date": "2024-00-00", "reason": "This paper introduces the DiLoCo algorithm, which is the subject of improvement and extension in the current paper."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Learning neural network subspaces", "publication_date": "2022-00-00", "reason": "This paper introduces the concept of model merging and the subspace approach, providing context and theoretical foundation for techniques used in distributed model training."}, {"fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-00-00", "reason": "This paper introduces the Chinchilla scaling laws, providing crucial insights into the optimal scaling of large language models which the current paper builds upon."}]}