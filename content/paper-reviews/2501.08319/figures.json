[{"figure_path": "https://arxiv.org/html/2501.08319/x1.png", "caption": "Figure 1: We posit that a faithful description of a feature should consider both model inputs that activate it (left, marked words cause the highest activations) and the effect it introduces to the model\u2019s outputs (right).", "description": "This figure illustrates the importance of considering both input and output when describing a model's feature. The left side shows example input phrases that strongly activate the feature, while the right side demonstrates how the feature's activation influences the model's output.  This highlights that a complete understanding of the feature requires analyzing both how it is triggered and what effect it has on the model's generation.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.08319/x2.png", "caption": "Figure 2: Illustration of our feature description evaluation, considering the description\u2019s faithfulness with respect to both the input (middle panel) and output (lower panel) of the model.", "description": "This figure illustrates the dual evaluation methodology for feature descriptions. The input-based evaluation (middle panel) assesses how well the generated description captures the inputs that activate the feature.  This is done by comparing the maximum activations elicited by examples generated from the description, that are expected to activate the feature versus examples that are not expected to activate the feature. The output-based evaluation (lower panel) assesses how effectively the feature description reflects the influence of the feature on the model's output. This is performed by comparing model outputs when the feature is amplified versus when a random feature is amplified.", "section": "3 Evaluation of Feature Descriptions"}, {"figure_path": "https://arxiv.org/html/2501.08319/x3.png", "caption": "(a) Residual stream SAE features of width 65k from Gemma-2.", "description": "This figure displays the results of the input and output evaluations for residual stream SAE features of width 65k from the Gemma-2 language model.  It compares the performance of different methods for automatically generating feature descriptions: MaxAct, VocabProj, TokenChange, and various ensembles of these methods.  The input evaluation measures how accurately the descriptions identify inputs that trigger the feature, while the output evaluation assesses how well the descriptions capture the feature's causal effect on the model's outputs.", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2501.08319/x4.png", "caption": "(b) MLP SAE features of width 65k from Gemma-2.", "description": "This figure displays the results of an experiment evaluating different methods for describing features (atomic units of computation) within the Gemma-2 language model. Specifically, it focuses on features extracted from the Multi-Layer Perceptron (MLP) layers of a Sparse Autoencoder (SAE) with a width of 65,000.  The figure likely shows the performance of various feature description methods across different metrics, potentially including how accurately these descriptions capture the inputs that activate the features and how well they reflect the causal impact of feature activation on the model's outputs.  The results are likely presented as a comparison across various methods for feature description (e.g., input-centric vs. output-centric).", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2501.08319/x5.png", "caption": "(c) Residual stream SAE features of width 32k from Llama-3.1.", "description": "This figure displays the performance of various methods (MaxAct, VocabProj, TokenChange, and their ensembles) on input and output evaluations.  Specifically, it focuses on residual stream Sparse Autoencoder (SAE) features with a width of 32,000, extracted from the Llama-3.1 language model.  The graph shows the accuracy of each method across different layer groups within the model, helping to illustrate how the methods' performance varies depending on depth within the network architecture and whether the evaluation is input- or output-centric.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08319/x6.png", "caption": "(d) MLP features from Llama-3.1 8B Instruct.", "description": "This figure displays the performance of various feature description methods (MaxAct, VocabProj, TokenChange, and their ensembles) on Llama-3.1 8B Instruct model.  It specifically shows input-based and output-based evaluation results for MLP features within this model, categorized by layer group.  The performance is measured by accuracy, indicating how well the methods capture the causal impact of feature activation on model outputs and how accurately they identify inputs that trigger the feature.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08319/x7.png", "caption": "Figure 3: Performance of the various methods on the proposed metrics, for Gemma-2 2B (upper row), Llama-3.1 8B (lower left), and Llama-3.1 8B Instruct (lower right). For the output metric, the baseline (dashed black line) is 1/3131/31 / 3 since the judge LLM picks between three sets of texts.", "description": "This figure displays the performance of different methods for generating feature descriptions in large language models (LLMs).  It compares input-centric and output-centric approaches across three different LLMs: Gemma-2 2B, Llama-3.1 8B, and Llama-3.1 8B Instruct.  The performance is evaluated using two metrics: an input-based metric (measuring how well the description captures the inputs that activate the feature) and an output-based metric (measuring how well the description captures the causal effect of the feature on model outputs). The output-based metric uses a baseline of 1/3 because the evaluation involves a judge LLM choosing between three sets of texts.  The figure shows the results broken down by layer group within each LLM, providing a detailed view of the performance variation across different model components.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08319/x8.png", "caption": "Figure 4: Prompt given to the judge LLM for the input-based evaluation.", "description": "This figure shows the prompt used to evaluate the input-centric aspect of the feature descriptions generated by different methods. The prompt instructs the Large Language Model (LLM) judge to produce two sets of sentences for a given feature description: one set of sentences that are expected to activate the feature, and another set that are not expected to.  The LLM's ability to correctly identify activating and non-activating sentences based on the feature description is assessed as a measure of how well the description captures the model's input-side behavior.", "section": "3 Evaluation of Feature Descriptions"}, {"figure_path": "https://arxiv.org/html/2501.08319/x9.png", "caption": "Figure 5: Prompt given to the judge LLM for the output-based evaluation.", "description": "This figure shows the prompt used for the output-based evaluation in the paper.  The evaluation aims to assess how accurately a feature description captures the feature's causal impact on model outputs. The prompt presents the judge LLM (a large language model) with a feature description and three sets of generated texts: one where the feature is amplified, one where a random feature is amplified, and one with no feature amplification.  The judge LLM must identify which set of texts corresponds to the amplified feature based on the given description. This tests the description's ability to predict the effect of the feature on the model's output, a key aspect of output-centric feature descriptions.", "section": "3 Evaluation of Feature Descriptions"}, {"figure_path": "https://arxiv.org/html/2501.08319/x10.png", "caption": "Figure 6: Prompt given to the explainer model for the VocabProj method.", "description": "This figure shows the prompt used to instruct the explainer model in the VocabProj method.  The prompt instructs the model to interpret a list of tokens, which represent a vector in the model's embedding space. It emphasizes that the list may contain noise (unrelated terms, symbols, etc.) and that the model should focus on identifying a central theme or function represented by the tokens. The model's response is expected to be a concise summary of the vector's meaning, avoiding overly generic or broad descriptions.", "section": "4 Interpretability Methods"}, {"figure_path": "https://arxiv.org/html/2501.08319/x11.png", "caption": "Figure 7: Text generated when amplifying a feature pronounced to be dead, which we managed to activate using the explanation generated by VocabProj, which was \u201cgaming, focusing on players, gameplay, and game mechanics\u201d.", "description": "This figure displays text generated by a language model when a specific feature, initially deemed \"dead\" (inactive), is artificially stimulated.  The feature's description, created using the VocabProj method, was \"gaming, focusing on players, gameplay, and game mechanics.\" The generated text demonstrates that activating this feature, even though it was previously considered inactive, produces outputs consistent with its description (text relating to gaming). This highlights the effectiveness of output-centric methods like VocabProj in identifying and characterizing features, even those that appear inactive using traditional input-centric methods.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08319/x12.png", "caption": "(a) MLP 32k SAE features from Llama-3.1.", "description": "The figure shows the performance of various methods for describing features in a large language model (LLM). Specifically, it presents the results for input-based and output-based evaluations on MLP 32k SAE features extracted from Llama-3.1.  Different methods for generating feature descriptions are compared, including MaxAct, VocabProj, TokenChange, and ensembles of these methods. The results are visualized to show the accuracy of each method in capturing the input and output aspects of the features.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08319/x13.png", "caption": "(b) Mid residual stream 32k SAE features from GPT-2 small.", "description": "This figure shows the results of the output-based and input-based evaluations for the GPT-2 small language model. Specifically, it focuses on the performance of different methods in describing features extracted from the model's mid residual stream layer using 32k-dimensional sparse autoencoders (SAEs). The evaluation metrics assess how well the descriptions capture both the model inputs that activate the features and the causal impact of feature activation on the model's output.", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2501.08319/x14.png", "caption": "(c) Residual stream 32k SAE features from GPT-2 small.", "description": "This figure displays the results of evaluating different methods for generating feature descriptions on residual stream 32k SAE (Sparse Autoencoder) features extracted from the GPT-2 small language model.  The evaluation assesses how well the descriptions capture the causal impact of feature activation on the model's output, as well as how accurately they identify the inputs that trigger the features.  The figure likely shows performance metrics such as accuracy or F1-score, possibly broken down by layer group within the model.", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2501.08319/x15.png", "caption": "(d) MLP 32k SAE features from GPT-2 small.", "description": "The figure displays the performance of various feature description methods on GPT-2 small's 32k-dimensional sparse autoencoder (SAE) features extracted from multilayer perceptrons (MLPs).  It shows the input and output evaluation metrics (accuracy) for different methods, including MaxAct, VocabProj, TokenChange, and ensembles of these methods. The x-axis represents different layer groups within the model, and the y-axis represents the accuracy of the methods in capturing input triggers and output effects of the features.", "section": "5.2 Results"}, {"figure_path": "https://arxiv.org/html/2501.08319/x16.png", "caption": "Figure 8: Performance of the various methods on the proposed metrics, for Llama-3.1 8B (upper left) and GPT-2 small (upper right and lower row). For the output metric, the baseline (dashed black line) is 1/3131/31 / 3 since the judge LLM picks between three sets of texts.", "description": "Figure 8 presents a comparative analysis of various methods for generating feature descriptions, focusing on their performance in input-based and output-based evaluations. The results are shown for two different language models: Llama-3.1 8B and GPT-2 small. The input-based evaluation assesses how well the methods capture the features' activating inputs, while the output-based evaluation measures their effectiveness in capturing the features' causal impact on the models' outputs. The output metric uses a baseline of 1/3 because the judge LLM chooses between three sets of texts.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08319/x17.png", "caption": "Figure 9: Three demonstrations of tokens and their descriptions for each model, added to the base prompt forming a fine-tuned prompt.", "description": "This figure displays three examples of how fine-tuned prompts were constructed for the VocabProj method.  For each of the three models used in the study (Gemma-2 2B, Llama-3.1 8B, and GPT-2 small), a set of tokens was identified and a corresponding description was generated.  These tokens and descriptions, which represent a feature of the model's inner workings, were then combined with a base prompt to create the fine-tuned prompt used in the VocabProj method.  The figure visually presents the tokens and explanations to show how model features were described and used to create more effective prompts.", "section": "4 Interpretability Methods"}, {"figure_path": "https://arxiv.org/html/2501.08319/x18.png", "caption": "Figure 10: Prompt given to the explainer model for the Ensemble Raw method.", "description": "This figure shows the prompt used to instruct the GPT-40 mini language model (the explainer model) when generating descriptions for the Ensemble Raw method. The prompt provides instructions on how to evaluate a given neuron in a neural network by considering both the neuron's input and output.  The input section explains how activation values are presented, indicating that a higher value represents a stronger match.  The output section defines how the related words are provided.  The overall instructions direct the model to generate a concise description (one to two sentences) that includes both the input and the output information.", "section": "4 Interpretability Methods"}, {"figure_path": "https://arxiv.org/html/2501.08319/x19.png", "caption": "Figure 11: The basic fine-tuned prompt VocabProj method.", "description": "This figure shows the prompt used for the VocabProj method in the paper.  The prompt instructs the model to generate a description of a feature vector based on a list of tokens that represent its components.  It emphasizes focusing on the most relevant tokens and avoiding noise or unrelated terms, while instructing the model to provide a concise summary.", "section": "4 Interpretability Methods"}, {"figure_path": "https://arxiv.org/html/2501.08319/x20.png", "caption": "Figure 12: An example of steered text set for the output-based metric.", "description": "This figure shows an example of text generated by a language model when a specific feature within the model is amplified or 'steered'.  The output-based evaluation method uses such steered text to assess whether a feature's description accurately reflects how the feature affects the model's overall output. The figure helps illustrate how the controlled amplification of a feature influences the model's text generation, offering insights into the feature's causal role in the model's behavior.", "section": "3 Evaluation of Feature Descriptions"}, {"figure_path": "https://arxiv.org/html/2501.08319/x21.png", "caption": "Figure 13: A first variant of a generic prompt for the VocabProj method.", "description": "This figure displays the prompt used for the VocabProj method in the paper's experiments.  The prompt instructs the model to take a list of tokens representing a vector and generate a single, concise description summarizing its meaning or function. The description should be based on the most relevant tokens and disregard noise such as unrelated terms or symbols.  The prompt also specifies that the response should be in JSON format with fields for reasoning, explanation, and observed patterns.", "section": "4 Interpretability Methods"}, {"figure_path": "https://arxiv.org/html/2501.08319/x22.png", "caption": "Figure 14: A second variant of a generic prompt for the VocabProj method.", "description": "This figure shows a slightly different prompt used in the VocabProj method for generating feature descriptions.  The prompt instructs the model to analyze input strings, identify common patterns, and provide a concise, single-sentence description of the strings and their patterns, avoiding noise or unrelated elements. The response should be structured in JSON format with fields for reasoning, explanation, and observed patterns.", "section": "4 Interpretability Methods"}]