{"references": [{"fullname_first_author": "Bai", "paper_title": "Qwen2.5-VL Technical Report", "publication_date": "2025-02-01", "reason": "This technical report provides details on the Qwen2.5-VL model, a key comparator in the benchmark evaluations, making it crucial for understanding relative performance."}, {"fullname_first_author": "Team, Gemma", "paper_title": "Gemma 3 Technical Report", "publication_date": "2025-03-01", "reason": "This technical report details the Gemma 3 model, an open-source VLM used for comparison in the current work"}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "publication_date": "2025-01-01", "reason": "This work is important as it demonstrates a method to improve reasoning capabilities in language models using reinforcement learning which served as inspiration for the Kimi-VL-Thinking."}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "DeepSeek-V3 Technical Report", "publication_date": "2024-12-01", "reason": "This technical report details the DeepSeek-V3 model, a key model used as inspiration in the architecture of the Kimi-VL language model."}, {"fullname_first_author": "Liu, Jingyuan", "paper_title": "Muon is Scalable for LLM Training", "publication_date": "2025-02-01", "reason": "This paper introduces Muon and the scalable optimizer for LLM training, which enhances the model's performance and memory efficiency and is the base optimizer used in this work."}]}