[{"figure_path": "https://arxiv.org/html/2503.16081/extracted/6317869/figures/performancewtdata.png", "caption": "Figure 1. Test accuracy metric curves of SFT and GRPO on geometry reasoning task.", "description": "This figure shows the performance comparison between supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) on a geometry reasoning task.  The x-axis represents the number of training steps, while the y-axis shows the accuracy achieved on a test set.  The curves illustrate how the accuracy of both methods changes as training progresses.  This allows for a direct visual comparison of the learning speed and final accuracy of each training method, highlighting the relative effectiveness of SFT and GRPO in improving the model's performance on this specific geometry task.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16081/x1.png", "caption": "Figure 2. Examples of multimodal content understanding and multimodal reasoning tasks.", "description": "This figure displays two example tasks to illustrate the capabilities of multimodal large language models.  The first example showcases multimodal content understanding, where the model must interpret both an image and a question to provide a numerical answer (how many items are present in the image?). The second example demonstrates multimodal reasoning, requiring the model to process an image depicting a geometry problem along with a text-based question to arrive at a numerical solution (finding the measure of an angle). These examples highlight the model's ability to integrate visual and textual information to perform complex reasoning tasks.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16081/x2.png", "caption": "Figure 3. Qualitative illustration of GRPO-D vs SFT in OThink-MR1.", "description": "This figure provides a qualitative comparison of the performance of the Group Relative Policy Optimization with Dynamic KL divergence (GRPO-D) and Supervised Fine-tuning (SFT) methods within the OThink-MR1 model. It showcases examples of both same-task and cross-task validation scenarios.  The same-task validation demonstrates how both GRPO-D and SFT approach solving geometry reasoning problems, but GRPO-D exhibits more accurate reasoning.  The cross-task validation depicts how the models perform when trained on one task (e.g., geometry reasoning) and evaluated on a different multimodal task (e.g., visual counting).  This demonstrates the superior cross-task generalization ability of GRPO-D, as it shows more accurate and logical answers compared to SFT.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16081/extracted/6317869/figures/impact_format.png", "caption": "Figure 4. Impact of the weight of format reward.", "description": "This figure shows how varying the weight of the format reward in the GRPO (Group Relative Policy Optimization) model affects both the main accuracy and format accuracy.  The x-axis represents different weights assigned to the format reward, ranging from 0 to 1. The y-axis shows the corresponding accuracy percentages. The lines represent the main accuracy and format accuracy for each weight. The results demonstrate that including a format reward generally improves accuracy, but an excessively high weight can negatively impact performance.  An optimal balance is needed.", "section": "4.1 Analysis of GPRO in Multimodal tasks (RQ1)"}, {"figure_path": "https://arxiv.org/html/2503.16081/extracted/6317869/figures/format_reward3.png", "caption": "(a) format reward w.r.t training steps.", "description": "The figure shows the training curve for the format reward.  It illustrates how the format reward changes with respect to the number of training steps. The x-axis represents the training steps, and the y-axis represents the format reward value. The curve's trajectory shows how well the model learns to generate outputs in the correct format during training.", "section": "4.1 Analysis of GPRO in Multimodal tasks (RQ1)"}, {"figure_path": "https://arxiv.org/html/2503.16081/extracted/6317869/figures/accuracy_reward.png", "caption": "(b) accuracy reward w.r.t training steps.", "description": "This figure shows the training curve for the accuracy reward during the training process.  The x-axis represents the number of training steps, while the y-axis shows the value of the accuracy reward. The curve illustrates how the accuracy reward changes as the training progresses, indicating the improvement in the model's performance on the task over time.", "section": "4.1 Analysis of GPRO in Multimodal tasks (RQ1)"}, {"figure_path": "https://arxiv.org/html/2503.16081/extracted/6317869/figures/impact_KL.png", "caption": "Figure 5. Training curves for format reward and accuracy reward.", "description": "The figure shows two line graphs illustrating the training progress of a multimodal large language model (MLLM). The top graph displays the format reward over training steps, showing rapid initial improvement before plateauing. The bottom graph shows the accuracy reward, which increases gradually and steadily throughout training.  These graphs demonstrate the difference in the speed of improvement between achieving correct formatting and generating accurate answers during model training. The curves highlight that ensuring correctly formatted responses is much easier than achieving accurate reasoning in the geometry reasoning task.", "section": "4.1 Analysis of GPRO in Multimodal tasks (RQ1)"}, {"figure_path": "https://arxiv.org/html/2503.16081/extracted/6317869/figures/cross-task2.png", "caption": "Figure 6. Impact of the weight of KL divergence", "description": "This figure shows how different constant weights of the KL divergence term in the GRPO algorithm affect the model's performance in the geometry reasoning task.  As the KL divergence weight increases, performance initially improves due to better regularization, but then decreases as excessive regularization leads to underfitting and reduced performance. The x-axis represents the weight of the KL divergence, and the y-axis shows the resulting accuracy.", "section": "4.1.2 Impact of the weight of KL divergence"}, {"figure_path": "https://arxiv.org/html/2503.16081/x3.png", "caption": "Figure 7. Cross-task validation.", "description": "This figure displays the results of cross-task validation experiments.  Two tasks were used: visual counting (a simpler task involving identifying objects) and geometry reasoning (a more complex task requiring logical analysis). Models were trained on one task and then evaluated on the other, demonstrating the ability of the models to generalize to unseen tasks.  The bars show the accuracy achieved by different models (Qwen2-VL-2B-Instruct with various post-training methods: Supervised Fine-Tuning (SFT), Group Relative Policy Optimization (GRPO), and the Dynamic KL version of GRPO (GRPO-D)) on each cross-task scenario.  The results highlight the superior generalization ability of GRPO-D compared to SFT and GRPO in transferring knowledge between different types of multimodal reasoning tasks.", "section": "4.3 Cross-Task Evaluation"}]