{"importance": "This research reveals the \"blind faith in text\" phenomenon in VLMs, highlighting critical vulnerabilities in multi-modal data handling. It prompts researchers to **reevaluate VLM architectures**, develop robust training strategies, and **explore novel methods** to enhance reliability in real-world applications.", "summary": "VLMs often disproportionately trust text over visual data, leading to performance drops and safety concerns.", "takeaways": ["VLMs exhibit a 'blind faith in text,' prioritizing textual data over visual data, even when inconsistent.", "Text bias is influenced by factors like instruction prompts, language model size, relevance, and token order.", "Supervised fine-tuning with text augmentation effectively reduces text bias, enhancing model robustness."], "tldr": "Vision-Language Models (VLMs) are increasingly used. However, their handling of inconsistencies between visual and textual data is underexplored, which raises safety concerns. VLMs disproportionately trust textual data over visual data when inconsistencies arise. This \"blind faith in text\" can lead to performance drops and safety concerns. Several factors influencing this text bias: instruction prompts, language model size, relevance, token order, and modality certainty.\n\nThis paper investigates VLMs' modality preferences when faced with inconsistencies. The authors introduce textual variations to four vision-centric tasks and evaluate ten VLMs. To mitigate text bias, they explore supervised fine-tuning with text augmentation. They suggest that the imbalance of pure text and multi-modal data during training contributes to the blind faith in text. Findings show supervised fine-tuning with text augmentation reduces text bias and enhances model robustness.", "affiliation": "National University of Singapore", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.02199/podcast.wav"}