[{"content": "| Model | Method | in-domain MATH | out-domain AMC12 | out-domain AMC10 | out-domain AQUA | out-domain MathBench(C) | out-domain MathBench(H) | out-domain OlympiadBench-TO |\n|---|---|---|---|---|---|---|---|---|\n| GPT-4o | 0-shot | 73.4 | 53.6 | 55.8 | 81.1 | 80.0 | 77.3 | 40.6 |\n|  | few-shot | 73.8 | 56.5 | 56.7 | 83.9 | 80.7 | 79.3 | 39.3 |\n|  | **Ours** | **76.4** | **63.0** | **60.4** | **85.4** | **82.0** | **84.0** | **43.3** |\n| Qwen | 0-shot | 83.0 | 67.4 | 67.7 | 84.6 | 80.6 | 82.0 | 49.7 |\n|  | few-shot | 83.8 | 67.4 | 66.8 | 85.0 | 81.3 | 82.7 | 49.9 |\n|  | **Ours** | **85.2** | **69.2** | **69.6** | **86.6** | **82.7** | **84.7** | **52.7** |", "caption": "Table 1: A comparison of different in-context learning strategies on different benchmarks on GPT-4o and Qwen2.5-Math-72B-Instruct. The example problem bank is constructed from PRM800K, so MATH500 is an in-domain benchmark while others are all out-domain benchmarks. Best results are in bold.", "description": "This table compares the performance of different in-context learning methods on various mathematical reasoning benchmarks using two large language models: GPT-4 and Qwen-2.5-Math-72B.  The methods compared are zero-shot, few-shot (traditional problem-level in-context learning), and the proposed BoostStep method (step-level in-context learning).  The benchmarks include both in-domain (MATH500, built using the same dataset as the example problem bank) and out-of-domain tasks (AMC10, AMC12, AQuA, MathBench, OlympiadBench). The table highlights the improved accuracy of the BoostStep approach, especially on out-of-domain benchmarks.", "section": "4 Experiments"}, {"content": "| Method | MathVision-Mini | MathVerse-Mini |\n|---|---|---|\n| 0-shot | 30.6 | 53.2 |\n| few-shot | 28.7 | 53.2 |\n| **Ours** | **35.2** | **54.2** |", "caption": "Table 2: Comparison of different strategies in multi-modal mathematical benchmarks with lower similarity with our problem bank. Base models are all GPT-4o.", "description": "This table presents a comparison of the performance of different reasoning strategies on multi-modal mathematical benchmarks.  These benchmarks (MathVision-Mini and MathVerse-Mini) have a lower similarity to the example problem bank used in the study than other benchmarks. The comparison focuses on the impact of different strategies on solving problems in these visually-rich settings.  All strategies utilize the GPT-40 language model as a base.", "section": "4 Experiments"}, {"content": "| Method | Math-level5 | AMC12 | AMC10 |\n|---|---|---|---|\n| 0-shot | 50.7 | 53.6 | 55.8 |\n| few-shot R_1 | 52.2 | 56.5 | 56.7 |\n| few-shot R_4 | 46.3 (-5.9) | 52.2(-4.3) | 53.7 (-3.0) |\n| Ours R_1 | 56 | 62.3 | 60.4 |\n| Ours R_4 | 52.2 (-3.8) | 61.6 (-0.7) | 58.1 (-2.3) |", "caption": "Table 3: Experiments on the sensitivity of the similarity between the question and the example problem bank. R_t indicates that the examples are the t_th similar for different method without any rejection strategy. Given a less similar example, our method suffers an 2.26% performance loss, which is much lower comparing to few-shot learning(4.4%).", "description": "This table investigates how sensitive different methods are to the similarity between the question and the examples in the problem bank.  It tests the performance of a 0-shot method, few-shot learning (using the top 1 and top 4 most similar examples), and the proposed method. The experiment measures the performance drop when using less similar examples and compares the impact on various methods. The results demonstrate that the proposed method shows significantly better robustness than traditional few-shot learning when faced with less similar examples.", "section": "4.3 Construction of Example Problem Bank"}, {"content": "| Strategy | AMC12 | AMC10 | MATH |\n|---|---|---|---|\n| Grammatical Separation | 56.5 | 58.1 | 74.8 |\n| **Reasoning Content** | **63.0** | **60.4** | **76.4** |", "caption": "Table 4: Comparison of different step-level example problem Bank construction methods.", "description": "This table compares the performance of different methods for constructing a step-level example problem bank.  It shows how different strategies for segmenting problems into steps (grammatical separation versus reasoning content) affect the performance of a model on mathematical reasoning tasks. The results are presented as accuracy scores on three benchmark datasets: AMC12, AMC10, and MATH.", "section": "3.2 Step-Level Example Problem Bank"}, {"content": "| Strategy | AMC12 | AMC10 | MATH | MathVision |\n|---|---|---|---|---|\n| Path | 56.5 | 58.1 | 73.8 | 31.7 |\n| Pre-Step | 57.2 | 56.7 | 74.0 | 31.0 |\n| **First-try** | **63.0** | **60.4** | **76.4** | **35.2** |", "caption": "Table 5: Comparison on different retrieval strategies in step-level in-context learning. The base model is GPT-4o and all the prompts are the same. \u2019Path\u2019 represents retrieving by the reasoning path including all previous step si\u22121,si\u22122,\u2026,s1subscript\ud835\udc60\ud835\udc561subscript\ud835\udc60\ud835\udc562\u2026subscript\ud835\udc601s_{i-1},s_{i-2},\\ldots,s_{1}italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_i - 2 end_POSTSUBSCRIPT , \u2026 , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and question q\ud835\udc5eqitalic_q, while \u2019Pre-Step\u2019 represents retrieving by only the immediately preceding step si\u22121subscript\ud835\udc60\ud835\udc561s_{i-1}italic_s start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT. Best results are in bold.", "description": "Table 5 presents a comparison of different retrieval strategies within a step-level in-context learning approach for mathematical reasoning.  The study uses GPT-40 as the base language model, and all experiments employ the same prompts to ensure consistency.  Three retrieval methods are compared:\n\n1. **Path:** Retrieves relevant examples based on the entire reasoning path up to the current step, including all previous steps (s\u1d62\u208b\u2081, s\u1d62\u208b\u2082, ..., s\u2081) and the initial question (q).\n2. **Pre-Step:** Retrieves examples based only on the immediately preceding step (s\u1d62\u208b\u2081).\n3. **First-try:** This is the proposed method from the paper. It involves a 'first-try' step where the model attempts the reasoning task before retrieving examples, using this initial attempt to guide the selection of highly relevant examples.\nThe table displays the performance of each retrieval strategy across four mathematical benchmarks: AMC12, AMC10, MATH, and MathVision. The best performance for each benchmark is highlighted in bold.", "section": "3.3 Step-Level ICL with First-try Strategy"}, {"content": "| Reason | Verify | AMC12 | AMC10 | MATH |\n|---|---|---|---|---|\n| w/o MCTS |  | 53.6 | 55.8 | 73.4 |\n| \u2717 | \u2717 | 58.7 | 59.0 | 77.8 |\n| \u2713 | \u2717 | 64.4 | 62.2 | 79.2 |\n| \u2717 | \u2713 | 61.6 | 60.4 | 78.2 |\n| \u2713 | \u2713 | **65.2** | **63.6** | **79.4** |", "caption": "Table 6: A detailed ablation on incorporating retrieving similar steps to provide fine-grained guidance during the reasoning and verifying phases of Monte Carlo Tree Search (MCTS) methods. Base models are GPT-4o and prompts are the same. Best results are in bold.", "description": "This table presents an ablation study on the impact of incorporating retrieved similar steps into the Monte Carlo Tree Search (MCTS) process for mathematical reasoning.  It examines how providing similar examples during both the reasoning and verification phases of the MCTS algorithm affects performance.  The study uses GPT-4o as the base reasoning model and compares different configurations of using or not using similar examples in the reasoning and verification steps, measuring the impact on accuracy across three benchmark datasets (AMC12, AMC10, MATH).  The prompts used across the different configurations were kept constant to isolate the effect of example retrieval.", "section": "3.4 Step-Level Example Guidance in MCTS"}]