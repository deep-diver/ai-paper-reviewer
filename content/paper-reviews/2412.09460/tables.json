[{"content": "| Dataset | Documents | Words |\n|---|---|---|\n| base | 60,182,586 | 40,122,626,81 |\n| extended | 125,285,547 | 82,149,281,266 |", "caption": "Table 1: Number of documents and words in each of the core datasets. Words refer to whitespace-separated sub-strings.", "description": "This table presents the size of the two core datasets used in the study: 'base' and 'extended'.  The 'base' dataset contains only publicly available or non-copyrighted material, while the 'extended' dataset includes copyrighted material in addition to the content of the 'base' dataset.  The table shows the number of documents and the number of words (counting whitespace-separated substrings as individual words) in each dataset. This provides context for understanding the scale of data used to train the different language models.", "section": "3 Data Collection"}, {"content": "| Subset | Documents | Words |\n|---|---|---|\n| books | 492,281 | 18,122,699,498 |\n| newspapers | 46,764,024 | 9,001,803,515 |\n| books + newspapers | 47,256,305 | 26,078,915,554 |\n| fiction books | 117,319 | 5,287,109,366 |\n| nonfiction books | 359,979 | 12,384,323,012 |\n| nonfiction books + newspapers | 42,083,532 | 20,340,539,068 |\n| original books | 392,887 | 13,352,261,605 |\n| original books + newspapers | 47,156,911 | 22,354,065,120 |\n| translated books | 96,258 | 4,695,814,506 |", "caption": "Table 2: Number of documents and words in each copyrighted subset of the \u2018extended\u2019 dataset.", "description": "This table presents a detailed breakdown of the copyrighted material within the 'extended' dataset used in the study.  It shows the number of documents and the total word count for various subsets of copyrighted data. These subsets are categorized based on different criteria, such as the type of publication (books vs. newspapers), content genre (fiction vs. nonfiction), and origin of the text (original Norwegian vs. translations). This granular breakdown allows for a precise analysis of the impact of different types of copyrighted material on the performance of the language models.", "section": "3 Data Collection"}, {"content": "| Model | Initialization | GPU/hours | Accelerator |\n|---|---|---|---| \n| **1.5 Core Models** |  |  |  |\n| base | From scratch | 50K | AMD MI250X |\n| extended | From scratch | 50K | AMD MI250X |\n| base (warm) | Mistral 7B v0.1 | 13.8K | NVIDIA H100 |\n| extended (warm) | Mistral 7B v0.1 | 55.6K | AMD MI250X |\n| **1 Domain Tuned Models** |  |  |  |\n| base + fiction books | base | 7.5K | AMD MI250X |\n| base + nonfiction books | base | 7.5K | AMD MI250X |\n| base + nonfiction books + newspapers | base | 7.5K | AMD MI250X |\n| base + newspapers | base | 4.8K | Google TPUv4 |\n| base + books | base | 4.8K | Google TPUv4 |\n| base + books + newspapers | base | 4.8K | Google TPUv4 |\n| base + original books + newspapers | base | 9.1K | AMD MI250X |\n| base + original books | base | 9.1K | AMD MI250X |\n| base + translated books | base | 9.1K | AMD MI250X |\n| **1 Instruction Fine Tuned Models** |  |  |  |\n| base _instruct_ | base | 14.2 | NVIDIA H100 |\n| extended _instruct_ | extended | 14.2 | NVIDIA H100 |\n| base (warm) _instruct_ | base (warm) | 14.2 | NVIDIA H100 |\n| extended (warm) _instruct_ | extended (warm) | 14.2 | NVIDIA H100 |", "caption": "Table 3: Model training specifications, where Model represents the model identifier and the data used for training, Initialization represents the base model used for training, GPU/hours indicates the total GPU hours required for model training, and Accelerator represents the type of accelerator used.", "description": "Table 3 details the training configurations for 17 different large language models (LLMs).  It shows the model name, the initial model used (if any, otherwise 'from scratch'), the dataset used for training, the total GPU hours consumed during training, and the type of hardware accelerator utilized (e.g., AMD MI250X, NVIDIA H100, Google TPUv4). This table provides a comprehensive overview of the computational resources used to build the models described in the paper, enabling comparison and reproducibility.", "section": "4 Model Training"}, {"content": "| Model | SA | FT | RC | WK | RC | NL | S | T | VR |\n|---|---|---|---|---|---|---|---|---|---| \n| extended | 3 | 2 | 3 | 3 | 2 | 2 | 1 | 3 | 2 |\n| base | 4 | 3 | 4 | 4 | 3 | 4 | 3 | 4 | 3 |\n| extended (warm) | 2 | 3 | 1 | 2 | 1 | 1 | 2 | 1 | 1 |\n| base (warm) | 1 | 1 | 2 | 1 | 1 | 3 | 2 | 2 | 4 |", "caption": "Table 4: Results for ranking the core models on all tasks by skill via (i) finding the best k-shot configuration for each task and (ii) aggregating metric-wise rankings. SA=Sentiment Analysis. FT=Fairness & Truthfulness. RC=Reading Comprehension. NL=Norwegian Language. WK=World Knowledge. CR=Commonsense Reasoning. S=Summarization. T=Translation. VR=Variation & Readability. Lower is better.", "description": "This table presents a ranking of core language models across various tasks, categorized by skill.  The ranking considers the best performance (lowest score) achieved using different numbers of examples (k-shot learning).  Each skill encompasses several sub-tasks, and the final ranking is based on aggregating the individual scores for these sub-tasks.  The skills evaluated are Sentiment Analysis (SA), Fairness & Truthfulness (FT), Reading Comprehension (RC), Norwegian Language proficiency (NL), World Knowledge (WK), Commonsense Reasoning (CR), Summarization (S), Translation (T), and Variation & Readability (VR). Lower scores indicate better performance.", "section": "6 Results"}]