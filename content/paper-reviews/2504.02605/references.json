{"references": [{"fullname_first_author": "Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-03", "reason": "This paper is highly cited and seminal in the field of evaluating large language models specifically for code-related tasks."}, {"fullname_first_author": "Jimenez", "paper_title": "SWE-bench: Can language models resolve real-world github issues?", "publication_date": "2023-10-06", "reason": "SWE-bench is the primary benchmark that Multi-SWE-bench extends and improves upon, making it a crucial reference point."}, {"fullname_first_author": "Xia", "paper_title": "Agentless: Demystifying llm-based software engineering agents", "publication_date": "2024-07-01", "reason": "Agentless is one of the core methods evaluated within the Multi-SWE-bench framework, making it critical for comparative analysis."}, {"fullname_first_author": "Yang", "paper_title": "SWE-agent: Agent-computer interfaces enable automated software engineering", "publication_date": "2024-05-15", "reason": "SWE-agent is a core method evaluated in the benchmark."}, {"fullname_first_author": "Wang", "paper_title": "OpenHands: An Open Platform for AI Software Developers as Generalist Agents", "publication_date": "2024-07-16", "reason": "OpenHands is a key method evaluated for issue resolution in the Multi-SWE-bench and provides important comparative data."}]}