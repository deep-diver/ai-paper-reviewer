{"importance": "This paper presents Multi-SWE-bench, **a crucial benchmark for multilingual code repair**. It fosters research in code intelligence and automated software engineering with diverse languages and real-world scenarios. It propels forward practical AI development and the development of autonomous intelligent systems.", "summary": "Multi-SWE-bench: A new benchmark to evaluate multilingual issue resolving across diverse software ecosystems, vital for advancing AGI.", "takeaways": ["Multi-SWE-bench enables comprehensive evaluation of LLMs across various programming languages, addressing limitations in existing benchmarks.", "The research identifies key factors influencing model performance, such as issue description length and fix patch characteristics.", "The Multi-SWE-RL community fosters open-source collaboration for scalable RL data creation in software engineering."], "tldr": "Existing benchmarks for issue resolving, like SWE-bench, predominantly focus on Python, neglecting the diversity in software ecosystems. This paper introduces Multi-SWE-bench, a multilingual benchmark encompassing Java, TypeScript, JavaScript, Go, Rust, C, and C++. It contains 1,632 high-quality instances annotated by experts to ensure accurate evaluation. It evaluates the performance of state-of-the-art models and provides a comprehensive analysis of empirical insights. \n\nThe paper launches Multi-SWE-RL, an open-source community, aimed at building RL training datasets for issue-resolving tasks, releasing 4,723 structured instances across seven languages. The goal is to encourage community contributions, and to expand the dataset. The paper envisions Multi-SWE-bench and the Multi-SWE-RL community as catalysts for advancing RL, to bring us closer to AGI. ", "affiliation": "ByteDance", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2504.02605/podcast.wav"}