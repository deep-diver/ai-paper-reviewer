[{"figure_path": "https://arxiv.org/html/2412.14173/x2.png", "caption": "Figure 1: AniDoc colorizes a sequence of sketches based on a character design reference with high fidelity, even when the sketches significantly differ in pose and scale. Additionally, the model supports sparse sketch inputs, enabling effective interpolation and high-quality colorization simultaneously, as shown in the last row.", "description": "AniDoc effectively colorizes a sequence of character sketches, referencing a provided character design image. The model maintains high fidelity even with variations in pose and scale across sketches.  Furthermore, it supports sparse sketch input, facilitating both interpolation and colorization between keyframes, as demonstrated in the last row where only the first and last frame sketches are provided.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2412.14173/x3.png", "caption": "Figure 2: Illustration of the workflow of 2D animation production.", "description": "This figure provides a simplified overview of the 2D animation production workflow.  It starts with *Character Design*, where artists create reference sheets defining the visual appearance of characters.  This is followed by *Keyframe sketches*, which depict the main poses and movements at key moments in a scene. *Inbetweening* follows, where artists draw the intermediate frames between keyframes to create smooth motion and transitions. Finally, *Colorization* is the process of adding color to the line art sketches, bringing the characters and scenes to life.  This workflow is the standard process typically used in the animation industry, and this paper aims to automate the colorization step while keeping quality and consistency with the original character design.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.14173/x4.png", "caption": "Figure 3: Overview of AniDoc pipeline. We adopt a two-stage training strategy. In the dense-sketch training stage, we explicitly extract matching keypoints pairs between the reference image and each frame of the training video, constructing point maps to represent the correspondences. In the sparse-sketch training stage, we remove the intermediate frame sketches and use the matching points from the start and end frames to interpolate point trajectories, which guide the generation of the intermediate frames.", "description": "AniDoc uses a two-stage training approach. The dense-sketch stage extracts matching keypoints between a reference image and each frame of a video, creating point maps to represent correspondences. The sparse-sketch stage removes intermediate sketches and interpolates point trajectories from the start and end frame keypoints, guiding intermediate frame generation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.14173/x5.png", "caption": "Figure 4: Illustration of color leakage issue in non-binarized sketch. For previous video colorization method\u00a0[21], when given non-binarized sketch, even if the reference is an empty image, it can still generate colorized results with similar color pattern to the ground truth. After binarizing the sketch, the colorization results degrade significantly.", "description": "This figure illustrates the color leakage issue present in previous video colorization methods when using non-binarized sketches.  Specifically, using the method from [21], even with an empty reference image, the model can still generate a colorized output that somewhat resembles the ground truth due to the leaked color information within the non-binarized sketch. However, when a binarized sketch (with true black and white values) is used, the output degrades significantly, demonstrating the reliance of previous methods on these leaked color cues.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.14173/x6.png", "caption": "Figure 5: Visual comparison of reference-based colorization with four methods LVCD\u00a0[21], LVCD+IP-Adapter\u00a0[55], ID-animator\u00a0[17], ToonCrafter\u00a0[52].", "description": "This figure presents a visual comparison of our AniDoc method with four other reference-based video colorization methods: LVCD, LVCD combined with IP-Adapter, ID-animator, and ToonCrafter. Two distinct anime sequences are used for this comparison. The first sequence features a character with varying poses against a predominantly dark background. The second sequence showcases a character interacting with a bookshelf. For each sequence, a reference image of the character, along with the input line art sketches, are displayed. The output colorized animations from each method are shown for comparison. It's evident that AniDoc produces results with superior visual quality and better maintains character identity compared to the other methods.  Noticeably, LVCD and ID-animator struggle to accurately colorize, even when using IP-Adapter to color the first frame.  While ToonCrafter enhances temporal smoothness by employing colorized start and end frames with IP-Adapter, its overall color accuracy and character preservation remain inferior to AniDoc.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14173/x7.png", "caption": "Figure 6: Ablations on each component. \u201cw/o matching\u201d indicates without the corresponding matching module, \u201cw/o binarize\u201d indicates without binarization and background augmentation.", "description": "This figure presents an ablation study showcasing the impact of key components in the proposed AniDoc model.  It compares the full model's output with versions where either the correspondence matching module or the binarization/background augmentation is removed.  The reference image is a character portrait. The first row shows the ground truth animation frames.  The second row displays the results from the full AniDoc model. The third row (\"w/o matching\") demonstrates the effect of removing the correspondence matching, resulting in color bleeding and inaccuracies. The fourth row (\"w/o binarize\") shows the outcome of training without binarized sketches and background augmentation, leading to washed-out colors and artifacts.", "section": "4.4. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.14173/x8.png", "caption": "Figure 7: Illustration of the flexible usage of our model. Figure (a) shows the ability of using same reference to colorize different sketches. Figure (b) demonstrates the robustness to different references. Figure (c) shows the sparse-sketch generation results.", "description": "This figure illustrates the flexibility and versatility of the AniDoc model by showcasing its performance under three different scenarios: (a) using the same reference image to colorize a variety of sketches with varying poses, demonstrating consistency in colorization; (b) applying different reference images to the same sketch sequence, demonstrating adaptability to diverse styles while maintaining character identity; and (c) generating consistent and smooth animations using only sparse sketches (start and end frames), reducing the need for detailed in-between drawings.", "section": "4.3. Flexible Usage"}, {"figure_path": "https://arxiv.org/html/2412.14173/x9.png", "caption": "Figure S1: Illustration of reference with different backgrounds.", "description": "This figure shows how the model can generate colorizations of line art with different backgrounds when provided with character reference images featuring those backgrounds. It showcases the model's ability to transfer the style from the reference images to create new backgrounds while maintaining consistency with the character's core visual features, like their expression and clothing.", "section": "A. Reference with Different Background"}, {"figure_path": "https://arxiv.org/html/2412.14173/x10.png", "caption": "Figure S2: Illustration of the multiple characters situation. When the reference image contains multiple characters, our method can correctly infer the correspondence and apply colorization to each character accordingly.", "description": "This figure shows how the model handles multiple characters in a reference image, demonstrating its ability to correctly colorize each character individually even when there are variations in pose, angle, and position between the reference and the line art sketches.", "section": "Appendix"}, {"figure_path": "https://arxiv.org/html/2412.14173/x11.png", "caption": "Figure S3: Impact of different line art extraction methods.", "description": "This figure showcases the impact of employing various line art extraction methods on the final colorization results. Besides the default method used in the paper, three additional methods \u2013 Anime Lineart, HED, and PiDiNet \u2013 were tested. The figure displays the colorization outputs produced by the model when using these different line art extraction techniques as input. Anime Lineart is specifically trained on anime datasets, HED is an edge detection method leading to thick line art, while PiDiNet results in simplistic, near hand-drawn style line art. Post-extraction, the same binarization process described in the main text is applied. Results reveal the method's capacity to effectively colorize across diverse extraction techniques, yet the variations in line art characteristics influence the resultant output.", "section": "Appendix. C. Different Line Art Extraction Methods"}, {"figure_path": "https://arxiv.org/html/2412.14173/x12.png", "caption": "Figure S4: In the early training stage (10k step), the video generation model produces static videos that closely resemble the given reference design.", "description": "During the early stages of training the stable video diffusion (SVD) model, specifically at the 10,000th step, the generated video exhibits a static behavior. The frames within this video closely resemble the provided reference design image, primarily due to SVD's inherent tendency to prioritize information from the input image during its initial training phase.", "section": "Appendix D"}, {"figure_path": "https://arxiv.org/html/2412.14173/x13.png", "caption": "Figure S5: Semantic feature can effectively find matching keypoints between reference color image and binarized sketch.", "description": "During training, the authors use LightGlue with SIFT descriptor for keypoint selection and matching between the reference image and the training video frames. However, during inference, they lack access to the ground truth color image. As techniques that rely on low-level image features like SIFT descriptors are ineffective at accurately matching keypoints between sketches and color reference images due to the significant domain gap between them, the authors use the semantic level keypoint matching method DIFT to establish the correspondence between the color reference image and the sketches, as shown in this figure. In this image, matching keypoints between the reference color image (left) and binarized sketch (right) are visualized. Red lines connecting circular markers on both images indicates matched keypoints. It can be seen that semantic keypoints effectively find correspondences between color and sketch images.", "section": "Appendix E: Illustration of DIFT Matching"}]