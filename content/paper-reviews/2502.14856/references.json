{"references": [{"fullname_first_author": "Rishi Bommasani", "paper_title": "On the opportunities and risks of foundation models", "publication_date": "2021-08-07", "reason": "This paper provides a broad overview of foundation models, which are central to the advancements discussed in the paper, and provides a broad perspective of LLMs."}, {"fullname_first_author": "Wayne Xin Zhao", "paper_title": "A survey of large language models", "publication_date": "2023-03-18", "reason": "This paper is a comprehensive survey of large language models, offering relevant background information on the technology being accelerated in the paper."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "publication_date": "2023-07-09", "reason": "This paper introduces Llama 2, a foundational model used in this work for experimentation and comparison, making it a central reference for understanding the models under discussion."}, {"fullname_first_author": "Yaniv Leviathan", "paper_title": "Fast inference from transformers via speculative decoding", "publication_date": "2023-01-01", "reason": "This paper is one of the earlier works on speculative sampling, the core technique that the current paper aims to optimize and accelerate, making it fundamentally important."}, {"fullname_first_author": "Yuhui Li", "paper_title": "EAGLE-2: Faster inference of language models with dynamic draft trees", "publication_date": "2024-01-01", "reason": "This paper introduces EAGLE-2, which is the current state-of-the-art method that the current paper improves upon, making it a central baseline and comparison point."}]}