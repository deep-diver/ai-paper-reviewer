[{"content": "| Dataset | Type | # Samples | Mask Quality | Video Quality |\n|---|---|---|---|---|\n| YouTubeVOS [44] | Video | 4,453 | High | Low |\n| YouTubeVIS [44] | Video | 2,883 | High | Low |\n| UVO [7] | Video | 10,337 | High | Low |\n| MOSE [6] | Video | 2,149 | High | High |\n| VIPSeg [22] | Video | 3,110 | High | High |\n| VSPW [23] | Video | 3,536 | High | High |\n| SAM2 [28] | Video | 51,000 | High | High |\n| Pexel | Video | 6,000 | Medium | High |\n| MVImgNet [47] | Video | 219,188 | High | High |\n| ViViD [8] | Video | 9,700 | High | High |\n| CHDTF [53] | Video | 362 | High | High |\n| CelebV-HQ [54] | Video | 35,666 | High | High |\n| Pexel | Image | 95,000 | Medium | High |", "caption": "Table 1: Statistics of datasets used for training our VideoAnydoor. \u201cquality\u201d particularly refers to the image resolution.", "description": "This table presents the datasets used to train the VideoAnydoor model.  It details the type of data (video or image), the number of samples available, and the quality of both the mask (object segmentation) and the video/image resolution.  High-quality data indicates higher resolution, while low-quality signifies lower resolution.  This information is crucial because the quality and diversity of the training data significantly affect the performance of the model.", "section": "3. Method"}, {"content": "| Method | PSNR (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | CLIP-Score (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | DINO-Score (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | AJ (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | <img src=\"https://arxiv.org/html/2501.01427/delta_avg_vis.png\"> (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | OA (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) |\n|---|---|---|---|---|---|---|\n| ConsistI2V [29] | 25.1 | 64.7 | 40.6 | 49.3 | 51.1 | 57.2 |\n| I2VAdapter [41] | 24.3 | 67.1 | 42.2 | 51.2 | 53.7 | 59.9 |\n| AnyV2V [17] | 30.1 | 70.2 | 47.2 | 54.1 | 55.8 | 61.1 |\n| ReVideo [24] | 33.5 | 74.2 | 51.7 | 79.2 | 81.4 | 83.2 |\n| VideoAnydoor (ours) | **37.7** | **81.2** | **58.9** | **88.0** | **91.1** | **92.3** |", "caption": "Table 2: Quantitative comparison between our VideoAnydoor and other related work. Six automatic metrics are employed for the performance evaluation of both content and motion. VideoAnydoor outperforms these methods across all the metrics.", "description": "This table presents a quantitative comparison of VideoAnydoor against several other video editing methods.  Six automatic metrics evaluate both the content and motion quality of the video edits.  The metrics show that VideoAnydoor significantly outperforms the other methods in all aspects of video editing.", "section": "4. Experiments"}, {"content": "| Method | Quality (\u2191) | Fidelity (\u2191) | Smooth (\u2191) | Diversity (\u2191) |\n|---|---|---|---|---|\n| ConsistI2V [10] | 1.80 | 1.75 | 2.30 | 1.50 |\n| AnyV2V [17] | 1.90 | 1.85 | 1.50 | 2.10 |\n| ReVideo [24] | 2.65 | 2.55 | 2.50 | 2.25 |\n| VideoAnydoor (ours) | **3.75** | **3.80** | **3.65** | **3.70** |", "caption": "Table 3: User study on the comparison between our VideoAnydoor and existing alternatives.\n\u201cQuality\u201d, \u201cFidelity\u201d, \u201cSmooth\u201d, and \u201cDiversity\u201d measure synthesis quality, object identity preservation, motion consistency, and object local variation, respectively.\nEach metric is rated from 1 (worst) to 4 (best).", "description": "This table presents the results of a user study comparing VideoAnydoor to other video editing methods.  Four aspects of video quality were rated on a scale of 1 to 4 (1 being worst, 4 being best): Quality (overall synthesis quality), Fidelity (how well the object's identity was preserved), Smoothness (motion consistency), and Diversity (variation in the object's appearance).  The comparison helps illustrate VideoAnydoor's strengths in these areas.", "section": "4. Experiments"}, {"content": "| Method | PSNR (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | CLIP-Score (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | DINO-Score (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) |\n|---|---|---|---|\n| Only Real-video Data | 34.4 | 76.4 | 52.0 |\n| Only Static-image Data | 34.1 | 76.2 | 51.2 |\n| FrozenDINOv2 | 33.2 | 74.5 | 51.4 |\n| w/o <img src=\"https://arxiv.org/html/2501.01427/PixelWarper.png\"><sup>\u2020</sup> | 35.1 | 77.0 | 53.1 |\n| w/o Pixel Warper | 33.6 | 72.1 | 48.1 |\n| w/o Re-weighted Loss | 35.1 | 77.0 | 53.1 |\n| Ours-full | **37.7** | **81.1** | **58.9** |", "caption": "Table 4: Quantitative evaluation of core components in VideoAnydoor on ID preservation. \u2020\u2020\\dagger\u2020 denotes removing the semantic points in the key-point image.", "description": "Table 4 presents a quantitative analysis of VideoAnydoor's core components' impact on identity preservation.  The metrics used are PSNR (Peak Signal-to-Noise Ratio), CLIP-Score (a measure of visual similarity using CLIP embeddings), and DINO-Score (another visual similarity metric based on DINOv2). The table compares the full VideoAnydoor model's performance to several ablation studies. These studies examine the effects of removing key components, such as using only real video data during training, using only static image data, excluding the pixel warper module, and removing the reweighted loss function. The \"+ denotes an ablation study where semantic points are removed from the keypoint image. This allows for a clear understanding of each component's individual contribution to maintaining identity in the generated videos.", "section": "4.4. Ablation Studies"}, {"content": "| Method | AJ (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | <img src=\"https://arxiv.org/html/2501.01427/delta_avg_vis.png\"> (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | OA (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) |\n|---|---|---|---|\n| Only Real-video Data | 66.1 | 67.0 | 69.6 |\n| Only Static-image Data | 72.3 | 72.6 | 75.1 |\n| FrozenDINOv2 | 80.1 | 82.2 | 85.1 |\n| w/o <img src=\"https://arxiv.org/html/2501.01427/PixelWarper.png\"><sup>\u2020</sup> | 81.3 | 82.2 | 85.0 |\n| w/o <img src=\"https://arxiv.org/html/2501.01427/PixelWarper.png\"> | 78.3 | 81.7 | 84.0 |\n| w/o Re-weighted Loss | 75.4 | 84.2 | 85.1 |\n| Ours-full | **88.0** | **91.1** | **92.3** |", "caption": "Table 5: Quantitative evaluation of core components in VideoAnydoor on motion consistency. \u2020\u2020\\dagger\u2020 denotes removing the semantic points in the key-point image.", "description": "This table presents a quantitative analysis of VideoAnydoor's core components' impact on motion consistency.  It compares the performance of the full model against versions where key components are removed or modified.  Specifically, it shows how metrics such as Average Jaccard (AJ), Structural Similarity Index (SSIM), and Overall Accuracy (OA) change when elements like the pixel warper or the re-weighted loss are excluded.  The impact of training only on real videos versus a combination of real and static images is also examined.  The symbol \u2020 indicates the removal of semantic points from the keypoint image. The results illustrate the contribution of each component in achieving high motion consistency in video object insertion.", "section": "4.4 Ablation Studies"}, {"content": "| Method | AJ (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | <math display=\"inline\">{\">{delta}_{avg}^{vis}</math> (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) | OA (<img src=\"https://arxiv.org/html/2501.01427/uparrow.png\">) |\n|---|---|---|---| \n| Random X-Pose points | 80.4 | 82.2 | 82.8 |\n| Grid points | 82.6 | 83.7 | 85.2 |\n| w/o NMS | 82.3 | 83.1 | 84.6 |\n| Tight box | 83.2 | 85.4 | 86.1 |\n| Ours-full | **88.0** | **91.1** | **92.3** |", "caption": "Table 6: Detailed quantitative evaluation of the pixel warper in VideoAnydoor on motion consistency. \u201cTight box\u201d denotes training with tightly-surrounded boxes.", "description": "This table presents a detailed quantitative analysis of the pixel warper component within the VideoAnydoor model, focusing specifically on its impact on motion consistency during video generation.  The results are based on several different training configurations.  The key metrics evaluated are the average Jaccard index (AJ), the average Structural Similarity Index (SSIM), and the overall accuracy (OA), all commonly used to measure video quality and motion fidelity. The different training conditions assessed include: using randomly sampled X-Pose points for motion trajectory generation, using points on a grid, training without non-maximum suppression (NMS) of densely clustered points, and training with tightly-surrounded bounding boxes around objects.  The 'Ours-full' row indicates the performance with the full, optimized VideoAnydoor setup. The analysis helps to determine the relative effectiveness of various training and keypoint selection techniques in achieving optimal motion consistency in the video object insertion task.", "section": "4.4 Ablation Studies"}]