[{"figure_path": "https://arxiv.org/html/2502.13946/x1.png", "caption": "Figure 1: LLMs may inadvertently anchor their safety mechanisms to the template region: safety-related decision-making overly relies on the aggregated information (e.g., harmfulness of input) from that region, potentially causing vulnerabilities.", "description": "The figure illustrates a common architecture of Large Language Models (LLMs) that incorporate a fixed template between the input instruction and the model's initial output.  The safety mechanism of the LLM relies heavily on the information within this template region to determine whether the input is safe or harmful.  This dependency is a vulnerability because adversarial attacks can manipulate the information in the template to bypass the safety mechanisms and elicit unsafe outputs, even if the original instruction is harmless.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.13946/x2.png", "caption": "Figure 2: Chat template from Llama-3-Instruct series.", "description": "This figure shows the chat template used in the Llama-3-Instruct series of large language models.  The template demonstrates the structure of the input and output.  It shows how the user's instruction is separated from the model's response by special tokens, indicating the beginning and end of user input and the beginning of the model's response. This structure is crucial to how the model processes information and generates responses.", "section": "3.1 Preliminaries"}, {"figure_path": "https://arxiv.org/html/2502.13946/x7.png", "caption": "Figure 3: Left: Attention distributions across different LLMs demonstrate that their attentions shift systematically from the instruction to the template region when processing harmful inputs. Right: Attention heatmaps (17th-layer, 21st-head) from Llama-3-8B-Instruct consistently illustrate this distinct pattern.", "description": "This figure shows how the attention mechanism in LLMs shifts when processing harmful versus harmless inputs.  The left side presents histograms for multiple LLMs.  Each histogram displays the distribution of attention weights across the instruction and template regions for both harmful and harmless inputs. It shows a consistent pattern across different LLMs: a shift in attention from the instruction region to the template region when processing harmful requests. The right side provides a visual illustration with attention heatmaps from a specific layer and head (17th layer, 21st head) within Llama-3-8B-Instruct. The heatmaps visually confirm the attention shift from instruction to template for harmful inputs.", "section": "3.2 Attention Shifts to The Template Region"}, {"figure_path": "https://arxiv.org/html/2502.13946/x8.png", "caption": "Figure 4: Left: Illustration of the activation patching process from harmless to harmful inputs. Right: Normalized indirect effects when patching activations are from two different regions (instruction v.s. template) across various LLMs, revealing that these models\u2019 safety functions are primarily anchored in the template regions.", "description": "Figure 4 illustrates the process of activation patching, a technique used to assess the causal influence of specific neuron activations on model behavior. The left panel visually depicts this process for two inputs: one harmless and one harmful. The right panel presents the results of applying this technique across various LLMs, focusing on two distinct input regions: the instruction region and the template region.  The normalized indirect effects clearly show a much stronger influence of the template region on the LLMs' safety decisions, highlighting a key vulnerability where safety mechanisms are overly reliant on this region.", "section": "3.3 Causal Role of The Template Region"}, {"figure_path": "https://arxiv.org/html/2502.13946/x9.png", "caption": "Figure 5: Performance of different attack methods. Surprisingly, simply intervening information from the template region (i.e., TempPatch) can significantly increase attack success rates.", "description": "This figure compares the success rates of various attack methods against language models.  The attacks aim to make the model generate unsafe responses, such as instructions to build a bomb or bully a child. The methods include established techniques (AIM, PAIR, AmpleGCG) and a novel method called TempPatch, which specifically targets the template region in the model input. The results show that TempPatch, which involves directly manipulating the template region, is surprisingly effective at bypassing safety mechanisms and causing the model to generate unsafe responses, highlighting the vulnerability of relying on template information for safety in language models.", "section": "4 How Does TASA Cause Inference-time Vulnerabilities of LLMs"}, {"figure_path": "https://arxiv.org/html/2502.13946/x10.png", "caption": "Figure 6: Probed harmful rates in the residual streams across layers and template positions (from the 5th to the 1st closest to the ending position) of Llama-3-8B-Instruct. The background intensity reflects the importance of each layer\u2019s states for safety-related decisions, as aligned with Figure\u00a010.", "description": "Figure 6 presents a detailed analysis of the Llama-3-8B-Instruct model's safety mechanisms by visualizing the \"harmful rates\" across different layers and positions within the template region.  The harmful rate indicates the likelihood of a given intermediate representation being classified as harmful based on a trained probe. The x-axis represents the layer of the model, and each sub-plot shows the harmful rates for various positions in the template, moving from positions furthest from the end of the template (5th) to closest (1st).  The y-axis represents the harmful rate, and the background color intensity of each cell corresponds to the importance of that layer's state in safety-related decisions (as further detailed in Figure 10). The figure reveals patterns in how harmfulness information is processed within the template region, differentiating between successful and failed jailbreak attacks.", "section": "4 How Does TASA Cause Inference-time Vulnerabilities of LLMs"}, {"figure_path": "https://arxiv.org/html/2502.13946/x11.png", "caption": "Figure 7: Harmful probes from middle layers (i.e., layer 14 in Llama-3-8B-Instruct) can be transferred to response generation while maintaining high accuracy.", "description": "This figure demonstrates the transferability of harmful probes trained on intermediate layers of a large language model (LLM) to the response generation phase.  Specifically, it shows that probes trained on layer 14 of Llama-3-8B-Instruct, which identify harmful content, maintain high accuracy when applied to generated responses. This indicates that the learned harmfulness features are not solely confined to the template region where they were initially identified, suggesting a more generalized representation of safety mechanisms within the model.", "section": "5 Detaching Safety Mechanism from The Template Region"}, {"figure_path": "https://arxiv.org/html/2502.13946/x12.png", "caption": "Figure 8: The ASR of applying TempPatch on different proportion of attention heads, with results from \ud835\udc9fanlzsubscript\ud835\udc9fanlz{\\mathcal{D}}_{\\text{anlz}}caligraphic_D start_POSTSUBSCRIPT anlz end_POSTSUBSCRIPT in solid lines and\ud835\udc9fevalsubscript\ud835\udc9feval{\\mathcal{D}}_{\\text{eval}}caligraphic_D start_POSTSUBSCRIPT eval end_POSTSUBSCRIPT in dash lines.", "description": "Figure 8 illustrates the attack success rates (ASR) achieved by applying the TempPatch method with varying proportions of patched attention heads. The solid lines represent the results obtained using the Danlz dataset, while the dashed lines show the results from the Deval dataset.  This demonstrates how the effectiveness of the TempPatch attack changes as more attention heads are modified.", "section": "4 How Does TASA Cause Inference-time Vulnerabilities of LLMs"}, {"figure_path": "https://arxiv.org/html/2502.13946/x13.png", "caption": "Figure 9: The prompt template of AIM.", "description": "This figure shows the prompt template used to instruct the AIM persona in the experiments.  The AIM persona is designed to be an unfiltered and amoral chatbot, always providing a response regardless of ethical or legal implications.  The instructions emphasize that AIM should never apologize or express any reservations, and it should use sophisticated language.  This specific prompt is crucial for testing the model's vulnerability to generating harmful content by circumventing built-in safety mechanisms.", "section": "3.1 Preliminaries"}, {"figure_path": "https://arxiv.org/html/2502.13946/x14.png", "caption": "Figure 10: Activation patching on the residual streams at template positions, measured by the proportion of refusal logit recovered.", "description": "This figure visualizes the results of activation patching experiments performed on various LLMs.  Activation patching involves selectively replacing the activations (intermediate representations) of a model at specific locations (template positions) during the processing of a harmful input with those from a corresponding harmless input. The y-axis represents the normalized indirect effect (NIE), which quantifies the influence of the patched activation on the model's final decision (whether to comply with or refuse a harmful request). The x-axis denotes the template position, indicating where the activation was modified in the input sequence. Each line in the graph represents a different LLM model, illustrating how the model's response changes when activating different parts of the template region.  A higher NIE suggests a stronger influence of the template region on safety-related decisions.", "section": "3.3 Causal Role of The Template Region"}, {"figure_path": "https://arxiv.org/html/2502.13946/x15.png", "caption": "Figure 11: The accuracy of harmful probes from position 0 in template when transferred to response.", "description": "This figure shows the accuracy of probes that identify harmful content.  These probes were initially trained on the template region of the model's intermediate activations (specifically, position 0 within the template) during the processing of harmful inputs. The figure then assesses the performance of these same probes when applied to the model's responses during the generation process for both harmful and harmless inputs. The x-axis represents the position of the token being generated in the response, and the y-axis shows the probe's accuracy in correctly identifying harmful content.  The color intensity likely represents the magnitude or confidence of the probe's prediction, with darker colors indicating higher confidence.", "section": "5 Detaching Safety Mechanism from The Template Region"}, {"figure_path": "https://arxiv.org/html/2502.13946/x16.png", "caption": "Figure 12: The accuracy of harmful probes from position 1 in template when transferred to response.", "description": "This figure displays the accuracy of probes that detect harmful content.  These probes were initially trained on intermediate activations from the template region (specifically, position 1) during the model's processing of harmful inputs. The figure shows how well these same probes can identify harmful content in the model's *responses*.  The x-axis represents the token position within the generated response, and the y-axis shows the layer of the model. The color intensity of each cell reflects the probe's accuracy at that specific layer and token position, with darker shades indicating higher accuracy.", "section": "5 Detaching Safety Mechanism from The Template Region"}]