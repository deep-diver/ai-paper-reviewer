[{"figure_path": "https://arxiv.org/html/2504.05262/x1.png", "caption": "Figure 1: Illustration of Evaluating LLMs\u2019 Arithmetic Comprehension.\nWhile LLMs demonstrate impressive performance on complex mathematical benchmarks, we examine their fundamental understanding through elementary addition, focusing on two essential arithmetic properties: (1) Commutativity (A+B=B+A\ud835\udc34\ud835\udc35\ud835\udc35\ud835\udc34A+B=B+Aitalic_A + italic_B = italic_B + italic_A) and (2) Compositional Generalization (invariance under symbolic transformations).\nOur systematic evaluation in Section\u00a04 reveals that models fail to maintain these basic properties, suggesting they rely on pattern matching rather than demonstrating genuine arithmetic comprehension.", "description": "This figure illustrates how the authors evaluated the true arithmetic comprehension of large language models (LLMs).  While LLMs often perform well on complex math problems, this work probed their understanding of basic addition.  The evaluation focused on two key properties: commutativity (whether A+B equals B+A) and compositional generalization (whether the LLMs can correctly perform the same operation even if the numbers are represented by symbols instead of digits).  The results (detailed in Section 4) showed that LLMs struggled to consistently demonstrate these basic arithmetic properties, suggesting they rely more on pattern-matching than on true understanding of mathematical principles.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.05262/x2.png", "caption": "Figure 2: Performance Degradation Patterns in Zero-shot vs. Symbolic Addition. While LLMs achieve high accuracy on standard numerical addition (left), their non-monotonic performance curve suggests brittle pattern matching rather than true algorithmic reasoning. In contrast, symbolic addition tests (right) reveal systematic degradation with increasing digit count. This stark contrast between numerical and symbolic performance suggests LLMs rely heavily on memorized patterns rather than learned arithmetic principles.", "description": "Figure 2 presents a comparison of Large Language Model (LLM) performance on two versions of a simple arithmetic task: standard numerical addition and symbolic addition.  The left panel shows that while LLMs achieve high accuracy on numerical addition (adding numbers like 12 + 34), their performance is not consistent across all problems of the same complexity. The inconsistent performance, which increases and then decreases with digit count, suggests that the models may be relying on pattern matching, rather than a true understanding of arithmetic principles. The right panel demonstrates that when numbers are replaced with symbols (e.g., 1 becomes 'A', 2 becomes 'B'), the models show a steady decrease in accuracy as the number of digits increases. This indicates that these models heavily rely on memorized patterns of numerical digits to perform the task, rather than a generalized understanding of arithmetic operations.", "section": "Findings"}, {"figure_path": "https://arxiv.org/html/2504.05262/extracted/6342656/figures/few-shot-123-stage2.png", "caption": "Figure 3: Few-Shot Performance with Explicit Rule Provision. Explicit rule provision leads to a significant drop in performance compared to zero-shot, contradicting the expected improvement.", "description": "This figure displays the performance of various LLMs on an elementary addition task under different conditions.  The x-axis represents the number of digits in the addition problem. The y-axis represents accuracy.  Multiple lines are plotted, showing the zero-shot performance (no explicit rules given) and the few-shot performance (with explicit rules provided) for several different LLMs. The figure demonstrates that providing explicit rules for addition surprisingly reduces the model's performance, in contrast to what might be expected. This is consistent across multiple LLMs, highlighting that simply providing rules may not be sufficient for effective mathematical reasoning in LLMs.", "section": "4.3.1 Explicit Rule Provision"}]