{"importance": "This paper unveils that LLMs struggle with truly understanding elementary addition, relying on pattern matching rather than grasping core principles. **The work introduces a novel methodology for controlled arithmetic experiments, revealing key differences between LLM computation and human mathematical cognition.** These findings open new avenues for designing effective benchmarks that can better assess genuine mathematical reasoning in AI systems.", "summary": "LLMs may ace benchmarks, but do they truly grasp elementary addition? This paper probes the depths of their arithmetic understanding.", "takeaways": ["LLMs struggle to generalize addition rules, especially when using symbolic representations instead of standard digits.", "LLMs often violate fundamental mathematical properties like commutativity, indicating a lack of true understanding.", "Providing LLMs with explicit addition rules can actually worsen their performance, suggesting a conflict between external instructions and internalized knowledge."], "tldr": "Large Language Models (LLMs) have shown remarkable performance on complex benchmarks, but they often fail at simple problems. This raises the question: do LLMs learn math principles or memorize patterns?  Current benchmarks might not effectively distinguish between genuine rule comprehension and pattern matching, potentially overestimating LLMs' true mathematical capabilities. This paper investigates this by focusing on elementary two-integer addition (0 to 264), a task with a clear algorithmic structure. \n\nInstead of designing more complex benchmarks, this research uses two properties to test the addition skill of LLMs: commutativity(A+B=B+A) and compositional generalization (symbol mapping) when they are asked to do the addition. The results indicate that current LLMs primarily depend on memory pattern instead of learning the principle, indicating architectural limits.", "affiliation": "Zhejiang University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.05262/podcast.wav"}