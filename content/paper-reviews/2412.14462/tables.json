[{"content": "| Dataset | Sample No. | Category No. |\n|---|---|---|\n| DreamEditBench | 440 | 22 |\n| MureCom | 640 | 32 |\n| **SAM-FB (Ours)** | 3,160,403 | 3,439 |", "caption": "Table 1: Dataset comparison. Our dataset contains significantly more training samples and object categories.", "description": "This table compares the number of training samples and object categories across three datasets used for image composition tasks: DreamEditBench, MureCom, and SAM-FB (the authors' dataset).  It highlights that SAM-FB is significantly larger than the other two datasets, containing over 3 million samples and a substantially greater number of object categories.  This larger scale is a key advantage of SAM-FB for training models capable of robust and generalized image composition.", "section": "3. Dataset"}, {"content": "| Method | FID \u2193 (mask) | FID \u2193 (bbox) | CLIP Score \u2191 (mask) | CLIP Score \u2191 (bbox) | MSE \u2193 (mask) | MSE \u2193 (bbox) |\n|---|---|---|---|---|---|---|\n| <img src=\"https://arxiv.org/html/2412.14462/filename.png\" alt=\"\" width=\"10.22\" height=\"10.22\"> [39] | 15.41 | 15.47 | 0.7079 | 0.8058 | 860 | 883 |\n| <img src=\"https://arxiv.org/html/2412.14462/filename.png\" alt=\"\" width=\"10.22\" height=\"10.22\"> [47] | 33.68 | 24.59 | \u2013 | 0.7664 | 2373 | 1615 |\n| <img src=\"https://arxiv.org/html/2412.14462/filename.png\" alt=\"\" width=\"10.22\" height=\"10.22\"> [29] | \u2013 | 14.21 | \u2013 | 0.7944 | \u2013 | 830 |\n| <img src=\"https://arxiv.org/html/2412.14462/filename.png\" alt=\"\" width=\"10.22\" height=\"10.22\"> [24] | 14.49 | 14.42 | 0.8014 | 0.8637 | 857 | 845 |\n| **Ours** | **13.53** | **13.60** | **0.8727** | **0.8658** | **760** | **775** |", "caption": "Table 2: Method comparisons on the SAM-FB test set. \n\u00a0\u2006 Stable Diffusion, \n\u00a0\u2006 PBE, \n\u00a0\u2006GLIGEN, \n\u00a0\u2006 Human Affordance.", "description": "Table 2 presents a quantitative comparison of different image composition methods on the SAM-FB test set.  It shows the FID (Fr\u00e9chet Inception Distance) scores and CLIP (Contrastive Language\u2013Image Pre-training) scores for four existing methods (Stable Diffusion, PBE, GLI-GEN, Human Affordance) and the proposed method (MADD). Lower FID scores indicate better image quality, and higher CLIP scores indicate better semantic similarity between generated and reference images. The table demonstrates the superior performance of MADD in both image quality and semantic consistency compared to the state-of-the-art approaches.", "section": "5.1 Results on the SAM-FB Test Set"}, {"content": "| Prompt | Mask | Bbox | Point | Null | Avg. |\n|---|---|---|---|---|---| \n| FID | **13.53** | 13.60 | 13.66 | 13.96 | 13.69 |\n| MSE | **760** | 775 | 772 | 860 | 792 |\n| CLIP Score | **0.8727** | 0.8658 | 0.8567 | 0.8034 | 0.8415 |", "caption": "Table 3: Comparison of position prompts on the SAM-FB test set.", "description": "This table presents a quantitative comparison of the model's performance using different types of position prompts (mask, bounding box, point, and null) for affordance-aware object insertion.  The evaluation metrics include FID score (Frechet Inception Distance), a measure of image quality, and CLIP score, which assesses the semantic similarity between the generated image and the reference image. The table shows the average performance across all prompts and the individual results for each prompt type. Lower FID scores and higher CLIP scores indicate better performance.", "section": "5.1 Results on the SAM-FB Test Set"}, {"content": "| Method | FID (<img src=\"https://arxiv.org/html/2412.14462/downarrow.png\" alt=\"\u2193\">) | CLIP<sub>x100</sub> (<img src=\"https://arxiv.org/html/2412.14462/uparrow.png\" alt=\"\u2191\">) |\n|---|---|---|\n| Baseline | 25.89 | 89.12 |\n| + Classifier-Free | 21.93 | 91.13 |\n| + Dual diffusion | 21.75 | 91.57 |\n| + Expertise branch | **21.55** | **91.68** |", "caption": "Table 4: Ablation study on the SAM-FB test set with 128\u00d7128128128128\\times 128128 \u00d7 128 resolution using mask prompts.", "description": "This table presents the results of an ablation study conducted on the SAM-FB test set using mask prompts.  The study evaluates the impact of different model components on the performance metrics, specifically FID and CLIP scores. By progressively adding components (Classifier-Free Guidance, dual diffusion, and expert branches), the study demonstrates how each contributes to improved performance in the task of affordance-aware object insertion.", "section": "4. Experiments"}, {"content": "| Filter condition | Threshold | Reserved Percentage |\n|---|---|---|\n| None (Initial) | \u2013 | 100% |\n| Relative Size | [0.1, 0.75] | 7.10% |\n| Aspect Ratio | \u2264 3 | 6.88% |\n| Components Num. | \u2264 4 | 6.71% |\n| Color Std. | \u2265 45 | 1.69% |\n| ResNet50 Score | \u2265 0.7 | 0.25% |", "caption": "Table 5: Reserved percentage for foreground quality control filters. We combine different rule-based and learning-based conditions. Through this process, foreground objects with high quality are reserved.", "description": "This table presents the thresholds and resulting percentages of foreground objects retained after applying various quality control filters during the creation of the SAM-FB dataset.  These filters combine rule-based criteria (relative size, aspect ratio, number of components, color standard deviation) with a learning-based approach (ResNet50 score). The goal is to automatically select high-quality foreground objects, removing those with issues such as incomplete segmentation, blurry details, or unwanted background elements. The table shows that applying these filters reduces the initial set of foreground masks by over 99%, resulting in a highly refined dataset of only 0.25% of the initial candidates.", "section": "3. Dataset"}, {"content": "| Method | mask | bbox | point | null | Avg. |  | mask | bbox | point | null | Avg. |\n|---|---|---|---|---|---|---|---|---|---|---|---| \n| **FID\u2193** |  |  |  |  |  |  | **CLIP Score<sub>\u00d7100</sub>\u2191** |  |  |  |  |\n| Baseline | 25.89 | 26.21 | 26.37 | 27.35 | 26.46 |  | 89.12 | 89.50 | 79.92 | 79.31 | 84.46 |\n| +Classifier-Free | 21.93 | 22.03 | 22.31 | 22.74 | 22.25 |  | 91.13 | 90.95 | 85.49 | 85.26 | 88.21 |\n| +Dual Diffusion | 21.75 | 21.81 | 21.90 | 22.39 | 21.96 |  | 91.57 | **91.05** | 88.25 | **88.34** | **89.80** |\n| +Expertise branch | **21.55** | **21.66** | **21.76** | **22.24** | **21.80** |  | **91.68** | 90.96 | **89.61** | 88.30 | **90.14** |", "caption": "Table 6: Experimental results on SAM-FB test set. The difference between the four kinds of prompts indicates that the performance will be better with a more precise position prompt.", "description": "Table 6 presents a quantitative comparison of the Mask-Aware Dual Diffusion (MADD) model's performance on the SAM-FB test set across different types of position prompts. It shows the FID (Fr\u00e9chet Inception Distance) scores and CLIP (Contrastive Language\u2013Image Pre-training) scores for each prompt type: mask, bounding box, point, and null. Lower FID scores indicate better image quality, and higher CLIP scores suggest better semantic similarity between generated images and ground truth. The results reveal that using more precise position prompts (mask > bounding box > point) leads to better performance.", "section": "5.1 Results on the SAM-FB Test Set"}]