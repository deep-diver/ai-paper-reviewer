[{"content": "| Dataset | Train Size | Validation Size | Test Size |\n|---|---|---|---|\n| User-Product Review | 20,000 | 2,500 | 2,500 |\n| Multilingual Product Review | 20,000 | 2,500 | 2,500 |\n| Stylized Feedback | 20,000 | 2,500 | 2,500 |\n| Hotel Experiences | 9,000 | 2,500 | 2,500 |", "caption": "Table 1: Data Statistics for PGraph Benchmark. The table reports the average input length and average output length in words (done for the test set on GPT-4o-mini on BM25 back on all methods). The average profile size for each task is by user review size.", "description": "Table 1 presents a statistical overview of the Personalized Graph-based Benchmark for Text Generation.  It details the average input and output lengths (in words) for each of the 12 tasks within the benchmark. These averages were calculated using the GPT-40-mini model and the BM25 retrieval method applied to the test set.  Additionally, the table provides the average size of the user profiles (measured by the number of user reviews) used in each task. This information is crucial for understanding the characteristics and scale of the data used to evaluate personalized text generation models.", "section": "2 Personalized Graph-based Benchmark for LLMs"}, {"content": "| Long Text Generation | Metric | PGraphRAG | PGraphRAG\n(Neighbors Only) | PGraphRAG\n(User Only) |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | 0.173 | **0.177** | 0.168 |\n|  | ROUGE-L | 0.124 | **0.127** | 0.125 |\n|  | METEOR | 0.150 | **0.154** | 0.134 |\n| Task 2: Hotel Experience Generation | ROUGE-1 | 0.263 | **0.272** | 0.197 |\n|  | ROUGE-L | 0.156 | **0.162** | 0.128 |\n|  | METEOR | 0.191 | **0.195** | 0.121 |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | **0.226** | 0.222 | 0.181 |\n|  | ROUGE-L | **0.171** | 0.165 | 0.134 |\n|  | METEOR | **0.192** | 0.186 | 0.147 |\n| Task 4: Multi-lingual Review Generation | ROUGE-1 | **0.174** | 0.172 | **0.174** |\n|  | ROUGE-L | 0.139 | 0.137 | **0.141** |\n|  | METEOR | **0.133** | 0.126 | 0.125 |", "caption": "Table 2: Graph statistics for the datasets used in the personalized tasks. The table provides the number of users, items, edges (reviews), and the average degree for each dataset: User-Product Graph, Multilingual Product Graph, Stylized Feedback Graph, and Hotel Experiences Graph.", "description": "This table presents a summary of the key statistics for the four datasets employed in the personalized text generation tasks within the Personalized Graph-based Benchmark for LLMs.  For each dataset (User-Product Graph, Multilingual Product Graph, Stylized Feedback Graph, and Hotel Experiences Graph), it provides the total number of users, the number of items reviewed or mentioned, the total number of edges or reviews, and the average degree of the graph.  The average degree metric indicates the average number of connections each node (user or item) has in the graph.", "section": "2 Personalized Graph-based Benchmark for LLMs"}, {"content": "| Long Text Generation | Metric | PGraphRAG | PGraphRAG\n(Neighbors Only) | PGraphRAG\n(User Only) |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | **0.186** | 0.185 | 0.169 |\n|  | ROUGE-L | **0.126** | 0.125 | 0.114 |\n|  | METEOR | **0.187** | 0.185 | 0.170 |\n| Task 2: Hotel Experience Generation | ROUGE-1 | 0.265 | **0.268** | 0.217 |\n|  | ROUGE-L | 0.152 | **0.153** | 0.132 |\n|  | METEOR | 0.206 | **0.209** | 0.161 |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | **0.205** | 0.204 | 0.178 |\n|  | ROUGE-L | **0.139** | 0.138 | 0.121 |\n|  | METEOR | **0.203** | 0.198 | 0.178 |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | **0.191** | 0.190 | 0.164 |\n|  | ROUGE-L | **0.142** | 0.140 | 0.123 |\n|  | METEOR | **0.173** | 0.169 | 0.155 |", "caption": "Table 3: Dataset split sizes for training, validation, and testing across four datasets: User-Product Review, Multilingual Product Review, Stylized Feedback, and Hotel Experiences.", "description": "This table shows the sizes of the training, validation, and test sets for four different datasets used in the Personalized Graph-based Benchmark for Text Generation.  Each dataset represents a different type of user-generated text: User-Product Reviews, Multilingual Product Reviews, Stylized Feedback, and Hotel Experiences. The split sizes ensure that each user's review history is contained within only one set (train, validation, or test).  This design is crucial for evaluating personalized models effectively, as it prevents data leakage and ensures a fair comparison of model performance.", "section": "3 Dataset Splits"}, {"content": "| Long Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | 0.160 | 0.169 | **0.173** |\n|  | ROUGE-L | 0.121 | **0.125** | 0.124 |\n|  | METEOR | 0.125 | 0.138 | **0.150** |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.230 | 0.251 | **0.263** |\n|  | ROUGE-L | 0.141 | 0.151 | **0.156** |\n|  | METEOR | 0.152 | 0.174 | **0.191** |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.200 | 0.214 | **0.226** |\n|  | ROUGE-L | 0.158 | 0.165 | **0.171** |\n|  | METEOR | 0.154 | 0.171 | **0.192** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.163 | 0.169 | **0.174** |\n|  | ROUGE-L | 0.134 | 0.137 | **0.139** |\n|  | METEOR | 0.113 | 0.122 | **0.133** |", "caption": "Table 4: Zero-shot test set results for long text generation using LLaMA-3.1-8B. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "This table presents the results of a zero-shot evaluation of long text generation models using the LLaMA-3.1-8B model.  The performance is measured using ROUGE-1, ROUGE-L, and METEOR metrics across four different long text generation tasks. The best-performing retriever (BM25 or Contriever) and the optimal number of retrieved items (k) were determined through a prior validation process.  The table shows the scores for each metric and task, allowing for a comparison of the model's performance in various scenarios.", "section": "5.1 Baseline Comparison"}, {"content": "| Long Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 1: User-Product Review Generation | ROUGE-1 | 0.176 | 0.184 | **0.186** |\n|  | ROUGE-L | 0.121 | 0.125 | **0.126** |\n|  | METEOR | 0.168 | 0.180 | **0.187** |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.250 | 0.260 | **0.265** |\n|  | ROUGE-L | 0.146 | 0.150 | **0.152** |\n|  | METEOR | 0.188 | 0.198 | **0.206** |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.196 | 0.200 | **0.205** |\n|  | ROUGE-L | 0.136 | 0.136 | **0.139** |\n|  | METEOR | 0.186 | 0.192 | **0.203** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.163 | 0.169 | **0.174** |\n|  | ROUGE-L | 0.134 | 0.137 | **0.139** |\n|  | METEOR | 0.113 | 0.122 | **0.133** |", "caption": "Table 5: Zero-shot test set results for long text generation using GPT-4o-mini. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "This table presents the results of zero-shot testing on long text generation tasks using the GPT-40-mini language model.  The performance is measured across four different tasks: User Product Review Generation, Hotel Experiences Generation, Stylized Feedback Generation, and Multilingual Product Review Generation.  For each task, multiple metrics (ROUGE-1, ROUGE-L, and METEOR) are reported. The model's performance is determined without any fine-tuning on the test data. The best performing retriever (BM25 or Contriever) and the optimal number of retrieved items (k) were selected based on the validation set's performance.", "section": "5 Experiments"}, {"content": "| Short Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 5: User Product Review Title Generation | ROUGE-1 | **0.128** | 0.123 | 0.125 |\n|  | ROUGE-L | **0.121** | 0.118 | 0.119 |\n|  | METEOR | **0.123** | 0.118 | 0.117 |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | **0.122** | 0.121 | 0.121 |\n|  | ROUGE-L | 0.112 | **0.114** | 0.113 |\n|  | METEOR | **0.104** | 0.102 | 0.099 |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.129 | **0.132** | **0.132** |\n|  | ROUGE-L | 0.124 | 0.126 | **0.128** |\n|  | METEOR | 0.129 | **0.130** | 0.129 |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | 0.129 | 0.126 | **0.131** |\n|  | ROUGE-L | 0.120 | 0.119 | **0.123** |\n|  | METEOR | 0.117 | 0.116 | **0.118** |", "caption": "Table 6: Zero-shot test set results for short text generation using LLaMA-3.1-8B. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "This table presents the zero-shot test results for short text generation tasks using the LLaMA-3.1-8B language model.  It shows the performance of the PGraphRAG model, along with several baseline methods (LaMP, No-Retrieval, and Random-Retrieval), across various metrics (ROUGE-1, ROUGE-L, and METEOR).  The retriever and the number of retrieved items (k) used in PGraphRAG were optimized based on the validation set results. The table provides a comprehensive evaluation of the model's ability to generate short personalized texts. ", "section": "5.1 Baseline Comparison"}, {"content": "| Short Text Generation | Metric | k=1 | k=2 | k=4 |\n|---|---|---|---|---|\n| Task 5: User Product Review Title Generation | ROUGE-1 | 0.111 | 0.110 | **0.111** |\n|  | ROUGE-L | **0.106** | 0.105 | **0.106** |\n|  | METEOR | 0.093 | 0.094 | **0.097** |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | 0.114 | 0.114 | **0.118** |\n|  | ROUGE-L | 0.109 | 0.109 | **0.112** |\n|  | METEOR | 0.082 | 0.082 | **0.085** |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.100 | 0.103 | **0.109** |\n|  | ROUGE-L | 0.098 | 0.101 | **0.107** |\n|  | METEOR | 0.087 | 0.090 | **0.096** |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | 0.104 | 0.104 | **0.108** |\n|  | ROUGE-L | 0.098 | 0.098 | **0.104** |\n|  | METEOR | 0.077 | 0.078 | **0.082** |", "caption": "Table 7: Zero-shot test set results for short text generation using GPT-4o-mini. The choice of retriever and k\ud835\udc58kitalic_k were tuned using the validation set.", "description": "This table presents the results of zero-shot testing on short-text generation tasks using the GPT-40-mini language model.  The performance metrics (ROUGE-1, ROUGE-L, METEOR) are shown for four different tasks: User Product Review Title Generation, Hotel Experience Summary Generation, Stylized Feedback Title Generation, and Multi-lingual Product Review Title Generation.  The model's performance is evaluated without any fine-tuning on the test set; the best-performing retriever (BM25 or Contriever) and optimal number of retrieved items (k) were determined using the validation set. This allows for a comparison of the model's ability to generate short-text outputs in various scenarios, based solely on the provided input and user profile information.", "section": "5.1 Baseline Comparison"}, {"content": "| Long Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---| \n| Task 1: User-Product Review Generation | ROUGE-1 | 0.172 | **0.173** |\n|  | ROUGE-L | 0.122 | **0.124** |\n|  | METEOR | **0.153** | 0.150 |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.262 | **0.263** |\n|  | ROUGE-L | 0.155 | **0.156** |\n|  | METEOR | 0.190 | **0.191** |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.195 | **0.226** |\n|  | ROUGE-L | 0.138 | **0.171** |\n|  | METEOR | 0.180 | **0.192** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.172 | **0.174** |\n|  | ROUGE-L | 0.134 | **0.139** |\n|  | METEOR | **0.135** | 0.133 |", "caption": "Table 8: Zero-shot test set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for LLaMA-3.1-8B-Instruct .", "description": "This table presents the performance of the LLaMA-3.1-8B-Instruct language model on four ordinal classification tasks (Tasks 9-12 from the paper's benchmark). The model's performance is evaluated using two metrics: Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).  Lower MAE and RMSE values indicate better performance.  The results are for a zero-shot setting, meaning the model was not fine-tuned for these specific tasks, and BM25 was used as the retrieval method.", "section": "5.1 Baseline Comparison"}, {"content": "| Long Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---| \n| Task 1: User-Product Review Generation | ROUGE-1 | 0.182 | **0.186** |\n|  | ROUGE-L | 0.122 | **0.126** |\n|  | METEOR | 0.184 | **0.187** |\n| Task 2: Hotel Experiences Generation | ROUGE-1 | 0.264 | **0.265** |\n|  | ROUGE-L | **0.152** | **0.152** |\n|  | METEOR | **0.207** | 0.206 |\n| Task 3: Stylized Feedback Generation | ROUGE-1 | 0.194 | **0.205** |\n|  | ROUGE-L | 0.128 | **0.139** |\n|  | METEOR | 0.201 | **0.203** |\n| Task 4: Multilingual Product Review Generation | ROUGE-1 | 0.190 | **0.191** |\n|  | ROUGE-L | 0.141 | **0.142** |\n|  | METEOR | **0.174** | 0.173 |", "caption": "Table 9: Zero-shot test set results on ordinal classification on Tasks 9-12 on BM25 using MAE and RMSE metrics for GPT-4o-mini .", "description": "This table presents the results of a zero-shot evaluation of the PGraphRAG model on four ordinal classification tasks (Tasks 9-12) using the GPT-40-mini language model.  The evaluation metrics employed are Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), both common metrics for assessing the accuracy of regression models.  Lower MAE and RMSE values indicate better model performance. BM25 was used as the retrieval method.  The table likely shows the MAE and RMSE scores for each task, potentially broken down by different methods or baselines for comparison.", "section": "5.1 Baseline Comparison"}, {"content": "| Short Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---| \n| Task 5: User Product Review Title Generation | ROUGE-1 | 0.122 | **0.125** |\n|  | ROUGE-L | 0.116 | **0.119** |\n|  | METEOR | 0.115 | **0.117** |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | 0.117 | **0.121** |\n|  | ROUGE-L | 0.110 | **0.113** |\n|  | METEOR | 0.095 | **0.099** |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.125 | **0.132** |\n|  | ROUGE-L | 0.121 | **0.128** |\n|  | METEOR | 0.122 | **0.129** |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | 0.126 | **0.131** |\n|  | ROUGE-L | 0.118 | **0.123** |\n|  | METEOR | 0.112 | **0.118** |", "caption": "Table 10: Ablation study results using LLaMA-3.1-8B-Instruct on the validation set for the long text generation Tasks 1 - 4.", "description": "This table presents the results of an ablation study conducted to evaluate the impact of different components of the PGraphRAG model on the performance of long text generation tasks.  Specifically, it compares the performance of the full PGraphRAG model against two variants: one using only the target user's history ('User Only'), and another using only the history of neighboring users ('Neighbors Only'). The study uses the LLaMA-3.1-8B-Instruct language model and focuses on four long text generation tasks from the Personalized Graph-based Benchmark for Text Generation.", "section": "5.2 PGraphRAG Ablation Study"}, {"content": "| Short Text Generation | Metric | Contriever | BM25 |\n|---|---|---|---| \n| Task 5: User Product Review Title Generation | ROUGE-1 | **0.113** | 0.111 |\n|  | ROUGE-L | **0.108** | 0.106 |\n|  | METEOR | **0.097** | **0.097** |\n| Task 6: Hotel Experience Summary Generation | ROUGE-1 | 0.113 | **0.118** |\n|  | ROUGE-L | 0.107 | **0.112** |\n|  | METEOR | 0.080 | **0.085** |\n| Task 7: Stylized Feedback Title Generation | ROUGE-1 | 0.108 | **0.109** |\n|  | ROUGE-L | 0.106 | **0.107** |\n|  | METEOR | 0.094 | **0.096** |\n| Task 8: Multi-lingual Product Review Title Generation | ROUGE-1 | **0.108** | **0.108** |\n|  | ROUGE-L | 0.103 | **0.104** |\n|  | METEOR | **0.082** | **0.082** |", "caption": "Table 11: Ablation study results using GPT-4o-mini on the validation set for long text generation tasks across Tasks 1-4.", "description": "This table presents the ablation study results obtained using the GPT-40-mini model on the validation set. The study focuses on long text generation tasks (Tasks 1-4). It compares the performance of three variations of the PGraphRAG model: the full PGraphRAG model, a version using only neighboring user reviews, and a version using only the target user's reviews.  The comparison is done using ROUGE-1, ROUGE-L, and METEOR metrics, to assess the impact of different user context information sources on model performance.", "section": "5.2.1 PGraphRAG Ablation Study"}]