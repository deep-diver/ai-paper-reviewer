{"importance": "This paper presents **ILLUME+**, which significantly contributes to the development of more capable and versatile MLLMs. It is important for researchers by improving image understanding, generation, and editing and providing a scalable solution for future multimodal applications.", "summary": "ILLUME+ improves MLLMs with dual visual tokenization and diffusion refinement, enabling high-fidelity image generation and deep semantic understanding.", "takeaways": ["Dual visual tokenization preserves both fine-grained textures and text-aligned semantics, enabling coarse-to-fine image representation.", "Diffusion model enhances generation quality and enables efficient super-resolution.", "ILLUME+ achieves competitive performance against existing unified MLLMs and specialized models across understanding, generation, and editing benchmarks."], "tldr": "Existing unified models struggle to handle understanding, generation, and editing simultaneously. Models using VQGAN lack deep semantic interaction, while those with semantic encoders struggle with image editing due to poor texture preservation. Janus series limits abilities to handle interleaved image-text tasks. To address these issues, ILLUME+ introduces DualViTok to preserves textures and semantics while enabling a coarse-to-fine image representation. \n\nILLUME+ employs a diffusion model for enhanced generation quality and efficient super-resolution. It follows a continuous-input, discrete-output scheme and adopts a progressive training procedure for dynamic resolution. This design allows flexible image editing and generation. Experiments show that ILLUME+ exhibits competitive performance across multimodal tasks, enhances visual understanding, and enables detailed image synthesis.", "affiliation": "Huawei Noah's Ark Lab", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Generation"}, "podcast_path": "2504.01934/podcast.wav"}