{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces CLIP, the vision-language model upon which HySAC is built and is thus fundamental to the current work."}, {"fullname_first_author": "Karan Desai", "paper_title": "Hyperbolic image-text representations", "publication_date": "2023-07-01", "reason": "This paper introduces hyperbolic vision-language models, the inspiration for HySAC and the geometric properties leveraged in this work."}, {"fullname_first_author": "Samuele Poppi", "paper_title": "Safe-CLIP: Removing NSFW Concepts from Vision-and-Language Models", "publication_date": "2024-08-01", "reason": "This paper presents a state-of-the-art safety unlearning method in the VLM space that HySAC uses as a baseline for comparison."}, {"fullname_first_author": "Octavian Ganea", "paper_title": "Hyperbolic entailment cones for learning hierarchical embeddings", "publication_date": "2018-07-01", "reason": "This paper introduces hyperbolic entailment cones, a geometric property used in HySAC to model the relationships between safe and unsafe content."}, {"fullname_first_author": "Chao Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "publication_date": "2021-07-01", "reason": "This paper describes ALIGN, a similar model to CLIP that leverages vast amounts of web-scraped image-text data to learn rich multimodal representations."}]}