[{"content": "|           | # Sample | Query | Thought | Output |\n|-----------|----------|-------|---------|--------|\n| o1-journey | 327       | 41.53 | 486.05  | 3.41   |\n| Marco-O1 CoT data | 10,000    | 52.73 | 673.98  | 52.73  |\n| DRT-o1 data <math>{}_{\normalfont\normalsize{(training)}}</math> | 19,264    | 37.25 | 527.64  | 44.67  |\n| DRT-o1 data <math>{}_{\normalfont\normalsize{(validation)}}</math> | 1,000     | 37.43 | 531.36  | 44.98  |\n| DRT-o1 data <math>{}_{\normalfont\normalsize{(testing)}}</math> | 2,000     | 37.19 | 525.44  | 44.70  |", "caption": "Table 1: The number of samples and average token-level length of query, thought and output. \u201cQuery\u201d and \u201cOutput\u201d in DRT-o1 data mean the source sentences and the translated outputs, respectively.", "description": "Table 1 presents a statistical overview of the datasets used in the study, including the DRT-01 dataset and the Marco-01 CoT dataset.  It details the number of samples and the average token length for the query (source sentence), the thought process (chain-of-thought), and the output (translated sentence) in each dataset.  This allows for a comparison of data characteristics across different datasets, notably highlighting the length of the chain-of-thought in the DRT-01 dataset.", "section": "2.4 Data Statistics"}, {"content": "| Model | BLEU | CometKiwi | CometScore |\n|---|---|---|---| \n| Qwen2.5-7B-Instruct | 27.02 | 70.36 | 76.78 |\n| Qwen2.5-14B-Instruct | 30.23 | 72.01 | 78.84 |\n| QwQ-32B-preview | 27.46 | 71.48 | 78.68 |\n| Marco-o1-7B | 29.48 | 71.62 | 77.41 |\n| DRT-o1-7B | 35.28 | 71.67 | 80.14 |\n| DRT-o1-14B | **37.56** | **72.16** | **80.50** |", "caption": "Table 2: Experimental results on literature translation.", "description": "This table presents the performance comparison of different machine translation models on a literature translation task.  It shows BLEU scores, CometKiwi scores, and CometScore, comparing the performance of DRT-01 models (7B and 14B parameter versions) against baseline models like Qwen2.5-7B-Instruct, Qwen2.5-14B-Instruct, QwQ-32B-preview and Marco-01-7B.  The metrics assess both the grammatical accuracy (BLEU) and the semantic correctness (CometKiwi and CometScore) of the translations.  The results demonstrate the improvement achieved by incorporating the long chain-of-thought mechanism into the translation process.", "section": "3.2 Main Results"}]