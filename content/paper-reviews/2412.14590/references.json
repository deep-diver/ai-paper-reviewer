{"references": [{"fullname_first_author": "Elias Frantar", "paper_title": "GPTQ: accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-10-17", "reason": "This paper proposes GPTQ, a highly influential quantization method for LLMs, which is used as a baseline and compared against in the MixLLM paper."}, {"fullname_first_author": "Zhen Dong", "paper_title": "HAWQ: Hessian aware quantization of neural networks with mixed-precision", "publication_date": "2019-10-27", "reason": "This paper introduces HAWQ, a mixed-precision quantization technique that is relevant to MixLLM's approach of using different bit-widths for output features."}, {"fullname_first_author": "Tim Dettmers", "paper_title": "SpQR: A sparse-quantized representation for near-lossless LLM weight compression", "publication_date": "2024-05-07", "reason": "SpQR, presented in this paper, is another significant LLM quantization method that MixLLM builds upon and contrasts its approach."}, {"fullname_first_author": "Saleh Ashkboos", "paper_title": "Quarot: Outlier-free 4-bit inference in rotated llms", "publication_date": "2024-04-00", "reason": "Quarot is a state-of-the-art mixed-precision quantization method that MixLLM directly compares its performance against, highlighting improvements in accuracy and efficiency."}, {"fullname_first_author": "Yilong Zhao", "paper_title": "Atom: Low-bit quantization for efficient and accurate LLM serving", "publication_date": "2024-05-13", "reason": "The Atom method is a notable mixed-precision approach for LLMs; MixLLM contrasts its global vs. local precision search and mixed-precision application between input vs. output features."}]}