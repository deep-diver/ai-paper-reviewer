[{"content": "|               |       Llama 3.1/3.2       |             Qwen2.5             |        Mistral        |\n| :------------ | :-----------------------: | :--------------------------: | :-------------------: |\n| baselines     |           1B  8B  70B          |      0.5B  1.5B  7B  32B      |         7B v0.3        |\n| float16      | 9.75/12.72  6.24/8.95  2.81/6.68 | 13.07/17.55  9.26/13.11  6.85/10.44 |   5.02/8.95  5.32/7.84   |\n| W4A16        | 11.72/15.56  6.82/9.72  3.55/7.43 | 15.54/20.55 10.35/14.35  7.23/10.88 |   5.27/9.14  5.51/8.04   |\n| RTN W5A16    | 10.15/13.25  6.40/9.15  3.16/9.52 | 13.61/18.17  9.52/13.38  6.95/10.53 |   5.09/8.99  5.38/7.91   |\n| GPTQ W4A16   | 10.38/14.15  6.52/9.55  Abn/Abn  | 14.01/19.04  9.64/13.75  7.09/10.75 |   5.20/9.08  5.49/8.19   |\n| AWQ W4A16    | 10.81/14.12  6.65/9.48  3.28/6.96 | 15.04/19.75  9.95/13.85  7.10/10.71 |   5.23/9.08  5.44/7.98   |\n| SmoothQuant W8A8  | 9.89/12.91  6.34/9.08  2.92/6.77 | 13.84/18.40  9.63/13.49  7.17/10.85 |   5.12/9.04  5.35/7.88   |\n| QoQ W4A8     | Abn/Abn  6.64/9.49  3.49/7.07    | Abn/Abn  Abn/Abn  7.39/11.06  |   5.55/9.31  5.44/7.98   |\n| W4A4         | Abn/Abn  8.34/11.95  6.16/9.91   | NA/NA  Abn/Abn  8.15/12.05  |   6.26/9.98  5.83/8.50   |\n| QuaRot W4A8  | Abn/Abn  6.60/9.67  3.43/7.10    | NA/NA  Abn/Abn  7.03/10.68  |   5.23/9.10  5.40/7.99   |\n| W4A8 (p0)    | 10.36/14.09  6.54/9.62  3.30/7.24 | 14.43/19.61  9.66/13.79  7.03/10.75 |   5.21/9.08  5.42/8.02   |\n| W4.4A8 (p10) | 10.05/13.51  6.42/9.33  3.02/6.83 | 13.42/18.13  9.44/13.43  6.92/10.57 |   5.12/9.01  5.36/7.93   |\n| W4.8A8 (p20) | 9.95/13.25  6.37/9.22  2.97/6.79 | 13.32/17.99  9.40/13.35  6.90/10.53 |   5.09/9.00  5.35/7.90   |\n| W6A8 (p50)   | 9.85/12.98  6.30/9.09  2.86/6.73 | 13.21/17.78  9.33/13.25  6.88/10.49 |   5.05/8.98  5.33/7.87   |\n| MixLLM W8A8 (p100) | 9.76/12.75  6.25/8.97  2.81/6.68 | 13.12/17.60  9.28/13.14  6.86/10.45 |   5.02/8.96  5.32/7.84   |", "caption": "Table 1: Perplexity evaluation (\u2193\u2193\\downarrow\u2193) on wikitext2/c4 (gray for c4), sequence length 2048.\nNA means no support.\nAbn means the value is too large (>105absentsuperscript105>10^{5}> 10 start_POSTSUPERSCRIPT 5 end_POSTSUPERSCRIPT).\nFor MixLLM, pn means n%percent\ud835\udc5bn\\%italic_n % 8-bit.", "description": "Table 1 presents perplexity scores, a measure of how well a language model predicts a text sequence, for several LLMs (Large Language Models) on the Wikitext2 and C4 datasets.  Lower perplexity indicates better performance.  The table compares different quantization methods (techniques to reduce the size and computational cost of the models), including various bit-widths (the number of bits used to represent model weights and activations).  Different methods are compared, including weight-only quantization and weight-activation quantization.  The results are shown for different model sizes. Values marked as 'NA' signify that the method is not applicable to that model size, while 'Abn' indicates perplexity scores exceeding 10<sup>5</sup>.  For MixLLM, the 'pn' notation signifies the percentage of 8-bit precision used.", "section": "4.2 Perplexity Evaluation"}, {"content": "| LLaMA 2 | FP16 | SqueezeLLM | OminiQuant | AfineQuant | Atom | SpinQuant | MixLLM |\n|---|---|---|---|---|---|---|---| \n| 7B | 5.47 | 5.57 | 5.58/14.26 | 5.58/12.69 | 6.03 | 5.7 | 5.55 |\n| 13B | 4.88 | 4.96 | 4.95/12.30 | 4.95/11.45 | 5.27 | 5.0 | 4.93 |", "caption": "Table 2: PPL (wikitext2) comparison with the reported numbers in the related works.", "description": "This table compares the perplexity scores (PPL) achieved by MixLLM on the Wikitext2 dataset against various other state-of-the-art quantization methods. It specifically focuses on the results from Llama 2, showing a comparison of different quantization strategies (weight-only, mixed-precision) and bit-widths.  The purpose is to demonstrate the superior performance of MixLLM, in terms of achieving a lower perplexity score indicating better performance, using either comparable or fewer bits than existing approaches.", "section": "4.2 Perplexity Evaluation"}, {"content": "| Model       | Result       |\n|--------------|---------------|\n| SqueezeLLM   | W4A16 0.45%  |", "caption": "Table 3: Downstream tasks evaluation (\u2191\u2191\\uparrow\u2191) on Llama-3.1-8B/Qwen2.5-7B/Mistral-7B-v0.3.\nThe above is the average of the three models.\nBBH is 3 shot, MMLU pro is 5 shot, and others are zero shot.", "description": "Table 3 presents the results of downstream task evaluations performed on three different large language models (LLMs): Llama-3.1-8B, Qwen2.5-7B, and Mistral-7B-v0.3.  The table shows the average performance across these three models for several downstream tasks.  The evaluation metrics used are not explicitly stated in the caption.  Different numbers of shots were used for certain tasks: BBH used 3 shots, MMLU pro used 5 shots, while the remaining tasks employed zero shots.  The table compares performance using various quantization methods.", "section": "4.4 Downstream Tasks Evaluation"}, {"content": "| Model           | Architecture |\n|-----------------|----------------|\n| OminiQuant      | W4A16/W4A4     |", "caption": "Table 4: The average percentage of 8-bit out features in the seven classes of linear layers in Llama 3.1 8B, with 10% global 8-bit out features in MixLLM.", "description": "This table shows the distribution of 8-bit output features across different linear layer types within the Llama 3.1 8B model when using MixLLM with a global 10% 8-bit output feature setting.  It breaks down the proportion of high-precision (8-bit) channels within each of the seven categories of linear layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) to illustrate the non-uniform distribution of high-salience features across different parts of the model. This highlights MixLLM's approach of selectively applying higher precision to more important features rather than uniformly.", "section": "4.5.2 High Precision Distribution"}, {"content": "| AfineQuant |\n|---|---| \n| W4A16/W4A4 |", "caption": "Table 5: The overhead of global precision search in MixLLM.", "description": "This table presents the computational time required for the global precision search algorithm used in MixLLM to determine the optimal bit-width for each output feature. The search is performed once for each model during the quantization process.  The models vary in size, ranging from 1.5B to 70B parameters. The results show the trade-off between model size and search time.", "section": "4 Evaluation"}]