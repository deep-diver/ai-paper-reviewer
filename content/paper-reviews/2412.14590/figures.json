[{"figure_path": "https://arxiv.org/html/2412.14590/x1.png", "caption": "Figure 1: Illustration of the quantization with mixed-precision between output features and kernel execution.", "description": "This figure illustrates MixLLM's quantization method.  It shows how a global mixed-precision strategy is applied to output features before kernel execution.  Low-salience output features are quantized with 4-bit precision, while high-salience features use 8-bit precision. The process is designed to balance accuracy and efficiency by focusing higher precision on the most important parts of the model's output. This is followed by a parallel execution of the matrix multiplications for the different bit-widths, resulting in optimized computation, and finally a fused scatter operation to combine the results.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2412.14590/x2.png", "caption": "Figure 2: The percentage of high-salient out features within each linear layer of Llama 3.1 8B model according to each feature\u2019s contribution to the final loss after quantizing to 4-bit, with 10% high-salient features globally.\nEach decoder layer contains q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj in order.", "description": "This figure visualizes the distribution of high-saliency output features across different layers of the Llama 3.1 8B model after 4-bit quantization.  The analysis focuses on the top 10% of features globally identified as having the most significant impact on the final loss.  The x-axis represents the decoder layer indices, and the y-axis shows the percentage of high-saliency output features in each layer. Each decoder layer is composed of seven sub-layers (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj), and the figure shows the aggregate high-saliency percentage for the entire layer. This illustrates that the importance of various features differs significantly across different layers.", "section": "3.1 Quantization Design and Decision in MixLLM"}, {"figure_path": "https://arxiv.org/html/2412.14590/x3.png", "caption": "Figure 3: The float and integer value of binary (010010110xx...x), each within a consecutive range.", "description": "This figure illustrates how a two-step dequantization method allows efficient use of the int8 Tensor Cores on GPUs.  It shows that a float32 number can be represented as a 9-bit fixed integer part and a 23-bit variable fractional part.  The figure demonstrates that there's a range where the binary representation of an integer and its corresponding float are the same (for example, '010010110xx...x' where 'x' can be any bit). This range allows for fast integer-to-float conversion by adding and subtracting a bias, without explicitly using computationally expensive instructions. This method helps avoid performance drops that are typically associated with directly converting integers to floating point numbers for low-bit quantization.", "section": "3.3 Efficient Quantization Computation System"}, {"figure_path": "https://arxiv.org/html/2412.14590/x4.png", "caption": "Figure 4: The GPU kernel software pipeline of group-wise W4A8/W8A8 quantized MatMul.\nIt assumes perfect overlapping.\nG2S: load global to shared memory;\nS2R: load shared memory to register;\nMMA: matrix multiply-accumulation;\nI2F: integer to float conversion;\ndeq: dequantize;\nacc: accumulate.", "description": "Figure 4 illustrates the optimized GPU kernel execution pipeline for performing group-wise quantized matrix multiplications (MatMul).  It showcases the parallel processing stages involved in handling both 4-bit (W4A8) and 8-bit (W8A8) quantized weights and activations. The pipeline is designed for optimal performance by maximizing resource utilization and minimizing overhead. Key stages include loading data from global memory to shared memory (G2S), transferring data from shared memory to registers (S2R), performing the matrix multiply-accumulate operation (MMA) using Tensor Cores, converting integers to floating-point numbers (I2F), dequantization (deq), and accumulation (acc). The diagram assumes perfect overlapping between these stages for maximum efficiency.", "section": "3.3 Efficient Quantization Computation System"}, {"figure_path": "https://arxiv.org/html/2412.14590/x5.png", "caption": "Figure 5: The speedup of two types of single linear layers over torch float16 baseline on the A100 GPU.", "description": "This figure showcases the performance speedup achieved by various quantization methods compared to a baseline using Torch float16 on an NVIDIA A100 GPU. Two distinct types of single linear layers are examined, each with varying numbers of tokens (input sequence length).  The speedups are presented for different quantization methods with different bit-widths, offering insights into how each approach impacts the processing speed of large language models (LLMs).", "section": "4.3 System Performance"}, {"figure_path": "https://arxiv.org/html/2412.14590/x6.png", "caption": "Figure 6: The perplexity (wikitext2) of Llama 3.1 8B model with different configurations.", "description": "Figure 6 illustrates an ablation study analyzing the impact of various optimization techniques on the perplexity of the Llama 3.1 8B model when using the Wikitext2 dataset.  It systematically shows the effects of adding each component: 8-bit activation quantization, asymmetric weight quantization, group-wise quantization, global 10% 8-bit feature selection using a diagonal Fisher Information matrix (with and without the first-order term), and finally GPTQ with and without the reorder technique.  The baseline is 4-bit weight-only quantization with symmetric quantization per channel.", "section": "4.5 Detailed Analysis"}]