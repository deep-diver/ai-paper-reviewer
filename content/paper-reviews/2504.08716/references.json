{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "This is a foundational paper introducing the BERT architecture, which revolutionized NLP and is the basis for many subsequent models discussed in the paper."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "RoBERTa: A robustly optimized BERT pretraining approach", "publication_date": "2019-07-11", "reason": "This paper introduces RoBERTa, an improved version of BERT, and the current paper compares against models based on RoBERTa's architecture."}, {"fullname_first_author": "Pengcheng He", "paper_title": "Deberta: Decoding-enhanced bert with disentangled attention", "publication_date": "2021-01-01", "reason": "This paper introduces DeBERTa, an improvement to BERT that makes use of disentangled attention, which is compared against in this study."}, {"fullname_first_author": "Pengcheng He", "paper_title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing", "publication_date": "2021-01-01", "reason": "This is the paper introducing DeBERTaV3, a state-of-the-art model, which the current paper compares its performance against."}, {"fullname_first_author": "Benjamin Warner", "paper_title": "Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference", "publication_date": "2024-12-21", "reason": "This paper introduces ModernBERT, the model that the current study aims to analyze and evaluate."}]}