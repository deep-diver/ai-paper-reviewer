{"importance": "This study enhances understanding of **modern Transformer encoder architectures** by disentangling architectural and pretraining data effects. It provides a comprehensive analysis and resources for further research in efficient and performant NLP models, revealing nuances for model selection in various applications.", "summary": "ModernBERT's speed doesn't guarantee superior performance; data matters! This study untangles architecture vs. data influence on transformer encoders.", "takeaways": ["DeBERTaV3 outperforms ModernBERT in benchmark performance and training efficiency when dataset differences are controlled.", "ModernBERT offers faster training and inference speeds, making it suitable for time-sensitive applications.", "High-quality training data accelerates convergence but does not significantly improve final performance, indicating potential benchmark saturation."], "tldr": "Transformer encoder models like DeBERTaV3 and ModernBERT introduce architectural improvements for better efficiency. However, it is difficult to determine whether these gains come from architecture or differences in training data. This study conducts a controlled comparison by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. The study also pretrains another ModernBERT variant to explore the role of dataset quality in model performance.\n\nThe results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, but ModernBERT's advantage is faster training and inference speed. It still provides architectural improvements compared to older models. High-quality pre-training data accelerates convergence but does not significantly improve final performance. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.", "affiliation": "Inria", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.08716/podcast.wav"}