[{"figure_path": "https://arxiv.org/html/2503.16356/x1.png", "caption": "Figure 1: The current edit cannot propagate the new knowledge to the reasoning circuit for multi-hop reasoning. We propose a circuit-aware edit to improve the model\u2019s multi-hop reasoning performance involving the updated knowledge.", "description": "This figure illustrates the limitations of existing knowledge editing techniques in handling multi-hop reasoning tasks.  The left side shows a standard knowledge editing approach where an edit is made (e.g., changing the citizenship of a person), but this edit fails to propagate correctly through the neural pathways (reasoning circuits) used by the language model to answer complex, multi-step questions. The result is that the model still produces incorrect answers even when the new information is factually correct. The right side proposes a solution, \"circuit-aware editing,\" aimed at integrating the updated knowledge effectively within these reasoning circuits, thereby enabling the model to answer multi-hop questions correctly after the edit.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16356/x2.png", "caption": "Figure 2: An overview of our work.", "description": "This figure provides a visual summary of the research presented in the paper. It showcases the limitations of existing knowledge editing (KE) methods in handling multi-hop reasoning tasks. Panel (a) illustrates the multi-hop reasoning process within large language models (LLMs), highlighting the potential failure points in signal propagation and information flow. Panel (b) contrasts two prevalent KE strategies (WISE-style and ROME-style) and illustrates their failure to integrate new knowledge into the reasoning pathways effectively. Panel (c) presents the proposed Circuit-aware Knowledge Editing (CaKE) method, which strategically leverages additional features to guide the model towards utilizing updated knowledge more accurately and consistently.", "section": "Analyzing Reasoning Circuit in LLM"}, {"figure_path": "https://arxiv.org/html/2503.16356/x3.png", "caption": "Figure 3: Results of the intervention on the failure cases in multi-hop reasoning of LLAMA3 and Qwen2.5.", "description": "This figure displays the outcomes of experiments designed to address failures in multi-hop reasoning within LLAMA3 and Qwen2.5 language models.  The experiments involved interventions to enhance information flow at critical points within the models' reasoning circuits.  The results show success rates of the intervention methods across different failure categories, indicating the effectiveness of the approaches in improving multi-hop reasoning.", "section": "2.3 Circuit in Failure Phenomena"}, {"figure_path": "https://arxiv.org/html/2503.16356/x4.png", "caption": "Figure 4: The target answer token\u2019s rank in the vocabulary of different editing methods when editing the fact \u2018The official language of Japan is Japanese \u2192\u2192\\rightarrow\u2192 Korean.\u2019", "description": "This figure compares the ranking of the correct answer token (\"Korean\") within the model's vocabulary across different layers of the language model.  It demonstrates the effect of three different knowledge editing methods (MEMIT, WISE, and the original, unedited model) on the propagation of the edited fact: changing the official language of Japan from Japanese to Korean. The rank indicates how likely the model was to predict \"Korean\" as the correct answer at each layer, with lower ranks signifying a higher probability. This visualization helps understand how the different editing methods affect the flow of information and the integration of new knowledge within the language model's architecture.", "section": "3 Circuits-aware Knowledge Editing"}, {"figure_path": "https://arxiv.org/html/2503.16356/x5.png", "caption": "Figure 5: Accuracies of different number hops and edit-positions in MQuAKE-CF-3k-v2 on LLAMA3-8B-Instruct.", "description": "This figure displays the results of experiments conducted on the LLAMA3-8B-Instruct model using the MQuAKE-CF-3k-v2 dataset. It investigates the impact of the number of hops (the number of reasoning steps involved in a multi-hop question) and the position of the edit (where in the question sequence the knowledge is updated) on the model's accuracy. The x-axis represents the number of hops, and the y-axis shows the accuracy. Different colors represent different edit positions: 'pre' (before the multi-hop question), 'mid' (in the middle), and 'post' (after).  The graph visually demonstrates how the model's performance changes based on these factors, highlighting the influence of both the number of reasoning steps and the placement of the knowledge update within the multi-hop question.", "section": "4.2 Experiments Results"}, {"figure_path": "https://arxiv.org/html/2503.16356/x6.png", "caption": "Figure 6: e2subscript\ud835\udc522e_{2}italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and r2subscript\ud835\udc5f2r_{2}italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT\u2019s logits at t2subscript\ud835\udc612t_{2}italic_t start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT in models after different knowledge editing methods.", "description": "This figure displays the logit values of the bridge entity (e2) and the second relation (r2) at the final token position (t2) for various knowledge editing methods.  Logits represent the model's confidence in each entity or relation. The goal is to show how different knowledge editing techniques affect the model's ability to incorporate the updated knowledge, which is represented by e2 and r2, into its reasoning process. Higher logit values for e2 and r2 indicate a stronger signal for the model to correctly identify the intended entities and relations during multi-hop reasoning.", "section": "Case Analysis"}, {"figure_path": "https://arxiv.org/html/2503.16356/extracted/6213323/Figure/newplot.png", "caption": "Figure 7: The failure case of the multi-hop reasoning.", "description": "This figure visualizes a case where multi-hop reasoning fails.  It shows the layer-wise activation probabilities for various entities and relations in a language model processing a multi-hop question.  The model successfully identifies intermediate elements early on but then deviates from the correct reasoning path in later layers, ultimately leading to an incorrect final answer.  The heatmap allows for a visual analysis of where and when the model's reasoning goes astray. The visualization highlights the limitations of layer-localized knowledge editing methods, which often fail to correct faulty multi-hop reasoning patterns.", "section": "2 Analyzing Reasoning Circuit in LLM"}, {"figure_path": "https://arxiv.org/html/2503.16356/x7.png", "caption": "Figure 8: The distribution of the layers allows us to detect the information from critical positions in the model via patch_scope.", "description": "Figure 8 uses the PatchScope method to analyze the layers of a language model during multi-hop reasoning tasks.  It visualizes where key pieces of information (the bridge entity 'e2' and relation 'r2') are present in the model's layers for successful and unsuccessful reasoning cases. By showing the distribution of these key elements across layers, the figure helps explain why some multi-hop reasoning attempts succeed while others fail, highlighting the importance of timely and proper propagation of information within the model's architecture for successful multi-hop reasoning. The layer distribution demonstrates the success or failure of the model to properly use information during the multi-hop reasoning process. ", "section": "2.3 Circuit in Failure Phenomena"}, {"figure_path": "https://arxiv.org/html/2503.16356/x8.png", "caption": "Figure 9: The way we test the function of the second hop. If the model conducts the function at the later layers, changing the representation would change the output of the model.", "description": "This figure demonstrates a causal analysis to validate the hypothesis of a structured reasoning circuit in LLMs for multi-hop tasks. By replacing the representation of the bridge entity (e2) or the second relation (r2) at the last token position (t2) with alternatives, the researchers observe how these changes affect the model's output. Successful patching confirms the model's reliance on specific representations for reasoning in multi-hop scenarios, supporting the existence of a structured reasoning circuit.", "section": "Analyzing Reasoning Circuit in LLM"}, {"figure_path": "https://arxiv.org/html/2503.16356/x9.png", "caption": "Figure 10: The way we conduct the backpatch and e1subscript\ud835\udc521e_{1}italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT to e2subscript\ud835\udc522e_{2}italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT. We substitute the hidden representations from the source position to the target position.", "description": "This figure illustrates the causal analysis conducted to investigate the impact of modifying variables in the multi-hop reasoning circuit of large language models (LLMs).  Specifically, it shows how replacing the representation of the bridge entity (e<sub>2</sub>) or the second relation (r<sub>2</sub>) at the last token position (t<sub>2</sub>) affects the model's output. The 'backpatch-t1' and 'backpatch-t2' interventions involve replacing the representations of e<sub>2</sub> at t<sub>1</sub> (end of the first hop) and t<sub>2</sub> (end of the second hop), respectively, while 't1-to-t2' propagates the representation of e<sub>2</sub> from t<sub>1</sub> to t<sub>2</sub>. This causal analysis helps to understand the model's reliance on specific intermediate representations and to confirm whether it properly activates the updated knowledge in the reasoning circuit.", "section": "Analyzing Reasoning Circuit in LLM"}]