[{"figure_path": "https://arxiv.org/html/2502.07776/x1.png", "caption": "Figure 1: \nAn example illustrating prompt caching. (1) A victim sends a prompt to the API, which then becomes cached. (2) An attacker sends a new prompt, resulting in a cache miss and slow response time. (3) An attacker sends a prompt that shares a prefix with the victim\u2019s prompt, resulting in a cache hit. From the fast response time, the attacker can infer that a cache hit occurred, which potentially reveals information about other users\u2019 prompts.", "description": "This figure illustrates a side-channel attack leveraging prompt caching in a language model API.  First, a victim's prompt is sent (1) and cached by the API. Next, an attacker sends a different prompt (2), resulting in a slower response time (cache miss). Finally, the attacker sends a prompt sharing a prefix with the victim's prompt (3).  The fast response time reveals the cache hit and indicates the prefix of the victim's prompt has been cached, potentially leaking private information.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.07776/x2.png", "caption": "Figure 2: \nOrganizations contain users, and the global level contains all users and organizations of an API.", "description": "This figure illustrates the hierarchical structure of users and organizations within an API's context.  At the lowest level are individual users.  Multiple users belong to an organization (e.g., a company or research group).  Finally, the global level encompasses all users and organizations associated with that specific API.", "section": "2. Preliminaries and Assumptions"}, {"figure_path": "https://arxiv.org/html/2502.07776/x3.png", "caption": "Figure 3: Histograms of response times from the cache hit and cache miss procedures in APIs where we detected caching. The distributions of times are clearly distinguishable, with cache hits tending to be faster. Each histogram title states the API provider, model, level of cache sharing (per-org. or global), timing source (client-side or server-side timing), and the NumVictimRequests used, denoted v.", "description": "Figure 3 presents histograms illustrating response time distributions from cache hit and cache miss procedures across several LLMs.  The x-axis represents time (in seconds), and the y-axis shows frequency.  Each histogram is clearly bimodal, with a distinct peak for cache hits (faster times) and another peak for cache misses (slower times). The separation of the two peaks demonstrates the effectiveness of using timing differences to distinguish between cache hits and misses.  Each histogram title provides detailed information for context: the API provider, the specific LLM model used, the level of cache sharing (per-organization or global), the timing measurement method (client-side or server-side), and the number of victim requests (denoted as 'v'). The clear separation of the response times for cache hits and misses across various models highlights the reliability of utilizing timing analysis to identify prompt caching.", "section": "Auditing Real-World APIs"}, {"figure_path": "https://arxiv.org/html/2502.07776/x4.png", "caption": "Figure 4: Selected precision-recall curves for distinguishing between times from the cache hit and cache miss procedures. Cache hits are the positive class. The curves show that cache hits can be detected with near perfect precision up to moderate recall scores. Figure\u00a06 in the appendix contains curves for other APIs.", "description": "This figure displays precision-recall curves that evaluate the performance of distinguishing between cache hit and cache miss response times.  The positive class represents cache hits. The curves demonstrate that the method achieves near-perfect precision at moderate recall levels.  The Appendix includes similar curves for other APIs, which were also evaluated in the study.", "section": "4. Auditing Real-World APIs"}, {"figure_path": "https://arxiv.org/html/2502.07776/x5.png", "caption": "Figure 5: \nAblations on the effects of PromptLength, PrefixFraction, and model size on the average precision. In (a)\u2013(c), as the prompt length or prefix match length decreases, the average precision decreases to random chance. In (d), we detect caching across all model sizes, with no clear relationship between model size and average precision.", "description": "This figure displays ablation studies on the effect of three factors on the average precision of detecting prompt caching: prompt length, prefix match length (proportion of the prompt used for matching), and model size.  Subfigures (a), (b), and (c) show that reducing either prompt length or prefix match length causes the average precision to decrease, eventually approaching that of random chance (0.5).  However, subfigure (d) shows that the model size (tested on the Llama 3.1 family of models) does not impact the detectability of prompt caching.", "section": "4.3 Ablations"}, {"figure_path": "https://arxiv.org/html/2502.07776/x6.png", "caption": "Figure 6: Precision-recall curves for distinguishing between times produced by the cache hit and cache miss procedures in APIs where we detected caching in our audits (Table\u00a01). Cache hits are the positive class, and cache misses are the negative class. The curves show that cache hits can be detected with near perfect precision up to moderate recall scores. Note that our cache hit procedure attempts to produce cache hits but cannot guarantee cache hits (e.g., due to server routing), so some times in the cache hit distribution may actually be cache misses, which would hurt recall scores.", "description": "This figure displays precision-recall curves illustrating the performance of distinguishing between cache hits and misses using timing data from various Language Model APIs.  Each curve represents a different API, showing the trade-off between precision (correctly identifying cache hits) and recall (correctly identifying all actual cache hits).  The curves demonstrate that high precision (near perfect) can be achieved up to a moderate recall level.  It's important to note that the process for generating cache hits doesn't guarantee a hit every time (due to factors like server routing), which can impact the overall recall.", "section": "4.2 Audit Results"}, {"figure_path": "https://arxiv.org/html/2502.07776/x7.png", "caption": "Figure 7: \nAblations on the effects of PromptLength, PrefixFraction, and model size on the audit p-values. Each test is run using NumSamples=250NumSamples250\\textsc{NumSamples}=250NumSamples = 250. The top and bottom rows display the p-values on linear and logarithmic scales, respectively. In (a)\u2013(c), as the prompt length or prefix match length decreases, the p-values grow larger. In (d), we detect caching across all model sizes, with no clear relationship between model size and p-values.", "description": "This figure displays ablation studies on the effects of different parameters on the statistical significance (p-values) of prompt caching detection.  Four experiments were performed, varying:\n\n(a) The length of prompts in the \"same prompt\" test (i.e., using identical prompts to induce cache hits).\n(b) The length of prompts in the \"same prefix, different suffix\" test (i.e., using prompts with common prefixes to induce cache hits).\n(c) The length of the matching prefix in the \"same prefix, different suffix\" test.\n(d) The size of the language model used in the test.\n\nThe results are shown in both linear and logarithmic scales for better visualization.  The key finding is that shorter prompt lengths or shorter matching prefixes lead to less statistically significant results (higher p-values) indicating a weaker ability to detect prompt caching. Model size, however, showed no discernible impact on the effectiveness of caching detection.", "section": "4.3 Ablations"}]