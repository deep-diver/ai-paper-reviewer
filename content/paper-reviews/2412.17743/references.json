{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-13", "reason": "This paper introduces the Llama model, a foundational model for YuLan-Mini's development, significantly impacting the architecture and training methodology."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "Llama 3's training techniques, particularly the annealing stage, directly influenced YuLan-Mini's training strategy, enhancing its efficiency and performance."}, {"fullname_first_author": "Shengding Hu", "paper_title": "Minicpm: Unveiling the potential of small language models with scalable training strategies", "publication_date": "2024-04-06", "reason": "MiniCPM's data-efficient pre-training approach and its training stability methods were adopted and improved in YuLan-Mini, resulting in better performance."}, {"fullname_first_author": "Kosuke Nishida", "paper_title": "Initialization of large language models via reparameterization to mitigate loss spikes", "publication_date": "2024-11-12", "reason": "The WeSaR re-parameterization technique from this paper was crucial in stabilizing YuLan-Mini's training process, reducing instability issues."}, {"fullname_first_author": "Mitchell Wortsman", "paper_title": "Small-scale proxies for large-scale transformer training instabilities", "publication_date": "2024-05-07", "reason": "This study's findings on training instability in large language models guided YuLan-Mini's training stability improvements, leading to better model performance."}]}