{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a large language model that serves as a foundational model for many of the vision-language models discussed in this paper."}, {"fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An open-source framework for training large autoregressive vision-language models", "publication_date": "2023-08-01", "reason": "OpenFlamingo is an open-source framework for training large vision-language models, representing an important contribution to the field and a key model compared in the experiments."}, {"fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A versatile vision-language model for understanding, localization, text reading, and beyond", "publication_date": "2023-08-12", "reason": "Qwen-VL is a versatile vision-language model that is compared in this paper's experiments, thus is an important reference for evaluating performance."}, {"fullname_first_author": "Wenliang Dai", "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning", "publication_date": "2023-11-28", "reason": "InstructBLIP is another key vision-language model that serves as a comparison point in the paper's experiments, highlighting its importance in the field."}, {"fullname_first_author": "Shilong Liu", "paper_title": "LLaVA-plus: Learning to use tools for creating multimodal agents", "publication_date": "2023-11-05", "reason": "LLaVA-plus is a significant model that introduces the use of tools for creating multimodal agents, a concept directly related to this paper's proposed MMFactory and compared in experiments."}]}