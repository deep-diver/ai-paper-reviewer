[{"content": "Method | Depth | Spatial | Jigsaw | Vis corr. | Sem. Corr. | Art | Count | Fun. Corr. | Local. | Multi-view | Refl. | Fore. | IQ | Sim. |---|---|---|---|---|---|---|---|---|---|---|---|---|---|---| OpenFlamingo-v2 [5] | 54.0 | 43.4 | 47.3 | 25.6 | 30.2 | 52.1 | 21.7 | 36.2 | 52.0 | 41.4 | 43.3 | 15.9 | 23.3 | 55.2 | InstructBLIP-7B [14] | 51.6 | 56.6 | 52.7 | 30.8 | 30.9 | 47.9 | 29.2 | 23.9 | 44.8 | 58.7 | 29.9 | 29.6 | 23.3 | 46.3 | InstructBLIP-13B [14] | 51.6 | 65.7 | 52.7 | 29.7 | 32.4 | 50.4 | 30.8 | 22.3 | 52.0 | 54.1 | 46.3 | 13.6 | 26.0 | 46.3 | CogVLM [55] | 50.8 | 67.1 | 52.7 | 20.9 | 23.6 | 49.6 | 46.3 | 23.9 | 43.2 | 57.1 | 26.9 | 24.2 | 26.7 | 46.3 | LLaVA-v1.5-7B [35] | 52.4 | 61.5 | 11.3 | 25.6 | 23.0 | 47.9 | 43.3 | 21.5 | 48.8 | 49.6 | 36.6 | 28.0 | 24.0 | 46.3 | LLaVA-v1.5-13B [35] | 53.2 | 67.8 | 58.0 | 29.1 | 32.4 | 47.9 | 50.0 | 20.8 | 47.2 | 41.4 | 45.5 | 27.3 | 28.0 | 46.3 | Ours (LLaVA-7B) | 51.6 | 78.8 | 56.7 | 33.1 | 32.4 | 54.7 | 41.2 | 21.5 | 56.6 | 55.6 | 37.0 | 26.5 | 23.3 | 58.5 | Ours (LLaVA-13B) | 58.1 | 69.9 | 64.0 | 34.3 | 34.5 | 58.1 | 47.2 | 23.9 | 51.6 | 51.1 | 45.1 | 26.5 | 28.0 | 45.9 | Qwen-VL-Max [7] | 58.9 | 77.6 | 3.3 | 22.7 | 29.3 | 37.6 | 55.8 | 28.5 | 49.6 | 53.4 | 49.3 | 47.7 | 22.0 | 51.5 | Gemini Pro [20] | 50.0 | 67.1 | 54.0 | 37.2 | 22.1 | 49.5 | 65.0 | 32.3 | 46.4 | 41.4 | 46.3 | 45.5 | 27.3 | 55.9 | Claude 3 OPUS [4] | 57.3 | 57.3 | 32.7 | 31.4 | 20.7 | 60.7 | 49.2 | 22.3 | 46.4 | 57.9 | 27.6 | 62.1 | 21.3 | 70.6 | GPT-4o [42] | 74.2 | 69.2 | 55.3 | 75.0 | 54.0 | 82.9 | 51.7 | 39.2 | 56.0 | 60.2 | 38.8 | 85.6 | 30.0 | 65.4 | GPT-4o (+ SoM + orig.)\u2020 | 75.0 | 82.5 | - | - | - | - | - | - | - | - | - | - | - | - | GPT-4o (+ Visprog)\u2020 | 46.8 | 37.8 | - | - | - | - | - | - | - | - | - | - | - | - | GPT-4o (+ Sketchpad) | 83.9\u2020 | 81.1\u2020 | 70.7\u2020 | 80.8\u2020 | 58.3\u2020 | 77.19\u2217 | 66.7\u2217 | 42.1\u2217 | 65.4\u2217 | 45.6\u2217 | 33.1\u2217 | 79.0\u2217 | 22.8\u2217 | 84.2\u2217 | Ours (GPT-4o) | 80.3 | 81.8 | 75.3 | 85.5 | 58.3 | 83.0 | 61.7 | 55.4 | 59.0 | 60.2 | 35.1 | 84.8 | 28.7 | 75.3 |", "caption": "Table 1: Quantitative results.  Experimental results on the BLINK benchmark\u00a0[19]. \u2020 denotes results from the previous work\u00a0[26], and \u2217 represents results collected via official codebase. The best result is highlighted in Bold and the second underlined.", "description": "This table presents a quantitative comparison of different vision-language models on the BLINK benchmark [19], a dataset designed to evaluate visual perception capabilities.  The models are evaluated across various tasks, and their performance is measured using several metrics.  The table includes both open-source and API-based models.  Results marked with \u2020 are reproduced from a prior work [26], and those with * represent results obtained from the official model code.  The best performing model for each metric is highlighted in bold, while the second-best is underlined.", "section": "4. Experiments"}, {"content": "| Model | Avg. | Scene | Id | Attri. | Locat. |\n|---|---|---|---|---|---| \n| InstructBLIP [14] | 51.5 | 58.9 | 49.7 | 61.7 | 35.1 |\n| LLaVA-v1.5-7B [35] | 57.7 | 63.7 | 62.4 | 66.7 | 51.3 |\n| MiniGPT-4 [63] | 45.9 | 56.3 | 49.2 | 45.8 | 37.9 |\n| OpenFlamingo [5] | 36.1 | 46.7 | 42.3 | 31.7 | 33.4 |\n| Qwen-VL-Chat [7] | 50.9 | 56.5 | 47.6 | 54.8 | 46.9 |\n| CogVLM [55] | 42.4 | 51.7 | 43.5 | 38.9 | 33.8 |\n| InternLM [62] | 69.2 | 77.5 | 73.5 | 74.8 | 65.4 |\n| GPT-4o [42] | 75.6 | 77.3 | 79.7 | 79.2 | 71.0 |\n| Ours (GPT-4o) | 75.8 | 78.3 | 78.3 | 79.7 | 70.1 |\n| Model | Count. | Spatial | Inter. | Reason. | Text |\n|---|---|---|---|---|---| \n| InstructBLIP [14] | 58.1 | 34.9 | 47.4 | 55.9 | 61.4 |\n| LLaVA-v1.5-7B [35] | 60.2 | 38.5 | 47.4 | 59.8 | 69.0 |\n| MiniGPT-4 [63] | 45.3 | 32.6 | 47.4 | 57.1 | 41.8 |\n| OpenFlamingo [5] | 27.4 | 29.8 | 29.9 | 47.7 | 35.6 |\n| Qwen-VL-Chat [7] | 54.2 | 40.3 | 55.7 | 55.0 | 47.4 |\n| CogVLM [55] | 29.4 | 33.6 | 45.4 | 53.5 | 51.5 |\n| InternLM [62] | 65.8 | 57.5 | 71.1 | 75.8 | 61.2 |\n| GPT-4o [42] | 68.1 | 63.8 | 78.6 | 81.2 | 69.8 |\n| Ours (GPT-4o) | 67.7 | 62.8 | 80.6 | 84.5 | 69.9 |", "caption": "Table 2: Quantitative results on Seedbench\u00a0[31].", "description": "This table presents a quantitative comparison of various vision-language models on the SeedBench benchmark [31].  The benchmark focuses on evaluating a model's ability to understand spatial relationships in images. The table shows the average performance scores achieved by each model across several sub-tasks within the benchmark, providing a detailed performance comparison across different aspects of spatial reasoning and visual understanding.", "section": "4. Experiments"}, {"content": "| Model | Acc | Error rate | Avg. # sols |\n|---|---|---|---| \n| Full model | **50.5** | **0.0** | **3.0** |\n| (-) _code debugger_ | 40.0 | 1.7 | 2.8 |\n| (-) _code checker_ | 33.3 | 20.8 | **3.0** |\n| (-) _requirement checker_ | 48.1 | 0.5 | 2.4 |\n| (-) _repetition checker_ | 40.5 | 17.8 | 2.0 |", "caption": "Table 3: Ablation. of significance of multi-agent conversation.", "description": "This table presents an ablation study analyzing the impact of each component within the multi-agent conversation system used in the MMFactory's solution router.  It shows how removing different agents (code debugger, code checker, requirement checker, and repetition checker) affects the model's accuracy, error rate, and the average number of solutions generated. This helps to understand the contribution of each agent to the overall performance and efficiency of the solution generation process.", "section": "4.3 Model Analysis"}, {"content": "| Model | Execution cost (sec) |  | Routing cost (sec) |  |\n|---|---|---|---|---|\n| Sketchpad [26] | 19.96 | 43.86 | 18.20 | 30.90 |\n| Ours | 9.74 | 29.43 | \u2248 0.00 | \u2248 0.00 |", "caption": "Table 4: API calling cost analysis per 10 samples (in USD).", "description": "This table presents a cost analysis of API calls, specifically focusing on the expense incurred per 10 samples processed within the MMFactory framework. The results are categorized by different models, showing the mean and variance of API calling costs, providing insights into the economic efficiency of various approaches within the MMFactory system.", "section": "4. Experiments"}]