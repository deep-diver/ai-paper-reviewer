[{"figure_path": "https://arxiv.org/html/2502.01237/x1.png", "caption": "Figure 1: \nImpact of the \u03b2\ud835\udefd\\betaitalic_\u03b2 Parameter on ASFT and ORPO Alignment Quality. The plot shows how tuning \u03b2\ud835\udefd\\betaitalic_\u03b2 (Section\u00a03.1.2) affects both ASFT and ORPO performance. Results are reported for GPT-4 Win Rate in the Llama 3.2 3B TL;DR setup and for AlpacaEval 2 LC Win Rate in the Llama 3.1 8B UF scenario. All other hyperparameters (e.g., learning rates) are selected via grid search, using each method\u2019s best configuration at \u03b2=1\ud835\udefd1\\beta=1italic_\u03b2 = 1 as the baseline. See Section\u00a05.2 for more details.", "description": "This figure shows the effect of tuning the beta parameter (\u03b2) on the performance of two direct alignment algorithms: ASFT and ORPO.  The x-axis likely represents different values of \u03b2, while the y-axis shows the performance metric (GPT-4 win rate for Llama 3.2 3B TL;DR, and AlpacaEval 2 LC win rate for Llama 3.1 8B UF).  The graph likely contains bars or lines comparing the performance of ASFT and ORPO at different values of \u03b2.  Each algorithm's performance at \u03b2=1 is likely used as a baseline for comparison.  The results illustrate how adjusting \u03b2 impacts the trade-off between alignment quality and potential overfitting.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2502.01237/x3.png", "caption": "Figure 2: GPT-4 Evaluation of Llama 3.2 3B TL;DR setup. The comparison shows multiple alignment methods (rows) using their best hyperparameters, where each approach aims to generate concise and accurate summaries. Most methods exceed 90% Win Rate; ASFT achieves 87.2%, maintaining robust summarization performance. See Section\u00a05.2 for more details.", "description": "This figure displays the results of evaluating various language model alignment methods on a summarization task using the Llama 3.2 3B model and the Reddit TL;DR dataset.  The evaluation is performed using GPT-4 to compare the quality of summaries generated by different methods. The y-axis represents the win rate (percentage of times a method's summary was judged superior by GPT-4), and the x-axis shows different methods.  The results indicate that most alignment methods achieved a high win rate (over 90%), with ASFT showing a slightly lower but still strong performance of 87.2%. This demonstrates the effectiveness of the alignment techniques in generating high-quality summaries.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2502.01237/x4.png", "caption": "Figure 3: Pareto front for alignment quality and KL divergence. Results for Llama 3.1 8B UF on AlpacaEval 2 LC. Methods are grouped into pairwise and pointwise categories, with pairwise achieving higher LC values while remaining within overlapping confidence intervals. See Section 5.3 for more details.", "description": "This figure displays Pareto fronts illustrating the trade-off between alignment quality (measured by AlpacaEval 2 Length-Controlled Win Rate) and KL divergence from a reference model for Llama 3.1 8B UF.  The data points are categorized into those using pairwise and pointwise ranking methods.  The results show that pairwise methods generally achieve better alignment quality (higher LC Win Rate) compared to pointwise methods, while maintaining KL divergence within overlapping confidence intervals.", "section": "5. Results"}]