[{"figure_path": "https://arxiv.org/html/2504.03011/x2.png", "caption": "Figure 1: We introduce Comprehensive Relighting, a generalizable and consistent model for relighting and harmonization, which controls the lighting property from a single image or video of humans with arbitrary body parts. Given target lighting coefficients, e.g., Spherical harmonics (second), background scenes (third), or their combination (fourth), our model performs consistent and harmonized relighting.", "description": "This figure demonstrates the capabilities of the Comprehensive Relighting model.  The model takes as input a single image or video of a person (or multiple people), and allows for precise control over the lighting in the output image.  The figure shows four examples: the original input, relighting controlled by spherical harmonics, relighting controlled by a background image, and relighting controlled by a combination of spherical harmonics and a background image.  In each example, the output is a harmonized image where lighting and background are seamlessly blended.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.03011/x3.png", "caption": "Figure 2: Comparison of various baseline methods for relighting settings and functionalities.", "description": "This figure provides a comparison of different baseline methods used for human relighting. It shows whether each method supports various functionalities such as image relighting, spherical harmonics (SH) relighting, background harmonization, video relighting, video consistency, and generalizability.  The table visually represents the capabilities of each method across different relighting settings and tasks, making it easy to compare their strengths and weaknesses.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2504.03011/x4.png", "caption": "Figure 3: Our model generalizes to various body parts (portrait, half-body, full-body, multiperson) for relighting and harmonization, with lighting control variables shown in the insets.", "description": "This figure demonstrates the generalizability of the Comprehensive Relighting model.  It showcases consistent and harmonized relighting and harmonization results across a wide range of human poses and body configurations, including portraits, half-body shots, full-body shots, and multi-person scenes.  The inset images show different lighting control variables used for each example, highlighting the model's ability to manipulate lighting conditions effectively.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.03011/x5.png", "caption": "Figure 4: System overview. (a) Given an input image of humans with coarse lighting and background image, our diffusion model generates the relit images harmonized with background scenes (Sec.\u00a03.2). (b) The external temporal modules learn the temporal cycle consistency from many real-world videos to construct temporal lighting features (Sec.\u00a03.3). (c) In inference time, we blend the features from lighting and temporal modules spatially and temporally to enable coherent and generalizable human relighting (Sec.\u00a03.4).", "description": "This figure provides a high-level overview of the Comprehensive Relighting system.  Panel (a) illustrates the core coarse-to-fine relighting and harmonization process. It shows how an input image, along with coarse lighting parameters and a background image, is used by a diffusion model to generate a finely detailed relit image that is seamlessly integrated with the background. Panel (b) details the unsupervised temporal lighting model. This model learns temporal consistency from real-world videos to predict future lighting conditions, which improves the temporal coherence of the relit videos. Panel (c) describes the inference process, showing how the lighting and temporal modules are blended (both spatially and temporally) to produce consistent and generalizable relighting results. ", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.03011/x6.png", "caption": "Figure 5: Qualitative comparison of synthetic video frames (corresponding to Tab.\u00a01). From left to right: composite input with target lighting parameters (inset), our relit result, baseline methods, and normalized L2\u2193\u2193\\downarrow\u2193 photometric error map (inset).", "description": "Figure 5 presents a qualitative comparison of synthetic video frames, directly corresponding to the results presented in Table 1.  The figure displays a sequence of frames for each method. Each sequence starts with a composite input image. This composite image shows the original video frame with the target lighting parameters overlaid as an inset.  Following the input, the figure shows the relighting result produced by the proposed method ('Ours'). Then, it shows the relighting results obtained by four existing baseline methods (GFR, RHW, DPR, LPBR). Finally, a normalized L2 photometric error map (as an inset) quantifies the visual difference between the ground truth and each method's output.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x7.png", "caption": "Figure 6: Comparison with LPBR\u00a0[41] on DeepFashion\u00a0[33] real images for background harmonization testing. The first row shows the relit output, and the second shows the magnified results.", "description": "This figure compares the performance of the proposed Comprehensive Relighting model against the LPBR [41] method for background harmonization using real images from the DeepFashion [33] dataset.  The top row displays the results of the proposed method's relighting, demonstrating its ability to harmonize the foreground with the background image. The second row provides a magnified view of the relighting, highlighting the fine details and demonstrating the method's ability to achieve a natural and cohesive result.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x8.png", "caption": "Figure 7: Comparison with RHW\u00a0[51] on Pexels\u00a0[37] real images. The lighting control variables are shown as insets. While RHW produces reasonable relighting for full-body images, its quality degrades on half-body and multi-person cases.", "description": "Figure 7 presents a comparison of the proposed Comprehensive Relighting model with the RHW method [51] using real images sourced from Pexels [37].  The results showcase the performance of both methods under various lighting conditions controlled by user-specified parameters (shown in insets).  While RHW demonstrates acceptable relighting results for full-body images, its performance noticeably diminishes in quality when applied to images featuring half-bodies or multiple people. This highlights a limitation of RHW's generalizability compared to the proposed approach.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x9.png", "caption": "Figure 8: Comparison with GFR\u00a0[23] on Pexels\u00a0[37] real images. The lighting control variables are shown as insets. Limited generalizability of GFR results in reduced output quality for half-body and multi-person cases.", "description": "This figure compares the performance of the proposed Comprehensive Relighting model against the GFR [23] method on real images from the Pexels [37] dataset.  The input image, along with the target lighting conditions (shown as insets), are provided to both models.  The results demonstrate that the Comprehensive Relighting model produces higher-quality relighting outputs, particularly for scenes with half-bodies or multiple people.  The limited generalizability of GFR leads to significantly degraded results in these scenarios compared to the proposed method.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x10.png", "caption": "Figure 9: Relighting and Harmonization diffusion model training and denoising pipeline.", "description": "This figure illustrates the training and sampling (denoising) pipeline of the diffusion model for fine-grained human relighting and harmonization.  The process begins with an input image and target lighting parameters, including coarse shading and background images. The image is then encoded, combined with the lighting variables through the Lighting ControlNet module and processed within the Stable Diffusion model (a U-Net architecture with an encoder, middle block, and decoder).  The model outputs a fine-grained relit image harmonized with the conditioning background image.  The denoising process involves progressively removing noise from a latent representation of the image until a clear relit image is obtained.  This pipeline aims to generate a high-quality, fine-grained relit image of humans, harmonized with a background scene, all conditioned on the coarse shading and background image.", "section": "3.2 Coarse-to-Fine Relighting and Harmonization"}, {"figure_path": "https://arxiv.org/html/2504.03011/x11.png", "caption": "Figure 10: Left side: Training data scale comparisons; Right side: Breakdown of our training and evaluation dataset information.", "description": "This figure provides a comparison of the scale of our training dataset with other related works and also shows a breakdown of the demographic information of our dataset. The left side shows a bar chart comparing the number of training images and subjects across several published datasets related to human relighting. It highlights that our dataset, while smaller than other datasets, is sufficiently large and diverse for our purposes.  The right side displays a breakdown of our training and evaluation datasets' demographic information, including gender, skin tone, and body part coverage. This information helps in understanding the representativeness of our dataset and ensures we considered factors like diversity in gender, race and body shapes to avoid bias in our model.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x12.png", "caption": "Figure 11: Left: Our shading estimation network, Right: Convolutional and deconvolutional blocks.", "description": "This figure illustrates the architecture of the shading estimation network used in the Comprehensive Relighting model. The left panel shows the overall network structure, which consists of an encoder that processes the input normal map and mask, and a decoder that generates the shading map.  The encoder uses convolutional blocks, while the decoder utilizes deconvolutional blocks to upsample the feature maps and reconstruct the high-resolution shading. The right panel details the structures of these convolutional and deconvolutional blocks, which are the fundamental building blocks of the network.  Each block includes multiple convolutional or deconvolutional layers, along with activation functions (LReLU) and instance normalization layers to enhance performance.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.03011/x13.png", "caption": "Figure 12: Training samples of the relighting data with half-body portraits (up) and simulation data with full-body images (bottom) .", "description": "This figure displays examples from the training dataset used for the Comprehensive Relighting model. The top half showcases examples of half-body portraits, while the bottom half presents full-body images generated through simulation. Each row consists of several images depicting various aspects of the data, including the mask (representing the subject's silhouette), albedo (the base color of the subject), the original input image, different background scenes, corresponding lighting conditions, and the final relit image. This visual representation highlights the variety and complexity of the training data used to enable the model's capability to perform generalizable and consistent human relighting and harmonization.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.03011/x14.png", "caption": "Figure 13: Comparison with harmonization methods (IC-Light). Left side is multi-person testing, right side is zoom in result.", "description": "This figure compares the results of the proposed Comprehensive Relighting model with the IC-Light harmonization method. The left side shows multi-person relighting results, where multiple individuals are present in the scene.  The right side provides a zoomed-in view of a specific portion of the image, highlighting finer details and allowing for a more detailed comparison of the relighting and harmonization effects between the two methods.  This visual comparison aims to demonstrate the effectiveness of the proposed model in generating high-quality, temporally consistent, and generalizable human relighting and harmonization results, even in complex scenes with multiple people.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x15.png", "caption": "Figure 14: Training pipeline for coarse lighting estimation network.", "description": "This figure illustrates the training process of the coarse shading estimation network, a crucial component in the Comprehensive Relighting model.  The network takes as input the surface normal map (N), a foreground mask (M), and the target lighting parameters (\u03c6, Spherical Harmonics). An encoder processes N and M into a latent space.  In this latent space, the lighting parameters (\u03c6) are conditioned.  The decoder then decodes the combined information in the latent space to generate the coarse shading map (S). This map represents a coarse estimation of the shading under the target lighting conditions, crucial for the subsequent fine-grained relighting stage.", "section": "3.2 Coarse-to-Fine Relighting and Harmonization"}, {"figure_path": "https://arxiv.org/html/2504.03011/x16.png", "caption": "Figure 15: Qualitative comparisons conducted on synthetic data. From top to bottom: full-body testing, multi-person testing. The ground truth data is displayed in the last column.", "description": "This figure presents a qualitative comparison of different human relighting methods using synthetic data.  It showcases results for both full-body and multi-person scenarios.  The top row displays the original input images with their respective target lighting. Subsequent rows show results generated by various relighting models. The final column in each section displays the ground truth results for comparison, allowing for a visual assessment of the accuracy and quality of each method. ", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x17.png", "caption": "Figure 16: Comparison with DPR on face and half-body relighting on Pexels\u00a0[37] real images.", "description": "This figure compares the performance of the proposed Comprehensive Relighting model against the DPR (Deep Portrait Relighting) method on real images from the Pexels dataset.  The comparison focuses on face and half-body relighting scenarios.  Multiple lighting conditions are applied to demonstrate the generalizability and robustness of each model in handling variations in lighting and body pose. The results showcase the ability of each model to realistically relight the images, highlighting the differences in quality and detail preservation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x18.png", "caption": "Figure 17: Our LigtStage data testing (Left) and comparison with other relighting baselines (Right).", "description": "This figure shows a comparison of relighting results on LightStage data. The left side displays the results obtained using the proposed method, showcasing the input image, the estimated coarse shading, and the final relit image. The right side presents a comparison with other state-of-the-art relighting baselines (DPR, GFR, and RHW), demonstrating the superior performance of the proposed method in terms of visual quality and accuracy.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x19.png", "caption": "Figure 18: Strong shadow testing results (left) and failure cases (right) on real images from Pexels\u00a0[37].", "description": "This figure showcases the performance of the Comprehensive Relighting model under challenging lighting conditions. The left half presents successful relighting results where strong shadows are effectively handled, demonstrating the model's robustness. In contrast, the right half displays instances where the model struggles, highlighting limitations in handling complex shadow patterns in real-world scenarios. These failure cases provide insights into areas for future model improvement and refinement.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x20.png", "caption": "Figure 19: Video relighting comparison results on synthetic testing data: from left to right, we show comparison results for Scenario 1, 2, 3. From top to bottom, the first row shows the composite input (foreground human albedo composited with background image), the second row shows the ground truth (GT) shading, and the third row shows the GT image.", "description": "This figure compares video relighting results across different methods for three scenarios.  Each scenario features a unique lighting condition and human movement. The top row shows the input for each scenario, a composite of the human albedo (a representation of the person's surface color and texture, independent of lighting) and the background. The second row shows the ground truth shading (how light would realistically fall on the human in the scene), and the third row displays the ground truth image with correct lighting for comparison. The subsequent rows depict the results from different relighting methods, allowing for a visual comparison of their accuracy against the ground truth.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x21.png", "caption": "Figure 20: Real image comparisons with other human relighting approaches on the DeepFashion dataset\u00a0[33]. We test on different identities and body parts (full body, half body). Our model shows consistent and feasible relighting with varying target lighting parameters (Spherical harmonics).", "description": "This figure compares the performance of the proposed Comprehensive Relighting model against other state-of-the-art human relighting methods on real images from the DeepFashion dataset.  The test encompasses diverse human identities and body types (full-body and half-body poses).  The results demonstrate the model's consistent and reliable ability to produce realistic relighting effects under a variety of target lighting conditions defined using Spherical Harmonics coefficients.", "section": "B. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2504.03011/x22.png", "caption": "Figure 21: We present real image comparisons with the harmonization method. Given a composite input image, our model can achieve effective harmonization. When provided with target lighting parameters (Spherical harmonics), our model can achieve both background harmonization and relighting. The top section displays the outputs of our background harmonization method compared to the results from [41]. The lower section presents harmonization and relighting comparisons with [23]. Due to the higher generative prior of LPBR, noticeable distortions are present on the human face. Although GFR can achieve both harmonization and relighting, it exhibits obvious color noise.", "description": "This figure compares the performance of the proposed Comprehensive Relighting model against existing harmonization methods ([41] and [23]) using real images.  The top half demonstrates background harmonization only, showing the results of the proposed method and LPBR [41]. The bottom half shows results for both harmonization and relighting, comparing the proposed method to both LPBR [41] and GFR [23].  The comparison highlights that while LPBR achieves harmonization, it introduces noticeable distortions, particularly on faces. GFR, while capable of both tasks, exhibits significant color noise in its results.  In contrast, the proposed method demonstrates effective harmonization and relighting without artifacts.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x23.png", "caption": "Figure 22: User study results: Preferences between our model and other relighting and harmonization models, including our general object testing.", "description": "This figure presents the results of a user study comparing the performance of different relighting and harmonization models, including the authors' proposed model and several state-of-the-art baselines.  The study assessed three aspects of model performance: overall relighting quality, how well each model preserved the subject's identity, and how effectively each model harmonized the subject with the background scene.  The results are displayed using pie charts that show the percentage of user preferences for each model on each of the three criteria.  The inclusion of \"general object testing\" suggests that the models were evaluated on a broader range of images than just human portraits, encompassing various objects or scenes.", "section": "B.4. User study"}, {"figure_path": "https://arxiv.org/html/2504.03011/x24.png", "caption": "Figure 23: Our model can achieve realistic relighting with lighting 1 and background harmonization.", "description": "This figure showcases the model's ability to perform realistic relighting and background harmonization.  Multiple subjects are shown, each with varied poses and clothing.  The first column displays the input image. The next column depicts the subject with relighting applied, based on Lighting Condition 1, maintaining consistency in the subject's appearance. The following columns demonstrate the harmonization of the subject with five different background images, illustrating how the lighting and subject seamlessly integrate into each scene. This highlights the model's capacity to realistically blend foreground and background elements, demonstrating both relighting and harmonization capabilities.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03011/x25.png", "caption": "Figure 24: Our model can achieve realistic relighting with lighting 2 and background harmonization.", "description": "This figure shows the results of applying the proposed Comprehensive Relighting model to generate realistic relighting effects with different background scenes.  The input image shows individuals against a neutral background.  The 'Relighting' column demonstrates the consistent relighting of these individuals across various lighting conditions (lighting parameter set 2).  The subsequent columns showcase the harmonization of the individuals with five distinct backgrounds, demonstrating the model's capability to seamlessly integrate them into different environmental contexts while maintaining consistent lighting.", "section": "Experiments"}]