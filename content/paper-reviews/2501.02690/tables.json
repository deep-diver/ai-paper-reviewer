[{"content": "| Methods | Kinetics AJ \u2191 | Kinetics <\u03b4xavg\u2191 | Kinetics OA \u2191 | DAVIS AJ \u2191 | DAVIS <\u03b4xavg\u2191 | DAVIS OA \u2191 | RGB-Stacking AJ \u2191 | RGB-Stacking <\u03b4xavg\u2191 | RGB-Stacking OA \u2191 | Average AJ \u2191 | Average <\u03b4xavg\u2191 | Average OA \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| TAP-Net [11] | 38.5 | 54.4 | 80.6 | 33.0 | 48.6 | 78.8 | 54.6 | 68.3 | 87.7 | 42.0 | 57.1 | 82.4 |\n| PIPs [20] | 31.7 | 53.7 | 72.9 | 42.2 | 64.8 | 77.7 | 15.7 | 28.4 | 77.1 | 29.9 | 50.0 | 75.9 |\n| OmniMotion [49] | - | - | - | 46.4 | 62.7 | 85.3 | 69.5 | 82.5 | 90.3 | - | - | - |\n| TAPIR [12] | 49.6 | 64.2 | 85.0 | 56.2 | 70.0 | 86.5 | 54.2 | 69.8 | 84.4 | 53.3 | 68.0 | 85.3 |\n| BootsTAPIR [13] | 54.6 | 68.4 | 86.5 | 61.4 | 73.6 | 88.7 | - | - | - | - | - | - |\n| CoTracker [27] | 48.7 | 64.3 | 86.5 | 60.6 | 75.4 | 89.3 | 63.1 | 77.0 | 87.8 | 57.4 | 72.2 | 87.8 |\n| DOT [32] | 48.4 | 63.8 | 85.2 | 60.1 | 74.5 | 89.0 | 77.1 | 87.7 | 93.3 | 61.9 | 75.3 | 89.2 |\n| SpatialTracker [53] | 50.1 | 65.9 | 86.9 | 61.1 | 76.3 | 89.5 | 63.5 | 77.6 | 88.2 | 58.2 | 73.3 | 88.2 |\n| Ours + ZoeDepth | 51.9 | 64.6 | 86.1 | 61.3 | 74.5 | 89.4 | 77.0 | 86.4 | 92.8 | 63.4 | 75.2 | 89.4 |", "caption": "Table 1: Comparison of 2D point tracking on TAPVid.", "description": "This table presents a comparison of various 2D point tracking methods on the TAPVid benchmark dataset.  The TAPVid benchmark consists of three subsets: Kinetics (human-related actions), DAVIS (real-world videos), and RGB-Stacking (synthetic robotic videos).  For each method and dataset, the table shows three metrics: Average Jaccard (AJ), which measures the average fraction of visible points within a certain distance of their ground truth positions; the average fraction of visible points within a certain distance of their ground truth positions (< \u03b4avg), and Overall Accuracy (OA), representing the overall success rate of point tracking. Higher values for AJ and OA indicate better performance.", "section": "4.1 Dense 3D Point Tracking"}, {"content": "| Methods | Aria 3D-AJ \u2191 | Aria APD \u2191 | Aria OA \u2191 | DriveTrack 3D-AJ \u2191 | DriveTrack APD \u2191 | DriveTrack OA \u2191 | PStudio 3D-AJ \u2191 | PStudio APD \u2191 | PStudio OA \u2191 | Average 3D-AJ \u2191 | Average APD \u2191 | Average OA \u2191 |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| BootsTAPIR [13] + ZoeDepth [4] | 8.6 | 14.5 | 86.9 | 5.1 | 8.7 | 83.5 | 10.2 | 17.7 | 82.0 | 8.0 | 13.6 | 84.1 |\n| SpatialTracker [53] | 9.2 | 15.1 | 89.9 | 5.8 | 10.2 | 82.0 | 9.8 | 17.7 | 78.4 | 8.3 | 14.3 | 83.4 |\n| Ours + ZoeDepth [4] | 10.0 | 16.1 | 90.0 | 7.2 | 12.0 | 81.1 | 9.8 | 17.3 | 80.5 | 9.0 | 15.1 | 83.9 |\n| Ours + Depth Anything V2 [54] | 14.5 | 21.9 | 90.0 | 8.6 | 13.8 | 81.1 | 11.3 | 19.4 | 80.5 | 11.4 | 18.3 | 83.9 |\n| Ours + UniDepth V2 [37] | 15.0 | 22.2 | 90.0 | 11.6 | 18.1 | 81.1 | 6.6 | 12.2 | 80.5 | 11.0 | 17.5 | 83.9 |", "caption": "Table 2: Comparison of 3D point tracking on TAPVid-3D minival split.", "description": "This table presents a comparison of different 3D point tracking methods on the TAPVid-3D minival dataset split.  It shows the performance of each method across three metrics: 3D Average Jaccard Index (3D-AJ), Average Point Depth (APD), and Occlusion Accuracy (OA). The results are presented for three different subsets within the TAPVid-3D dataset: Aria, DriveTrack, and PStudio, offering a comprehensive evaluation across various video types and motion characteristics.  The table helps demonstrate the superior performance of the proposed D3D-PT method compared to existing state-of-the-art sparse 3D point tracking techniques.", "section": "4.1 Dense 3D Point Tracking"}, {"content": "| Methods | DAVIS PSNR \u2191 | DAVIS SSIM \u2191 | DAVIS LPIPS \u2193 | Sora PSNR \u2191 | Sora SSIM \u2191 | Sora LPIPS \u2193 | Pixabay PSNR \u2191 | Pixabay SSIM \u2191 | Pixabay LPIPS \u2193 |\n|---|---|---|---|---|---|---|---|---|---| \n| MonST3R | 14.12 | 0.59 | 0.31 | 15.32 | 0.59 | 0.30 | 19.78 | 0.74 | 0.22 |\n| GCD | 15.04 | 0.41 | 0.48 | 11.96 | 0.32 | 0.52 | 13.71 | 0.42 | 0.50 |\n| Ours | 19.18 | 0.60 | 0.23 | 17.92 | 0.60 | 0.20 | 22.66 | 0.73 | 0.15 |", "caption": "Table 3: Comparison of the multi-shooting video generation quality.", "description": "This table presents a quantitative comparison of the quality of multi-camera shooting videos generated by different methods.  The metrics used are PSNR, SSIM, and LPIPS, calculated for videos generated on three datasets: DAVIS, Sora, and Pixabay.  Each dataset represents different characteristics of videos, allowing for a more robust assessment across diverse video content.", "section": "4.2. 4D Video Control"}, {"content": "| Methods | DAVIS |  |  |  |\n|---|---|---|---|---|\n| AJ \u2191 | &lt;{\n\u03b4}^{x}_{avg}\n\u2191 | OA \u2191 |  |  |\n| RGB-RAFT (DOT) [32] | 60.1 | 74.5 | 89.0 |  |\n| RGBD-RAFT | 55.7 | 71.6 | 86.7 |  |\n| D3D-PT (Ours) | **63.4** | **75.2** | **89.4** |  |", "caption": "Table A1: Ablation Study on DAVIS.", "description": "This table presents the results of an ablation study conducted on the DAVIS dataset to evaluate the effectiveness of the proposed dense 3D point tracking method.  It compares the performance of three different approaches: RGB-RAFT (DOT), which uses the RAFT network for RGB image data; RGBD-RAFT, an extension of the RAFT network that incorporates depth information; and the authors' proposed D3D-PT method.  The comparison is based on three metrics: Average Jaccard (AJ), average number of pixels within a given threshold of ground truth (\u03b4\u0394uv), and Occlusion Accuracy (OA).  The results demonstrate the superiority of the proposed D3D-PT method.", "section": "Appendix"}]