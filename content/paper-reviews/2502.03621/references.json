{"references": [{"fullname_first_author": "Omer Bar-Tal", "paper_title": "Lumiere: A Space-Time Diffusion Model for Video Generation", "publication_date": "2024-01-12", "reason": "This paper introduces a novel space-time diffusion model for video generation, which is directly leveraged by the DynVFX method for its core video generation capabilities."}, {"fullname_first_author": "Wenyi Hong", "paper_title": "CogVideo: Large-scale pretraining for text-to-video generation via transformers", "publication_date": "2022-05-15", "reason": "This work provides the foundation for the text-to-video diffusion model (CogVideoX) used in DynVFX, which is crucial for the generation of dynamic content in real videos."}, {"fullname_first_author": "Zhuoyi Yang", "paper_title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer", "publication_date": "2024-08-06", "reason": "This paper introduces CogVideoX, a specific text-to-video diffusion model variant that enhances upon the original CogVideo model and is directly applied in the DynVFX framework."}, {"fullname_first_author": "Chenlin Meng", "paper_title": "SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations", "publication_date": "2022-00-00", "reason": "The SDEdit method is used as a baseline for DynVFX's video editing approach; understanding SDEdit is vital to comprehending DynVFX's improvements and novelties."}, {"fullname_first_author": "Jiaming Song", "paper_title": "Denoising Diffusion Implicit Models", "publication_date": "2020-00-00", "reason": "This work introduces denoising diffusion implicit models, a fundamental concept underpinning several methods used in DynVFX, particularly the diffusion model's theoretical framework."}]}