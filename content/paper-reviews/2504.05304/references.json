{"references": [{"fullname_first_author": "Vaswani, A.", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which is fundamental to many modern deep learning models, including the DiT used in this paper."}, {"fullname_first_author": "Ho, J.", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This paper introduces Denoising Diffusion Probabilistic Models (DDPMs), a foundational generative model which is widely used and referenced."}, {"fullname_first_author": "Song, Y.", "paper_title": "Score-based generative modeling through stochastic differential equations", "publication_date": "2021-01-01", "reason": "This paper generalizes diffusion models to score-based generative modeling and connects them to stochastic differential equations (SDEs), providing a theoretical framework that underlies the work."}, {"fullname_first_author": "Peebles, W.", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "This work introduces Diffusion Transformers (DiT), which directly leverages transformers as a backbone in diffusion models, the architecture used for ImageNet generation in this submission."}, {"fullname_first_author": "Rombach, R.", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces latent diffusion models (LDMs), which operate in a lower-dimensional latent space to reduce computational costs, it also allows for high resolution image synthesis."}]}