[{"content": "| Benchmark | Evaluation Scenarios | Data Generation | Evaluation Metrics | Evaluation Models |\n|---|---|---|---|---| \n| | Task-spe. | Manual | Rule | Retriever |\n| | Topic-spe. | Auto. | Model | Generator |\n| | | | Human | |\n| PIXIU [Xie et al. (2023)] | \u2713 | \u2717 | \u2713 | \u2717 |\n| | \u2717 | \u2717 | \u2717 | \u2713 |\n| DISC-FinLLM [Chen et al. (2023)] | \u2713 | \u2717 | \u2713 | \u2713 |\n|  | \u2717 | \u2713 | \u2713 | \u2713 |\n| FinanceBench [Islam et al. (2023)] | \u2713 | \u2713 | \u2717 | \u2717 |\n| | \u2713 | \u2713 | \u2717 | \u2717 |\n| AlphaFin [Li et al. (2024)] | \u2713 | \u2717 | \u2713 | \u2713 |\n| | \u2717 | \u2717 | \u2713 | \u2713 |\n| FinBen [Xie et al. (2024)] | \u2713 | \u2717 | \u2713 | \u2713 |\n| | \u2717 | \u2717 | \u2717 | \u2713 |\n| FinTextQA [Chen et al. (2024a)] | \u2713 | \u2717 | \u2713 | \u2713 |\n|  | \u2717 | \u2717 | \u2717 | \u2713 |\n| OmniEval | \u2713 | \u2713 | \u2713 | \u2713 |\n|  | \u2713 | \u2713 | \u2713 | \u2713 |", "caption": "Table 1: The comparison between our proposed benchmark with existing financial benchmarks. \u201cAuto.\u201d is short for \u201cAutomated-generated\u201d, \u201cSpe.\u201d is short for \u201cSpecific\u201d.", "description": "This table compares OmniEval with other existing financial benchmarks like PIXIU, DISC-FinLLM, FinanceBench, AlphaFin, FinBen, and FinTextQA across different evaluation aspects. These aspects include whether the benchmark supports Task-specific or Topic-specific scenarios, utilizes Manual or Automated data generation methods, employs rule-based or model-based evaluation metrics, and offers evaluations based on retriever and generator models.", "section": "2 Related Work"}, {"content": "| Data Source | Data Type | Doc. Count | Length Sum. | Avg. Length per Doc. |\n|---|---|---|---|---| \n| BSCF-DB | DB - JSON | 193,774 | 23,631,875 | 122 |\n| BSCF-PDF | PDF - TXT | 3,082 | 10,587,648 | 3,435 |\n| FinGLM | PDF - TXT | 55,595 | 97,296,690 | 1,750 |\n| Wiki-Fin | JSON | 3,367 | 5,679,758 | 1,687 |\n| BAAI-Fin | JSON | 48,124 | 70,014,858 | 1,455 |\n| Official Websits | JSON | 58,616 | 45,837,298 | 782 |", "caption": "Table 2: Statistical information of our diverse data sources. \u201cDoc.\u201d, \u201cSum.\u201d, and \u201cAvg.\u201d are short for \u201cDocument\u201d, \u201csummation\u201d, and \u201cAverage\u201d.", "description": "This table presents statistics of the diverse data sources used to construct the knowledge corpus.  It lists the data source, the data type, the number of documents, the total length of text across all documents, and the average length of each document.", "section": "3. Construction Pipeline of OmniEval"}, {"content": "| Setting | Base Model | $\\kappa$ | Accuracy |\n|---|---|---|---| \n| Zero-shot | Llama3.1-8B-Inst | 39.70 | 55.60 |\n| Zero-shot | Llama3.1-70B-Inst | 54.14 | 66.40 |\n| Zero-shot | Qwen2.5-7B-Inst | 48.05 | 62.00 |\n| Zero-shot | Qwen2.5-32B-Inst | <u>61.44</u> | <u>71.60</u> |\n| Zero-shot | Qwen2.5-72B-Inst | 55.38 | 67.20 |\n| Lora | Llama3.1-8B-Inst | 48.63 | 62.80 |\n| Lora | Qwen2.5-7B-Inst | **64.86** | **74.40** |", "caption": "Table 3: Experimental results of model-based evaluator.", "description": "This table presents the accuracy scores of different large language models (LLMs) when evaluated against human annotations on five model-based metrics for assessing Retrieval-Augmented Generation (RAG) systems in the financial domain. These metrics are: accuracy, completeness, hallucination, utilization, and numerical accuracy. The LLMs are evaluated in zero-shot and Lora fine-tuned settings and compared against a fine-tuned LLM specifically trained for evaluation tasks. The table demonstrates the performance improvement achieved by the supervised fine-tuning of the LLM evaluator.", "section": "3.3 Evaluation of RAG Models"}, {"content": "| Models | MAP \u2191 | MRR \u2191 | Rouge-L \u2191 | F1 \u2191 | ACC \u2191 | HAL \u2193 | COM \u2191 | UTL \u2191 | NAC \u2191 |\n|---|---|---|---|---|---|---|---|---|---|\n|  |  |  |  |  |  |  |  |  |  |\n| Automated-generated evaluation set |  |  |  |  |  |  |  |  |  |\n| Jina-zh | 0.3395 | 0.3469 | 0.1662 | 0.2553 | 0.3908 | 0.0794 | 0.5981 | 0.5078 | 0.2837 |\n| BGE-large-zh | 0.3777 | 0.3865 | 0.1693 | 0.2541 | 0.4080 | **0.0597** | 0.6048 | 0.5194 | **0.3124** |\n| BGE-M3 | **0.3961** | **0.4057** | **0.1746** | **0.2593** | **0.4091** | 0.0634 | **0.6092** | **0.5203** | 0.3060 |\n| GTE-Qwen2-1.5b | **0.4370** | **0.4491** | **0.1778** | **0.2563** | **0.4326** | **0.0467** | **0.6256** | **0.5613** | **0.3293** |\n|  |  |  |  |  |  |  |  |  |  |\n| Human-annotated evaluation set |  |  |  |  |  |  |  |  |  |\n| Jina-zh | 0.3458 | 0.3533 | 0.2341 | 0.3821 | 0.4089 | 0.0886 | 0.5930 | 0.5163 | 0.3073 |\n| BGE-large-zh | **0.4153** | **0.4252** | 0.2435 | 0.3870 | 0.4325 | 0.0718 | **0.6224** | 0.5367 | **0.3545** |\n| BGE-M3 | 0.4152 | 0.4236 | **0.2517** | **0.3913** | **0.4450** | **0.0709** | **0.6208** | **0.5410** | 0.3472 |\n| GTE-Qwen2-1.5b | **0.4443** | **0.4574** | **0.2528** | **0.3919** | **0.4476** | **0.0618** | 0.6190 | **0.5576** | **0.3595** |", "caption": "Table 4: The overall results of retrieval models with the generator being set as Qwen2.5-72B.", "description": "This table presents the evaluation results of various retrieval models (Jina-zh, BGE-large-zh, BGE-M3, and GTE-Qwen2-1.5b) when combined with a fixed large language model (Qwen2.5-72B) for retrieval-augmented generation. The metrics used for evaluation include Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Rouge-L, F1 score, Accuracy (ACC), Hallucination (HAL), Completeness (COM), Utilization (UTL), and Numerical Accuracy (NAC).  The results are presented for both automated-generated and human-annotated evaluation sets.", "section": "4.1 Overall Experimental Results"}, {"content": "| Retriever | Generator | Rouge-L \u2191 | F1 \u2191 | ACC \u2191 | HAL \u2193 | COM \u2191 | UTL \u2191 | NAC \u2191 |\n|---|---|---|---|---|---|---|---|---| \n|---|---:|---:|---:|---:|---:|---:|---:|---:|\n| Automated-generated evaluation set | | | | | | | | |\n| CLOSE | Yi15-34B | 0.0326 | 0.0673 | 0.1573 | - | 0.5063 | - | 0.0693 |\n| CLOSE | Deepseek-v2-chat | 0.1861 | 0.3709 | 0.3587 | - | 0.5755 | - | 0.1121 |\n| CLOSE | Qwen2.5-72B | 0.1607 | 0.3222 | 0.3788 | - | 0.6017 | - | 0.1256 |\n| CLOSE | Llama3-70B-Instruct | 0.1993 | 0.3989 | 0.3238 | - | 0.5284 | - | 0.0677 |\n| GTE-Qwen2-1.5B | Yi15-34B | 0.0593 | 0.0958 | 0.3402 | **0.0597** | 0.5778 | 0.4229 | 0.1682 |\n| GTE-Qwen2-1.5B | Deepseek-v2-chat | **0.2279** | **0.3300** | 0.4099 | 0.0634 | **0.6072** | **0.5197** | **0.3175** |\n| GTE-Qwen2-1.5B | Qwen2.5-72B | 0.1778 | 0.2563 | **0.4326** | **0.0467** | **0.6256** | **0.5613** | **0.3293** |\n| GTE-Qwen2-1.5B | Llama3-70B-Instruct | **0.3235** | **0.4810** | **0.4398** | 0.0792 | 0.5926 | 0.4754 | 0.3088 |\n| Human-annotated evaluation set | | | | | | | | |\n| CLOSE | Yi15-34B | 0.0497 | 0.1161 | 0.1461 | - | 0.4987 | - | 0.0749 |\n| CLOSE | Deepseek-v2-chat | 0.2250 | 0.4353 | 0.3306 | - | 0.5541 | - | 0.1153 |\n| CLOSE | Qwen2.5-72B | 0.2082 | 0.4191 | 0.3405 | - | 0.5754 | - | 0.1241 |\n| CLOSE | Llama3-70B-Instruct | 0.2195 | 0.4183 | 0.2859 | - | 0.5133 | - | 0.0659 |\n| GTE-Qwen2-1.5B | Yi15-34B | 0.0887 | 0.1583 | 0.3366 | **0.0648** | 0.5821 | 0.4234 | 0.1856 |\n| GTE-Qwen2-1.5B | Deepseek-v2-chat | **0.2916** | **0.4353** | 0.4234 | 0.0750 | **0.6006** | **0.5160** | 0.3213 |\n| GTE-Qwen2-1.5B | Qwen2.5-72B | 0.2528 | 0.3919 | **0.4476** | **0.0618** | **0.6190** | **0.5576** | **0.3595** |\n| GTE-Qwen2-1.5B | Llama3-70B-Instruct | **0.3390** | **0.5042** | **0.4433** | 0.1131 | 0.5745 | 0.4764 | **0.3268** |", "caption": "Table 5: The overall evaluation results on final responses of RAG models.", "description": "This table presents the evaluation results of different RAG models using various LLMs as generators. The table is divided into two sections: one for the automated-generated evaluation set and the other for the human-annotated evaluation set.  Within each set, different LLMs (Yi15-34B, Deepseek-v2-chat, Qwen2.5-72B, Llama3-70B-Instruct) are used as generators, and their performance is evaluated with GTE-Qwen2-1.5B as the retriever. \"CLOSE\" indicates that only the LLMs are used, without any retrieval augmentation. The evaluation metrics include Rouge-L, F1, ACC (accuracy), HAL (hallucination), COM (completeness), UTL (utilization), and NAC (numerical accuracy).", "section": "3.3 Evaluation of RAG Models"}]