[{"figure_path": "https://arxiv.org/html/2503.04222/x1.png", "caption": "Figure 2: Overview of our proposed FuseChat-3.0 framework for implicit model fusion.", "description": "This figure illustrates the FuseChat-3.0 framework's three-stage process for implicit model fusion.  First, data is constructed by generating multiple responses from various source LLMs for each prompt, then these responses are evaluated using an external reward model (for instruction-following) or rule-based methods (for math and coding).  Second, supervised fine-tuning (SFT) addresses distribution shifts by fine-tuning target models on optimal responses.  Finally, Direct Preference Optimization (DPO) incorporates controlled preference signals from same-source response pairs to further fine-tune the target model.", "section": "3 FUSECHAT-3.0 DATASET"}]