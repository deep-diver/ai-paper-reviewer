[{"figure_path": "https://arxiv.org/html/2503.23913/x1.png", "caption": "Figure 1: Comparison between the traditional self-training pipeline and EAST. The LLM generates n\ud835\udc5bnitalic_n responses per question, clustered by final answers. Questions with all incorrect answers are discarded. Self-training fine-tunes uniformly on the rest, while EAST assigns higher weights to questions with diverse (uncertain) answers and lower weights to consistent (confident) ones.", "description": "This figure illustrates the difference between standard self-training and the proposed EAST method.  In standard self-training, a large language model (LLM) generates multiple responses to a given question.  Responses are grouped by their final answer. If all responses for a question are incorrect, that question is discarded.  The remaining correct responses are used for fine-tuning the model, with each response given equal weight. EAST modifies this process by assigning weights to the questions based on the diversity of the responses. Questions with a wide variety of answers (indicating uncertainty in the model) receive higher weights, while those with consistent answers (showing model confidence) receive lower weights. This adaptive weighting strategy allows EAST to focus the model's training on more challenging and informative examples.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.23913/x2.png", "caption": "Figure 2: The framework of EAST. For each training question, the LLM generates n\ud835\udc5bnitalic_n responses, clustered by final answers. Entropy value is computed from the cluster distribution, transformed via mapping function, and integrated as weight into the loss objective.", "description": "Figure 2 illustrates the EAST framework, a novel weighting method for self-training.  For each training question, a large language model (LLM) generates multiple responses. These responses are grouped (clustered) based on their final answers.  The diversity of answers within each question is quantified using entropy. This entropy value represents the model's uncertainty about the question; higher entropy indicates more uncertainty. The entropy is then transformed by a mapping function, which introduces a tunable parameter to control the sharpness of the weighting. Finally, this transformed entropy value (the weight) is incorporated into the loss function to guide the self-training process.  Questions with high entropy (more uncertainty) receive higher weights during training, effectively steering the model towards focusing on more challenging and informative examples.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.23913/x3.png", "caption": "Figure 3: Performance(accuracy (%)) of various exponent parameters a\ud835\udc4eaitalic_a on GSM8K and MATH datasets using LLaMA-3.2-1B.", "description": "This figure displays the accuracy achieved by the EAST model on the GSM8K and MATH datasets using different values for the exponent parameter 'a'.  The x-axis represents the values of 'a', ranging from -3 to +3. The y-axis shows the accuracy percentage achieved. Two separate lines are shown, one for each dataset (GSM8K and MATH). This graph illustrates how different values of the exponent parameter affect the model's performance across various mathematical reasoning tasks.  The results show the impact of the mapping function's sharpness on the model's performance, highlighting how the emphasis on uncertain data influences accuracy.", "section": "4.2 Experiment Results"}, {"figure_path": "https://arxiv.org/html/2503.23913/x4.png", "caption": "Figure 4: Comparison of iterative learning performance (accuracy (%)) between vanilla SFT and EAST on LLaMA-3.2-1B.", "description": "Figure 4 presents a comparison of the performance of vanilla supervised fine-tuning (SFT) and the proposed Entropy-Based Adaptive Weighting for Self-Training (EAST) method over multiple iterative training rounds.  The results are shown separately for the GSM8K and MATH datasets, using the LLaMA-3.2-1B language model.  The plots display the accuracy achieved at each iteration, highlighting the difference in performance between vanilla SFT and EAST.  This visualization allows for the assessment of the impact of iterative training and the efficacy of EAST in preventing overfitting and maintaining performance over multiple iterations.", "section": "4.2 Experiment Results"}, {"figure_path": "https://arxiv.org/html/2503.23913/x5.png", "caption": "Figure 5: The figure illustrates the distribution of training data in entropy-based, accuracy-based, and rejected-based values. Each point represents a training example (xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT), with coordinates (H\u2062(xi),1\u2212A\u2062(xi)\ud835\udc3bsubscript\ud835\udc65\ud835\udc561\ud835\udc34subscript\ud835\udc65\ud835\udc56H(x_{i}),1-A(x_{i})italic_H ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , 1 - italic_A ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )) for entropy-based and accuracy-based values, and color indicating the rejected-based value (R\u2062(xi)\ud835\udc45subscript\ud835\udc65\ud835\udc56R(x_{i})italic_R ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )). The accompanying table reports the performance (accuracy(%)) of three weighting strategies on the GSM8K and MATH datasets.", "description": "Figure 5 visualizes the relationship between three different weighting strategies (entropy-based, accuracy-based, and rejection-based) and their impact on model performance.  Each point on the scatter plot represents a single training example, plotted according to its entropy and accuracy. The color of the point indicates the rejected-based value for that example.  The accompanying table summarizes the accuracy achieved by each weighting method on the GSM8K and MATH datasets, allowing for a direct comparison of their effectiveness in improving model performance.", "section": "4.3 Ablation Study: Effect of Accuracy-Based and Reject-Based Weights"}]