[{"figure_path": "https://arxiv.org/html/2501.11425/x1.png", "caption": "Figure 1: Illustration of language agents struggling with error correction in trajectory generation. These errors can cause agents to enter loops, hindering recovery in long trajectories and resulting in suboptimal outcomes.\nAgent-R enables agents to effectively detect and address errors in real-time, handling long-horizon tasks and avoiding loops with greater self-reflection capabilities.", "description": "The figure illustrates the challenges language models face when correcting errors during multi-step task completion.  The left panel shows an example where the agent takes a wrong path ('Open door to hallway'), realizing its mistake later, but finding it difficult to recover. The right panel showcases a scenario where the agent gets stuck in a loop ('Open door to kitchen'), repeatedly performing an ineffective action.  Agent-R, the proposed method, is designed to overcome these issues. It allows agents to detect and correct errors in real-time, improving the ability to complete long and complex tasks without getting stuck in unproductive loops.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.11425/x2.png", "caption": "Figure 2: The framework of Agent-R consists of two phases. In Phase I, we adopt MCTS and a model-guided reflection mechanism to construct revision trajectories. In Phase II, the agents are trained using the collected revision trajectories. These two phases can be repeated iteratively. rs is the revision signal, t\u2032superscript\ud835\udc61\u2032t^{\\prime}italic_t start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT is the transition point between the bad and good trajectories, and L\u2062(\u03b8)\ud835\udc3f\ud835\udf03L(\\theta)italic_L ( italic_\u03b8 ) is the loss function to be optimized.", "description": "Agent-R's framework consists of two iterative phases.  Phase 1 uses Monte Carlo Tree Search (MCTS) and a model-guided reflection mechanism to identify and correct errors in agent trajectories, creating 'revision trajectories'.  These corrected trajectories are then used in Phase 2 to fine-tune the agent's model using a loss function (L(\u03b8)) that balances learning from both the revised and originally correct trajectories. This two-phase process repeats iteratively for continuous improvement.  Key elements are the revision signal (rs), which marks the point of correction within the trajectory, and the transition point (t'), which is where the corrected segment of the trajectory begins.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.11425/x3.png", "caption": "Figure 3: Results of different training trajectories under different iterations on three interactive environments.", "description": "This figure displays the average final scores achieved by Llama-3.1-8B language model agents trained on different trajectory types across three iterations in three interactive environments (WebShop, SciWorld, TextCraft).  The training trajectories include those generated by Agent-R (the proposed method), optimal trajectories only, and trajectories using a direct revision strategy.  The graph visually compares the performance gains obtained through each training method across the three environments over the course of three iterative training phases, showcasing the effectiveness of Agent-R in improving agent performance compared to baselines.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2501.11425/x4.png", "caption": "Figure 4: Average count of repeated action lengths for different training trajectories and different iterations in three interactive environments.", "description": "This figure displays the average number of times an agent repeats the same sequence of actions in the WebShop, SciWorld, and TextCraft environments.  It compares the performance of agents trained with different methods: a vanilla model, a model trained with optimal trajectories, and a model trained with Agent-R's trajectories (at various iterations). The x-axis represents the length of repeated action sequences, and the y-axis shows the average count of those sequences.  This visualization helps to illustrate how Agent-R training reduces repetitive actions, a sign of the agent getting stuck in unproductive loops or failing to make progress towards the goal.", "section": "4.4. Findings with Analysis"}, {"figure_path": "https://arxiv.org/html/2501.11425/x5.png", "caption": "Figure 5: Average revision length of different iterations on three interactive environments.", "description": "This figure shows the average number of steps taken by the language agent from the start of a bad trajectory to the detection of the first error, across three iterations of the Agent-R training process.  The three interactive environments used are WebShop, SciWorld, and TextCraft. Shorter average lengths indicate quicker error detection and correction by the agent, showcasing the effectiveness of Agent-R's iterative self-training in improving real-time self-reflection and error-recovery capabilities.", "section": "4.4. Findings with Analysis"}, {"figure_path": "https://arxiv.org/html/2501.11425/x6.png", "caption": "Figure 6: Comparison of different training methods on three interactive environments.", "description": "The figure displays a comparison of the average final scores achieved by different training methods across three interactive environments (WebShop, SciWorld, and TextCraft).  The methods compared include single-task training with Agent-R, multi-task training with Agent-R, single-task training using a direct revision approach, and multi-task training using a direct revision approach.  The graph visually represents the performance of each method across three iterations, illustrating how the average final score changes over multiple training iterations.  This demonstrates the impact of the different training methods and whether multi-task learning offers better scalability and performance.", "section": "4. Experiment"}]