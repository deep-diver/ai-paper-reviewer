{"references": [{"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-12-01", "reason": "This paper introduces the foundation of diffusion models, which are the core of many video generation models, including the model used in this paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "CLIP, introduced in this paper, is a crucial element in the proposed Ingredients framework for aligning visual and textual inputs."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-06-01", "reason": "This paper's work on transformers for image generation is foundational to the video diffusion transformers used in the proposed framework."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-10-01", "reason": "This work directly addresses the use of transformers to improve the scalability and efficiency of diffusion models for video generation, a key aspect of the Ingredients approach."}, {"fullname_first_author": "Zhuoyi Yang", "paper_title": "Cogvideox: Text-to-video diffusion models with an expert transformer", "publication_date": "2024-08-01", "reason": "Cogvideox serves as the baseline model in the experimental evaluation, demonstrating its significance in the field and its relevance to the proposed method."}]}