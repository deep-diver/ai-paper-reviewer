[{"figure_path": "https://arxiv.org/html/2504.02828/x1.png", "caption": "Figure 1: Given a source image and the editing task, our proposed CoLan generates a concept dictionary and performs sparse decomposition in the latent space to precisely transplant the target concept.", "description": "This figure illustrates the Concept Lancet (CoLan) framework for image editing.  Given a source image (e.g., a cat sitting on grass) and a desired edit (e.g., change the cat to a dog), CoLan first constructs a dictionary of visual concepts. This dictionary represents a collection of visual features extracted from a large dataset, allowing CoLan to understand the components of the image.  The source image is then decomposed into a sparse linear combination of these concepts in the latent space (either text embedding or diffusion score space). Finally, CoLan performs a concept transplant, replacing the cat concept with the dog concept, while preserving other aspects of the image such as background and pose. This precise manipulation of the latent space allows CoLan to generate an edited image where the target concept is seamlessly integrated. ", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.02828/x2.png", "caption": "Figure 2: Representation manipulation in diffusion models involves adding an accurate magnitude of edit direction (e.g., Image (3) by CoLan) to the latent source representation. Figure\u00a05 and Figure\u00a07 show more examples.", "description": "This figure illustrates the core concept of Concept Lancet (CoLan), a method for precise image editing using diffusion models.  It highlights the problem of accurately estimating the magnitude of an edit direction when modifying an image's representation in the latent space.  Simply shifting the representation towards a target concept (as in existing methods) often leads to either an overestimation (too strong an edit, harming visual consistency) or an underestimation (too weak an edit, failing to achieve the desired change). CoLan addresses this by decomposing the source image's latent representation into a sparse linear combination of concept vectors, which allows for an accurate estimation of the presence and strength of different concepts within the image. This informed approach enables CoLan to precisely transplant the target concept with the correct magnitude, preserving visual consistency while effectively implementing the edit.  Further examples illustrating the effectiveness of this approach are shown in Figures 5 and 7.", "section": "2. Preliminaries in Diffusion-Based Editing"}, {"figure_path": "https://arxiv.org/html/2504.02828/x3.png", "caption": "Figure 3: The CoLan framework. Starting with a source image and prompt, a vision-language model extracts visual concepts (e.g., cat, grass, sitting) to construct a concept dictionary. The source representation is then decomposed along this dictionary, and the target concept (dog) is transplanted to replace the corresponding atom to achieve precise edits. Finally, the image editing backbone generates an edited image where the desired target concept is incorporated without disrupting other visual elements.", "description": "The figure illustrates the Concept Lancet (CoLan) framework for image editing.  The process begins with a source image and its corresponding prompt. A vision-language model (VLM) analyzes this input and identifies key visual concepts (e.g., \"cat,\" \"grass,\" \"sitting\"). These concepts form a \"concept dictionary,\" a set of vectors representing the concepts in the model's latent space. The source image's representation in this latent space is then decomposed into a sparse linear combination of these concept vectors.  To perform the edit (e.g., changing a cat to a dog), the concept vector corresponding to the source concept (\"cat\") is replaced with the target concept vector (\"dog\"). This modified representation is then fed into an image editing backbone (a diffusion model), generating a new image with the desired change.  The framework is designed to ensure that the edit is precise and only affects the target concept, preserving the other visual elements of the original image.", "section": "3. Our Method: Concept Lancet"}, {"figure_path": "https://arxiv.org/html/2504.02828/x4.png", "caption": "Figure 4: Samples of the concept stimuli from CoLan-150K. Additional samples are attached in the Appendix \u00a78.", "description": "This figure displays example stimuli from the CoLan-150K dataset, a collection of diverse visual concept descriptions used to train the Concept Lancet model.  Each concept within the dataset is represented by numerous examples. These samples show the variety of phrasing and contexts included, illustrating that the descriptions are not just single words but also phrases and descriptive sentences covering multiple aspects of a concept.  The goal of the dataset is to ensure the model can accurately represent and manipulate concepts in various scenarios, improving editing precision and consistency.  Additional examples beyond those shown here can be found in the paper's Appendix \u00a78.", "section": "3.1 Concept Dictionary Synthesis"}, {"figure_path": "https://arxiv.org/html/2504.02828/x5.png", "caption": "Figure 5: Visual comparisons of CoLan in the text embedding space of P2P-Zero. Texts in gray are the original captions of the source images from PIE-Bench, and texts in blue are the corresponding edit task (replace, add, remove). [x] represents the concepts of interest, and [] represents the null concept.", "description": "This figure showcases the effectiveness of Concept Lancet (CoLan) in image editing by comparing its performance with the P2P-Zero baseline.  Each row displays a source image, the results of using P2P-Zero alone, and the results of using P2P-Zero combined with CoLan.  The source image captions are presented in gray, while the specific editing tasks (replace, add, or remove a concept) are highlighted in blue.  Brackets, [ ], around a word or phrase denote the specific concept targeted by the edit.  Empty brackets, [], indicate the removal of a concept. The figure highlights how CoLan improves the accuracy and consistency of edits by effectively transplanting the desired concepts while preserving other aspects of the original image.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2504.02828/x6.png", "caption": "Figure 6: The histograms of solved magnitudes of the concept atoms in CoLan decomposition (text embedding space). As there are tens of concepts in a single dictionary, the histogram includes the concepts whose CoLan coefficients have the top 10 largest magnitudes.", "description": "This figure displays bar charts illustrating the magnitudes of the top 10 concept atoms derived from the Concept Lancet (CoLan) decomposition method applied to text embedding space. Each bar chart represents a different image from the dataset. The height of each bar corresponds to the magnitude of a particular concept's coefficient in the sparse decomposition.  The x-axis labels represent the names of the concepts while the y-axis denotes the magnitude of the concept vector. This visualization helps to understand the relative importance of different concepts in each image and how CoLan decomposes the image representation into a sparse combination of these concepts.", "section": "4.3 Representation Analysis in CoLan-150K"}, {"figure_path": "https://arxiv.org/html/2504.02828/x7.png", "caption": "Figure 7: Visual comparisons of CoLan in the score space (first row) and text embedding space (second row) of InfEdit. Texts in gray are the original captions of the source images from PIE-Bench, and texts in blue are the corresponding edit task (replace, add, remove).", "description": "Figure 7 presents a visual comparison of the image editing results obtained using Concept Lancet (CoLan) integrated with the InfEdit model.  The top row showcases results from using CoLan in InfEdit's score space, while the bottom row demonstrates its application in InfEdit's text embedding space.  Each image pair shows the original source image and the corresponding edited image. The original image caption is displayed in gray, and the specific editing task (replace, add, or remove a concept) is indicated in blue. This figure illustrates CoLan's ability to effectively and consistently edit images regardless of the chosen latent space (score or text embedding).", "section": "4.2.1 Visual Comparison"}, {"figure_path": "https://arxiv.org/html/2504.02828/x8.png", "caption": "Figure 8: The histograms of solved magnitudes of the concept atoms in CoLan decomposition (score space). The histogram includes the concepts whose CoLan coefficients have the top 10 largest magnitudes.", "description": "This figure displays bar charts illustrating the magnitudes of the top 10 most significant concept components derived from the sparse decomposition process within Concept Lancet (CoLan) when operating in the score space. Each bar chart represents a distinct image from the dataset, with the height of each bar reflecting the magnitude of its corresponding concept vector. This visualization helps demonstrate how CoLan effectively identifies and weights the most relevant concepts for each image in the editing process.", "section": "4.3.2 Comparing Editing Strengths"}, {"figure_path": "https://arxiv.org/html/2504.02828/x9.png", "caption": "Figure 9: Visualizations of edited images with decreasing strength of the concept [fresh] extracted from our CoLan-150K dataset. The values on top correspond to the coefficient \ud835\udc98freshsubscript\ud835\udc98fresh{\\boldsymbol{w}}_{\\text{fresh}}bold_italic_w start_POSTSUBSCRIPT fresh end_POSTSUBSCRIPT for removing the concept \ud835\udc85freshsubscript\ud835\udc85fresh{\\boldsymbol{d}}_{\\text{fresh}}bold_italic_d start_POSTSUBSCRIPT fresh end_POSTSUBSCRIPT. CoLan solves \ud835\udc98fresh\u2217subscriptsuperscript\ud835\udc98fresh{\\boldsymbol{w}}^{*}_{\\text{fresh}}bold_italic_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT fresh end_POSTSUBSCRIPT of \u22120.9770.977-0.977- 0.977 for the apple and \u22121.161.16-1.16- 1.16 for the lotus.", "description": "This figure displays the results of applying concept removal editing to apples and lotuses using the Concept Lancet (CoLan) method. By varying the coefficient  \ud835\udc98fresh{\nw}_{\nfresh} for removing the concept of \"freshness\" (represented by \ud835\udc85fresh{\nd}_{\nfresh}), the image is progressively altered.  The top row shows the original images, while subsequent rows show the images edited with decreasing amounts of \"freshness\", corresponding to increasing negative values of  \ud835\udc98fresh{\nw}_{\nfresh}. The specific coefficients  \ud835\udc98fresh\u2217{\nw}_{\nfresh}^{*} obtained by CoLan are -0.977 for the apple and -1.16 for the lotus.  The figure demonstrates how CoLan controls the intensity of the editing effect.", "section": "4.3.2 Comparing Editing Strengths"}, {"figure_path": "https://arxiv.org/html/2504.02828/x10.png", "caption": "Figure 10: Visualizations of edited images with increasing strength of the concept [green] extracted from our CoLan-150K dataset. The values on top correspond to the coefficient \ud835\udc98greensubscript\ud835\udc98green{\\boldsymbol{w}}_{\\text{green}}bold_italic_w start_POSTSUBSCRIPT green end_POSTSUBSCRIPT for adding the concept vector \ud835\udc85greensubscript\ud835\udc85green{\\boldsymbol{d}}_{\\text{green}}bold_italic_d start_POSTSUBSCRIPT green end_POSTSUBSCRIPT. CoLan solves \ud835\udc98green\u2217subscriptsuperscript\ud835\udc98green{\\boldsymbol{w}}^{*}_{\\text{green}}bold_italic_w start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT green end_POSTSUBSCRIPT of 0.5860.5860.5860.586 for the apple and 0.6950.6950.6950.695 for the rose.", "description": "This figure visualizes how changing the coefficient  **w<sub>green</sub>** in the concept vector **d<sub>green</sub>** affects the generated image.  The coefficient controls the intensity of the 'green' concept added to the base image.  The experiment uses apples and roses as examples.  The results show that a coefficient of 0.586 produces the optimal 'green' hue for the apple, while 0.695 is optimal for the rose.  Increasing the coefficient beyond these values leads to unnatural and over-saturated results. This demonstrates the importance of precisely controlling the magnitude of concept addition for effective image editing.", "section": "4.3.2 Comparing Editing Strengths"}, {"figure_path": "https://arxiv.org/html/2504.02828/x11.png", "caption": "Figure 11: Additional visual comparison of CoLan in the text embedding space of P2P-Zero. We observe that the backbone plugging with CoLan has editing results that visually better align with the task.", "description": "This figure displays a visual comparison of image editing results obtained using the P2P-Zero model with and without the Concept Lancet (CoLan) framework.  Multiple image editing tasks are shown, each with the original image, the result from P2P-Zero alone, and the result from P2P-Zero enhanced by CoLan. The goal is to demonstrate how CoLan improves the visual quality of edited images by more accurately aligning them with the user's specified edits.", "section": "4.2. Qualitative Observation"}, {"figure_path": "https://arxiv.org/html/2504.02828/x12.png", "caption": "Figure 12: Visualizations of editing results. The first row shows the source images, the second row shows the results with the fixed edit strength of 0.70.70.70.7 for the concept [dog] without CoLan analysis, and the third row shows the edit results with CoLan analysis.", "description": "This figure demonstrates the impact of CoLan on image editing by comparing results with and without its usage.  The top row displays the original source images. The middle row shows the results of applying a fixed edit strength (0.7) to the concept '[dog]' in the source images without employing CoLan. This fixed strength approach lacks the adaptability to account for the varying prominence of the '[dog]' concept across different images. The bottom row presents the results obtained with CoLan. CoLan's adaptive concept analysis and magnitude estimation ensures that the editing strength is appropriately determined for each image. This leads to more consistent and effective editing results compared to the fixed strength method.", "section": "4. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2504.02828/x13.png", "caption": "Figure 13: Visualizations of concept grounding for sampled concepts from our CoLan-150K dataset. We observe that the extracted concept vectors from our dataset corresponds to the desired semantics by visualization.", "description": "This figure displays examples from the CoLan-150K dataset, demonstrating the concept grounding of three selected concept vectors: [watercolor], [dog], and [wearing hat]. For each concept, several source images were chosen, and the corresponding edited images are shown after applying the concept vector using the P2P-Zero backbone with CoLan. The results visually confirm that the extracted concept vectors effectively convey the intended semantics, showcasing the quality of the CoLan-150K dataset and the effectiveness of the CoLan method in generating grounded concept representations.", "section": "4.3 Concept Grounding"}, {"figure_path": "https://arxiv.org/html/2504.02828/x14.png", "caption": "Figure 14: Additional samples of the concept stimuli from CoLan-150K. Each concept consists of approximately 30303030 stimuli and this figure samples the first three for a concept.", "description": "Figure 14 presents a subset of the CoLan-150K dataset, showcasing example stimuli for various concepts relevant to image editing.  Each concept within the dataset has around 30 associated stimuli, offering diverse descriptions and contextual variations. This figure displays only the first three stimuli for a selection of these concepts, illustrating the rich and varied nature of the data used to train the Concept Lancet model. The stimuli encompass various styles, contexts, and perspectives to enhance the model's understanding and application of each concept.", "section": "3.1 Concept Dictionary Synthesis"}]