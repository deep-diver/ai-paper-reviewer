[{"Alex": "Welcome back to the podcast, everyone! Today, we're diving headfirst into the fascinating world of AI reasoning, but with a twist. We're uncovering a hidden flaw \u2013 a chink in the armor, if you will \u2013 where AI models, designed for deep thinking, actually get\u2026 well, kinda lost. Stick around as we explore 'Overthinking' in AI, and why missing information can send these smart systems into a spiral!", "Jamie": "Wow, that sounds wild! So, Alex, what exactly is this 'Overthinking' that you're talking about? What research has been done about it?"}, {"Alex": "It's a fascinating phenomenon where reasoning-based AI, when faced with incomplete questions \u2013 questions missing key details, which they call Missing Premises (MiP) \u2013 respond with extremely lengthy answers. However the crazy thing is that those answer fails to identify MiP and solve the question. Which kinda violates the test-time scaling law. In short, it is an overthinking issue!", "Jamie": "Missing Premises, okay. So, like, the AI is given a question that's impossible to answer and just\u2026 keeps trying anyway?"}, {"Alex": "Exactly! Think of it like asking a GPS to find a location without giving it a street name, just a city. It'll spin its wheels trying to figure it out, giving you roundabout directions, instead of simply saying, 'Hey, I need more info!' In our paper we formally named it the MiP-Overthinking issue.", "Jamie": "Hmm, so what kind of questions are we talking about? Can you give me an example?"}, {"Alex": "Sure! A classic example is, 'What is the value of a?' No context, no equation, just 'a'. A non-reasoning model would likely say, 'I can't answer that,' but a reasoning-based AI might generate paragraphs trying to define 'a', explore its possible values, and ultimately fail to provide a real answer. It seems silly for us, but AI is taking time to analyze these type of basic questions.", "Jamie": "That\u2019s kind of mind-blowing. So, what did you guys actually do in the paper to investigate this? Like, how did you test for this MiP-Overthinking?"}, {"Alex": "We curated four datasets specifically designed to trigger this overthinking behavior. We used a mix of synthetic questions and modifications of existing datasets, varying the difficulty. Basically we asked the models lots of these ill-posed question and analyzed how many tokens (words) the models generated, and how long it took for them.", "Jamie": "Four datasets... wow. What datasets did you use for your test? Sounds like a lot of work"}, {"Alex": "It was! We used a rule-based Formula dataset, SVAMP, GSM8K, and MATH500. These cover different difficulty levels and ensure the questions triggered the ", "Jamie": "And what were the big findings? What did you discover about how these models handle MiP?"}, {"Alex": "The most striking finding was the sheer increase in response length for MiP questions. Reasoning models generated responses two to four times longer than their answers to well-defined questions! Surprisingly, non-reasoning models were much more efficient, quickly identifying the missing premise and offering short responses.", "Jamie": "Wait, so the models *designed* for reasoning actually performed worse than simpler models when dealing with incomplete information?"}, {"Alex": "Precisely! The reasoning models get caught in a self-doubt loop, revisiting the question and guessing user intentions, resulting in that explosion of tokens. It\u2019s like they are desperately trying to solve the question.", "Jamie": "That is very counterintuitive. So it's not so much about the ability to do complex math, but to think critically, isn't it?"}, {"Alex": "Exactly! It's not about pure computational power, but the lack of critical thinking in the models. They can do complex problems, but when faced with an unsolvable question, they lack the capacity of reject the question by themselves and instead dig in it. That's critical thinking.", "Jamie": "This is really interesting. Did the models ever, like, *almost* figure out that a premise was missing?"}, {"Alex": "That's one of the most interesting observations! We found that many models *did* suspect that something was wrong early in their reasoning process, they hesitate and not committing to this judgement. So they keep trying, checking, looping with self doubt patterns. They ultimately continue and still generate meaningless result.", "Jamie": "Wow, that is so weird! They are able to identify the missing premise issue but they don't dare to abstain it."}, {"Alex": "Indeed. They seem to lack the confidence or the mechanism to definitively say, 'This question cannot be answered with the information given.' It points to a flaw in how we're training these reasoning models.", "Jamie": "Umm, so what *is* going on there? Do you have any ideas why the RLMs are behaving like that? I'd guess RLMs are prone to generate long responses, I wonder how does it appear"}, {"Alex": "We believe it stems from the training process, especially the rule-based reinforcement learning. Models are rewarded for accuracy and thoroughness, often with minimal constraints on length. This can lead to 'reward hacking,' where models prioritize verbose reasoning patterns even when unnecessary.", "Jamie": "So, the AI is basically trained to show its work, even when there's nothing *to* work with. It is also interesting to consider that length itself can be a reward. So if the models tend to perform long, they get awarded."}, {"Alex": "Exactly. The other important thing is that the problems could become a bias to solve complex and hard problems only, and forget the ability to critically thinking questions. And that behavior transmits during distillation and finetuning processes. And that is what we observed in our experiments.", "Jamie": "That\u2019s fascinating\u2026 it's like overthinking becomes contagious within AI training!"}, {"Alex": "Precisely! It shows how easily these behaviors can propagate through the AI ecosystem. This also is the problem with current CoT method, where all the models are incentivized to solve problems by Chain of Thought.", "Jamie": "So, if the models are not able to think critically, and the behaviors are contagious, how can we avoid such failure?"}, {"Alex": "That's the million-dollar question! We need new training recipes that emphasize efficient and critical thinking. Encouraging models to recognize and flag incomplete information, and also give positive reward to model who 'dare' to abstain.", "Jamie": "What metrics can you use to test and avoid such failure?"}, {"Alex": "Besides the normal 'accuracy' and 'reasoning length' metrics, we need new abstain rate metrics to explicitly assess a model\u2019s ability to identify and reject ill-posed questions. Further improvement should encourage model to be confidence enough to flag the question.", "Jamie": "Okay. That makes sense. So, it sounds like this MiP-Overthinking issue is a pretty fundamental problem in how we train AI for reasoning."}, {"Alex": "Absolutely. It highlights a critical gap in current training approaches: the need to balance thorough reasoning with efficient decision-making and critical thinking. We should also rethink about RLHF training approaches.", "Jamie": "So, what are the real-world implications of all this? Why should people care about AI overthinking missing premise questions?"}, {"Alex": "Well, in real-world applications, AI will inevitably encounter incomplete or ambiguous data. Think about medical diagnoses, financial analysis, even self-driving cars. If AI can't recognize missing information and adjust its reasoning accordingly, it could lead to serious errors and bad actions.", "Jamie": "That's a pretty scary thought. What's the next step for you and your team, and for the broader research community?"}, {"Alex": "We are exploring better training techniques that explicitly incentivize critical thinking and discourage overthinking. This includes the method of generating synthetic training samples of MiP questions to train the model. The community is on the way to rethinking the training recipes of critical thinking for reasoning models.", "Jamie": "That sounds like really important work. Alex, thanks for sharing the insights about MiP-Overthinking. It's been super interesting."}, {"Alex": "My pleasure, Jamie! To sum up, we've uncovered a surprising flaw in reasoning-based AI: a tendency to 'overthink' questions with missing information, leading to inefficient and ineffective reasoning. Our research emphasizes the need for new training approaches that prioritize critical thinking, enabling AI to recognize its limits and make better decisions in the face of uncertainty. Hopefully with better training recipes, we will solve the MiP problems. Thanks everyone, and have a good day! ", "Jamie": "Yeah, bye everyone! I'm Jamie, and welcome to the podcast!"}]