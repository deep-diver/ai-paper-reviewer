{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduces the Transformer architecture, which is fundamental to the current work."}, {"fullname_first_author": "Felix A Gers", "paper_title": "Learning to forget: Continual prediction with lstm", "publication_date": "2000-01-01", "reason": "This paper is important because it details the forget gate, a key component which the current work incorporates into the Transformer."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-01-01", "reason": "This paper is important because it describes the LLaMA architecture, which the current work uses as a baseline and modifies."}, {"fullname_first_author": "Angelos Katharopoulos", "paper_title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "publication_date": "2020-01-01", "reason": "This paper is important because it discusses linear attention, a concept which is used in the current work."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-01-01", "reason": "This paper is an important reference, since the current work benchmarks against the Mamba model and discusses the similarities between the forget gate of Mamba and the Forgetting Attention mechanism."}]}