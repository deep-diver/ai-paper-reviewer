{"references": [{"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the MATH dataset", "publication_date": "2021-12-XX", "reason": "This paper introduces a challenging dataset for evaluating mathematical problem-solving abilities, which is crucial for assessing the performance of LLMs in complex reasoning tasks."}, {"fullname_first_author": "DeepSeek", "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning", "publication_date": "2025-XX-XX", "reason": "This paper presents a model (DeepSeek-R1) that exemplifies the state-of-the-art in deep reasoning capabilities, making it a key reference for understanding and comparing o1-like LLMs."}, {"fullname_first_author": "OpenAI", "paper_title": "Learning to reason with LLMs", "publication_date": "2024-11-XX", "reason": "This paper introduces the seminal OpenAI's o1 model and its capabilities for deep reasoning, setting the stage for the research on o1-like models and their limitations."}, {"fullname_first_author": "Xingyu Chen", "paper_title": "Do not think that much for 2+3=? on the overthinking of o1-like LLMs", "publication_date": "2024-12-XX", "reason": "This paper investigates the problem of overthinking in o1-like LLMs, which is a complementary issue to underthinking and contributes to a broader understanding of reasoning inefficiencies."}, {"fullname_first_author": "David Rein", "paper_title": "GPQA: A graduate-level Google-proof Q&A benchmark", "publication_date": "2023-11-XX", "reason": "This paper introduces a challenging dataset (GPQA) for evaluating advanced reasoning abilities, which is directly relevant for benchmarking the performance of o1-like LLMs and testing the proposed underthinking mitigation technique."}]}