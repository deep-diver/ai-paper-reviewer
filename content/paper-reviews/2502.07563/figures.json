[{"figure_path": "https://arxiv.org/html/2502.07563/x1.png", "caption": "Figure 1: Computation Decomposition in LASP-2 with masking. Colored chunks represent inter-chunks.", "description": "This figure illustrates how LASP-2 handles sequence parallelism with masking, a crucial aspect of autoregressive tasks.  It shows the decomposition of computations into intra-chunk (within a single chunk) and inter-chunk (between multiple chunks) operations.  The colored chunks highlight the inter-chunk computations, which are performed independently and in parallel across different devices because they don't depend on the results of other chunks. This parallel processing improves efficiency. The intra-chunk computations, on the other hand, involve sequential operations due to the masking requirements of autoregressive tasks.  The figure visually demonstrates how LASP-2 efficiently combines parallel and sequential processing to improve the scalability of linear attention models with masking.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.07563/x2.png", "caption": "Figure 2: Visualization of LASP-2H on Linear Attention and Standard Attention hybrid model. We exemplify LASP-2H on the hybrid layers of linear attention and standard attention modules with both TP and SP (both have a dimension of 2). The communication operations colored in yellow and green are for TP and SP, respectively. AG/RS: all-gather in forward and reduce-scatter in backward, and vice versa. AG/No: all-gather in forward and no-op in backward, and vice versa. Note that the SP communication operations for linear attention operate on the memory state \ud835\udc0ct\u2208\u211dd\u00d7dsubscript\ud835\udc0c\ud835\udc61superscript\u211d\ud835\udc51\ud835\udc51\\mathbf{M}_{t}\\in\\mathbb{R}^{d\\times d}bold_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_d \u00d7 italic_d end_POSTSUPERSCRIPT, while for standard attention, they operate on states \ud835\udc0at,\ud835\udc15t\u2208\u211dC\u00d7dsubscript\ud835\udc0a\ud835\udc61subscript\ud835\udc15\ud835\udc61superscript\u211d\ud835\udc36\ud835\udc51\\mathbf{K}_{t},\\mathbf{V}_{t}\\in\\mathbb{R}^{C\\times d}bold_K start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_V start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_C \u00d7 italic_d end_POSTSUPERSCRIPT.", "description": "Figure 2 illustrates the LASP-2H approach applied to a hybrid model containing both linear and standard attention layers.  The diagram showcases two dimensions of parallelism: Tensor Parallelism (TP) and Sequence Parallelism (SP), each split into two parts.  Communication patterns, whether all-gather (AG), reduce-scatter (RS), or no-operation (No-op), are indicated for both forward and backward passes.  The key difference highlighted is that Sequence Parallelism in linear attention layers operates on memory states (Mt) of dimensions d x d, whereas in standard attention, it operates on key (Kt) and value (Vt) states of dimensions C x d.  The colors yellow and green distinguish between TP and SP communication operations respectively.", "section": "3.5. Hybrid Model Sequence Parallelism"}, {"figure_path": "https://arxiv.org/html/2502.07563/x3.png", "caption": "Figure 3: Speed Comparison (tokens/s). Experiments were carried out on a pure Linear-Llama3-1B model, utilizing the basic linear attention module. A total of 64 A100 GPUs were employed, and the SP size T\ud835\udc47Titalic_T was also set to 64. To accommodate very-long sequence lengths, such as 2048K, the batch size was kept fixed at 1 throughout this experiment.", "description": "Figure 3 presents a performance comparison of different sequence parallelism (SP) methods for training a large language model (LLM).  The experiment uses a Linear-Llama3-1B model, a variant of the Llama3 model where standard attention is replaced with basic linear attention, making the training time linear with sequence length. A total of 64 A100 GPUs were used in parallel to accelerate training. The SP size (T) was set to 64, and to enable training with very-long sequences (up to 2048K tokens), the batch size was maintained at 1. The plot displays the throughput (tokens/second) of LASP-2 against other methods such as Megatron-SP, Ring Attention, and LASP-1, across a range of sequence lengths.  The results demonstrate the superior speed and scalability of LASP-2, particularly as sequence lengths increase beyond 64K tokens.", "section": "4.2. Speed"}]