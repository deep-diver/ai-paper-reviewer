[{"figure_path": "https://arxiv.org/html/2502.06786/x1.png", "caption": "(a)", "description": "This figure shows a multi-scale quantization training technique leveraging the nested structure of integer data types.  Specifically, it illustrates how smaller bit-width integers (int4, int2) are nested within larger ones (int8).  This nested structure is exploited to train a single model that can be efficiently served at different precision levels, addressing the challenge of needing multiple quantized models for different accuracy/latency trade-offs.", "section": "3. Matryoshka Quantization"}, {"figure_path": "https://arxiv.org/html/2502.06786/x2.png", "caption": "(a)", "description": "MatQuant is a multi-scale quantization training technique that leverages the inherent nested structure of integer data types (int8, int4, int2).  It allows training and maintaining just one model, which can then be served at different precision levels. The figure shows how the most significant bits (MSBs) of an int8-quantized weight can be directly sliced to yield an int4 or int2 model, demonstrating the nested Matryoshka structure.", "section": "3. Matryoshka Quantization"}, {"figure_path": "https://arxiv.org/html/2502.06786/x3.png", "caption": "(b)", "description": "This figure shows the empirical gains achieved by Matryoshka Quantization (MatQuant) on downstream tasks compared to baseline methods.  The x-axis represents the effective bits per feed-forward network (FFN) parameter, reflecting different quantization levels (int2, int4, int8). The y-axis shows the performance, measured as task average. MatQuant demonstrates a significant performance boost for int2 quantization, surpassing the accuracy of the baseline model by more than 8%. For int4 and int8, MatQuant's performance is comparable to the baseline.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06786/x4.png", "caption": "(b)", "description": "This figure shows the empirical gains achieved by Matryoshka Quantization (MatQuant) on downstream tasks compared to baseline methods for various bit-widths (int2, int4, int8).  The graph highlights the significant improvement in accuracy, particularly for int2 models, demonstrating MatQuant's effectiveness in achieving high accuracy at low precisions. The x-axis represents the effective bits per FFN parameter (a measure of model size and computational cost), while the y-axis represents the frequency.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06786/x5.png", "caption": "(c)", "description": "The figure shows the weight distribution for the Gemma-2 9B model when trained with Matryoshka Quantization (MatQuant) and OmniQuant. The x-axis represents the quantized buckets, and the y-axis represents the frequency.  The figure highlights how MatQuant shifts the distribution of quantized weights toward higher values compared to the baseline.  This shift is particularly noticeable for int2 models, where using more of the higher-valued weights leads to increased accuracy. This demonstrates that MatQuant allows for better utilization of the integer data type, particularly at low precisions.", "section": "3. Matryoshka Quantization"}]