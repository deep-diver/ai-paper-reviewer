{"references": [{"fullname_first_author": "Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-01", "reason": "This paper is important as it presents foundational work on evaluating LLMs specifically trained on code, a core aspect of the LeetCodeDataset benchmark."}, {"fullname_first_author": "Hendrycks", "paper_title": "Measuring coding challenge competence with apps", "publication_date": "2021-01-01", "reason": "This paper presents APPS, a significant benchmark for coding challenges that serves as a comparison point for LeetCodeDataset."}, {"fullname_first_author": "Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-01", "reason": "This paper introduces MBPP, another widely used benchmark for Python programming, used for comparing LeetCodeDataset in the experiments."}, {"fullname_first_author": "Jain", "paper_title": "Livecodebench: Holistic and contamination free evaluation of large language models for code", "publication_date": "2024-03-01", "reason": "This work is directly related as it tackles similar challenges of evaluating LLMs for code in a contamination-free manner, providing a key comparative benchmark."}, {"fullname_first_author": "Wei", "paper_title": "Magicoder: Empowering code generation with oss-instruct", "publication_date": "2024-01-01", "reason": "This paper is important as it describes the Magicoder dataset, a widely used coding dataset against which LeetCodeDataset's training efficiency is compared."}]}