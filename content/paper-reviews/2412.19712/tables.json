[{"content": "| Methods | (i) | (ii) | (iii) | (iv) | (v) | Val | Ove | Ali | Und<sub>l</sub> | Und<sub>s</sub> |\n|---|---|---|---|---|---|---|---|---|---|---|\n| FlexDM [11] | 5.34 | 5.29 | 5.41 | 5.09 | 4.54 | 0.8757 | 0.3242 | **0.0016** | 0.7286 | 0.7298 |\n| GPT-4o [1] | 6.53 | 6.49 | 6.60 | 6.27 | 5.69 | 0.9968 | 0.0595 | 0.0001 | 0.3780 | 0.5708 |\n| LaDeCo (Ours) | **8.08** | **7.92** | **8.00** | **7.82** | **6.98** | **0.9365** | **0.0865** | 0.0013 | **0.6922** | **0.6580** |\n| GT | 8.35 | 8.21 | 8.30 | 8.01 | 7.26 | 0.9265 | 0.0768 | 0.0015 | 0.6848 | 0.6732 |", "caption": "Table 1: Quantitative comparison on the design composition task. LLaVA-OV evaluation includes the following aspects: (i) design and layout, (ii) content relevance, (iii) typography and color, (vi) graphics and images, and (v) innovation and originality. The score closest to the one calculated from real data (denoted as GT) is highlighted in bold, indicating the best performance among different methods.", "description": "This table presents a quantitative comparison of different methods for automated graphic design composition, using the LLaVA-OV evaluation metric.  LLaVA-OV assesses five key aspects of design quality: (i) design and layout, (ii) content relevance, (iii) typography and color, (iv) graphics and images, and (v) innovation and originality.  The scores for each method are compared to a ground truth (GT) score derived from human-created designs.  The method with the score closest to the GT score for each aspect is highlighted in bold, indicating superior performance in that specific area.  Overall, the table allows for a comprehensive comparison of the different methods in terms of their ability to produce high-quality, creative graphic designs.", "section": "4. Experiments"}, {"content": "| Settings | (i) | (ii) | (iii) | (iv) | (v) | Val | Ove | Ali | Und<sub>l</sub> | Und<sub>s</sub> |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Llama-3.1-8B (rank 16) | 8.03 | 7.89 | 8.00 | 7.75 | 6.90 | 0.9347 | 0.0796 | 0.0012 | 0.6900 | 0.6564 |\n| Llama-3.1-8B (rank 64) | 8.10 | 7.94 | 8.04 | 7.83 | 6.98 | 0.9352 | 0.0787 | 0.0013 | 0.7084 | 0.6715 |\n| llava-v1.5-7b (rank 32) | 8.00 | 7.86 | 8.02 | 7.78 | 6.90 | 0.9403 | 0.0940 | 0.0015 | 0.6703 | 0.6208 |\n| Llama-3.1-8B-Instruct (rank 32) | 8.08 | 7.89 | 8.03 | 7.82 | 6.99 | 0.9388 | 0.0804 | 0.0015 | 0.6867 | 0.6640 |\n| w/o LP, w/o LDC (rank 32) | 7.23 | 7.12 | 7.28 | 6.99 | 6.29 | 0.9325 | 0.0954 | 0.0013 | 0.6194 | 0.5875 |\n| w/ LP, w/o LDC (rank 32) | 7.84 | 7.67 | 7.78 | 7.56 | 6.66 | 0.9389 | 0.0843 | 0.0013 | 0.6568 | 0.6242 |\n| Llama-3.1-8B* (rank 32) | 8.22 | 8.06 | 8.22 | 7.94 | 7.09 | 0.9335 | 0.1029 | 0.0005 | 0.7321 | 0.7116 |\n| Llama-3.1-8B (rank 32) | 8.08 | 7.92 | 8.00 | 7.82 | 6.98 | 0.9365 | 0.0865 | 0.0013 | 0.6922 | 0.6580 |\n| GT | 8.35 | 8.21 | 8.30 | 8.01 | 7.26 | 0.9265 | 0.0768 | 0.0015 | 0.6848 | 0.6732 |", "caption": "Table 2: Ablation studies. Our investigation covers four aspects (from top to bottom): (1) the rank number in LoRA, (2) the base model, (3) the key techniques in LaDeCo, where LP denotes layer planning , and LDC represents layered design composition, (4) dataset size.\nThe model with * to is trained on the combined Crello and LargeCrello datasets, while the models without * are trained on Crello only.", "description": "This table presents the results of ablation studies conducted to analyze the impact of different components of the LaDeCo model on its performance.  The study investigates four key aspects: 1) The rank of the LoRA (Low-Rank Adaptation) technique used for efficient fine-tuning of the large language model, varying the rank number to assess its influence. 2) The base language model used, testing different models to evaluate their suitability for the task. 3) The core techniques of LaDeCo, namely, Layer Planning (LP) and Layered Design Composition (LDC), which are systematically removed to understand their individual contributions. 4) The size of the training dataset, comparing the results obtained using only the Crello dataset and a combined Crello and LargeCrello dataset.  The asterisk (*) indicates models trained on the larger combined dataset.", "section": "4. Experiments"}, {"content": "| Methods | Val | Ove | Ali | Und<sub>l</sub> | Und<sub>s</sub> | Uti | Occ | Rea |\n|---|---|---|---|---|---|---|---|---|\n| PosterLLaVa [32] | **0.9269** | 0.0685 | 0.0011 | 0.7879 | 0.7375 | 0.4199 | 0.1936 | **0.0747** |\n| PosterLlama [27] | 0.8701 | 0.0868 | 0.0014 | 0.8483 | 0.7798 | 0.4115 | **0.1772** | 0.0694 |\n| LaDeCo (Ours) | 0.9340 | **0.0805** | **0.0016** | **0.6851** | **0.6540** | **0.4414** | 0.1835 | 0.0768 |\n| GT | 0.9265 | 0.0768 | 0.0015 | 0.6848 | 0.6732 | 0.4737 | 0.1628 | 0.0709 |", "caption": "Table 3: Quantitative results on the content-aware layout generation subtask. The score closest to\nthe one calculated from real data (denoted as GT) is highlighted in bold, indicating the best performance among different methods.", "description": "This table presents a quantitative comparison of different methods for content-aware layout generation.  The metrics used assess various aspects of the generated layouts, comparing them to ground truth (GT) layouts created by human designers.  The scores closest to the ground truth values are highlighted in bold, indicating the best-performing method in each metric.  This helps to evaluate the accuracy and effectiveness of each approach in creating visually appealing and well-organized layouts.", "section": "4. Experiments"}]