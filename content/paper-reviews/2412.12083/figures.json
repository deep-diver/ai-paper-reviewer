[{"figure_path": "https://arxiv.org/html/2412.12083/x1.png", "caption": "Figure 1: IDArb tackles intrinsic decomposition for an arbitrary number of views under unconstrained illumination. Our approach (a) achieves multi-view consistency compared to learning-based methods and (b) better disentangles intrinsic components from lighting effects via learnt priors compared to optimization-based methods. Our method could enhance a wide range of applications such as image relighting and material editing, photometric stereo, and 3D reconstruction.", "description": "IDArb performs intrinsic image decomposition from images with varying numbers of views and illumination conditions. It disentangles intrinsic properties (albedo, normal, etc.) from lighting effects, leading to multi-view consistency.  Compared to learning-based methods, IDArb maintains consistency across multiple views. Compared to optimization-based methods, IDArb is less susceptible to artifacts from lighting.  This figure showcases these advantages and suggests applications in relighting, material editing, stereo, and 3D reconstruction.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2412.12083/x2.png", "caption": "Figure 2: Top: Overview of \u00a0IDArb. Bottom: Illustration of the attention block within the UNet.\nOur training batch consists of N\ud835\udc41Nitalic_N input images, sampled from Nvsubscript\ud835\udc41\ud835\udc63N_{v}italic_N start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT viewpoints and Nisubscript\ud835\udc41\ud835\udc56N_{i}italic_N start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT illuminations. The latent vector for each image is concatenated with Gaussian noise for denoising. Intrinsic components are divided into three triplets (D\ud835\udc37Ditalic_D=3): Albedo, Normal and Metallic&Roughness. Specific text prompts are used to guide the model toward different intrinsic components. For attention block inside UNet, we introduce cross-component and cross-view attention module into it, where attention is applied across components and views, facilitating global information exchange.", "description": "IDArb takes N images as input, which are sampled from N_v viewpoints and N_i illumination conditions. The input images are encoded into a latent space via VAE. The latent code is then concatenated with Gaussian noise for denoising. The intrinsic components are divided into 3 groups: albedo, normal, and metallic&roughness.  Cross-component and cross-view attention modules are introduced in the UNet to exchange information across different components and views, enforcing global consistency.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2412.12083/x3.png", "caption": "Figure 3: Overview of the Arb-Objaverse dataset. Our custom dataset features a diverse collection of objects rendered under various lighting conditions, accompanied by their intrinsic components.", "description": "This figure gives an overview of the Arb-Objaverse dataset construction process. It shows examples of objects from ABO, G-Objaverse, and A12-Objaverse datasets, comparing their features regarding lighting conditions and object diversity.  It then shows how Arb-Objaverse combines the strengths of other datasets with diverse objects from Objaverse, rendered under multiple illumination conditions using both HDR environment maps and point lights. Finally, the figure shows examples of generated intrinsic components (albedo, normal, metallic, and roughness) for an object from the Arb-Objaverse dataset.", "section": "3.2 ARB-OBJAVERSE DATASET"}, {"figure_path": "https://arxiv.org/html/2412.12083/x4.png", "caption": "(a) Albedo estimation. Our method effectively removes highlights and shadows.", "description": "This figure presents a qualitative comparison of albedo estimation results on synthetic data, showcasing the effectiveness of the proposed method (IDArb) in removing highlights and shadows from the estimated albedo. It compares IDArb with IID, RGB\u2192X, IntrinsicAnything and ground truth.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.12083/x5.png", "caption": "(b) Normal estimation. Our method gives shape geometry while correctly predicting flat surface.", "description": "IDArb produces normals maps that accurately represent the shape geometry, including flat surfaces. It avoids artifacts and inconsistencies commonly produced by other methods, such as embedding texture details into the normal map. This is evident in the example shown, where IDArb correctly predicts smooth normals for the flat portions of the object.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.12083/x6.png", "caption": "(c) Metallic estimation. Our method outperforms IID and RGB\u2194\u2194\\leftrightarrow\u2194X with plausible results free of interference from texture patterns and lighting.", "description": "Comparison of metallic estimation results among IID, RGB\u2192X, the proposed method (IDArb), and ground truth. IDArb produces plausible metallic maps without interference from texture patterns and lighting, outperforming IID and RGB\u2192X.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2412.12083/x7.png", "caption": "(d) Roughness estimation. Our method outperforms IID and RGB\u2194\u2194\\leftrightarrow\u2194X with plausible results free of interference from texture patterns and lighting.", "description": "This figure presents a qualitative comparison of roughness map estimation by different methods.  The input image is shown alongside results from IID, RGB\u2192X, and the proposed method (IDArb), along with the ground truth. The comparison highlights IDArb's superior performance in producing plausible roughness maps that are free from the influence of texture patterns and lighting artifacts, unlike the other methods.", "section": "4.2 EXPERIMENTAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.12083/x8.png", "caption": "Figure 4: Qualitative comparison on synthetic data. \u00a0IDArb demonstrates superior intrinsic estimation compared to all other methods.", "description": "Qualitative comparison of intrinsic decomposition results from different methods (IID, RGB\u2192X, IntrinsicAnything, GeoWizard, IDArb) against ground truth on synthetic data. It shows example results for albedo, normal, metallic, and roughness estimation.  IDArb produces more accurate and visually plausible intrinsic estimations compared to other methods, especially in removing highlights, shadows and texture interference caused by lighting.", "section": "4.2 EXPERIMENTAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2412.12083/x9.png", "caption": "Figure 5: Qualitative comparison on real-world data. \u00a0IDArb generalizes well to real data, with accurate, convincing decompositions and high-frequency details.", "description": "This figure showcases qualitative results of IDArb on real-world images, comparing its performance against IntrinsicAnything for albedo estimation. The input images are shown in the first column, the albedo predicted by IntrinsicAnything in the second, and the albedo, normal, metallic, and roughness predicted by IDArb are in the third, fourth, fifth, and sixth columns respectively. IDArb generates accurate and detailed decompositions, preserving high-frequency details and correctly predicting albedo, particularly for metallic objects. In contrast, IntrinsicAnything tends to predict darker albedo for metallic materials and exhibits blurred details. Despite being trained solely on synthetic data, IDArb demonstrates robust generalization capabilities when applied to real-world images.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.12083/x10.png", "caption": "(a)", "description": "This figure showcases how IDArb ensures multi-view consistency, unlike other learning-based methods which often produce inconsistent intrinsic properties across different views. Specifically, it visually compares the albedo and normal maps generated by IDArb and a competing learning-based approach when applied to multiple views of the same object.  The close-up views emphasize the inconsistencies in the competitor's outputs and highlight the consistent decompositions achieved by IDArb.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.12083/x11.png", "caption": "(b)", "description": "IDArb separates intrinsic images from lighting effects compared to traditional optimization-based methods. The optimization-based method NVDiffRecMC embeds some lighting effects in the intrinsic components, particularly in the material properties, as shown on the third column of the figure. The fourth column, representing the results of IDArb, successfully separates materials and lighting effects. This is attributed to the strong priors learned by IDArb from its training data on diverse objects under various lighting conditions. ", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.12083/x12.png", "caption": "Figure 6: Ablative studies on (a) cross-component attention and (b) training strategy.", "description": "This figure presents two ablative studies conducted to assess the impact of key components of the IDArb model. **(a) Cross-component Attention:** This study investigates the effectiveness of fusing information across different intrinsic components (albedo, normal, metallic, and roughness). The results demonstrate that incorporating cross-component attention improves the accuracy of intrinsic decomposition, particularly by reducing material ambiguities for metallic and roughness properties. **(b) Training Strategy:** This study evaluates the impact of different training strategies on model performance. Specifically, it compares the results of training the model exclusively on multi-view inputs versus using a combined multi-view and single-view training approach. The results show that the combined approach leads to better generalization and improved performance on single-image inputs, highlighting the importance of incorporating both general object material priors and cross-view information during training.", "section": "4.3 ANALYSIS AND ABLATIVE STUDY"}, {"figure_path": "https://arxiv.org/html/2412.12083/x13.png", "caption": "Figure 7: Effects of number of viewpoints and lighting conditions. We find increasing the number of viewpoints and the lighting conditions generally improves decomposition performance.", "description": "This figure analyzes the impact of varying the number of input viewpoints and lighting conditions on the performance of IDArb. The results generally indicate that increasing either the number of viewpoints or the diversity of lighting leads to improved intrinsic decomposition accuracy, especially in predicting metallic and roughness properties. However, the benefits of adding viewpoints appear to plateau beyond eight viewpoints.", "section": "4.3 ANALYSIS AND ABLATIVE STUDY"}, {"figure_path": "https://arxiv.org/html/2412.12083/x14.png", "caption": "Figure 8: Relighting and material editing results. From in-the-wild captures (a), our model allows for relighting under novel illumination (b) and material property modifications (c).", "description": "Figure 8 showcases the results of applying the IDArb model to perform relighting and material editing on real-world images.  The figure is divided into three subfigures: (a) shows the original captured images, (b) displays the results of relighting the objects under novel illumination conditions, and (c) presents examples of material editing, demonstrating how IDArb can be used to modify material properties such as color, roughness, and metallic properties of the objects. The relighting results in (b) show that IDArb is robust to different lighting conditions. And material editing results in (c) show that IDArb can accurately decompose the intrinsic properties of the objects, thereby enabling their editing and creating realistic effects.", "section": "4.4 APPLICATIONS"}, {"figure_path": "https://arxiv.org/html/2412.12083/x15.png", "caption": "Figure 9: Optimization-based inverse rendering results. Our method guides NVDiffecMC generate more plausible material results.", "description": "This figure showcases the results of using the proposed method in conjunction with an optimization-based inverse rendering approach, specifically NVDiffRecMC. It demonstrates that by incorporating the predictions of the proposed method as a prior, NVDiffRecMC can generate more plausible material properties, especially for albedo, which is prone to color shifting issues. In the figure, the first column displays the relighted results from two different novel environment maps using albedo produced by original NVDiffRecMC. The second column shows the relighted results utilizing albedo estimated by the proposed method together with NVDiffRecMC.  The third column provides the ground truth. In these experiments, the proposed method's output is used as pseudo-labels to guide the optimization of NVDiffRecMC.", "section": "4.4 Applications"}, {"figure_path": "https://arxiv.org/html/2412.12083/x16.png", "caption": "Figure 10: Photometric stereo results using 4 OLAT images in OpenIllumination and NeRFactor.", "description": "This figure shows qualitative results of photometric stereo, which aims to estimate surface normal and albedo maps from images with varying lighting conditions using a fixed viewpoint, under the One-Light-At-a-Time (OLAT) setup, where each image is illuminated by a single point light source without ambient light.  The first row demonstrates OLAT images and the estimated albedo and normal from OpenIllumination dataset (real data). The other rows present the decomposition on NeRFactor dataset (synthetic data).", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.12083/x17.png", "caption": "Figure 11: More results on real-world data.", "description": "This figure showcases additional qualitative results of IDArb on real-world images, complementing the results presented earlier in the paper.  Each row features a different object, and within each row, from left to right: the original input image, the estimated albedo, the estimated normal map, the estimated metallic map, and the estimated roughness map. These diverse examples illustrate IDArb's capacity to decompose real-world images into their intrinsic components.  The model, while trained on synthetic data, generalizes reasonably well to real-world photos of objects with varied material properties.  The objects presented include a metal skull, a sofa, a bouquet of flowers, toy figure, a plate of fruit, a wooden easel, a metallic teapot, and a ceramic figurine of rabbits. These results provide a visual demonstration of the model's ability to handle a range of object shapes, materials, and lighting conditions encountered in real-world photography.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2412.12083/x18.png", "caption": "Figure 12: More results on real-world data. We also provide the reconstructed and relighting images.", "description": "This figure shows additional qualitative results of IDArb on real-world images. The first column displays the original input images. The following columns present the predicted albedo, normal, metallic, and roughness maps. The 'Recon' column shows the reconstructed images using the predicted intrinsic components, while the 'Relit' columns display the results after relighting with different environment maps, showcasing the applicability of IDArb to downstream tasks like single-image relighting and material editing.  Specifically, it shows examples of a motorcycle, a car, trumpets, and a breakfast setting.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.12083/x19.png", "caption": "Figure 13: More results on multi-view data.", "description": "This figure showcases additional results of the IDArb model on multi-view data, further demonstrating its ability to decompose intrinsic properties (albedo, normal, metallic, and roughness) from multiple input images.  Each row presents a different object, with the columns displaying the input images and the corresponding predicted intrinsic components.  The consistent appearance of the intrinsic components across different viewpoints highlights the model's multi-view consistency.", "section": "4. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2412.12083/x20.png", "caption": "Figure 14: Multiview images with extreme lighting variation.\nFor each scene in NeRD dataset\u00a0(Boss et\u00a0al., 2021a), we input 4 views.", "description": "This figure showcases the results of IDArb on multiview images with extreme lighting variations from the NeRD dataset. Each scene includes four input views demonstrating varying illumination conditions, alongside the predicted albedo, normal, metallic, and roughness maps.", "section": "4.3 Analysis and Ablative Study"}, {"figure_path": "https://arxiv.org/html/2412.12083/x21.png", "caption": "Figure 15: Failure cases.", "description": "This figure presents a few failure cases of the IDArb model. The first row shows an outdoor scene, where the model struggles with generalization since it is primarily trained on object-centric synthetic data. The second row shows a product box with text. The model has difficulty recovering complex text structures. The third row shows a telephone, where the model overly simplifies the material details, due to the limited material variations presented in the synthetic training data.", "section": "4.3. ANALYSIS AND ABLATIVE STUDY"}, {"figure_path": "https://arxiv.org/html/2412.12083/x22.png", "caption": "Figure 16: Results on Mip-NeRF 360\u00a0(Barron et\u00a0al., 2022) (Part 1, outdoor). We input 4 views for each scene.", "description": "This figure presents the results of intrinsic image decomposition on outdoor scenes from the Mip-NeRF 360 dataset. For each scene, four input views are provided, and the model predicts albedo, normal, metallic, and roughness maps. The figure demonstrates the model's ability to generalize to complex outdoor environments and maintain consistency across different viewpoints under various lighting conditions. Although trained primarily on object-centric data, the model exhibits reasonable performance on these scene-level images. However, it can be observed that some material predictions are oversimplified, particularly for metallic and roughness, where the model tends to assign global values rather than capturing finer details within the scene.", "section": "4.4 APPLICATIONS"}]