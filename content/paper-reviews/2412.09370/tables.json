[{"content": "|       | Models   | Params | SE07 | ALL | ALL<sub>FULL</sub> |\n| :----: | :------- | :-----: | :--: | :-: | :---------------: |\n| <div style=\"width:5.7pt;height:39.7pt;vertical-align:-17.4pt;transform:rotate(-90deg);display:inline-block;\"><span class=\"ltx_text ltx_font_italic\">Sequence</span></div> | ESCHER   | 400M   | 76.3 | 80.7 | 81.2             |\n|         | KELESC   | 400M   | 76.7 | 81.2 | 81.4             |\n|         | ESR      | 350M   | 77.0 | 81.1 | 81.3             |\n|         | ConSeC   | 400M   | **77.4** | **82.0** | **82.5**         |\n| <div style=\"width:6.9pt;height:26.4pt;vertical-align:-9.7pt;transform:rotate(-90deg);display:inline-block;\"><span class=\"ltx_text ltx_font_italic\">Token</span></div> | WMLC     | 340M   | 72.2 | 77.6 | 78.1             |\n|         | EWISER   | 340M   | 71.0 | 78.3 | 78.9             |\n|         | BEM      | 220M   | 74.5 | 79.0 | 79.7             |\n|         | Our Model | 295M   | **75.2** | **80.2** | **80.8**         |", "caption": "Table 1: WSD results for sequence-level and token-level classifiers.", "description": "This table presents the performance of various Word Sense Disambiguation (WSD) models, categorized into sequence-level and token-level classifiers, on the benchmark dataset ALL.  It shows the F1 scores achieved by each model on the SE07 and ALL datasets, providing a comparison of different WSD approaches and their effectiveness in disambiguating word senses.  The table also includes model parameters to allow for an analysis of the model's complexity and performance trade-offs.  The results highlight the strengths and weaknesses of various WSD systems and their suitability for sequence vs. token-based disambiguation.", "section": "4.3 Word Sense Disambiguation"}, {"content": "| Models | SE07 P | SE07 R | SE07 F1 | ALL<sub>FULL</sub> P | ALL<sub>FULL</sub> R | ALL<sub>FULL</sub> F1 |\n|---|---|---|---|---|---|---|\n| BEM<sub>SUP</sub> | 67.6 | 40.9 | 51.0 | 74.8 | 50.7 | 60.4 |\n| BEM<sub>HEU</sub> | 70.8 | 51.2 | 59.4 | 76.6 | 61.2 | 68.0 |\n| ConSeC<sub>SUP</sub> | 76.4 | 46.5 | 57.8 | 78.9 | 53.1 | 63.5 |\n| ConSeC<sub>HEU</sub> | 76.7 | 55.4 | 64.3 | 80.4 | 64.3 | 71.5 |\n| Our Model | 73.8 | **74.9** | **74.4** | 75.2 | **76.7** | **75.9** |", "caption": "Table 2: WSL results with no CD oracle.", "description": "This table presents the results of the Word Sense Linking (WSL) experiment conducted without the Concept Detection (CD) oracle. It compares the performance of different models, including ConSeC, BEM, and the proposed model, in the WSL setting where the system must identify the spans to disambiguate itself.  The table shows Precision, Recall, and F1-score metrics for each model across the SE07 and ALLFULL datasets.  It demonstrates the performance impact of removing the CD oracle from the standard WSD setting and highlights the robustness of the proposed model when compared to state-of-the-art WSD systems adapted to this challenging WSL setting.", "section": "4.4 Word Sense Linking: Dropping the Concept Detection Oracle"}, {"content": "| Models | Lemmas | P | R | F1 | \u0394 F1 |\n|---|---|---|---|---|---|---|\n| ConSeC<sub>HEU</sub> | all | 80.4 | 64.3 | 71.5 | \u2013 |\n| ConSeC<sub>HEU</sub> | one | 71.6 | 56.4 | 63.1 | -8.4 |\n| ConSeC<sub>HEU</sub> | no | 0.0 | 0.0 | 0.0 | -71.5 |\n| Our Model | all | 75.2 | 76.7 | 75.9 | \u2013 |\n| Our Model | one | 70.4 | 73.1 | 71.7 | -4.2 |\n| Our Model | no | 68.5 | 62.5 | 65.4 | -10.5 |", "caption": "Table 3: WSL analysis on CG oracle.", "description": "This table presents the results of the Word Sense Linking (WSL) experiment, focusing on the impact of relaxing the Candidate Generation (CG) oracle.  It compares the performance of the proposed model and the ConSeCHEU system under three different conditions: \n1. **all**: using all available lemmas.\n2. **one**: using only the most frequent lemma for each sense.\n3. **no**: using no lemmas at all. The comparison highlights how the models behave with increasingly limited CG information, demonstrating the robustness of the proposed model in scenarios where the CG oracle is incomplete or absent.", "section": "4.5 Word Sense Linking: Relaxing the Candidate Generation Oracle"}, {"content": "| Models | Params | ALL R@100 (<span class=\"ltx_Math\" display=\"inline\">\u0394</span>) | \n|---|---|---| \n| baseline | 109M | 96.5 | \n| - bert-base-uncased | 109M | 88.7 (<span class=\"ltx_text\" style=\"color:#FF0000;\">-7.8</span>) | \n| - <span class=\"ltx_Math\" display=\"inline\">E5_{small}</span> | 33M | 94.2 (<span class=\"ltx_text\" style=\"color:#FF0000;\">-2.3</span>) | \n| - just main lemma | 109M | 92.5 (<span class=\"ltx_text\" style=\"color:#FF0000;\">-4.0</span>) | \n| - no lemma | 109M | 85.3 (<span class=\"ltx_text\" style=\"color:#FF0000;\">-11.2</span>) |", "caption": "Table 4: Results in terms of the ablation study on the Retriever Module. Each row represents a change made to the baseline model and the corresponding impact on performance.", "description": "This table presents an ablation study on the retriever module of a Word Sense Linking (WSL) model. The baseline model uses a BERT-base uncased architecture. Each row shows the results of modifying the baseline model. The changes include using different encoder architectures (E5small, bert-base-uncased), varying the textual representation of the senses in the inventory (using only the most frequent lemma or no lemma at all), and measuring the effect of these modifications on the model's recall@100 (R@100) performance.  The table highlights the impact of these changes on retrieval accuracy, indicating the importance of different design choices for the WSL system.", "section": "4.1 Model Details"}, {"content": "|       | Dataset       | Sentences | Tokens   | Instances | New Instances |\n| :----: | :-----------: | :-------: | :-------: | :--------: | :------------: |\n| Train | SemCor        | 37176     | 820410   | 226036    | -             |\n|       | SemCor<sub class=\"ltx_sub\"><span class=\"ltx_text ltx_font_italic\">C</span></sub> | 37176     | 820410   | 359763    | -             |\n|Eval   | semeval2007   | 135       | 3219     | 455       | 941 (+206%)   |\n|       | semeval2013   | 306       | 8533     | 1644      | 2194 (+133%)  |\n|       | semeval2015   | 138       | 2643     | 1022      | 157 (+15%)    |\n|       | senseval2     | 242       | 5829     | 2282      | 444 (+19%)    |\n|       | senseval3     | 352       | 5640     | 1850      | 634 (+34%)    |\n|       | all           | 1173      | 25864    | 7253      | 4370 (+60%)   |", "caption": "Table 5: Statistics for training and evaluation corpora. The columns represent the number of sentences, the total number of tokens, the number of annotated terms, and the number of newly annotated instances added in each dataset.", "description": "Table 5 presents a detailed statistical overview of the training and evaluation corpora used in the Word Sense Linking (WSL) study.  It breaks down the datasets into several key metrics: the original number of sentences, the total count of tokens (words and punctuation), the pre-existing number of annotated terms (spans of text associated with specific meanings), and crucially, the number of *new* annotated instances added as part of the current research. This last column is particularly important because it highlights the significant expansion of the datasets achieved through this work.  This augmented annotation addresses gaps in previous datasets, improving the accuracy and overall quality of the WSL model evaluation.", "section": "4.2 WSL Benchmark"}, {"content": "| Models | SemCor | SemCor<sub>C</sub> | \n|---|---|---| \n| BEM | 79.0 | 78.8 (-0.2) | \n| ESCHER | 80.7 | 80.3 (-0.4) | \n| ConSeC | 82.0 | 81.2 (-0.8) | ", "caption": "Table 6: WSD F1 score results on the SemCorC the dataset containing the silver annotations annotations from ConSecHEU.", "description": "This table presents the Word Sense Disambiguation (WSD) F1 scores achieved by different models on the SemCorC dataset. SemCorC is a version of the SemCor dataset that includes additional annotations generated by the ConSeC HEU model, to address the issue of missing annotations in the original dataset.  The F1 score, a metric that balances precision and recall, provides a comprehensive measure of the models' accuracy in assigning the correct word sense to each word.", "section": "4.3 Word Sense Disambiguation"}, {"content": "| Models | SE07 P | SE07 R | SE07 F1 | ALL<sub>FULL</sub> P | ALL<sub>FULL</sub> R | ALL<sub>FULL</sub> F1 |\n|---|---|---|---|---|---|---|\n| ConSeC<sub>HEU</sub> | **76.7** | 55.4 | 64.3 | **80.4** | 64.3 | 71.5 |\n| EntQA | 75.1 | 64.7 | 69.5 | 78.4 | 66.5 | 72.0 |\n| Our Model | 73.8 | **74.9** | **74.4** | 75.2 | **76.7** | **75.9** |", "caption": "Table 7: Our model comparison with EntQA in the WSL task tested on ALLFULL dataset.", "description": "This table presents a comparison of the performance of the proposed WSL model against the EntQA model.  Both models were evaluated on the ALLFULL dataset using the Word Sense Linking (WSL) task. The table shows the precision, recall, and F1-score achieved by each model. The results highlight the superior performance of the proposed model in terms of overall F1-score, recall, and efficiency.", "section": "4.5 Word Sense Linking: Relaxing the Candidate Generation Oracle"}, {"content": "| Example Text | WSL disambiguation |\n|---|---| \n| Training and development of ageing workers in both the workplace and the community. | a place where work is done |\n| In the amount USD 45 billion (nearly EUR 30 billion) in one go. | the basic monetary unit of most members of the European Union |\n| Auditors found crookery the first day on the job. | verbal misrepresentation intended to take advantage of you in some way |\n| Played on the 23rd of November against Ajax in European Champions League | - any number of entities (members) considered as a unit; |\n|  | - an active diversion requiring physical exertion and competition |\n| Ctrl Q Quit Shuts the program. | cease to operate or cause to cease operating |", "caption": "Table 8: This table showcases examples of model\u2019s disambiguation capabilities and lexical recognition gaps, showing specific instances where the model accurately identifies and annotates lexical variants not directly mapped in standard sense inventories", "description": "This table presents examples where the model successfully disambiguates words, highlighting instances of lexical variants that are not directly mapped in standard sense inventories like WordNet.  It demonstrates the model's ability to handle such variations, and simultaneously points out limitations in the sense inventory itself, where certain forms or nuances are missing.", "section": "4.5 Word Sense Linking: Relaxing the Candidate Generation Oracle"}, {"content": "| Example Text | WSL disambiguation |\n|---|---| \n| Trouble is following hard on the heels of the uproar around <span style=\"color:#FF0000;\">Josef Ackermann</span>, CEO of Deutsche Bank. | the corporate executive responsible for the operations of the firm; |\n| In his program, <span style=\"color:#FF0000;\">Fran\u00e7ois Hollande</span> confines himself to banalities. | a human being |\n| The <span style=\"color:#FF0000;\">World Labor Organisation</span> estimates that for example in Germany.. | an international alliance involving many different countries |\n| Friendly game today, at 3:05 pm at the <span style=\"color:#FF0000;\">National Stadium</span> in San Jose. | location, a point or extent in space |\n| The two justices have been attending <span style=\"color:#FF0000;\">Federalist Society</span> events for years. | any number of entities (members) considered as a unit |", "caption": "Table 9: This table showcases examples of how the model abstracts named entities into broader conceptual categories. Each row shows the model\u2019s disambiguation of specific named entities.", "description": "This table presents instances where the model's word sense disambiguation (WSD) output for named entities is categorized under broader conceptual categories in WordNet, rather than being linked to their specific entries. Each row provides an example sentence, the identified named entity, and how the model classified it within WordNet's hierarchy. This demonstrates the model's tendency to generalize named entities, which may reflect either limitations of WordNet's coverage or the model's preference for higher-level classifications.", "section": "4.5 Word Sense Linking: Relaxing the Candidate Generation Oracle"}]