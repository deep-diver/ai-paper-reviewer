{"importance": "This paper is crucial for researchers working with synthetic data for language model training.  It directly addresses the prevalent issue of **model collapse**, offering a novel theoretical framework and practical solution. This work is highly relevant given the increasing use of synthetic data and has the potential to significantly improve model performance and generalization capabilities. It opens exciting new avenues for research into data augmentation techniques and optimizing training data quality.", "summary": "Token-level editing prevents language model collapse from synthetic data by theoretically bounding test error and empirically improving model performance.", "takeaways": ["Synthetic data, when used in large proportions, negatively impacts language model performance.", "Token-level editing, a novel data augmentation technique, effectively mitigates model collapse by improving data quality and distribution coverage.", "Theoretical analysis proves that token-level editing prevents model collapse by bounding the test error."], "tldr": "The widespread use of synthetic data in training large language models (LLMs) presents a significant challenge: **model collapse**, where iterative training on self-generated data leads to performance degradation.  This paper investigates the impact of synthetic data on LLMs and explores methods to synthesize data without causing model collapse. The authors find a negative correlation between the proportion of synthetic data and model performance, attributing this to distributional shift and feature over-concentration in synthetic datasets.\nTo address this challenge, the authors propose **token-level editing**, a novel approach that modifies human-produced data at a token level using a pre-trained language model to create semi-synthetic data. This method is theoretically proven to prevent model collapse by keeping the test error bounded. Extensive experiments confirm the effectiveness of token-level editing across various training scenarios (pre-training from scratch, continual pre-training, and supervised fine-tuning), demonstrating improved model performance and data quality.", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2412.14689/podcast.wav"}