[{"content": "|                     | ArXiv | Books2 | Books3 | Math  | Enron | EuroParl | FreeLaw | GitHub | PG-19  | HackerNews | NIH   | Avg   |\n| :------------------ | :---- | :----- | :----- | :---- | :---- | :------ | :------ | :----- | :----- | :-------- | :---- | :---- |\n| Human data          | 22.26 | 25.39  | 22.87  | 10.84 | 23.50 | 30.73   | 12.04   | 4.15   | 16.88  | 32.54     | 23.53 |       |\n| 25% Synthetic Data | 21.86 | 26.32  | 23.87  | 11.05 | 24.85 | 35.02   | 12.84   | 4.35   | 17.99  | 33.80     | 23.76 |       |\n| 50% Synthetic Data | 22.50 | 28.01  | 25.75  | 10.84 | 26.56 | 41.99   | 14.02   | 4.67   | 19.70  | 36.12     | 24.61 |       |\n| 75% Synthetic Data | 24.35 | 31.19  | 28.98  | 11.81 | 30.30 | 56.32   | 16.03   | 5.30   | 22.75  | 40.44     | 26.19 |       |\n| Synthetic Data      | 35.60 | 43.72  | 47.72  | 17.25 | 66.97 | 129.75  | 29.62   | 12.00  | 50.14  | 87.95     | 39.48 |       |\n|                     | OpenSubts | OWT2   | Phil   | Pile-CC | PubMed-A | PubMed-C | StackEx | Ubuntu | USPTO  | Wikipedia | Youtube | Avg   |\n| Human data          | 28.08  | 25.77  | 33.56  | 26.78 | 18.97 | 15.49   | 10.81  | 20.86 | 19.32  | 24.31     | 21.54 | 21.37 |\n| 25% Synthetic Data | 29.25  | 26.94  | 34.63  | 27.83 | 19.55 | 15.38   | 11.03  | 22.32 | 19.58  | 25.88     | 22.63 | 22.31 |\n| 50% Synthetic Data | 31.00  | 28.76  | 37.48  | 29.36 | 20.51 | 15.89   | 11.54  | 23.53 | 20.51  | 27.57     | 24.91 | 23.90 |\n| 75% Synthetic Data | 34.18  | 32.04  | 42.39  | 32.17 | 22.33 | 16.92   | 12.55  | 26.54 | 22.21  | 30.68     | 28.98 | 27.03 |\n| Synthetic Data      | 57.83  | 53.94  | 78.18  | 54.69 | 34.82 | 23.87   | 20.47  | 51.78 | 37.24  | 46.12     | 65.49 | 49.30 |", "caption": "Table 1: PPL evaluation results for GPT-2 Small (124M) pre-trained on data mixture. The PPL increases as the proportion of synthetic data grows, providing further confirmation of Figure\u00a02.", "description": "This table presents the perplexity (PPL) scores achieved by a GPT-2 Small language model (124M parameters) after being pre-trained on various mixtures of human-generated and synthetic text data.  Different rows represent different proportions of synthetic data in the training dataset (ranging from 0% to 100%). Each column shows the PPL on a specific downstream task evaluation dataset.  Higher PPL indicates lower performance. The results demonstrate a clear trend: as the proportion of synthetic data increases, the overall PPL increases across all downstream tasks, confirming the negative correlation between the amount of synthetic data and model performance shown graphically in Figure 2 of the paper.", "section": "2 Non-iterative Model Collapse"}, {"content": "| Models | MQP | ChemProt | PubMedQA | RCT | USMLE | Average |\n|---|---|---|---|---|---|---|\n| OLMo-1B | 52.59 | 17.2 | 51.40 | 32.70 | 28.90 | 36.63 |\n| CPT | 52.29 | 21.00 | 58.50 | 34.90 | 27.49 | 38.83 |\n| \u0394 ToEdit | 54.59 | 22.40 | 65.00 | 34.50 | 27.96 | 40.89 |\n| LLama-3-8B | 66.80 | 28.59 | 60.8 | 73.85 | 40.61 | 54.13 |\n| CPT | 72.29 | 29.4 | 69.1 | 72.65 | 36.76 | 56.04 |\n| \u0394 ToEdit | 76.39 | 30.2 | 65.3 | 73.30 | 37.23 | 56.48 |\n| Models | HeadLine | FPB | FiQA-SA | ConvFinQA | NER | Average |\n|---|---|---|---|---|---|---|\n| OLMo-1B | 69.00 | 47.03 | 48.05 | 4.83 | 62.19 | 46.22 |\n| CPT | 70.31 | 49.78 | 40.36 | 18.72 | 60.44 | 47.92 |\n| \u0394 ToEdit | 71.77 | 51.39 | 46.06 | 18.85 | 62.97 | 50.21 |\n| LLama-3-8B | 81.28 | 63.58 | 81.60 | 52.88 | 72.53 | 70.37 |\n| CPT | 85.68 | 54.22 | 81.88 | 67.78 | 67.43 | 71.40 |\n| \u0394 ToEdit | 83.83 | 61.61 | 80.82 | 67.31 | 67.62 | 72.24 |\n| Models | ARC-c | GPQA | GSM8K | MATH | MMLU | Average |\n|---|---|---|---|---|---|---|\n| OLMo-1B | 28.67 | 24.23 | 1.67 | 0.00 | 26.56 | 16.23 |\n| CPT | 28.41 | 24.03 | 1.52 | 0.10 | 27.23 | 16.26 |\n| \u0394 ToEdit | 28.92 | 28.12 | 2.20 | 0.10 | 23.63 | 16.59 |", "caption": "Table 2: Performance on domain-specific tasks for continual pre-training models. CPT indicates continual pre-training. \u0394\u0394\\Deltaroman_\u0394 denotes training with our edited data. Our method demonstrates consistent improvements across three domains on both OLMo-1B and Llama-3-8B.", "description": "This table presents the results of continual pre-training experiments on language models, comparing performance across three domains (Biomedicine, Finance, Math).  The models used are OLMo-1B and Llama-3-8B.  For each model, the performance is measured with and without using the authors' token-level editing technique (ToEdit) on the training data. The standard continual pre-training approach (CPT) is compared to the CPT approach that incorporates ToEdit. The table shows the average performance improvement across multiple tasks within each domain, demonstrating the effectiveness of the proposed data editing method in enhancing the models' performance in domain-specific tasks.", "section": "2 Non-Iterative Model Collapse"}, {"content": "|               | PIQA   | BoolQ  | OBQA   | ARC-c  | ARC-e  | HellaSwag | SIQA   | Winogrande | Average     |\n| :------------ | :----- | :----- | :----- | :----- | :----- | :-------- | :----- | :--------- | :---------- |\n| OLMo-1B (PT) | 53.97  | 38.26  | 12.20  | 17.23  | 28.36  | 26.02     | 34.80  | 51.14      | 32.75       |\n| \u0394 ToEdit      | 54.13  | 38.65  | 12.80  | 18.43  | 27.48  | 25.94     | 34.95  | 52.49      | 33.11       |", "caption": "Table 3: General performance of the pre-trained base models. PT indicates we pre-train OLMo-1B from scratch. Experimental results demonstrate that our method can also enhance the effectiveness of pre-training.", "description": "Table 3 presents the general performance comparison of pre-trained language models, specifically OLMo-1B, before and after applying the token-level editing technique introduced in the paper.  The table shows results across various downstream tasks, highlighting the impact of pre-training from scratch (PT) versus pre-training enhanced by token-level editing on model performance.  This demonstrates the effectiveness of the proposed method, even in the foundational stage of pre-training language models.", "section": "2.1 PRE-TRAINING ON DATA MIXTURE"}, {"content": "|       | Models       | PIQA   | BoolQ   | HellaSwag | SIQA   | Winogrande | Average    |\n| :---- | :----------- | :----- | :------ | :-------- | :----- | :--------- | :--------- |\n|       |             |         |          |           |         |            |            |\n| **Instruction Tuning** |             |         |          |           |         |            |            |\n| _Natural Instructions_ | Llama-3-8B | 79.82  | 87.06   | 58.32     | 46.83  | 74.66      | 69.34      |\n| \u0394 ToEdit |             | 80.58  | 87.80   | 58.27     | 46.93  | 74.90      | 69.70      |\n| _CoT_ | Llama-3-8B   | 79.87  | 81.28   | 59.72     | 49.69  | 74.51      | 69.01      |\n| \u0394 ToEdit |             | 80.25  | 81.16   | 59.74     | 50.56  | 74.59      | 69.26      |\n| _FLAN v2_ | Llama-3-8B   | 80.79  | 84.04   | 59.98     | 51.43  | 74.66      | 70.18      |\n| \u0394 ToEdit |             | 80.69  | 85.20   | 59.99     | 52.00  | 75.37      | 70.65      |\n| _Open Assistant 1_ | Llama-3-8B | 79.65  | 83.18   | 60.51     | 48.52  | 74.11      | 69.19      |\n| \u0394 ToEdit |             | 79.98  | 83.91   | 60.34     | 48.31  | 74.66      | 69.44      |", "caption": "Table 4: Performance of the SFT models. We fine-tune LLaMA-3-8B using instruction tuning and code reasoning tasks, comparing performance with the edited version produced by our method. The experimental results indicate that our approach can enhance the data for instruction-tuning and code reasoning tasks.", "description": "This table presents the results of fine-tuning the LLaMA-3-8B language model using two sets of data: one original, and one processed using the authors' token-level editing method.  The model was fine-tuned on instruction tuning and code reasoning tasks.  The table compares the performance of the model on various downstream tasks after training on both datasets, illustrating the improved performance achieved by using the edited dataset.", "section": "4 EXPERIMENTS"}, {"content": "|   | Models | ARC-c | GPQA | GSM8K | MMLU | Average |\n|---|---|---|---|---|---|---|\n| *Code Reasoning* |  |  |  |  |  |  |\n| *OSS-Instruct-75K* | Llama-3-8B | 51.28 | 27.46 | 49.58 | 62.14 | 45.76 |\n|  | \u0394 ToEdit | 51.79 | 28.79 | 49.36 | 62.04 | 46.13 |\n| *Evol-Instruct-110K* | Llama-3-8B | 52.90 | 27.90 | 50.87 | 62.40 | 46.62 |\n|  | \u0394 ToEdit | 52.22 | 29.69 | 50.87 | 62.60 | 46.92 |", "caption": "Table 5: \nResults of different sampling strategies.", "description": "This table compares the performance of three different sampling strategies: top-k, top-p, and rejection sampling, on the PubMedQA, MedMCQA, and MedQA (4 options) tasks.  It shows how different sampling methods affect the model's ability to generalize and perform on these specific downstream tasks. The results are useful for understanding the trade-offs between these sampling techniques, helping to choose the best strategy for optimal model performance.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"content": "| Sampling Strategy | PubMedQA | MedMCQA | MedQA (4 options) |\n|---|---|---|---| \n| Top-k | 64.5 | 26.13 | 24.82 |\n| Top-p | 63.8 | 27.11 | 25.61 |\n| Reject Sampling | 64.5 | 28.90 | 28.20 |", "caption": "Table 6: \nAblation study on sampling size k\ud835\udc58kitalic_k for top-k.", "description": "This table presents the results of an ablation study investigating the impact of different sampling sizes (k) on the performance of the top-k sampling strategy.  The study examines how varying the value of k affects the performance on three downstream tasks: PubMedQA, MedMCQA, and MedQA (with 4 options).  The results are used to determine the optimal k value that balances performance and computational efficiency.", "section": "4.2 Datasets and Models"}, {"content": "| Sampling Size (<math>k</math>) | PubMedQA | MedMCQA | MedQA (4 options) |\n|---|---|---|---|\n| <math>k=8</math> | 64.5 | 26.13 | 24.82 |\n| <math>k=64</math> | 63.8 | 28.14 | 27.34 |", "caption": "Table 7: \nPerformance impact of different resampled token condition (p\ud835\udc5dpitalic_p) in Biomedicine domain.", "description": "This table presents the results of an ablation study investigating the impact of different resampling probability thresholds (p) on the performance of a language model trained on a biomedical dataset. Specifically, it shows how varying the threshold p, which determines the probability of replacing a token during data editing, affects the model's performance across multiple evaluation metrics in the Biomedicine domain.  The metrics shown are typical evaluation metrics for language models like MQP, ChemProt, PubMedQA, RCT, and USMLE.", "section": "4.3 Results"}, {"content": "| p  | PubMedQA | MQP | RCT | USMLE | ChemProt | Avg |\n|---|---|---|---|---|---|---|\n| $p \\geq 0.99$ | 64.5 | 55.73 | 30.95 | 27.65 | 14.6 | 38.69 |\n| $p \\geq 0.999$ | 63.6 | 55.4 | 29.09 | 28.12 | 16.2 | 38.48 |\n| $p \\leq 0.1$ | 62.4 | 51.47 | 25.6 | 29.14 | 10.0 | 35.72 |\n| $p \\leq 0.01$ | 65.4 | 54.91 | 28.19 | 27.80 | 11.0 | 37.46 |\n| $p \\leq 0.001$ | 64.2 | 56.39 | 35.0 | 27.80 | 12.4 | 39.16 |", "caption": "Table 8: \nToken distribution across different probability ranges in BioMed dataset.", "description": "This table shows the distribution of tokens within the BioMed dataset, categorized by their probability ranges.  It illustrates the proportion of tokens falling into various probability intervals (e.g., 0.0-0.1, 0.1-0.2, etc.), providing insight into the data's token probability distribution. This is relevant to understanding the characteristics of the data and how token probabilities relate to data quality and model training.", "section": "2.2 WHY DOES SYNTHETIC DATA FAIL IN LANGUAGE MODEL PRE-TRAINING?"}, {"content": "| Probability Range | Percentage | Token Count |\n|---|---|---|\n| 0.0-0.1 | 34.7% | 388,626,330 |\n| 0.1-0.2 | 8.1% | 90,716,809 |\n| 0.2-0.3 | 5.4% | 60,477,872 |\n| 0.3-0.4 | 4.4% | 49,278,266 |\n| 0.4-0.5 | 3.8% | 42,558,503 |\n| 0.5-0.6 | 3.6% | 40,318,546 |\n| 0.6-0.7 | 3.7% | 41,438,924 |\n| 0.7-0.8 | 4.0% | 44,798,424 |\n| 0.8-0.9 | 5.2% | 58,238,944 |\n| 0.9-1.0 | 27.1% | 303,543,988 |", "caption": "Table 9: Percentage of tokens requiring edits in the Natural-Instructions dataset. The total number of tokens is 4,671,834. and \u201cGen\u201d is short for \u201cGeneration\u201d.", "description": "This table presents the percentage of tokens in the Natural Instructions dataset that required editing during the token-level editing process.  The dataset contains a total of 4,671,834 tokens. The columns represent the generation number (Gen), indicating the iteration of the editing process, and the percentage of tokens requiring edits in that generation. The data shows a gradual decrease in the percentage of tokens requiring edits across generations, demonstrating the effectiveness of the token-level editing method in refining the data over successive iterations.", "section": "3.3 TEST ERROR UNDER DATA EDITING"}, {"content": "| **Tokens (p&gt;0.99)** | **Gen 1 (source)** | **Gen 2** | **Gen 3** |\n|---|---|---|---|\n| 584,103 | 12.5% | 11.76% | 11.08% |", "caption": "Table 10: \nComparison of human and synthetic data performance across downstream tasks in\u00a0(Maini et\u00a0al., 2024), based on training with GPT-2.", "description": "This table presents a comparison of the performance of language models trained on different proportions of human-generated and synthetic text data.  The models were evaluated on several downstream tasks from the Maini et al. (2024) benchmark, using GPT-2 as the base model. The results show how the model's performance on these tasks changes with increasing amounts of synthetic data in the training set.  It demonstrates the impact of synthetic data on language model generalization and performance.", "section": "2. NON-ITERATIVE MODEL COLLAPSE"}, {"content": "|                     | TruthfulQA | LogiQA | Wino. | PIQA | ARC-E | BoolQ | OBQA | Avg |\n| :------------------ | :---------: | :-----: | :----: | :---: | :----: | :----: | :----: | :----: |\n| Human Data          |   32.68     | 23.03   | 51.3   | 64.42 | 44.4   | 60.98  |  15    | 41.69 |\n| 25% Synthetic Data |   27.91     | 21.37   | 50.12  | 63.93 | 43.94  | 62.29  |  15.4  | 40.71 |\n| 50% Synthetic Data |   30.84     | 22.58   | 52.41  | 63.33 | 44.02  | 62.14  |  16    | 41.62 |\n| 75% Synthetic Data |   29.5      | 22.65   | 49.8   | 63.44 | 44.53  | 61.56  |  17.2  | 41.24 |\n| Synthetic Data      |   28.89     | 22.58   | 49.72  | 63   | 46.3   | 54.53  |  16.8  | 40.26 |", "caption": "Table 11: \nComparison of human and synthetic data performance across downstream tasks in\u00a0(Maini et\u00a0al., 2024), based on training with OLMo-237M. \u00b1 indicates the standard error.", "description": "This table presents a detailed comparison of the performance achieved by language models trained on various mixtures of human and synthetic data. Specifically, it evaluates the performance on multiple downstream tasks using the OLMo-237M model. The results highlight the impact of using different ratios of synthetic data in the training process.  The standard error for each result is included, providing a measure of the reliability of the results. The comparison includes results for models trained entirely on human data, models trained on mixtures of human and synthetic data at different ratios, and models trained on pure synthetic data. The table offers insights into the effects of synthetic data on the generalizability and performance of language models.", "section": "2.1 Pre-training on Data Mixture"}, {"content": "|                   | TruthfulQA | LogiQA | Wino. | PIQA  | ARC-E | OBQA  | Avg   |\n|-------------------|-------------|---------|--------|--------|-------|--------|-------|\n| Human Data        | 26.81 \u00b1 1.550 | 21.06 \u00b1 1.028 | 52.01 \u00b1 1.404 | 56.69 \u00b1 1.156 | 31.73 \u00b1 0.9550 | 13.80 \u00b1 1.543 | 33.68 |\n| 25% Synthetic Data | 26.44 \u00b1 1.543 | 21.25 \u00b1 1.032 | 52.64 \u00b1 1.403 | 57.02 \u00b1 1.155 | 31.78 \u00b1 0.9552 | 12.40 \u00b1 1.475 | 33.59 |\n| 50% Synthetic Data | 25.95 \u00b1 1.534 | 20.04 \u00b1 1.099 | 52.25 \u00b1 1.408 | 56.64 \u00b1 1.126 | 31.82 \u00b1 0.9557 | 12.80 \u00b1 1.495 | 33.25 |\n| 75% Synthetic Data | 25.34 \u00b1 1.522 | 20.87 \u00b1 1.025 | 50.43 \u00b1 1.405 | 55.60 \u00b1 1.159 | 32.74 \u00b1 0.9629 | 12.00 \u00b1 1.454 | 32.83 |\n| Synthetic Data     | 23.01 \u00b1 1.473 | 20.29 \u00b1 1.014 | 49.33 \u00b1 1.405 | 55.93 \u00b1 1.158 | 33.33 \u00b1 0.9673 | 14.20 \u00b1 1.562 | 32.68 |", "caption": "Table 12: PPL results of GPT-2 124M pretraining on mixture of human and synthetic data.", "description": "This table presents the perplexity (PPL) scores achieved by the GPT-2 124M language model during pretraining.  The model was trained on various mixtures of human-generated data (Dolma) and synthetic data (Cosmopedia), with the proportions of each type of data varying across different rows. The columns represent different datasets used for evaluation (Wikitext-103, RedPajama, Falcon-RefinedWeb, c4-en, mc4-en). The table shows how the model's performance, as measured by PPL, changes as the ratio of synthetic to human data in the training set is altered.", "section": "2 Non-iterative Model Collapse"}, {"content": "| Synthetic Data Ratio | 25% | 25% | 25% | 25% | 25% | 50% | 50% | 50% | 50% | 50% | 75% | 75% | 75% | 75% | 75% |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| Tokens Size | 8.4B | 16.8B | 25.2B | 33.6B | 42B | 8.4B | 16.8B | 25.2B | 33.6B | 42B | 8.4B | 16.8B | 25.2B | 33.6B | 42B |\n| Epochs | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 |\n| Wikitext-103 | 45.97 | 39.87 | 37.65 | 36.91 | 36.32 | 50.29 | 43.15 | 40.46 | 39.43 | 38.65 | 58.66 | 48.75 | 45.20 | 43.42 | 42.95 |\n| RedPajama | 42.28 | 37.62 | 35.72 | 34.66 | 34.24 | 46.89 | 41.42 | 39.37 | 38.21 | 37.72 | 55.72 | 49.26 | 46.27 | 44.81 | 44.30 |\n| Falcon-RefinedWeb | 56.40 | 50.62 | 48.26 | 47.13 | 46.66 | 61.06 | 54.34 | 51.72 | 50.39 | 49.87 | 69.32 | 61.50 | 58.28 | 56.77 | 56.19 |\n| c4-en | 48.15 | 43.14 | 40.98 | 39.91 | 39.41 | 51.79 | 46.06 | 43.90 | 42.73 | 42.23 | 58.60 | 52.22 | 49.26 | 47.87 | 47.27 |\n| mc4-en | 62.46 | 56.80 | 54.35 | 53.06 | 52.71 | 70.43 | 62.48 | 59.61 | 57.66 | 57.07 | 80.37 | 71.77 | 67.90 | 65.31 | 64.82 |", "caption": "Table 13: PPL results of OLMo-237M pretraining on mixture of human and synthetic data.", "description": "This table presents the perplexity (PPL) scores achieved by the OLMo-237M language model during pretraining.  The model was trained on various mixtures of human-produced and synthetic data. Different proportions of synthetic data (0%, 25%, 50%, 75%, and 100%) were used in the training process. The table shows the PPL on several downstream datasets (Wikitext-103, RedPajama, Falcon-RefinedWeb, c4-en, mc4-en, M2D2-Wiki, M2D2-S2ORC), demonstrating the impact of varying synthetic data ratios on model performance.", "section": "2 Non-Iterative Model Collapse"}, {"content": "| Synthetic Data Ratio | 0% | 25% | 50% | 75% | 100% | DSIR (1M) | DSIR (10M) | Edu Classifier (1M) | Edu Classifier (10M) | PPL Filter (1M) | PPL Filter (10M) | Density Sampling (1M) | Density Sampling (10M) |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---| \n| Unique Tokens | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 0.6B | 8.4B | 0.75B | 7.4B | 0.97B | 9B | 0.6B | 7.1B |\n| Training Tokens | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 8.4B | 10.5B | 7.4B | 13.68B | 9B | 8.9B | 7.1B |\n| Epochs | 1 | 1 | 1 | 1 | 1 | 14 | 1 | 14 | 1 | 14 | 1 | 14 | 1 |\n| Wikitext-103 | 187.36 | 185.5 | 260.08 | 367.46 | 1605.73 | 1309.53 | 1757.03 | 1111.29 | 1612.95 | 738.36 | 1193.25 | 1188.40 | 1753.89 |\n| RedPajama | 175.38 | 183.93 | 236.33 | 301.09 | 907.91 | 649.36 | 916.51 | 811.14 | 1104.75 | 376.36 | 645.82 | 789.67 | 896.18 |\n| Falcon-RefinedWeb | 165.17 | 166.69 | 199.68 | 245.15 | 523.93 | 573.61 | 510.96 | 522.97 | 612.72 | 344.82 | 449.86 | 501.99 | 560.92 |\n| c4-en | 123.88 | 127.68 | 147.69 | 174.48 | 410.19 | 457.96 | 404.63 | 415.88 | 487.97 | 286.95 | 367.44 | 414.55 | 457.71 |\n| mc4-en | 208.91 | 208.94 | 263.35 | 324.91 | 800.40 | 861.01 | 823.12 | 769.86 | 955.70 | 476.81 | 662.00 | 740.75 | 844.53 |\n| M2D2-Wiki | 88.24 | 87.34 | 107.77 | 114.19 | 189.06 | 234.45 | 183.17 | 161.58 | 206.45 | 130.43 | 162.08 | 167.20 | 205.50 |\n| M2D2-S2ORC | 86.15 | 81.53 | 97.61 | 100.64 | 204.22 | 170.78 | 496.40 | 145.27 | 201.52 | 117.44 | 163.38 | 131.22 | 192.97 |", "caption": "Table 14: Comparison of different synthetic data methods.", "description": "This table compares three different methods for generating synthetic data for language models: Cosmopedia, Rephrasing the Web, and ToEdit (the proposed method).  It shows the type of synthetic data generated (purely synthetic vs. semi-synthetic), the approach used to generate the data, and the resulting impact on model training (model collapse or performance improvement).", "section": "2.3 Proposed Strategy"}, {"content": "| Method | Data Type | Approach | Result |\n|---|---|---|---| \n| Cosmopedia (Ben\u00a0Allal et\u00a0al., 2024) | Pure synthetic | Using a prompt to induce data from LLMs. | Reveal non-iterative model collapse. |\n| Rephrasing the Web (Maini et\u00a0al., 2024) | Semi-synthetic | Using a prompt and source content to guide LLMs to reformat source content. | Improve training performance. |\n| ToEdit (Ours) | Semi-synthetic | Using the distribution of source content estimated by LLMs (single forward pass) to replace tokens. | Improve training performance. |", "caption": "Table 15: PPL results of GPT-2 124M pretraining on pure Human or Synthetic data.", "description": "This table presents the perplexity (PPL) scores achieved by a GPT-2 language model (124M parameters) after being pre-trained on either purely human-generated text data or purely synthetic text data.  It compares the model's performance across various dataset sizes and training epochs, illustrating the impact of using only human-generated versus only synthetic data on the model's ability to generalize well.", "section": "2 Non-iterative Model Collapse"}, {"content": "| Data Type | Human Data (Dolma) |  |  |  |  | Synthetic Data (Cosmopedia) |  |  |  |  | \n|---|---|---|---|---|---|---|---|---|---|---|\n| Tokens Size | 8.4B | 16.8B | 25.2B | 33.6B | 42B | 8.4B | 16.8B | 25.2B | 33.6B | 42B |\n| Epochs | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 |\n| Wikitext-103 | 43.62 | 38.57 | 36.11 | 34.89 | 34.55 | 169.38 | 147.73 | 135.23 | 131.78 | 128.05 |\n| RedPajama | 40.18 | 35.84 | 33.97 | 32.74 | 32.34 | 116.37 | 103.25 | 99.27 | 96.81 | 96.03 |\n| Falcon-RefinedWeb | 54.85 | 49.10 | 46.93 | 45.43 | 44.90 | 146.97 | 132.60 | 127.68 | 124.32 | 122.69 |\n| c4-en | 45.87 | 41.00 | 39.10 | 37.95 | 37.56 | 128.25 | 114.41 | 109.73 | 107.53 | 106.55 |\n| mc4-en | 61.00 | 54.44 | 52.11 | 50.38 | 49.74 | 171.44 | 153.70 | 150.28 | 145.44 | 144.99 |", "caption": "Table 16: Dolma dataset statistics (v1.6), quoted from source\u00a0(Soldaini et\u00a0al., 2024).", "description": "Table 16 provides a detailed breakdown of the Dolma dataset (version 1.6), which is a large-scale dataset used for training language models. It lists the source of the data, the type of documents included (e.g., web pages, code, social media posts, scientific papers), and the size of the dataset in terms of UTF-8 bytes, the number of documents, and the number of Unicode words.  This information is valuable for understanding the composition and scale of the dataset used in training language models and helps to compare it to other datasets used in similar research.", "section": "4.2 Datasets and Models"}]