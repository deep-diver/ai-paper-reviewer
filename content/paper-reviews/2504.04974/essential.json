{"importance": "This paper is important for researchers because it introduces **TRIG-Bench**, a novel benchmark to evaluate MLLMs' text grounding in documents, filling a critical gap in the research. It also propose  methods based on instruction tuning and efficient embedding, **providing baseline performances and more insight into document grounding**.", "summary": "TRIG: MLLMs get grounded in text-rich documents!", "takeaways": ["TRIG-Bench: a novel benchmark for evaluating text-rich image grounding in MLLMs.", "Instruction tuning and efficient embedding methods improve grounding capabilities on text-rich images.", "Existing MLLMs have limitations in grounding capability on text-rich images, needing specific training."], "tldr": "MLLMs struggle with visual text grounding, especially in text-rich document images due to complex layouts. Existing benchmarks focus on natural images, not document images. To address this, the paper introduces Text-Rich Image Grounding (**TRIG**), a novel task and instruction dataset for benchmarking and improving text-rich image grounding capabilities of MLLMs. The dataset consists of manually annotated question-answer pairs and synthetic data based on diverse datasets. \n\nThe paper proposes an OCR-LLM-human interaction pipeline to create the dataset and exposes the limitations of current MLLMs on text-rich images. Additionally, two methods, instruction tuning and plug-and-play efficient embedding, are proposed. **By finetuning MLLMs on the synthetic dataset, they improved spatial reasoning and grounding capabilities.** The dataset and benchmark provide a standardized framework for evaluating MLLMs in this domain.", "affiliation": "University of Maryland", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.04974/podcast.wav"}