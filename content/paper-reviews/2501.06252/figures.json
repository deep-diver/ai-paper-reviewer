[{"figure_path": "https://arxiv.org/html/2501.06252/x1.png", "caption": "Figure 1: Overview of Transformer2superscriptTransformer2\\text{Transformer}^{2}Transformer start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. In the training phase, we tune the scales of the singular values of the weight matrices to generate a set of \u201cexpert\u201d vectors, each of which specializes in one type of tasks. In the inference phase, a two-pass process is adopted where the first applies the task-specific expert and the second generates the answer.", "description": "Transformer\u00b2 is a self-adaptive large language model (LLM) framework.  During training, it uses singular value decomposition (SVD) on the weight matrices of the base LLM.  It then tunes the singular values to create \"expert\" vectors, each specialized for a particular type of task (e.g., math, coding). During inference, a two-pass process is used.  The first pass identifies the task type using a dispatch system. The second pass uses the corresponding expert vector to modify the LLM's weights, thereby generating a task-specific response.", "section": "3.2 TRANSFORMER\u00b2"}, {"figure_path": "https://arxiv.org/html/2501.06252/extracted/6116234/images/cem_code.png", "caption": "Figure 2: Method overview.\nLeft) At training time, we employ SVF and RL to learn the \u201cexpert\u201d vectors z\ud835\udc67zitalic_z\u2019s that scale the singular values of the weight matrices.\nRight) At inference time, we propose three distinct methods to adaptively select/combine the learned expert vectors.", "description": "This figure illustrates the two-stage process of the Transformer\u00b2 framework. The left panel shows the training phase, where Singular Value Fine-tuning (SVF) and reinforcement learning (RL) are used to train expert vectors that modulate the singular values of the model's weight matrices.  These expert vectors represent specialized skills or knowledge for different tasks. The right panel depicts the inference phase, where three different methods are employed to dynamically select and combine these pre-trained expert vectors to adapt the model's behavior to the current input task.  This adaptive combination allows the model to handle unseen tasks efficiently without requiring full retraining.", "section": "3.2 TRANSFORMER\u00b2"}, {"figure_path": "https://arxiv.org/html/2501.06252/x2.png", "caption": "Figure 3: Prompt based adaptation. Self-adaptation prompt used by Transformer2superscriptTransformer2\\text{Transformer}^{2}Transformer start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT to classify the task prompt into pre-defined categories.", "description": "This figure illustrates the prompt-based adaptation strategy used in Transformer\u00b2.  A new prompt is crafted to instruct the LLM to classify the input prompt into pre-defined categories (e.g., 'code', 'math', 'reasoning', 'others'). This classification determines which pre-trained 'expert' vector (obtained via Singular Value Fine-tuning and reinforcement learning) is selected to adapt the LLM's weights before processing the original input prompt, thereby tailoring the model's response to the specific task.", "section": "3.2 TRANSFORMER2"}, {"figure_path": "https://arxiv.org/html/2501.06252/x3.png", "caption": "Figure 4: SVF learning curves. The dashed lines indicate the performance of Llama3-8B-Instruct on the test split of each task. SVF effectively fine-tunes to surpass the base performance. While we use the best validation score to select our checkpoint for evaluation (marked by red dots), we present the entire training curve without early stopping to demonstrate SVF\u2019s learning capabilities. Tasks with only hundreds of training samples like Coding and Reasoning were stopped early. In our experiments, we update the parameters at the end of each epoch.", "description": "This figure displays the learning curves for Singular Value Fine-tuning (SVF) across four different tasks: Math, Code, Reasoning, and Vision-Language.  Each curve shows the training and testing performance over several epochs. The dashed lines represent the baseline performance of the Llama3-8B-Instruct model on the test set for each task.  The red dots highlight the validation checkpoints selected for evaluation.  Note that training for tasks with limited data (Coding and Reasoning) stopped early due to the small number of samples.  Parameter updates occurred at the end of each epoch.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.06252/x4.png", "caption": "Figure 5: Results for the VLM domain.", "description": "This figure displays the performance of the Transformer\u00b2 model on a vision-language modeling (VLM) task, specifically TextVQA.  It compares the performance of the base LLAMA3-LLAVA-NEXT-8B model to the performance after fine-tuning with LoRA and with the authors' proposed Singular Value Fine-tuning (SVF) method.  The graph likely shows a significant performance boost achieved by using SVF, demonstrating the effectiveness of the approach on VLM tasks. The y-axis might represent accuracy or a similar performance metric, and the x-axis is not specified but likely represents model variations or experimental setups. This demonstrates Transformer\u00b2's capacity to adapt and improve model performance even in a new, out-of-distribution visual domain.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.06252/x5.png", "caption": "Figure 6: Confusion matrices. These matrices display the classification percentages, where rows represent the task classes (ground truth) and columns indicate the predicted categories. Some samples are misclassified as \u201cOthers,\u201d which is reflected in rows where the totals do not sum to one.", "description": "This figure presents confusion matrices illustrating the performance of the classification-based adaptation strategies in Transformer\u00b2. Each matrix shows the percentage of samples correctly and incorrectly classified into different task categories (code, math, reasoning).  Rows represent the ground truth task, and columns represent the predicted task.  The 'Others' category accounts for samples that were not confidently classified into any of the main categories; this is why some rows do not add up to 100%. The matrices offer a visual comparison of the classification accuracy across various scenarios.", "section": "4 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.06252/x6.png", "caption": "Figure 7: \ud835\udf36\ud835\udc8csubscript\ud835\udf36\ud835\udc8c\\bm{\\alpha_{k}}bold_italic_\u03b1 start_POSTSUBSCRIPT bold_italic_k end_POSTSUBSCRIPT learned weights.", "description": "This figure visualizes the learned weights (\u03b1k) for each expert vector (zk) used in the few-shot adaptation strategy of the Transformer2 model.  Each bar represents the weight assigned to a specific expert for a given task. The height of the bar indicates the importance of that expert in solving the task.  The figure allows for a comparison of how different tasks utilize the various experts and helps illustrate the dynamic combination of expertise in the Transformer2 framework.  Different colors may represent different tasks, and different sections within the bars may represent different data sets used to train the experts.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.06252/x7.png", "caption": "Figure 8: Sample problem and answer. Math data sample used for LoRA instruction fine-tuning, text in blue is the unmasked solution.", "description": "This figure shows an example of a math problem from the GSM8K dataset used for training LoRA models. The problem involves a simple word problem about selling clips. The solution is shown, with the numbers and calculations in blue indicating parts that were unmasked during training.", "section": "A.2 LORA TRAINING"}, {"figure_path": "https://arxiv.org/html/2501.06252/x8.png", "caption": "Figure 9: Training LoRA with policy gradient. The dashed line shows the performance of Llama3-8B-Instruct on the test split. LoRA collapses at the beginning of the training stage and fails to recover, leading to negative effects on test performance. We swept a wide range of learning rates (2\u00d710\u22124,5\u00d710\u22124,\u2026,2\u00d710\u22122,5\u00d710\u22122)2superscript1045superscript104\u202621025superscript102(2\\times 10^{-4},5\\times 10^{-4},\\dots,2\\times 10{-2},5\\times 10^{-2})( 2 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , 5 \u00d7 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , \u2026 , 2 \u00d7 10 - 2 , 5 \u00d7 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT ), and all learning curves were similar to the one presented.", "description": "Figure 9 illustrates the instability of LoRA training using a policy gradient.  The plot shows the training and testing accuracy of a Llama3-8B-Instruct model fine-tuned with LoRA. Despite attempting a wide range of learning rates, the model's performance dramatically decreases at the beginning of training, and it fails to recover. This demonstrates the limitations of LoRA in scenarios where the objective function is optimized via reinforcement learning.  All learning curves exhibited a similar pattern.", "section": "Analysis 3: Ablation studies"}, {"figure_path": "https://arxiv.org/html/2501.06252/x9.png", "caption": "Figure 10: PCA of Llama3-8B-Instruct. We show the ratio of the variance captured by the top r\ud835\udc5fritalic_r singular components on the y-axis, and the layer indices on the x-axis. Except for the Query, Key and Value projection matrices, small r\ud835\udc5fritalic_r values only capture a tiny fraction of variance in singular values in the parameter matrices.", "description": "This figure displays the results of a Principal Component Analysis (PCA) performed on the weight matrices of the Llama3-8B-Instruct large language model.  The y-axis represents the proportion of variance in the weight matrices captured by the top 'r' principal components, while the x-axis shows the layer index.  The analysis reveals that for most layers (except for Query, Key, and Value projection matrices), a small number of principal components only accounts for a small fraction of the total variance within the weight matrices. This implies that a significant amount of information is contained in the less prominent principal components which are usually discarded in low-rank adaptation methods.  This finding supports the authors' claim that their Singular Value Fine-tuning (SVF) method is effective because it leverages all singular components of the weight matrices.", "section": "3 METHODS"}, {"figure_path": "https://arxiv.org/html/2501.06252/x10.png", "caption": "Figure 11: PCA of Mistral-7B-Instruct-v0.3. We show the ratio of the variance captured by the top r\ud835\udc5fritalic_r singular components on the y-axis, and the layer indices on the x-axis. Except for the Query, Key and Value projection matrices, small r\ud835\udc5fritalic_r values only capture a tiny fraction of variance in singular values in the parameter matrices.", "description": "This figure displays the results of a Principal Component Analysis (PCA) performed on the weight matrices of the Mistral-7B-Instruct-v0.3 large language model.  The y-axis represents the proportion of variance captured by the top r singular components (where r is the number of principal components considered), while the x-axis shows the different layers within the model. The analysis reveals that, except for the Query, Key, and Value projection matrices, only a tiny fraction of the variance in the singular values of the parameter matrices is captured by small values of r. This suggests that the model's most significant information isn't concentrated in just a few principal components.", "section": "3 METHODS"}]