{"importance": "**Value-guided decoding** offers a cost-effective way to control language models, but inaccuracies hinder its performance. This paper presents Iterative Value Function Optimization(IVO), a new way to address these inaccuracies, achieving alignment while reducing computational costs.", "summary": "IVO: Iterative Value Function Optimization for Guided Decoding", "takeaways": ["Introduces Iterative Value Function Optimization (IVO) for more accurate value estimation and enhanced exploration.", "IVO combines Monte Carlo Value Estimation and Iterative On-Policy Optimization to improve value estimation and exploration.", "Demonstrates IVO's effectiveness in text summarization, dialogue, and instruction following, showing improvements over existing methods."], "tldr": "**Reinforcement Learning from Human Feedback (RLHF)** aligns language models with human values, but is computationally intensive. **Guided decoding** offers a more cost-effective alternative, particularly with value-guided methods. Accuracy of the value function is crucial for these methods, as inaccuracies can lead to suboptimal decision-making and degraded performance. Existing methods struggle with estimating the optimal value function accurately, limiting their effectiveness. \n\nTo tackle these challenges, the paper introduces **Iterative Value Function Optimization (IVO)**, a framework with two key components: **Monte Carlo Value Estimation** reduces variance by exploring diverse trajectories. **Iterative On-Policy Optimization** progressively improves value estimation by collecting trajectories from value-guided policies. Experiments show that IVO enhances the effectiveness of value-guided decoding, achieving alignment with reduced computational costs.", "affiliation": "Shanghai Artificial Intelligence Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Text Generation"}, "podcast_path": "2503.02368/podcast.wav"}