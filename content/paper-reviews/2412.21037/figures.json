[{"figure_path": "https://arxiv.org/html/2412.21037/x1.png", "caption": "Figure 1: A depiction of the overall training pipeline of TangoFlux.", "description": "The figure illustrates the training pipeline of the TangoFlux model. It consists of two main stages: pre-training and online iterative alignment.  Pre-training involves training a base model using a large audio dataset and a language model (WavCaps and AudioCaps). Online iterative alignment refines the pre-trained model by utilizing the CLAP-Ranked Preference Optimization (CRPO) framework.  CRPO involves iterative cycles of generating audio samples, ranking them using CLAP, and constructing preference pairs. This process is designed to improve the model's alignment with textual descriptions, enhancing the quality and fidelity of generated audio. The figure shows the flow of data, prompts, and audio between different model components, highlighting the key steps in the training pipeline.", "section": "2 METHOD"}, {"figure_path": "https://arxiv.org/html/2412.21037/x2.png", "caption": "Figure 2: The trajectory of CLAP score and KL divergence across the training iterations. This plot shows the stark difference between online and offline training. Offline training clearly peaks early, by the second iteration, indicated by the peaking CLAP score and increasing KL. In contrast, the CLAP score of online training continues to increase until iteration 4, while the KL divergence has a clear downward trend throughout.", "description": "Figure 2 illustrates the performance of two training methods (online and offline) for a text-to-audio model across multiple iterations.  The y-axis shows the CLAP score (a measure of audio quality and relevance to the input text) and KL divergence (a measure of the difference between the generated audio distribution and the target distribution). The x-axis represents the training iteration.  The offline training method demonstrates rapid initial improvement, peaking around the second iteration, but then shows diminishing returns with a subsequent decline in CLAP score and an increase in KL divergence.  In contrast, the online training method exhibits sustained improvement, with the CLAP score increasing steadily until the fourth iteration and a simultaneous decrease in KL divergence, suggesting a more stable and effective training process.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21037/extracted/6101841/images/clap_vs_time_with_steps_legend.png", "caption": "Figure 3: Winning and Losing losses of \u2112DPO-FMsubscript\u2112DPO-FM\\mathcal{L}_{\\text{DPO-FM}}caligraphic_L start_POSTSUBSCRIPT DPO-FM end_POSTSUBSCRIPT and \u2112CRPOsubscript\u2112CRPO\\mathcal{L}_{\\text{CRPO}}caligraphic_L start_POSTSUBSCRIPT CRPO end_POSTSUBSCRIPT at each iteration. Winning and Losing losses increase each iteration, as well as their margin.", "description": "Figure 3 illustrates the training dynamics of two different loss functions: \u2112DPO-FM and \u2112CRPO, used for preference optimization in a text-to-audio model.  The plot shows how the 'winning' and 'losing' losses (representing the difference in model preference between better and worse audio samples) change over five iterations of training. Notably, both the winning and losing losses increase in magnitude with each training iteration. Importantly, the difference between the winning and losing losses (their margin) also increases, suggesting successful preference alignment with each iteration. This visualization underscores the iterative nature of the proposed preference optimization process and how the chosen loss function affects the training dynamics, leading to improved audio quality.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21037/extracted/6101841/images/fd_vs_time_without_legend.png", "caption": "(a) CLAPscorescore{}_{\\text{score}}start_FLOATSUBSCRIPT score end_FLOATSUBSCRIPT", "description": "The figure shows the trajectory of CLAP score and KL divergence across training iterations.  The CLAP score is a metric measuring the alignment between generated audio and text descriptions. KL divergence measures the difference between the generated audio distribution and the target distribution. The plot illustrates the performance of two training methods: online CRPO (with new data generated each iteration) and offline CRPO (using the same data repeatedly). The online CRPO shows a steady increase in CLAP scores and a decrease in KL divergence across iterations, indicating improved performance and better alignment. The offline CRPO, in contrast, shows early peaking of the CLAP score and an increase in KL divergence, demonstrating performance saturation and degradation after a few iterations. This highlights the importance of online data generation in the CRPO framework for consistently improving TTA model alignment.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2412.21037/extracted/6101841/images/annotation_form_tangoflux.png", "caption": "(b) FDopenl3openl3{}_{\\text{openl3}}start_FLOATSUBSCRIPT openl3 end_FLOATSUBSCRIPT", "description": "Figure 2 shows the trends of two key metrics, the KL divergence and CLAP score, across five iterations of the CLAP-Ranked Preference Optimization (CRPO) process. The blue line represents the CRPO approach with online data generation at each iteration, whereas the red line represents CRPO without online data generation. The KL divergence is a measure of the difference between the generated audio distribution and the true audio distribution, while the CLAP score measures the alignment between the generated audio and text descriptions. As the iterations proceed, the online CRPO consistently exhibits a decreasing KL divergence, signifying improved alignment with the true distribution, while the CLAP score steadily increases, indicating better adherence to the provided textual prompts.  In contrast, the offline CRPO approach demonstrates a more erratic behavior, showing early improvement followed by degradation in both metrics after the second iteration. This highlights the importance of incorporating online data generation in CRPO to prevent performance saturation and ensure consistent progress.", "section": "3 EXPERIMENTS"}]