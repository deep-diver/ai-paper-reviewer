[{"content": "| Model | #Params. | Duration | Steps | FD<sub>openl3</sub> \u2193 | KL<sub>passt</sub> \u2193 | CLAP<sub>score</sub> \u2191 | IS \u2191 | Inference Time (s) |\n|---|---|---|---|---|---|---|---|---|\n| AudioLDM 2-large | 712M | 10 sec | 200 | 108.3 | 1.81 | 0.419 | 7.9 | 24.8 |\n| Stable Audio Open | 1056M | 47 sec | 100 | 89.2 | 2.58 | 0.291 | 9.9 | 8.6 |\n| Tango 2 | 866M | 10 sec | 200 | 108.4 | 1.11 | 0.447 | 9.0 | 22.8 |\n| TangoFlux-base | 515M | 30 sec | 50 | 80.2 | 1.22 | 0.431 | 11.7 | 3.7 |\n| TangoFlux-base | 515M | 30 sec | 50 | 75.1 | 1.15 | 0.480 | 12.2 | 3.7 |", "caption": "Table 1: Comparison of audio generation models across various metrics. Output length represents the duration of the generated audio. Metrics include FDopenl3openl3{}_{\\text{openl3}}start_FLOATSUBSCRIPT openl3 end_FLOATSUBSCRIPT for Frechet Distance, KLpasstpasst{}_{\\text{passt}}start_FLOATSUBSCRIPT passt end_FLOATSUBSCRIPT for KL divergence, and CLAPscorescore{}_{\\text{score}}start_FLOATSUBSCRIPT score end_FLOATSUBSCRIPT for alignment. All inference time is computed on the same A40 GPU. We report the trainable parameters in the #Params column.", "description": "Table 1 presents a comparison of several text-to-audio (TTA) generation models, evaluating their performance across various metrics.  The models are compared based on their output audio length (duration), Frechet Distance (FD), Kullback-Leibler divergence (KL), CLAP score (measuring alignment between audio and text), and Inception Score (IS), a metric for image quality that is sometimes adapted for audio.  All inference times were measured using the same A40 GPU for fair comparison.  The number of trainable parameters in each model is also provided to give an indication of model complexity. This comprehensive comparison allows for a detailed analysis of the strengths and weaknesses of each model, providing insights for researchers and developers in the field of TTA.", "section": "3.3 Objective Evaluation"}, {"content": "| Inference | Time (s) |\n|---|---|", "caption": "Table 2: Comparison of text-to-audio models on multi-event inputs.", "description": "This table presents a comparison of various text-to-audio models' performance on audio clips containing multiple events.  The models compared are AudioLDM 2-large, Stable Audio Open, Tango 2, TANGOFLUX-base, and TANGOFLUX.  For each model, the table lists the number of parameters, the duration of the generated audio, the Fr\u00e9chet distance (FDopenl3), Kullback-Leibler divergence (KLpasst), CLAP score, and Inception Score (IS). Lower FDopenl3 and KLpasst scores, and higher CLAP and IS scores indicate better performance.  This allows for a direct comparison of how well each model handles complex audio scenarios with multiple events.", "section": "3.3 Objective Evaluation"}, {"content": "| Model | #Params. | Duration | FD<sub>openl3</sub> \u2193 | KL<sub>passt</sub> \u2193 | CLAP<sub>score</sub> \u2191 | IS \u2191 |\n|---|---|---|---|---|---|---|\n| AudioLDM 2-large | 712M | 10 sec | 107.9 | 1.83 | 0.415 | 7.3 |\n| Stable Audio Open | 1056M | 47 sec | 88.5 | 2.67 | 0.286 | 9.3 |\n| Tango 2 | 866M | 10 sec | 108.3 | **1.14** | 0.452 | 8.4 |\n| **TangoFlux-base** | **515M** | 30 sec | 79.7 | 1.23 | 0.438 | 10.7 |\n| **TangoFlux** | **515M** | 30 sec | **75.2** | 1.20 | **0.488** | **11.1** |", "caption": "Table 3: Comparison of difference preference dataset used for preference tuning. Metrics include FDopenl3openl3{}_{\\text{openl3}}start_FLOATSUBSCRIPT openl3 end_FLOATSUBSCRIPT for Frechet Distance, KLpasstpasst{}_{\\text{passt}}start_FLOATSUBSCRIPT passt end_FLOATSUBSCRIPT for KL divergence, and CLAPscorescore{}_{\\text{score}}start_FLOATSUBSCRIPT score end_FLOATSUBSCRIPT for alignment.", "description": "This table compares the performance of three different preference datasets used to fine-tune a text-to-audio model.  The datasets are BATON, Audio-Alpaca, and the novel CRPO dataset proposed in this paper. The comparison is based on three metrics:  FDopenl3 (Fr\u00e9chet distance for measuring the similarity of audio distributions), KLpasst (Kullback-Leibler divergence to quantify the difference between generated and reference audio label distributions), and CLAP score (measuring the alignment of generated audio with text descriptions). Lower FDopenl3 and KLpasst scores and a higher CLAP score indicate better performance.  The results show how the choice of preference dataset affects the model's ability to generate audio that is both high quality and semantically aligned with the input text.", "section": "3.3 Objective Evaluation"}, {"content": "| Dataset | FD<sub>openl3</sub> \u2193 | KL<sub>passt</sub> \u2193 | CLAP<sub>score</sub> \u2191 |\n|---|---|---|---|\n| BATON | 80.5 | 1.20 | 0.437 |\n| Audio Alpaca | 80.0 | 1.20 | 0.448 |\n| CRPO | 79.1 | 1.18 | 0.453 |", "caption": "Table 4: Comparison of different preference datasets used for preference tuning. Metrics include FDopenl3openl3{}_{\\text{openl3}}start_FLOATSUBSCRIPT openl3 end_FLOATSUBSCRIPT for Fr\u00e9chet Distance, KLpasstpasst{}_{\\text{passt}}start_FLOATSUBSCRIPT passt end_FLOATSUBSCRIPT for KL divergence, and CLAPscorescore{}_{\\text{score}}start_FLOATSUBSCRIPT score end_FLOATSUBSCRIPT for alignment.", "description": "This table presents a comparison of different preference datasets used in the preference tuning process of the TANGOFLUX model.  The effectiveness of each dataset is evaluated using three metrics:  FDopenl3, a measure of Fr\u00e9chet distance reflecting the overall similarity between the generated audio and a reference dataset; KLpasst, the Kullback-Leibler divergence indicating how closely the generated audio's probability distribution matches that of the reference data; and CLAP score, an alignment metric assessing how well the generated audio aligns with its corresponding text prompt.  The table facilitates the comparison and allows determining which preference dataset leads to the best model performance in terms of audio quality and alignment with textual input.", "section": "3 EXPERIMENTS"}, {"content": "| Model | N | FD<sub>openl3</sub> \u2193 | KL<sub>passt</sub> \u2193 | CLAP<sub>score</sub> \u2191 |\n|---|---|---|---|---|\n| **TangoFlux** | 1 | 75.0 | 1.15 | 0.480 |\n|  | 5 | 74.3 | 1.14 | 0.494 |\n|  | 10 | 75.8 | 1.08 | 0.499 |\n|  | 15 | 75.1 | 1.11 | 0.502 |\n| `Tango 2` | 1 | 108.4 | 1.11 | 0.447 |\n|  | 5 | 108.8 | 1.05 | 0.467 |\n|  | 10 | 108.4 | 1.08 | 0.474 |\n|  | 15 | 108.7 | 1.06 | 0.473 |", "caption": "Table 5: Human evaluation results on two attributes: OVL (overall quality) and REL (relevance). We report the z-scores, ranking, and Elo scores to mitigate individual annotator biases and present a relative performance comparison.", "description": "Table 5 presents the results of a human evaluation assessing two key aspects of audio generated by different models: Overall audio quality (OVL) and Relevance to the text prompt (REL).  To account for potential bias from individual assessors, the data is processed in three ways.  First, z-scores normalize the ratings, centering them around the average score for each annotator and scaling by their score spread.  Next, the models are ranked based on both the average (mean) and most frequent (mode) ranks.  Finally, Elo scores, using a pairwise comparison method, provide a continuous measure of each model's relative strength compared to others. The table displays these three metrics (z-scores, rankings, and Elo scores) for each model and attribute (OVL and REL). This multi-faceted approach allows for a comprehensive and robust comparison of the models, minimizing the effect of individual assessor bias and providing a nuanced understanding of the relative performance of each.", "section": "3.4 Human Evaluation"}, {"content": "| Model | z-scores | Ranking | Elo |\n|---|---|---|---|---| \n|  | OVL | REL | OVL | REL | OVL | REL |\n| **AudioLDM 2** | -0.3020 | -0.4936 | 3.5 | 4 | 3.7 | 4 | 1,236 | 1,196 |\n| **Stable Audio Open** | 0.0723 | -0.3584 | 2.4 | 1, 3 | 3.3 | 3 | 1,444 | 1,268 |\n| **Tango 2** | -0.019 | 0.1602 | 2.4 | 2 | 1.9 | 2 | 1,419 | 1,507 |\n| **TangoFlux** | **0.2486** | **0.6919** | **1.7** | **2** | **1.1** | **1** | **1,501** | **1,628** |", "caption": "Table 6: TangoFlux with different classifier free guidance (CFG) values.", "description": "This table presents the results of experiments conducted on the TANGOFLUX model using different classifier-free guidance (CFG) values.  The experiments aimed to analyze the impact of CFG on the model's performance, specifically focusing on the trade-off between audio fidelity (measured by FDopenl3) and semantic alignment (measured by CLAP score).  The table shows that varying the CFG scale affects both metrics, indicating an optimal CFG value that balances these two aspects.  Lower CFG values generally improve semantic alignment but may compromise audio quality, while higher values can improve audio quality at the cost of reduced semantic accuracy.", "section": "3 Experiments"}, {"content": "| Model | Steps | CFG | FD<sub>openl3</sub> \u2193 | KL<sub>passt</sub> \u2193 | CLAP<sub>score</sub> \u2191 |\n|---|---|---|---|---|---| \n| **TangoFlux** | 50 | 3.0 | 77.7 | **1.14** | 0.479 |\n|  | 50 | 3.5 | 76.1 | **1.14** | **0.481** |\n|  | 50 | 4.0 | 74.9 | 1.15 | 0.476 |\n|  | 50 | 4.5 | 75.1 | 1.15 | 0.480 |\n|  | 50 | 5.0 | **74.6** | 1.15 | 0.472 |", "caption": "Table 7: Prompts used in human evaluation and their characteristics.", "description": "Table 7 provides a detailed list of the 50 prompts used in the human evaluation section of the paper.  For each prompt, it indicates whether the prompt contains multiple distinct audio events and whether the temporal order of those events matters for accurate audio generation. This table is crucial for understanding the complexity and diversity of the prompts used to assess the models' abilities, especially in handling scenarios with multiple events.", "section": "3.4 Human Evaluation"}]