[{"figure_path": "https://arxiv.org/html/2504.00396/x1.png", "caption": "Figure 1: \nVisualization of the Attention Map. The salient regions directly reflect response intensity to the target text \u201da hat\u201d. Brighter regions indicate higher attention.", "description": "This figure visualizes attention maps, highlighting how the model focuses on different image regions when processing the text prompt. The example shows attention maps from different time steps during image generation when adding the text attribute \u201ca hat\u201d. Brighter areas in the attention map indicate stronger attention from the model. This visualization helps illustrate how the model focuses on the relevant areas for adding the hat attribute while minimizing impact on other elements in the image.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2504.00396/x2.png", "caption": "Figure 2: The Dual-Path Contrastive Learning Pipeline of SPF-Portrait.\nThe text in blue is the Base text, while those in red is the Target text. Reference Path takes only Base text\nas input, while Response Path takes complete text (Base text & Target text) as input.", "description": "This figure illustrates the dual-path contrastive learning pipeline used in SPF-Portrait.  The pipeline processes text input in two parallel paths: a Reference Path and a Response Path.  The Reference Path uses only the 'Base text' as input and remains frozen, acting as a baseline to maintain the original model's behavior. The Response Path receives both the 'Base text' and the 'Target text,' enabling it to learn and generate images incorporating both original features and the desired customizations. The figure visually depicts the flow of information through both paths, showcasing the interaction between them to achieve pure semantic understanding without the negative effects of 'semantic pollution'.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2504.00396/x3.png", "caption": "Figure 3: \nAnalysis of Alignment Process. (a) Vanilla aligning results in the over-alignment with original portrait.\n(b) For the same customization goal, reference image fine-tuning offers a more distinct target response region than T2I fine-tuning.", "description": "Figure 3(a) demonstrates the problem of over-alignment in vanilla alignment methods during text-driven portrait customization.  Vanilla methods, by focusing solely on aligning the overall image generation, cause the model to overemphasize matching attributes unrelated to the target changes, resulting in an image that is too similar to the original.  In contrast, Figure 3(b) illustrates that reference image fine-tuning generates a more clearly defined area of response focused on the target attribute change.  This is a crucial difference in performance because it ensures that the alterations are localized and don't inadvertently affect other aspects of the portrait. This comparison highlights the superiority of the proposed approach in maintaining the original image's integrity while still successfully adapting it to the requested attributes.", "section": "3.3. Semantic-Aware Fine Control Alignment"}, {"figure_path": "https://arxiv.org/html/2504.00396/x4.png", "caption": "Figure 4: \nComparison with Traditional Supervision on Image Fidelity.\n(a) illustrates the trend of Image-Reward (IR) and CLIP Score (CLIP-T) across fine-tuning steps. Image-Reward [66] is a metric used to evaluate image fidelity.\n(b) displays samples from traditional method [2] and ours.", "description": "Figure 4 demonstrates a comparison between traditional image fidelity supervision methods and the proposed SPF-Portrait approach. Part (a) shows graphs illustrating the Image-Reward (IR) and CLIP Score (CLIP-T) metrics over multiple fine-tuning steps, showcasing how the model's image fidelity (IR) and alignment with text descriptions (CLIP-T) evolve during training.  Part (b) provides a visual comparison, presenting example images generated by a traditional approach alongside those produced by the SPF-Portrait method. This visual comparison directly illustrates the improvements in image fidelity and adherence to the text prompts achieved by the proposed technique.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.00396/x5.png", "caption": "Figure 5: Qualitative Comparisons with SOTA methods. We compare ours with naive fine-tuning [55], PEFT-based methods (LoRA [26], AdaLoRA [71] ) and the decoupled methods (Tokencompose [64], Magenet [74]). Please zoom in for more details.", "description": "Figure 6 showcases a qualitative comparison of SPF-Portrait against several state-of-the-art (SOTA) methods for text-driven portrait customization.  The comparison includes naive fine-tuning as a baseline, along with prominent Parameter-Efficient Fine-Tuning (PEFT) approaches such as LoRA and AdaLoRA, and methods focused on decoupling text embeddings like TokenCompose and Magnet.  Each row represents a different portrait customization task, starting with an original image and caption. Subsequent columns display the results from each method, highlighting the differences in how well each method maintains the original portrait's identity and other attributes while incorporating the desired changes specified in the caption. The figure emphasizes the superior performance of SPF-Portrait in preserving the original model's behavior while accurately implementing the specified customizations. Detailed visual inspection is recommended for a thorough understanding of the results.", "section": "4.2. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.00396/x6.png", "caption": "Figure 6: Qualitative Ablation Study. We independently ablate the proposed loss and the SFCM mechanism.", "description": "This ablation study visually demonstrates the impact of individual components within the SPF-Portrait model. By removing either the text consistency loss, the fine-grained loss, the response enhancement loss, or the Semantic-Aware Fine Control Map (SFCM) mechanism, one at a time, we can observe how each component contributes to the overall performance.  The results show the effects on the generated images when specific parts of the model's training or alignment process are disabled.  This helps to isolate the effects of each module in terms of maintaining original image fidelity while also incorporating the desired target attributes.", "section": "4.5 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.00396/x7.png", "caption": "Figure 7: User Study Results. The percentages indicate the proportion of users who select the corresponding method.", "description": "This figure presents the results of a user study comparing SPF-Portrait with other state-of-the-art methods across three key aspects: Original Behavior Consistency (OBC), Target Attribute Responsiveness (TAR), and Aesthetic Preference (AP).  For each comparison, participants were shown pairs of images generated by different methods and asked to select the preferred image based on the evaluation criteria. The percentages shown in the bar chart within the figure represent the proportion of participants who selected each method as superior for each of the three criteria.  The figure visually demonstrates the relative strength of SPF-Portrait compared to the alternative methods in maintaining the original image characteristics while incorporating desired target attributes.", "section": "4.3. Quantitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.00396/x8.png", "caption": "Figure 8: Reconstruction Results. The three portraits for each case are only generated by the same Base text.", "description": "This figure demonstrates the ability of SPF-Portrait to retain the original characteristics of a portrait when only the base text prompt is used for generation, even after fine-tuning with additional target attributes.  Three portraits are shown for each example, the first being the original image generated by the Stable Diffusion model using only the base text. The second shows the result of naive fine-tuning on a new dataset, highlighting its inability to maintain the original characteristics. The third image, generated using SPF-Portrait with the same base text, clearly shows that the model retains the original details and style while successfully adding the target attributes. This experiment illustrates SPF-Portrait\u2019s ability to achieve incremental learning and prevent semantic pollution.", "section": "4.4. Original Capability Retention"}]