{"references": [{"fullname_first_author": "Edward J Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2021-06-00", "reason": "This paper introduces LoRA, a parameter-efficient fine-tuning method, which is a foundational technique for adapting large language models and is heavily cited in this paper."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper presents Latent Diffusion Models, the base of Stable Diffusion, which are foundational models used by this paper."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-00-00", "reason": "This paper introduces CLIP, a model that aligns visual and textual representations, which is used as a metric to measure consistency between the original and customized portrait."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This paper describes denoising diffusion probabilistic models, which are used to generate images based on text input through a forward diffusion process and a reverse denoising process."}, {"fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-00-00", "reason": "This paper introduces the attention mechanism, a key component of the Stable Diffusion model, which is adopted as T2I diffusion models."}]}