[{"content": "| Method | CLIP-I\\uparrow | DINO\\uparrow | CLIP-T\\uparrow |\n|---|---|---|---| \n| **Ours** | **77.60** | **40.11** | **27.71** |\n| Emu2 | 69.70 | 35.96 | 20.54 |\n| Collage Diffusion | 67.80 | 34.16 | 22.14 |\n| AnyDoor+ControlNet | 72.40 | 37.94 | 27.00 |\n| Paint-by-example+ControlNet | 64.50 | 34.60 | 23.77 |", "caption": "Table 1: Comparison with multi-object reference generation methods. The first three rows represent one pass multi-reference customization methods and the last two rows represent two stage inpainting pipeline based on pre-generated base images.", "description": "This table presents a comparison of FashionComposer with other multi-object reference generation methods using CLIP-I, DINO, and CLIP-T as evaluation metrics. The methods are grouped into one-pass multi-reference customization methods and two-stage inpainting pipeline methods based on pre-generated base images.", "section": "4.2. Comparisons for Compositional Generation"}, {"content": "| Methods | SSIM \u2191 | FID \u2193 | KID \u2193 | LPIPS \u2193 | FID \u2193 | KID \u2193 |\n|---|---|---|---|---|---|---| \n| DCI-VTON [10] | 0.8620 | 9.408 | 4.547 | 0.0606 | 12.531 | 5.251 |\n| StableVITON [15] | 0.8543 | 6.439 | 0.942 | 0.0905 | 11.054 | 3.914 |\n| StableGarment [30] | 0.8029 | 15.567 | 8.519 | 0.1042 | 17.115 | 8.851 |\n| MV-VTON [29] | 0.8083 | 15.442 | 7.501 | 0.1171 | 17.900 | 8.861 |\n| GP-VTON [32] | 0.8701 | 8.726 | 3.944 | **0.0585** | 11.844 | 4.310 |\n| LaDI-VTON [21] | 0.8603 | 11.386 | 7.248 | 0.0733 | 14.648 | 8.754 |\n| OOTDiffusion [33] | 0.8187 | 9.305 | 4.086 | 0.0876 | 12.408 | 4.689 |\n| **Ours** | **0.8771** | **5.842** | **0.906** | 0.0727 | **9.205** | **1.3606** |\n", "caption": "Table 2: Quantitative comparison for the standard virtual try-on task on the VITON-HD test dataset.", "description": "Quantitative comparison of standard virtual try-on methods on the VITON-HD dataset, evaluating performance metrics like SSIM, FID, KID, and LPIPS under paired and unpaired settings.", "section": "4.3. Comparisons of Virtual Try-on"}, {"content": "| Method | CLIP-I\u2191 | DINO\u2191 | CLIP-T\u2191 |\n|---|---|---|---| \n| DINOv2 Embeddings | 76.80 | 38.22 | 26.17 |\n| ControlNet | 75.94 | 33.47 | 27.10 |\n| Reference UNet | **77.30** | **39.39** | **27.74** |", "caption": "Table 3: Quantitative study for the reference UNet. We compare with other options for the appearance encoders like DINOv2 and ControlNet. Reference UNet shows the best performance.", "description": "This table presents a quantitative comparison of different appearance encoders for preserving details of reference garments. The table includes three different methods including DINOv2 embeddings, ControlNet and Reference UNet.  Each method is evaluated by CLIP-I, DINO, and CLIP-T.  CLIP-I and DINO measure the similarity between generated and reference garments, while CLIP-T measures text-image similarity.  The results show that Reference UNet outperforms the other two methods in all metrics.", "section": "4.4. Ablation Study"}, {"content": "| Method | CLIP-I \u2191 | DINO \u2191 | CLIP-T \u2191 | Quality \u2191 | Fidelity \u2191 |\n|---|---|---|---|---|---| \n| w/o Binding | 77.30 | 39.39 | 27.74 | 84 | 74 |\n| Conv-in | 77.60 | 39.39 | 27.86 | 90 | 122 |\n| Bind(1) | 77.20 | 39.42 | **28.10** | **169** | 95 |\n| Bind(1,2,3) | **77.60** | **40.11** | 27.71 | 140 | **192** |", "caption": "Table 4: Quantitative study for subject-binding attention. Bind(1) means only augmenting the self-attention modules of the UNet down and up blocks with the smallest resolution. Conv-in refers to injecting the text embeddings through the Convolution-in layer of the reference UNet.", "description": "This table presents the ablation study results for subject-binding attention, a key component of FashionComposer. Subject-binding attention links visual features of garments with corresponding text descriptions to better distinguish different items.  The table compares variations of subject-binding attention. \"Bind(1,2,3)\" applies the attention mechanism to all UNet blocks, while \"Bind(1)\" applies it only to the blocks with the smallest resolution. \"Conv-in\" refers to another approach where text embeddings are injected through the convolution layer of the reference UNet instead. The metrics used are CLIP-I (image similarity), DINO (image similarity), CLIP-T (text-image similarity), Quality (based on a user study), and Fidelity (based on a user study).  The results suggest that applying subject-binding attention to all UNet blocks achieves the best balance between image similarity and text alignment, with higher scores in both quality and fidelity assessments from users.", "section": "4.4 Ablation Study"}]