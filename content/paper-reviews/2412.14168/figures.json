[{"figure_path": "https://arxiv.org/html/2412.14168/x2.png", "caption": "Figure 1: \nDemonstration for the applications of FashionComposer.\nFashionComposer takes different kinds of conditions\u00a0(e.g., garment image, face image, parametric human model) equally as \u201cassets\u201d to composite diverse and realistic fashion images.\nThus supporting various fashion-related applications like controllable model image generation, virtual try-on, human album generation, etc.", "description": "FashionComposer takes various inputs like garment images, face images, and parametric human models as \"assets\" to create realistic and diverse fashion images. It supports applications such as controllable model image generation, virtual try-on with multiple garments or a consistent human identity across images (human album generation). The figure showcases example outputs of these applications.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/comp_quali_v3.jpg", "caption": "Figure 2: Overall pipeline of FashionComposer.\nFashionComposer takes garments composition and optional face, text prompt, and a densepose map projected from SMPL as inputs. The text prompt is encoded and fused with UNets through cross-attention and subject-binding attention, while the garment features are extracted and injected for denoising through Feature Injection Attention.", "description": "FashionComposer takes various inputs such as garment composition, optional face, text prompt, and densepose from SMPL to generate or edit fashion images.  It uses Reference UNet to extract garment/face features, which are then injected for denoising through Feature Injection Attention. The text prompt is fused with UNets via cross-attention and subject-binding attention.  Subject-binding attention associates extracted features with corresponding text descriptions (semantics) to avoid confusion and better maintain details. For example, if given a prompt \"A slim woman with short brown hair wears a green shirt and yellow skirt\", it will extract features for the shirt, skirt, face, and pose and bind them to descriptions like \"green shirt\" and \"yellow skirt\". It will then use these bound features together with human pose information to generate a realistic image.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/comp_v2.jpg", "caption": "Figure 3: Qualitative comparison with multi-reference customization methods, including Emu2\u00a0[27], Collage Diffusion\u00a0[25], Paint by Example\u00a0[34] and AnyDoor\u00a0[6].", "description": "This figure provides a qualitative comparison of FashionComposer with existing multi-reference customization methods, including Emu2, Collage Diffusion, Paint by Example, and AnyDoor, showcasing FashionComposer's superior performance in preserving garment details and handling multiple references in a single pass. The inputs consist of garment images and corresponding prompts, with FashionComposer receiving all garments in one pass while other methods receive single or multiple garments in multiple forwards. The results demonstrate FashionComposer's ability to effectively handle various garment references, while other methods struggle with maintaining fidelity and blending identities, making FashionComposer the better solution for realistic and detailed fashion image generation.", "section": "4.2 Comparisons for Compositional Generation"}, {"figure_path": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/tryon.jpg", "caption": "Figure 4: Qualitative comparison with garment-centric fashion image synthesis methods, including\nStableGarment\u00a0[30],\nIMAGDressing-v1\u00a0[26],\nand Magic Clothing\u00a0[4],\nwhere ours better preserves the identity of the target objects.\nNote that all approaches do not finetune the model on the test samples.", "description": "This figure presents a qualitative comparison among FashionComposer and existing garment-centric image synthesis methods, including StableGarment, IMAGDressing-v1, and MagicClothing. The task focuses on generating a fashion image given reference garment images and additional guidance such as face and densepose of a human. In each set of samples, the top row displays the provided conditions: the garment components, face image and the densepose.  The remaining rows show generated images by each method with the same input.  FashionComposer demonstrates better ability in preserving the identity of the garments, faces, and denseposes than other methods.", "section": "4.2. Comparisons for Compositional Generation"}, {"figure_path": "https://arxiv.org/html/2412.14168/extracted/6080147/fig/ablation_ref_v2.jpg", "caption": "Figure 5: Diverse virtual try-on results of FashionComposer for upper, lower, and outfit try-on tasks.", "description": "FashionComposer demonstrates its diverse virtual try-on capabilities, seamlessly integrating upper garments (like shirts and jackets), lower garments (like pants and skirts), and even complete outfits onto individuals.  The results showcase accurate garment fitting and maintain the texture and details of the original garment images.  This figure highlights the method's ability to handle various clothing categories and combinations, showcasing potential for realistic and flexible virtual try-on applications.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2412.14168/x3.png", "caption": "Figure 6: Qualitative comparison for the reference encoder. Reference UNet better preserves the fine details of the garments.", "description": "This figure shows qualitative results for preserving details when generating fashion images with references.  The first column displays the original garment images used as reference. The other columns present generated images conditioned on the garments and densepose, using different encoders for the reference information: DINOv2 embeddings, ControlNet, and the proposed Reference UNet. The comparison demonstrates that Reference UNet better preserves the details of the reference garments, such as patterns, logos, and textures, leading to higher fidelity in the generated images.", "section": "4.4. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2412.14168/x4.png", "caption": "Figure 7: Qualitative ablation study on subject-binding attention. Bind(1) means only modifying the self-attention modules of UNet blocks with the smallest resolution. Conv-in refers to injecting the mask map through the Convolution-in layer of the reference UNet. We highlight mistakes in rows 2-3 using red boxes.", "description": "Figure 7 shows qualitative results of subject-binding attention ablation study. The first row displays the input garments and densepose map. The second row presents results using convolution in layer, without subject-binding. The third row shows results of Bind(1). The fourth and fifth rows present results using Bind(1,2,3), the full model proposed, and Bind(1) which modifies self-attention only in lowest resolution UNet blocks, respectively. The red boxes in the second and third rows highlight regions with artifacts/mistakes, like blurry textures and unnatural fusion between garments and human body. This ablation study shows Bind(1,2,3) produces the best results, preserving both garment details and natural compositions.", "section": "4.4. Ablation Study"}]