{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces the Stable Diffusion model, which serves as the backbone for FashionComposer."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This work introduces CLIP, which is used extensively in FashionComposer for encoding text prompts and guiding the generation process."}, {"fullname_first_author": "Xintong Han", "paper_title": "VITON: An image-based virtual try-on network", "publication_date": "2018-01-01", "reason": "This paper introduces the VITON dataset, a benchmark used for evaluating virtual try-on tasks in FashionComposer."}, {"fullname_first_author": "Nataniel Ruiz", "paper_title": "DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation", "publication_date": "2023-01-01", "reason": "DreamBooth is used for evaluating the quality of generated images in FashionComposer, specifically in terms of CLIP-Score and DINO-Score."}, {"fullname_first_author": "Rinon Gal", "paper_title": "An image is worth one word: Personalizing text-to-image generation using textual inversion", "publication_date": "2023-01-01", "reason": "Textual Inversion provides a method for customizing text-to-image models and is utilized in FashionComposer for comparison and for evaluating prompt consistency using CLIP-T."}]}