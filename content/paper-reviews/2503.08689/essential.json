{"importance": "This paper is important because it addresses the critical challenge of **efficiently processing long videos in LVLMs**. QuoTA's training-free, plug-and-play design offers immediate benefits, paving the way for more effective video analysis and understanding in resource-constrained environments, and opens new research directions in query-focused attention mechanisms.", "summary": "QuoTA: Task-aware token assignment boosts long video comprehension in LVLMs via query-decoupled processing, without extra training!", "takeaways": ["QuoTA: a plug-and-play module, enhances long video comprehension in LVLMs by assigning visual tokens based on query relevance.", "It decouples the query using Chain-of-Thoughts, leading to more precise LVLM-based frame importance scoring.", "The method achieves state-of-the-art results on multiple benchmarks, demonstrating its effectiveness and generalizability."], "tldr": "Recent advances in long video understanding use token pruning. However, they overlook input-level semantic correlation between visual tokens and instructions. The paper introduces **QuoTA**, a modular, training-free method for visual token assignment that considers query-oriented frame-level importance in Large Video-Language Models.\n\n**QuoTA** strategically allocates frame-level importance scores based on query relevance, decoupling the query via Chain-of-Thoughts reasoning to facilitate more precise frame importance scoring.  This approach improves performance on six benchmarks by aligning visual processing with task-specific needs, optimizing token budget utilization.", "affiliation": "Xiamen University", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2503.08689/podcast.wav"}