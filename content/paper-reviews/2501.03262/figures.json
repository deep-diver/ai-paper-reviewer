[{"figure_path": "https://arxiv.org/html/2501.03262/extracted/6110288/imgs/llama3.png", "caption": "Figure 1: General domain results show that PPO and REINFORCE++ have smaller length hacking issues compared to GRPO in general scenarios with Bradley-Terry Reward Models.", "description": "The figure displays the training curves for three reinforcement learning algorithms (PPO, REINFORCE++, and GRPO) on a general domain dataset using the Bradley-Terry reward model.  It compares their performance across several metrics, including reward, KL divergence, policy loss, response length, actor learning rate, and total sequence length. The key observation is that PPO and REINFORCE++ exhibit less severe \"length hacking\" (where the model generates excessively long outputs to maximize reward) than GRPO.", "section": "5 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2501.03262/extracted/6110288/imgs/rule.jpg", "caption": "Figure 2: Mathematical scenario 1 shows that comparable results between REINFORCE++ and GRPO(Group Norm) under rule-based rewards.", "description": "Figure 2 presents a comparison of REINFORCE++ and GRPO (Group Norm) performance on a mathematical task using rule-based rewards.  The graphs illustrate key metrics such as reward, KL divergence, policy loss, actor learning rate, and total length (likely of generated text) across training steps.  The main takeaway is that both algorithms achieve comparable performance in this specific scenario, demonstrating REINFORCE++'s competitiveness with a more established method.", "section": "5 Results and Analysis"}, {"figure_path": "https://arxiv.org/html/2501.03262/extracted/6110288/imgs/math.jpg", "caption": "Figure 3: Mathematical scenario 2 results show that, under the same unit KL consumption, REINFORCE++ and RLOO achieve a greater reward increase compared to GRPO (Group Norm).", "description": "Figure 3 presents a comparison of the performance of REINFORCE++, RLOO, and GRPO (Group Norm) in a mathematical scenario.  The key finding is that, when consuming the same amount of KL divergence (a measure of the difference between the model's updated policy and its previous policy), both REINFORCE++ and RLOO achieve a significantly larger increase in reward than GRPO.  This highlights that REINFORCE++ and RLOO are more efficient at improving model performance in this specific context compared to the Group Norm variant of GRPO.", "section": "5 Results and Analysis"}]