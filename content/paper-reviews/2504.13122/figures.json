[{"figure_path": "https://arxiv.org/html/2504.13122/x1.png", "caption": "Figure 1: (a) Traditional textual DPO overlooks multimodal information, limiting video-language tasks. (b) Existing multimodal DPO methods rely on coarse alignment, missing rich temporal and perceptual details. (c&d) VistaDPO overcomes these limitations with a hierarchical spatiotemporal preference optimization framework, enabling fine-grained video-language alignment and precise reasoning over video dynamics. Here, ywsubscript\ud835\udc66\ud835\udc64y_{w}italic_y start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT is the preferred response over ylsubscript\ud835\udc66\ud835\udc59y_{l}italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT, and vwsubscript\ud835\udc63\ud835\udc64v_{w}italic_v start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT the visual input more likely to produce it than vlsubscript\ud835\udc63\ud835\udc59v_{l}italic_v start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT.", "description": "Figure 1 illustrates the limitations of previous direct preference optimization (DPO) methods and introduces the proposed VistaDPO framework. (a) shows traditional textual DPO ignoring visual information, resulting in limited performance on video-language tasks. (b) depicts existing multimodal DPO approaches with coarse alignment that overlook rich temporal and perceptual video details. In contrast, (c) and (d) present VistaDPO, which uses a hierarchical spatiotemporal preference optimization framework that enables fine-grained video-language alignment by considering temporal information and perceptual details. In (d), yw is shown as a preferred response over yl, with vw being the visual input more likely to produce yw than vl. ", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.13122/x2.png", "caption": "Figure 2: (a) The metadata of VistaDPO-7k highlights its focus on fine-grained video-language tasks, emphasizing temporal (44%percent4444\\%44 %) and perceptual (56%percent5656\\%56 %) reasoning. yli\u2062rsuperscriptsubscript\ud835\udc66\ud835\udc59\ud835\udc56\ud835\udc5fy_{l}^{ir}italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i italic_r end_POSTSUPERSCRIPT and ylr\u2062esuperscriptsubscript\ud835\udc66\ud835\udc59\ud835\udc5f\ud835\udc52y_{l}^{re}italic_y start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r italic_e end_POSTSUPERSCRIPT denote the irrelevant and relevant non-preferred responses respectively. (b) VistaDPO introduces a hierarchical spatiotemporal preference optimization framework. Instance (vvsuperscript\ud835\udc63\ud835\udc63v^{v}italic_v start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT) and perceptive (vfsuperscript\ud835\udc63\ud835\udc53v^{f}italic_v start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT) levels align global-to-local semantics with spatial visual features, leveraging both text-relevant and irrelevant rejected responses for robust cross-modal interaction. Temporal (vcsuperscript\ud835\udc63\ud835\udc50v^{c}italic_v start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT) level aligns clip-level semantics with temporal dynamics, enabling precise reasoning across spatial and temporal dimensions.", "description": "Figure 2 illustrates two key aspects of the VistaDPO framework.  (a) shows the breakdown of the VistaDPO-7k dataset, highlighting its focus on fine-grained video-language tasks by emphasizing both the temporal (44%) and perceptual (56%) aspects of video understanding.  It defines irrelevant (\ud835\udc66\ud835\udc56\ud835\udc5f) and relevant (\ud835\udc66\ud835\udc5f\ud835\udc52) non-preferred responses. (b) provides a visual representation of the VistaDPO's hierarchical spatiotemporal optimization framework. This framework operates across three levels: Instance (\ud835\udc63\ud835\udc63), aligning overall video content; Perceptive (\ud835\udc63\ud835\udc53), aligning spatial objects with language tokens; and Temporal (\ud835\udc63\ud835\udc50), aligning video temporal semantics with event descriptions.  The figure shows how these levels interact to enable precise reasoning across spatial and temporal dimensions.", "section": "4. VistaDPO-7k: A Spatial-temporal Grounded Video DPO Dataset"}, {"figure_path": "https://arxiv.org/html/2504.13122/x3.png", "caption": "Figure 3: Ablation study of hyperparameters on EventHallusion.", "description": "This ablation study investigates the impact of different hyperparameter values on the EventHallusion benchmark.  The figure displays the effect of varying four hyperparameters on binary accuracy: \u03bb (lambda), \u03bc (mu), \u03b2 (beta), and the ratio between relevant and irrelevant non-preferred responses (Bre/Bir). Each subplot shows the performance across a range of values for a single hyperparameter, illustrating how these values influence the model's ability to distinguish between preferred and non-preferred responses. The results help to determine the optimal hyperparameter settings for enhancing the model's performance on EventHallusion.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13122/x4.png", "caption": "Figure 4: T-SNE visualization of representation.\u00a0\n(a) Video-LLaVA shows substantial overlap between hallucinated (orange) and non-hallucinated (green) representations.\n(b) With Hound-DPO, there is no distinct improvement in the separation of the two clusters.\n(c) With VistaDPO, the representations achieve clear clustering, highlighting its superior discriminative capability.", "description": "This figure visualizes the results of t-distributed stochastic neighbor embedding (t-SNE) dimensionality reduction applied to video representations generated by three different models: (a) Video-LLaVA, (b) Video-LLaVA enhanced with Hound-DPO, and (c) Video-LLaVA enhanced with VistaDPO.  The visualizations reveal the model's ability to distinguish between hallucinated and non-hallucinated video representations.  Video-LLaVA shows significant overlap between the two types of representations. Hound-DPO offers little to no improvement in separating hallucinated from non-hallucinated representations. However, VistaDPO demonstrates a superior capability to clearly separate the representations, indicating its improved ability to discriminate between these two categories.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13122/x5.png", "caption": "Figure 5: Ablation study of visual non-preferred samples on two video hallucination benchmarks.", "description": "This ablation study investigates the impact of different types of negative visual samples on the performance of video hallucination benchmarks.  The study uses four different types of negative samples: randomness, blackness, reverse, and random mask.  The results show how these various negative samples affect the accuracy of video hallucination models on two specific benchmarks: VideoHallucer and EventHallusion.  The x-axis represents the type of negative sample and the y-axis represents the accuracy achieved. The figure helps to understand which type of negative samples contribute most to improving the performance of video hallucination models.", "section": "6.3. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2504.13122/x6.png", "caption": "Figure 6: Adversarial temporal testing on VideoHallucer. The gray regions indicate the performance drop under adversarial scenarios for each method.", "description": "This figure shows the results of an adversarial temporal test conducted on the VideoHallucencer benchmark.  Four different methods were compared: Video-LLaVA (baseline), Video-LLaVA with Hound-DPO, Video-LLaVA with only the Instance-level component of VistaDPO, and Video-LLaVA with the full VistaDPO model.  The x-axis represents different levels of adversarial modification applied to the videos, while the y-axis indicates the accuracy of the model's predictions.  The gray shaded areas highlight the performance drop experienced by each method under adversarial conditions. This experiment aims to evaluate the models' robustness to temporal inconsistencies and their ability to accurately predict video events despite such perturbations.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13122/x7.png", "caption": "Figure 7: Kernel Density Estimation (KDE) of log-likelihood differences in adversarial masking experiments. The log-likelihood difference measures the separation between original and adversarial distributions, with the shift representing the mean difference. Larger shifts indicate greater model robustness.", "description": "Figure 7 presents Kernel Density Estimation (KDE) plots illustrating the log-likelihood difference between original and adversarial video samples.  These plots demonstrate the robustness of different models against adversarial attacks.  The log-likelihood difference quantifies the separability of the original and perturbed data distributions, where a larger difference (shift) indicates improved model robustness to adversarial examples. The figure visually compares the distributions and their shifts for four different model scenarios:  the baseline Video-LLaVA model, the model with Hound-DPO applied, the model with only the Instance-level component of VistaDPO removed, and the full VistaDPO model. This allows for a direct visual assessment of how each model handles adversarial modifications and demonstrates VistaDPO's superior robustness.", "section": "7. Adversarial Spatial Testing"}, {"figure_path": "https://arxiv.org/html/2504.13122/x8.png", "caption": "Figure 8: Case Studies of Adversarial Testing for VistaDPO: We conduct case studies from three perspectives: (a) Temporal adversarial testing, which examines whether the model can infer the correct sequence of events by introducing reversed temporal order through video playback. (b) Spatial adversarial testing, which evaluates the model\u2019s ability to understand subject-object interactions by masking frames or pixels related to the target object. (c) Token adversarial testing, which tests the model\u2019s sensitivity to subtle linguistic differences by introducing similar action descriptions (e.g., contrasting \u201crun\u201d with \u201cstand\u201d and \u201cwalk\u201d). Each test compares VistaDPO with baselines (i.e., Video-LLaVA and Hound-DPO) and corresponding ablated versions to assess the impact of key components.", "description": "Figure 8 presents a comparative analysis of VistaDPO's robustness against adversarial attacks on video understanding. Three types of adversarial tests are conducted: temporal, spatial, and token-level.  Temporal adversarial testing reverses the video's temporal order to assess the model's ability to understand event sequences. Spatial adversarial testing masks parts of the video to evaluate the model's understanding of subject-object interactions.  Token-level adversarial testing introduces subtle linguistic variations (e.g., replacing \"run\" with \"walk\") to assess the model's sensitivity to fine-grained language differences. The results for VistaDPO are compared against two baseline models (Video-LLaVA and Hound-DPO) and their ablated versions to highlight the contribution of key components in VistaDPO.", "section": "7. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13122/x9.png", "caption": "Figure 9: Illustration of dataset pipeline for constructing augmented video-language QA pairs. (a) Original QA pairs are extracted from existing prevalent datasets, providing basic QA pairs. (b) These pairs are augmented by introducing chosen and rejected answers, where rejected answers include both irrelevant responses (e.g., \u201dshopping cart\u201d) and relevant but incorrect ones (e.g., \u201dtable\u201d). (c) To enhance spatiotemporal understanding, manual annotations are added, specifying object appearances, spatial coordinates (e.g., bounding boxes), and temporal dynamics (e.g., appearance and disappearance timestamps). This pipeline ensures richer, more nuanced data for hierarchical preference optimization in video-language tasks.", "description": "This figure illustrates the process of creating a dataset for training a video-language model using direct preference optimization.  It starts with extracting original question-answer pairs from existing video datasets. These pairs are then augmented by adding 'chosen' and 'rejected' answers, with the rejected answers categorized as either irrelevant or relevant but incorrect to the actual video content. Finally, manual annotations are added to enhance the spatiotemporal information. These annotations provide precise details about the objects present in the video, their location (bounding boxes), and their temporal dynamics (appearance and disappearance timestamps). The resulting dataset allows for a more nuanced and hierarchical optimization of the video-language alignment within the model.", "section": "4. VistaDPO-7k: A Spatial-temporal Grounded Video DPO Dataset"}, {"figure_path": "https://arxiv.org/html/2504.13122/x10.png", "caption": "Figure 10: A prompt template designed for generating hallucinated responses in multimodal models is presented. The template transforms original video QA pairs into a \u201dchosen response\u201d (a rephrased correct answer) and two \u201drejected responses\u201d (one contextually relevant but incorrect, and one entirely unrelated). This framework supports preference optimization by providing plausible yet inaccurate alternatives for training and evaluation. An example illustrates the process, highlighting the generation of both coherent and unrelated hallucinated responses.", "description": "Figure 10 details a prompt template designed to generate hallucinated responses for training multimodal models.  It transforms existing video question-answer (QA) pairs into three parts: a \"chosen response\" (a correctly rephrased answer), and two \"rejected responses\" (one plausible yet incorrect, and one entirely unrelated to the video content).  This design is crucial for preference optimization, as it provides the model with examples of both good and bad responses, allowing it to learn to prefer human-preferred answers. The example in the figure clarifies this process, showcasing the creation of both coherent and incoherent hallucinated responses.", "section": "4. VistaDPO-7k: A Spatial-temporal Grounded Video DPO Dataset"}, {"figure_path": "https://arxiv.org/html/2504.13122/x11.png", "caption": "Figure 11: Cases of VistaDPO in video understanding.", "description": "Figure 11 presents three example video questions and their answers generated by VistaDPO, Hound-DPO, and a variant of VistaDPO without the Temporal level optimization (only w/o LDPOt).  Each example demonstrates how the different models handle various aspects of video understanding, highlighting their strengths and weaknesses.  The questions probe different aspects of video content such as motion (car moving forward, person touching hair), object interactions (person riding a bicycle), and events (accidents). The answers show how accurately each model identifies the video content and how sensitive they are to subtleties in the question or video data. By comparing the responses, the figure demonstrates the impact of VistaDPO's hierarchical spatio-temporal approach in improving the accuracy and robustness of video understanding.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13122/x12.png", "caption": "Figure 12: Temporal data samples of VistaDPO-7K.", "description": "This figure showcases examples from the VistaDPO-7K dataset, illustrating how temporal information is annotated. Each example includes a question, the chosen and rejected answers along with their \"is_in_video\" labels, and detailed spatial-temporal grounding information. The grounding includes object bounding boxes (x, y, w, h) coordinates and timestamps (t) indicating the presence of objects at specific moments in the videos.", "section": "4. VistaDPO-7k: A Spatial-temporal Grounded Video DPO Dataset"}, {"figure_path": "https://arxiv.org/html/2504.13122/x13.png", "caption": "Figure 13: Perception data samples of VistaDPO-7K.", "description": "Figure 13 presents example data points from the VistaDPO-7k dataset, highlighting the annotation details for the perceptive level. Each example shows a question about an object in a video, the chosen correct answer, two rejected answers (one relevant but incorrect and another completely irrelevant), and the associated spatio-temporal grounding information. The spatio-temporal grounding information provides precise location (bounding boxes) and time stamps for each object mentioned in the question and answers.  This illustrates the fine-grained level of annotation in VistaDPO-7k, crucial for training the model to accurately perceive and understand spatial relationships in videos.", "section": "4. VistaDPO-7k: A Spatial-temporal Grounded Video DPO Dataset"}]