{"references": [{"fullname_first_author": "Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model.", "publication_date": "2024-01-01", "reason": "This paper introduced Direct Preference Optimization (DPO), a key technique used in this research for aligning language models with human preferences without needing explicit reward modeling."}, {"fullname_first_author": "Wang", "paper_title": "Videohallucer: Evaluating intrinsic and extrinsic hallucinations in large video-language models.", "publication_date": "2024-06-01", "reason": "This paper is important because it focuses on the problem of video hallucination in large video-language models (LVMs), directly relevant to the paper's goals of mitigating such issues."}, {"fullname_first_author": "Zhang", "paper_title": "Video-llama: An instruction-tuned audio-visual language model for video understanding.", "publication_date": "2023-06-01", "reason": "This paper serves as a fundamental baseline, presenting a model for video understanding that the current paper aims to improve upon."}, {"fullname_first_author": "Xu", "paper_title": "Pllava: Parameter-free llava extension from images to videos for video dense captioning.", "publication_date": "2024-04-01", "reason": "This paper provides a baseline model (PLLaVA) for video understanding that this work attempts to improve through the VistaDPO method."}, {"fullname_first_author": "Lin", "paper_title": "Video-llava: Learning united visual representation by alignment before projection.", "publication_date": "2023-11-01", "reason": "This paper introduces Video-LLaVA, which this work builds upon and compares to using the VistaDPO method, making it a key reference."}]}