{"references": [{"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This paper is a foundational work in visual instruction tuning, which is critical for developing large multimodal models."}, {"fullname_first_author": "Yue Zhang", "paper_title": "MMMU: A massive multi-discipline multi-modal understanding and reasoning benchmark for expert AGI", "publication_date": "2024-01-01", "reason": "This paper introduces the MMMU benchmark, one of the key datasets used in the InterFeedback-Bench, making it directly relevant to the study."}, {"fullname_first_author": "Pan Lu", "paper_title": "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts", "publication_date": "2024-01-01", "reason": "This paper introduces the MathVista dataset, another key dataset used in the InterFeedback-Bench for evaluating mathematical reasoning in LMMs."}, {"fullname_first_author": "Joon Sung Park", "paper_title": "Generative agents: Interactive simulacra of human behavior", "publication_date": "2023-01-01", "reason": "This paper's work on simulating human behavior is the basis for the approach for stimulating humans to give feedback, mimicking human-AI interactions."}, {"fullname_first_author": "Kenneth Marino", "paper_title": "Ok-vqa: A visual question answering benchmark requiring external knowledge", "publication_date": "2019-01-01", "reason": "This paper presents the Ok-VQA benchmark, a dataset with well-formed use cases for multimodal task benchmarks and evaluation."}]}