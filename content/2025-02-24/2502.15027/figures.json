[{"figure_path": "https://arxiv.org/html/2502.15027/x1.png", "caption": "Figure 1: Illustration of an interactive feedback scenario. When models generate incorrect responses, human users provide pertinent feedback to iteratively refine the answers.", "description": "The figure illustrates a human-in-the-loop interactive feedback process.  A multimodal model initially provides an answer to a question. If the answer is incorrect, a human user gives feedback, guiding the model to refine its answer through iterative steps. This demonstrates the importance of human feedback in improving the accuracy and problem-solving capabilities of large multimodal models.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.15027/x2.png", "caption": "Figure 2: Overview of the test data construction process for InterFeedback-Bench. For each LMM serving as the feedback receiver, we process each instance from a target dataset (e.g., MathVerse) and collect the error cases to form a negative set. The feedback provider then processes the same instances to build a positive set. Finally, we curate test data by selecting the intersection of both sets.", "description": "This figure illustrates the process of creating a benchmark dataset for evaluating large multimodal models (LMMs) using human feedback.  It starts with a target dataset (like MathVerse). For each LMM being tested (the 'feedback receiver'), the model processes each item in the dataset.  Any incorrect responses from this LMM create the 'negative set' for that LMM. A separate, highly accurate model (the 'feedback provider') is then run on the same dataset items, generating the 'positive set'. The intersection of the positive and negative sets creates the final curated test data used for further benchmark evaluations. This ensures only problems that the provider can solve correctly, but the receiver cannot, are included.", "section": "3.1 Interactive Benchmarking"}, {"figure_path": "https://arxiv.org/html/2502.15027/x3.png", "caption": "Figure 3: Overview of the proposed framework InterFeedback for assessing an LMM\u2019s ability to improve itself through feedback. The model interacts with humans to progressively solve a problem, and after each conversation round, we verify the correctness of the answer. If the answer is incorrect, an LMM-stimulated human will provide constructive feedback. We implement two types of feedback to investigate the behavior of LMMs.", "description": "The figure illustrates the InterFeedback framework, designed to evaluate a large multimodal model's (LMM) ability to learn from human feedback.  The process begins with the LMM attempting to solve a problem.  A human then verifies the answer. If incorrect, another LMM simulates a human providing feedback to help the first LMM improve its response.  This iterative process continues until the LMM solves the problem or a predetermined number of rounds are completed.  The framework explores two feedback types: simple (correct/incorrect) and detailed (with explanations).", "section": "3 INTERFEEDBACK-BENCH"}, {"figure_path": "https://arxiv.org/html/2502.15027/x8.png", "caption": "Figure 4: Distribution of samples being corrected in each round. We can observe that Claude-3.5-Sonnet archives the best performance in round 0.", "description": "This figure shows the percentage of samples that were initially incorrect but were later corrected in each round of interaction for four different large language models (LLMs): Gemini-2.0, GPT-40, OpenAI-01, and Claude-3.5-Sonnet.  Each model's initial accuracy (round 0) and accuracy after each subsequent round of feedback is represented by different colored segments.  The size of each segment is proportional to the percentage of samples corrected in that round. The figure illustrates how the models improve iteratively through human feedback, but also highlights that different models begin at different levels of accuracy and improve at varying rates.", "section": "4.2 EXPERIMENTAL ANALYSIS ON INTERACTIVE BENCHMARKING"}, {"figure_path": "https://arxiv.org/html/2502.15027/x9.png", "caption": "Figure 5: Distribution of corrected samples across various task categories. Visual logic tasks are mostly resolved within the first two rounds, whereas Math (Text-only) and MMMU-Pro tasks show little corrections in rounds 1 and 2. In contrast, Coding (Text-only) and MathVerse tasks exhibit corrections during rounds 1 and 2.", "description": "This bar chart displays the distribution of correctly answered questions across different task categories and rounds of interaction.  Visual logic problems saw the most improvements after the first two rounds of feedback.  In contrast, math problems (text-based) and MMMU-Pro problems showed less improvement across the first two rounds.  Coding (text-based) and MathVerse problems demonstrated a moderate level of correction across the first two rounds.", "section": "4.2 EXPERIMENTAL ANALYSIS ON INTERACTIVE BENCHMARKING"}, {"figure_path": "https://arxiv.org/html/2502.15027/x10.png", "caption": "Figure 6: Qualitative results on different LMMs.", "description": "This figure displays a qualitative comparison of how different Large Multimodal Models (LMMs) perform on a visual reasoning task.  The task involves selecting the correct option from four choices to complete a sequence or reveal a pattern in a series of images. The figure shows the responses of three different LMMs (Claude-3.5 Sonnet, Gemini 2.0 Flash, and OpenAI-01) to the same question.  Each model's response, including intermediate steps and any corrections made, is presented. This illustrates the various approaches and reasoning processes of each LMM. The differences in reasoning steps and the number of iterations needed to arrive at the correct answer highlight the varying capabilities and challenges associated with interactive intelligence in LMMs.", "section": "B Qualitative Examples"}, {"figure_path": "https://arxiv.org/html/2502.15027/x11.png", "caption": "Figure 7: Qualitative results on different LMMs.", "description": "This figure presents a qualitative comparison of different Large Multimodal Models (LMMs) in solving a geometry problem.  The problem involves finding the measure of angle CED given that DE is parallel to BC, angle A = 80\u00b0, and angle B = 60\u00b0. The figure displays the responses of three different LMMs (Claude-3.5-Sonnet, Gemini-2.0 Flash, and OpenAI-01) to the problem.  Each model's response shows the steps taken to arrive at the answer, including any corrections made after receiving feedback. This showcases the models' varying capabilities in problem-solving and reasoning, and their ability to utilize feedback to improve the solution.", "section": "4.3 EXPERIMENTAL ANALYSIS ON HUMAN BENCHMARKING"}, {"figure_path": "https://arxiv.org/html/2502.15027/x12.png", "caption": "Figure 8: An example that model tends to guess answers.", "description": "This figure demonstrates how Large Multimodal Models (LMMs) tend to guess answers when faced with challenging problems they cannot readily solve.  Two instances of the same question are shown, highlighting how the model, even with feedback, produces different incorrect answers on separate attempts, before ultimately settling on a final answer (which may or may not be correct).  The figure visually represents the limitations of LMMs and suggests a reliance on elimination rather than true reasoning in certain situations.", "section": "4.2 EXPERIMENTAL ANALYSIS ON INTERACTIVE BENCHMARKING"}, {"figure_path": "https://arxiv.org/html/2502.15027/x13.png", "caption": "Figure 9: An example that model tends to guess answers.", "description": "This figure demonstrates how large language models (LLMs) resort to guessing when faced with challenging problems.  It showcases two instances where, despite receiving feedback, the models' responses are inconsistent across different runs for the same problem. This suggests that the models may not be truly reasoning but are simply trying to eliminate incorrect answers rather than utilizing a logical problem-solving strategy. The inconsistency highlights the limitations of LLMs in tackling complex questions effectively, even with guidance.", "section": "4.3 EXPERIMENTAL ANALYSIS ON HUMAN BENCHMARKING"}]