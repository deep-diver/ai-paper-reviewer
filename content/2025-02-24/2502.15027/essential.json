{"importance": "This research introduces InterFeedback-Bench, a benchmark designed to evaluate how well large multimodal models interact with human feedback. It highlights the gaps in current LMMs' ability to use feedback effectively, **offering researchers a crucial tool for developing more interactive and adaptive AI systems**. The framework and datasets provide a new avenue for exploring human-AI interaction, pushing the boundaries of general-purpose AI assistants. ", "summary": "InterFeedback: LMMs need better human feedback to enhance AI assistants!", "takeaways": ["Interactive processes improve LMM performance but existing LMMs interpret feedback suboptimally.", "High-quality feedback is essential; subpar feedback degrades performance more than binary signals.", "LMMs often guess answers, revealing a need to enhance their reasoning capabilities."], "tldr": "Existing benchmarks lack tests for Large Multimodal Models (LMMs) regarding interactive intelligence with human users, a key aspect for general AI assistants. To address this, **the study introduces InterFeedback, a versatile framework applicable to any LMM and dataset**. It also presents InterFeedback-Bench, evaluating interactive intelligence using MMMU-Pro and MathVerse on 10 open-source LMMs. A newly collected dataset, InterFeedback-Human, assesses interactive performance manually in models like OpenAI-01 and Claude-3.5-Sonnet. \n\n**The evaluation reveals that even advanced LMMs like OpenAI-01 correct their results through human feedback less than 50% of the time**. The study highlights the need for methods to enhance LMMs' ability to interpret and benefit from feedback. Key findings include that interactive processes improve performance, existing LMMs struggle with feedback interpretation, and additional iterations don't guarantee correct solutions. The research also emphasizes that LMMs may not always use reasoning and sometimes guess the answer based on human's response.", "affiliation": "National University of Singapore", "categories": {"main_category": "Multimodal Learning", "sub_category": "Human-AI Interaction"}, "podcast_path": "2502.15027/podcast.wav"}