{"references": [{"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This work is important as it introduces Refusal, one of the three standard unlearning methods used in this study, which trains the model to output uninformative answers instead of knowledge from the forget set via RLHF."}, {"fullname_first_author": "Meng", "paper_title": "Locating and editing factual associations in gpt", "publication_date": "2022-01-01", "reason": "This paper is a commonly cited work as it introduces a model editing method, focusing on directly modifying model weights to forget target facts, serving as a comparative method in related work, and uses a similar LoRA finetuning approach in this study."}, {"fullname_first_author": "Jang", "paper_title": "Knowledge unlearning for mitigating privacy risks in language models", "publication_date": "2023-01-01", "reason": "This work is important because it introduces Gradient Ascent, one of the three standard unlearning methods used in this study, which maximizes the training loss on the forget set."}, {"fullname_first_author": "Maini", "paper_title": "Tofu: A task of fictitious unlearning for llms", "publication_date": "2024-01-01", "reason": "This paper is important because it proposes a benchmark for evaluating unlearning (which is utilized in this study), measures performance on preserved data points after unlearning, and uses it to measure model utility, which is a key concept in the approach of the presented study."}, {"fullname_first_author": "Cao", "paper_title": "Towards making systems forget with machine unlearning", "publication_date": "2015-01-01", "reason": "This paper is foundational as it categorizes machine unlearning into exact and approximate unlearning, concepts that frame the study."}]}