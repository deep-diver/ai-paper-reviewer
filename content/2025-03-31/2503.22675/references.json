{"references": [{"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which is widely used in sequential recommendation models."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Adam: A method for stochastic optimization", "publication_date": "2014-01-01", "reason": "This paper introduces the Adam optimizer, which is used to optimize all models in the paper."}, {"fullname_first_author": "Wang-Cheng Kang", "paper_title": "Self-attentive sequential recommendation", "publication_date": "2018-01-01", "reason": "This paper introduced SASRec, a classic sequential recommendation baseline which utilizes self-attention mechanism."}, {"fullname_first_author": "Fei Sun", "paper_title": "BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer", "publication_date": "2019-01-01", "reason": "This paper introduced BERT4Rec, which uses bidirectional encoder representations from transformer for sequential recommendation."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper demonstrates the use of Chain-of-Thought prompting to elicit reasoning in large language models."}]}