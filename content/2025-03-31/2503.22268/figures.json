[{"figure_path": "https://arxiv.org/html/2503.22268/x1.png", "caption": "Figure 1: Our method is capable of handling challenging scenarios, including articulated structures, shadow reflections, dynamic background motion, and drastic camera movements, while producing per object level fine-grained moving object masks.", "description": "This figure showcases the robustness and accuracy of the proposed method for moving object segmentation.  It demonstrates the ability to successfully segment moving objects even under challenging conditions such as articulated movements (e.g., a person running), shadow reflections obscuring object boundaries, dynamic background motion that could interfere with object detection, and rapid or significant camera movements. The resulting segmentation masks are fine-grained and accurately delineate the boundaries of individual moving objects.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.22268/x2.png", "caption": "Figure 2: The effectiveness of long-range tracks. Over longer periods of time, if a moving object experiences factors such as occlusion or changes in lighting, it can negatively affect the tracking performance of optical-flow-based methods for that object.", "description": "This figure demonstrates the superiority of long-range tracking over optical flow methods for moving object segmentation.  It shows that as time elapses, factors like occlusion and changing lighting conditions can severely hinder the accuracy of optical flow-based tracking, while long-range tracking remains more robust. The graph visually compares the performance of the two methods over time, highlighting how long-range tracks maintain their effectiveness even when encountering challenging conditions that disrupt optical flow.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2503.22268/x3.png", "caption": "Figure 3: Overview of Our Pipeline.\nWe take 2D tracks and depth maps generated by off-the-shelf models\u00a0[15, 66] as input, which are then processed by a motion encoder to capture motion patterns, producing featured tracks. Next, we use tracks decoder that integrates DINO feature\u00a0[45] to decode the featured tracks by decoupling motion and semantic information and ultimately obtain the dynamic trajectories(a). Finally, using SAM2\u00a0[51], we group dynamic tracks belonging to the same object and generate fine-grained moving object masks(b).", "description": "This figure illustrates the pipeline of the proposed method for moving object segmentation.  It starts with 2D tracks and depth maps from external models as input. These are fed into a motion encoder to extract motion features, generating 'featured tracks'. A tracks decoder, incorporating DINO features for semantic information, then processes these featured tracks, separating motion and semantic cues to identify dynamic trajectories. Finally, the identified trajectories are grouped using SAM2 to generate pixel-accurate masks for moving objects.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.22268/x4.png", "caption": "Figure 4: Qualitative comparison on DAVIS17-moving benchmarks.\nFor each sequence we show moving object mask results.\nOur method successfully handles water reflections (left), camouflage appearances (middle), and drastic camera motion (right).", "description": "Figure 4 presents a qualitative comparison of moving object segmentation results on the DAVIS17-moving benchmark dataset.  Three challenging scenarios are highlighted:  the left column showcases the successful segmentation of a moving object despite water reflections that would typically confuse simpler methods; the middle column demonstrates accurate segmentation of a camouflaged object, highlighting the model's robustness to visual ambiguity; the right column shows accurate segmentation even with significant, drastic camera motion, proving the algorithm's effectiveness under challenging conditions.  Each row displays the ground truth mask (GT), followed by the masks generated by our proposed method and various other state-of-the-art methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.22268/x5.png", "caption": "Figure 5: Qualitative comparison on FBMS-59 benchmarks. The masks produced by us are geometrically more complete and detailed.", "description": "Figure 5 presents a qualitative comparison of moving object segmentation results on the FBMS-59 benchmark dataset. It visually demonstrates that the proposed method generates masks that are more geometrically complete and detailed compared to several baseline methods.  The figure showcases examples where the model's output provides more accurate and precise boundaries for moving objects, capturing fine details that are missed by other approaches.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.22268/x6.png", "caption": "Figure 6: Qualitative comparison on SegTrack v2 benchmarks. Our method succeeds even under motion blur conditions.", "description": "Figure 6 presents a qualitative comparison of moving object segmentation results on the SegTrack v2 benchmark dataset.  The figure visually demonstrates the performance of the proposed method (Ours) against several other state-of-the-art techniques (GT, CIS, OCLR-TTA, ABR, RCF-All, EM).  Each column shows the ground truth segmentation mask (GT), followed by the segmentation masks generated by each method, including the authors' proposed method. This allows for a direct visual comparison of the accuracy and robustness of different approaches. The figure highlights the effectiveness of the proposed method in handling challenging conditions, particularly those involving significant motion blur, a scenario where other techniques often struggle. The figure showcases improved segmentation accuracy even in challenging scenarios like those with motion blur.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.22268/x7.png", "caption": "Figure 7: Qualitative comparison on Fine-grained MOS task which will produce per-object level masks.", "description": "Figure 7 displays a qualitative comparison of fine-grained moving object segmentation results.  It showcases the per-object level mask generation capabilities of the proposed model, highlighting its ability to produce detailed and accurate masks for each individual object, even in complex scenarios with multiple interacting objects.  This contrasts with methods that only produce a single mask for all moving objects.", "section": "4.4. Fine-grained Moving Object Segmentation"}, {"figure_path": "https://arxiv.org/html/2503.22268/x8.png", "caption": "Figure 8: \nVisual comparison for the ablation study on two critical and challenging cases. The top sequence shows scenarios involves drastic camera motion and complex motion patterns, while the bottom sequence with both static and dynamic objects of the same category. The experimental setup is detailed in Sec.\u00a04.5.", "description": "This figure presents a qualitative comparison of ablation study results on challenging scenarios. The top row shows the results on sequences with drastic camera motion and complex motion patterns, highlighting the impact of different components of the proposed model. The bottom row showcases results when both static and dynamic objects of the same category are present, demonstrating the model's ability to differentiate between them. The detailed experimental setup can be found in Section 4.5 of the paper.", "section": "4.5. Ablation Study"}]