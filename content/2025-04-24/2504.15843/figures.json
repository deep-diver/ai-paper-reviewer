[{"figure_path": "https://arxiv.org/html/2504.15843/x1.png", "caption": "Figure 1: Pre-DPO further enhances the performance of DPO and SimPO by leveraging a guiding reference model, consistently across different models and scales.", "description": "This figure displays the results of experiments comparing the performance of three preference optimization methods: Direct Preference Optimization (DPO), Simple Preference Optimization (SimPO), and the proposed Pre-DPO method.  The x-axis represents different language models (Llama-3.2 and Qwen-2.5, both with base and instruct versions) varying in size and training data. The y-axis represents the win rate percentage on two benchmark datasets, AlpacaEval 2.0 and Arena-Hard v0.1.  The bars show that Pre-DPO consistently outperforms both DPO and SimPO across all models and scales, demonstrating that leveraging a guiding reference model significantly improves performance in preference optimization.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.15843/x2.png", "caption": "Figure 2: An overview of Pre-DPO. DPO constrains training using the initial policy model as the reference, and SimPO is reference-free. Pre-DPO first obtains an optimized policy model via DPO or SimPO, resets it as a guiding reference model, and then re-optimizes the initial policy using DPO. This process improves data utilization and leads to a better optimized policy model.", "description": "Pre-DPO enhances existing preference optimization methods by using a guiding reference model.  Standard DPO uses the initial policy model as a constraining reference model during training, while SimPO is reference-free.  Pre-DPO improves on these by first optimizing the initial policy using either DPO or SimPO.  The resulting optimized model then serves as a *guiding* reference model. This guiding model informs a second round of DPO optimization on the initial policy model, leading to improved data utilization and a better final policy model. The figure visually illustrates this process, comparing the workflows of standard DPO, SimPO, and the proposed Pre-DPO method.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2504.15843/x3.png", "caption": "(a) Llama3.2-3B-Base", "description": "This violin plot shows the distribution of the reweighting factor \u03bb for Llama3.2-3B-Base model.  The x-axis represents the different methods (DPO and Pre-DPO with DPO as reference). The y-axis represents the value of \u03bb. Each violin plot displays the distribution of \u03bb values, with the width representing data density and the thick lines showing the mean and median.  The figure illustrates the differences in \u03bb distributions between Pre-DPO and standard DPO, highlighting how Pre-DPO results in higher \u03bb values.", "section": "5.3 Ablations"}, {"figure_path": "https://arxiv.org/html/2504.15843/x4.png", "caption": "(b) Llama3.2-3B-Instruct", "description": "This violin plot displays the distribution of the weighting factor '\u03bb' in the Llama3.2-3B-Instruct model during training.  The '\u03bb' values represent how much weight the model assigns to each training example, reflecting the model's focus. The plot compares the distribution of '\u03bb' obtained by the standard Direct Preference Optimization (DPO) method and the proposed Pre-DPO method.  A higher '\u03bb' suggests that the data sample is considered more valuable by the model. The difference in the distributions between DPO and Pre-DPO illustrates how Pre-DPO effectively leverages a guiding reference model to modify the data weighting scheme and improve training efficiency.", "section": "5.3 Ablations"}, {"figure_path": "https://arxiv.org/html/2504.15843/x5.png", "caption": "Figure 3: \nComparative differences in \u03bb\ud835\udf06\\lambdaitalic_\u03bb (when \u03b2\ud835\udefd\\betaitalic_\u03b2 is set to 0.01) distributions between Pre-DPO and DPO across Llama3.2-3B settings.\nEach subplot presents a violin plot illustrating the distribution of \u03bb\ud835\udf06\\lambdaitalic_\u03bb values on the training preference dataset.\nThe black horizontal line indicates the mean, while the dark gray line represents the median.\nThe width of the violin at each point represents the data density, where thicker sections correspond to regions with higher data concentration.", "description": "Figure 3 displays violin plots illustrating the distributions of the weighting factor lambda (\u03bb) for both Pre-DPO and standard DPO methods.  The data used comes from the training preference dataset, with beta (\u03b2) fixed at 0.01.  Separate plots are shown for the Llama3.2-3B Base and Llama3.2-3B Instruct model settings. The plots show the distributions' means and medians, with violin widths reflecting data density, allowing for a visual comparison of the weighting factor's distribution between the two methods across different model configurations.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.15843/x6.png", "caption": "(a) Qwen2.5-7B-Base", "description": "Figure 4(a) displays the distribution of the 'A' values (a weighting factor in the Pre-DPO algorithm) on the training preference dataset for both the standard DPO method and the proposed Pre-DPO method.  The data is visualized as violin plots, showing the distribution's density and central tendency (mean and median).  This specific subplot shows the results for the Qwen2.5-7B-Base language model. The figure demonstrates how Pre-DPO adjusts the weights more effectively to improve learning, as shown by the difference in the distributions of 'A' values between the two methods.", "section": "5.4 The A distribution after training of Pre-DPO and vanilla DPO"}, {"figure_path": "https://arxiv.org/html/2504.15843/x7.png", "caption": "(b) Qwen2.5-7B-Instruct", "description": "This violin plot displays the distribution of the weighting factor '\u03bb' for both the standard DPO method and the proposed Pre-DPO method, specifically for the Qwen2.5-7B-Instruct model. The '\u03bb' values, calculated with \u03b2 = 0.01, represent the weights assigned to each training example during preference optimization. A higher '\u03bb' indicates a greater influence of the example on the model's training.  The plot visually compares the distribution of '\u03bb' between the two methods, highlighting the impact of Pre-DPO on data weighting and consequently, model training.", "section": "5.3 Ablations"}, {"figure_path": "https://arxiv.org/html/2504.15843/x8.png", "caption": "(c) Llama3.2-3B-Base", "description": "This violin plot displays the distribution of the values of lambda (\u03bb) computed on the Llama3.2-3B-Base model's training preference dataset after training with both the original DPO and the Pre-DPO methods.  The plot shows the mean and median values of \u03bb for both methods, highlighting the differences in the distribution of \u03bb resulting from the Pre-DPO's adaptive data reweighting mechanism.", "section": "5.4 The \u03bb distribution after training of Pre-DPO and vanilla DPO"}, {"figure_path": "https://arxiv.org/html/2504.15843/x9.png", "caption": "(d) Llama3.2-3B-Instruct", "description": "This violin plot displays the distribution of the weighting factor '\u03bb' for both the standard DPO method and the proposed Pre-DPO method.  The data comes from training the Llama3.2-3B-Instruct model, with \u03b2 set to 0.01.  The x-axis shows the methods (DPO and Pre-DPO), and the y-axis represents the \u03bb values. The violin plot's shape shows the data density at various \u03bb values, indicating the frequency of different weights assigned to the training data samples.  The black line denotes the mean, and the gray line represents the median \u03bb value for each method.  The wider parts of the violin shape indicate higher density, while narrower parts mean fewer data points have those values of \u03bb.", "section": "5.3 Ablations"}, {"figure_path": "https://arxiv.org/html/2504.15843/x10.png", "caption": "(e) Qwen2.5-7B-Base", "description": "This violin plot displays the distribution of the weight parameter '\u03bb' for the Qwen2.5-7B-Base model.  The distribution shows the range of weights assigned to different data samples during training using both the standard DPO and the proposed Pre-DPO methods.  The plot helps to visualize how Pre-DPO alters the weighting of training data compared to the standard approach, offering insights into the effectiveness of Pre-DPO's data reweighting strategy.", "section": "5.4 The \u03bb distribution after training of Pre-DPO and vanilla DPO"}]