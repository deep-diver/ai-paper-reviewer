{"importance": "This paper introduces Pre-DPO, enhancing data utilization in preference optimization for LLMs without extra data or models. It improves existing methods and offers a flexible, deployable solution.", "summary": "Pre-DPO improves data use in Direct Preference Optimization for Large Language Models.", "takeaways": ["Pre-DPO improves the performance of both DPO and SimPO in LLMs.", "A guiding reference model enhances data utilization by adaptively weighting samples.", "Pre-DPO does not rely on external models or additional data, increasing its flexibility and ease of deployment."], "tldr": "**Direct Preference Optimization (DPO)** streamlines reinforcement learning from human feedback for LLMs by optimizing human preferences directly, without needing an explicit reward model. However, initializing policy and reference models identically in DPO leads to inefficient data use and a performance ceiling. Also, Simple Preference Optimization (SimPO) lacks a reference model which reduces robustness and needs stricter conditions to avoid catastrophic forgetting. All of these issues affect the performances of LLMs. \n\nIn response to the challenges, this paper proposes **Pre-DPO**. It enhances preference optimization by using a guiding reference model. This model provides foresight into optimal policy states achievable with preference data, adaptively weighting samples. Experiments on AlpacaEval 2.0 and Arena-Hard v0.1 show Pre-DPO consistently improves DPO and SimPO performance, without external models or extra data. The experiments shows that Pre-DPO is more effective.", "affiliation": "Zhejiang University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.15843/podcast.wav"}