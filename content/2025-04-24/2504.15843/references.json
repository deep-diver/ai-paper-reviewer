{"references": [{"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper introduces Direct Preference Optimization (DPO), a key method for directly optimizing language models based on human preferences without an explicit reward model."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper showcases the importance of Reinforcement Learning from Human Feedback (RLHF) in aligning large language models with human values and preferences."}, {"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-01-01", "reason": "This paper provides a technical overview of GPT-4, a significant advancement in large language models and a key benchmark for alignment techniques."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "This paper presents Proximal Policy Optimization (PPO), a widely used reinforcement learning algorithm often employed in RLHF pipelines for policy optimization."}, {"fullname_first_author": "Yu Meng", "paper_title": "SimPO: Simple preference optimization with a reference-free reward", "publication_date": "2024-01-01", "reason": "This paper presents Simple Preference Optimization (SimPO), a reference-free method for preference optimization, offering an alternative to DPO by eliminating the reference model."}]}