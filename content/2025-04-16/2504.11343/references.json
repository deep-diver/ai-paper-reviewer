{"references": [{"fullname_first_author": "Schulman", "paper_title": "Proximal policy optimization algorithms.", "publication_date": "2017-07-06", "reason": "This paper introduces Proximal Policy Optimization (PPO), a widely used reinforcement learning algorithm, making it a foundational reference for LLM post-training."}, {"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback.", "publication_date": "2022-01-01", "reason": "This paper details training language models to follow instructions using human feedback, a key aspect of reinforcement learning for LLMs."}, {"fullname_first_author": "Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model.", "publication_date": "2023-05-18", "reason": "This paper introduces Direct Preference Optimization (DPO), an alternative approach to reinforcement learning, which has become an important method in the field."}, {"fullname_first_author": "Dong", "paper_title": "RAFT: Reward ranked finetuning for generative foundation model alignment.", "publication_date": "2023-01-01", "reason": "This paper introduces Reward Ranked Finetuning (RAFT), a simple and effective method for aligning generative foundation models using reward ranking."}, {"fullname_first_author": "Williams", "paper_title": "Function optimization using connectionist reinforcement learning algorithms.", "publication_date": "1991-01-01", "reason": "This paper presents a foundational method using reinforcement learning algorithms, namely Reinforce, making it an important reference for LLM post-training."}]}