{"importance": "This paper introduces ERW to improve diffusion model training by using pre-trained representations. The new method accelerates convergence, enhances fidelity, and opens avenues for exploration of representation learning in generative models, which is significant for future research.", "summary": "ERW: Boost generative model training by injecting pre-trained representations early for faster convergence and higher quality results.", "takeaways": ["Embedded Representation Warmup (ERW) accelerates diffusion model training by leveraging pre-trained representations.", "ERW enhances both training efficiency and the quality of generated data.", "The method identifies and utilizes a critical representation processing region in early layers of the diffusion model."], "tldr": "Diffusion models are powerful for generating high-dimensional data but suffer from training inefficiency and limited representation quality. This paper identifies that the **underutilization of high-quality representations slows convergence**. Systematic analysis reveals a critical representation processing region where semantic and structural learning occurs before generation. To solve these issues, the paper proposes Embedded Representation Warmup (ERW).\n\nERW is a framework that **initializes the diffusion model's early layers** with pre-trained representations, minimizing the need to learn from scratch. This method enhances efficiency and effectiveness, leading to faster convergence and superior performance. Empirical results show a 40x acceleration in training speed compared to state-of-the-art methods. ERW achieves a new state-of-the-art.", "affiliation": "Westlake University", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2504.10188/podcast.wav"}