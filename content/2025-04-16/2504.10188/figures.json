[{"figure_path": "https://arxiv.org/html/2504.10188/x4.png", "caption": "Figure 1: \nEmbedded Representation Warmup (ERW).\nThroughout the training process, we demonstrate that incorporating representations at the early stages is highly beneficial.\nTo this end, we propose a representation warmup stage that employs a representation alignment loss to integrate representations from models such as Dinov2 \u00a0[38] into the ERW.\nThis initialized representation region is subsequently embedded into the diffusion model pipeline, providing a strong starting point for training.\nOur ERW\u00a0method thus enhances both efficiency and effectiveness, leading to faster convergence and superior performance compared to the REPA\u00a0[54] method, thereby establishing a new state-of-the-art.", "description": "The figure illustrates the Embedded Representation Warmup (ERW) method.  ERW consists of two stages: a representation warmup stage and a full training stage.  In the warmup stage, pretrained representations from a model like Dinov2 are integrated into the early layers of a diffusion model using a representation alignment loss. This provides a strong initialization for the model. The subsequent full training stage then fine-tunes the entire model.  The results show that ERW leads to significantly faster convergence and improved performance compared to the REPA method, achieving a new state-of-the-art.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.10188/extracted/6360108/resources/figures/merged_image_1.png", "caption": "Figure 2: Illustration of the Circuits in Hypothesis.\nFrom left to right, we first apply a VAE encoder (the Pixel-to-Latent or (P2L) stage) to map high-dimensional inputs to a compressed latent space. We then perform latent diffusion on these codes, dividing the backbone into two subregions: the Latent-to-Representation (L2R) region that capture and refine semantic features, and the Representation-to-Generation (R2G) region that decode the learned representation into final outputs.", "description": "This figure illustrates the three-stage diffusion circuit proposed in the paper: Pixel-to-Latent (P2L), Latent-to-Representation (L2R), and Representation-to-Generation (R2G).  The P2L stage uses a Variational Autoencoder (VAE) to compress high-dimensional image inputs into a lower-dimensional latent space.  The latent diffusion process is then divided into two sub-regions: L2R, which extracts and refines semantic features from the latent codes; and R2G, which decodes the learned representations into the final generated outputs. This decomposition helps to clarify the distinct roles of different network regions in the process of generating images from noisy inputs.", "section": "3. Hypothesis of Latent Diffusion Models"}, {"figure_path": "https://arxiv.org/html/2504.10188/x5.png", "caption": "Figure 3: Selected Samples on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256. Images generated by the SiT-XL/2 + REPA + ERW model using classifier-free guidance (CFG) with a scale of w=2.2\ud835\udc642.2w=2.2italic_w = 2.2 under 40 epochs.", "description": "This figure showcases sample images generated using the SiT-XL/2 model, enhanced with REPA and ERW techniques.  The images are 256x256 pixels and were produced using classifier-free guidance (CFG) with a scale of w=2.2, after training for 40 epochs.  The figure visually demonstrates the quality of images generated by the model after incorporating the ERW method, which aims to improve efficiency and representation quality during training. The samples are from the ImageNet dataset.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.10188/x6.png", "caption": "Figure 4: \nComparison of Training Efficiency and Cost Analysis with Warmup and Full Training Stages.\nLeft: Scatter plot depicting the relationship between the total training cost (in TFLOPs) and the FID score for various training strategies.\nEach point is annotated with a simplified label (e.g., \u201c10K+90K\u201d) representing the warmup and full training split, and the marker sizes are scaled based on a combination of the FID and FLOPs values to highlight the relative differences.\nRight: Bar chart comparing the computational costs of the warmup and full training stages for different strategies (all evaluated over 100K iterations).\nThe chart shows the warmup cost, full training cost, and their corresponding total cost.", "description": "This figure compares different training strategies for diffusion models, focusing on training efficiency and computational cost.  The left panel shows a scatter plot illustrating the relationship between total training cost (in TeraFLOPS) and FID (Fr\u00e9chet Inception Distance) score. Each point represents a different training strategy, with the label indicating the split between warmup and full training phases (e.g., \"10K+90K\" means 10,000 iterations of warmup and 90,000 iterations of full training). Point size reflects both FID and FLOPS values to highlight relative differences in performance and cost. The right panel provides a bar chart comparing the computational costs (in TFLOPS) of the warmup and full training phases for each strategy, all evaluated over 100,000 iterations.  This allows for a direct comparison of the computational investment in each phase of training. ", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.10188/x7.png", "caption": "(a) Alignment with ERW", "description": "This figure (a) shows the alignment between the model's internal representation (RoL2R(Ho(x))) and the target semantic features (f_rep(x)) across different layers of the network.  It demonstrates how well the representation learned by the model aligns with high-quality pretrained representations, which is crucial for the efficiency of ERW. The plot shows how this alignment changes over the course of training, initially declining as the model adjusts to the diffusion training objective, then recovers and improves, showing the effectiveness of the warmup phase.", "section": "5.4. Ablation Studies"}]