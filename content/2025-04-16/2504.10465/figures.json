[{"figure_path": "https://arxiv.org/html/2504.10465/x2.png", "caption": "Figure 1: Comparison of current MLLMs for pixel-wise understanding with our method. (a) and (b). Current MLLMs for pixel-wise understanding feature highly complex system architectures, including an LLM, a CLIP-like vision backbone, an object token extraction model, a segmentation vision backbone, and a SAM-like decoder. (c). Our method employs only a single transformer.", "description": "Figure 1 illustrates three different approaches to pixel-wise understanding using multimodal large language models (MLLMs).  (a) and (b) depict existing MLLM architectures that are complex, involving multiple components: a large language model (LLM), a CLIP-like visual encoder, an object token extractor, a segmentation backbone, and a SAM-like decoder.  In contrast, (c) shows the Pixel-SAIL method, which uses a single transformer to perform pixel-wise understanding, offering a more simplified and efficient architecture.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.10465/x16.png", "caption": "Figure 2: The architecture of our proposed plain baseline and Pixel-SAIL. Pixel-SAIL is as simple and elegant as the plain baseline but demonstrates significantly improved performance. The examples on the right demonstrate that Pixel-SAIL possesses the capability for general conversation and comprehensive pixel-grounded understanding.", "description": "Figure 2 illustrates the architectures of a plain baseline model and the proposed Pixel-SAIL model.  Both models utilize a single transformer, highlighting Pixel-SAIL's simplicity.  However, Pixel-SAIL incorporates several key improvements (learnable upsampling, visual prompt injection, and dense feature distillation) leading to superior performance.  The examples on the right showcase Pixel-SAIL's ability to engage in general conversation and perform detailed pixel-grounded understanding tasks, demonstrating its enhanced capabilities compared to the plain baseline.", "section": "3. Method"}]