[{"figure_path": "https://arxiv.org/html/2504.08672/x1.png", "caption": "Figure 1: Typical reinforce-like approaches to boost LLM reasoning. (a) and (b) abstract two types of reinforce-like methods, which require final answer and verification respectively. (c) depicts the ultimate goal of our research. x\ud835\udc65xitalic_x denotes the input query, a\ud835\udc4eaitalic_a denotes the LLM-generated responses that contain multiple steps, and y\ud835\udc66yitalic_y is the final answer (if exists).", "description": "Figure 1 illustrates different approaches to enhance Large Language Model (LLM) reasoning abilities.  (a) and (b) represent supervised reinforcement learning methods. (a) uses the final answer as a supervisory signal, while (b) employs a verification process.  Both require either ground truth data or an external reward model. (c) showcases the unsupervised self-training method proposed in the paper. It only uses input queries and LLM-generated multi-step responses to optimize the LLM without any external supervision or labeled data.  In all three cases, 'x' represents the input query, 'a' represents the multi-step response generated by the LLM, and 'y' (where applicable) represents the correct final answer.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.08672/x2.png", "caption": "Figure 2: The overall framework of Genius. It only receives the unsupervised NL queries as inputs. To complete goal of self-improving, the policy LLM goes through K\ud835\udc3eKitalic_K steps of sampling and rewarding for each query (Step 1-4), collects the high-quality response sequence as the training data (Step 5), and trains itself with the advantage calibrated optimization loss (Step 6).", "description": "The Genius framework is a purely unsupervised self-training method for enhancing Large Language Model (LLM) reasoning abilities.  It begins with unsupervised natural language (NL) queries as input. The LLM generates responses in a step-wise manner, where each step's quality is assessed using a 'foresight' mechanism that simulates future steps to estimate its value. Steps are sampled and re-sampled using a strategy balancing exploration and exploitation. High-quality response sequences are selected for training.  A novel Advantage-Calibrated Optimization (ACO) loss function is used to train the LLM, improving robustness by mitigating the noise inherent in unsupervised learning. This iterative process of sampling, rewarding, and training allows the LLM to continuously improve its reasoning capabilities without relying on external supervision.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2504.08672/x3.png", "caption": "Figure 3: Visualization of the calibration function. The x-axis denotes the the differences between Alsubscript\ud835\udc34\ud835\udc59A_{l}italic_A start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and Awsubscript\ud835\udc34\ud835\udc64A_{w}italic_A start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT, while the y-axis is the value of the calibration term. By adjusting \u03b1\ud835\udefc\\alphaitalic_\u03b1, we can control the decay rate of the curve.", "description": "This figure visualizes the advantage-calibrated function (ACO) used in the Genius framework.  The x-axis represents the difference in advantage values between a negative response (A<sub>l</sub>) and a positive response (A<sub>w</sub>).  The y-axis shows the value of the calibration term \u03c9(x, A) from Equation (11) in the paper, which adjusts the reward assigned to negative responses based on the difference in advantage. The parameter \u03b1 controls the curve's steepness; larger \u03b1 values lead to sharper transitions between regions where negative responses are given different weights.  This function ensures robustness in the self-training process by mitigating noisy estimations from the unsupervised setting.  The curve is divided into two regions: the normal region, where the difference in advantage is not substantial, and the calibration region where negative samples receive an adjusted weight.", "section": "2.3 Advantage-Calibrated Optimization"}, {"figure_path": "https://arxiv.org/html/2504.08672/x4.png", "caption": "(a) Base LLM: Qwen2.5-3B-Instruct", "description": "The bar chart visualizes the average performance gains across seven benchmark tasks achieved by various LLMs, including Genius, when using Qwen2.5-3B-Instruct as the base LLM.  The comparison highlights Genius's superior performance improvement compared to other methods such as SPIN, CoH, Self-Rewarding, and SCPO.", "section": "3.3 Generalization and Adaptation"}, {"figure_path": "https://arxiv.org/html/2504.08672/x5.png", "caption": "(b) Base LLM: Qwen2.5-7B-Instruct", "description": "This figure shows the average performance gains across seven reasoning benchmarks for the Qwen2.5-7B-Instruct large language model after applying different post-training methods, including Genius, Self-Rewarding, ScPO, CoH, SPIN, and SFT.  It showcases how Genius significantly outperforms other methods, demonstrating its effectiveness in enhancing reasoning capabilities via unsupervised self-training.", "section": "3.3 Generalization and Adaptation"}, {"figure_path": "https://arxiv.org/html/2504.08672/x6.png", "caption": "Figure 4: Generalization to Qwen2.5 series models. All methods are trained on the OpenHermes2.5 split. The numbers above the bars represent the average performance gain relative to the base model.", "description": "This figure demonstrates the generalization capabilities of the Genius framework when applied to different Large Language Models (LLMs).  Specifically, it shows the performance improvement achieved by Genius when fine-tuned on the OpenHermes2.5 dataset and then evaluated on various reasoning tasks using the Qwen-2.5-3B and Qwen-2.5-7B models. The bar graphs compare the average performance gains (relative to the baseline models) achieved by Genius and several other post-training methods. This illustrates Genius's ability to enhance various LLMs consistently.", "section": "3.3 Generalization and Adaptation"}, {"figure_path": "https://arxiv.org/html/2504.08672/x7.png", "caption": "Figure 5: Results on AIME 2024.", "description": "Figure 5 presents the results of the AIME 2024 competition-level benchmark.  It compares the performance of the base LLaMA3.1-8B-Instruct model with the Genius model (trained using the OpenHermes2.5 dataset).  The bar chart visually displays the Pass@1 score achieved by each model on the AIME 2024 dataset. This demonstrates Genius's ability to improve upon the base LLM's performance even in challenging, complex reasoning scenarios.", "section": "3.3 Generalization and Adaptation"}, {"figure_path": "https://arxiv.org/html/2504.08672/x8.png", "caption": "Figure 6: Post-training scaling law with LLaMA3.1-8B-Instruct as the base LLM.", "description": "This figure displays the results of the post-training scaling law experiment using the LLaMA3.1-8B-Instruct model as a base.  It shows how the average performance across multiple reasoning benchmarks improves as the number of training steps increases.  The graph illustrates the effect of increasing training data on model performance, highlighting the potential for continued improvement with more training.", "section": "3.3 Generalization and Adaptation"}]