[{"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T1.7.7\">\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T1.7.7.8.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.8.1.1\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.7.7.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.8.2.1\">GSM8K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.7.7.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.8.3.1\">MATH</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.7.7.8.4\">\n<span class=\"ltx_text\" id=\"S5.T1.7.7.8.4.1\"></span><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.8.4.2\"> <span class=\"ltx_text\" id=\"S5.T1.7.7.8.4.2.1\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T1.7.7.8.4.2.1.1\">\n<span class=\"ltx_tr\" id=\"S5.T1.7.7.8.4.2.1.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T1.7.7.8.4.2.1.1.1.1\">Olympiad</span></span>\n<span class=\"ltx_tr\" id=\"S5.T1.7.7.8.4.2.1.1.2\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S5.T1.7.7.8.4.2.1.1.2.1\">Bench</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T1.7.7.8.4.2.2\"></span></span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.7.7.8.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.8.5.1\">OmniMath</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.7.7.8.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.8.6.1\">Average F1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.9\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" id=\"S5.T1.7.7.9.1\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S5.T1.7.7.9.1.1\">LLM-as-judge</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.1.1.1\">o1-Mini<sup class=\"ltx_sup\" id=\"S5.T1.1.1.1.1.1\">\u22c4</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.1.2\">0.932</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.1.3\">0.889</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.1.4\">0.872</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.1.5\">0.824</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.1.1.6\">0.879</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.7.7.10.1\">\n<span class=\"ltx_text\" id=\"S5.T1.7.7.10.1.1\"></span><span class=\"ltx_text\" id=\"S5.T1.7.7.10.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T1.7.7.10.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T1.7.7.10.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S5.T1.7.7.10.1.2.1.1.1\">Deepseek-R1-Distill-32B</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T1.7.7.10.1.3\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.10.2\">0.817</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.10.3\">0.739</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.10.4\">0.659</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.10.5\">0.585</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.10.6\">0.700</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.7.7.11.1\">QwQ-32B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.11.2\">0.871</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.11.3\">0.834</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.11.4\">0.787</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.11.5\">0.771</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.11.6\">0.816</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.12\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" id=\"S5.T1.7.7.12.1\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S5.T1.7.7.12.1.1\">Process Reward Models (72B)</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.2.2.1\">Qwen2.5-Math-PRM-72B<sup class=\"ltx_sup\" id=\"S5.T1.2.2.2.1.1\">\u22c4</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.2.2.2\">0.873</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.2.2.3\">0.806</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.2.2.4\">0.743</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.2.2.5\">0.711</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.2.2.6\">0.783</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.13\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"6\" id=\"S5.T1.7.7.13.1\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S5.T1.7.7.13.1.1\">Process Reward Models (7B+)</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.3.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.3.3.3.1\">Math-Shepherd-PRM-7B<sup class=\"ltx_sup\" id=\"S5.T1.3.3.3.1.1\">\u22c4</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.2\">0.479</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.3\">0.295</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.4\">0.248</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.5\">0.238</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.3.3.3.6\">0.315</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.4.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.4.4.4.1\">Qwen2.5-Math-7B-Math-Shepherd<sup class=\"ltx_sup\" id=\"S5.T1.4.4.4.1.1\">\u22c4</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.4.4.4.2\">0.625</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.4.4.4.3\">0.316</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.4.4.4.4\">0.137</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.4.4.4.5\">0.077</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.4.4.4.6\">0.289</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.5.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.5.5.5.1\">EurusPRM-Stage2<sup class=\"ltx_sup\" id=\"S5.T1.5.5.5.1.1\">\u22c4</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.5.5.2\">0.473</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.5.5.3\">0.357</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.5.5.4\">0.212</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.5.5.5\">0.209</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.5.5.5.6\">0.313</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.6.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.6.6.6.1\">Qwen2.5-Math-7B-PRM800K<sup class=\"ltx_sup\" id=\"S5.T1.6.6.6.1.1\">\u22c4</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.6.6.2\">0.683</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.6.6.3\">0.626</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.6.6.4\">0.507</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.6.6.5\">0.443</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.6.6.6.6\">0.565</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.14\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.7.7.14.1\">\n<span class=\"ltx_text\" id=\"S5.T1.7.7.14.1.1\"></span><span class=\"ltx_text\" id=\"S5.T1.7.7.14.1.2\">\n<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T1.7.7.14.1.2.1\">\n<span class=\"ltx_tr\" id=\"S5.T1.7.7.14.1.2.1.1\">\n<span class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S5.T1.7.7.14.1.2.1.1.1\">Ensemble-PRM-PRM800K (ours)</span></span>\n</span></span><span class=\"ltx_text\" id=\"S5.T1.7.7.14.1.3\"></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.14.2\">0.705</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.14.3\">0.630</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.14.4\">0.472</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.14.5\">0.433</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.14.6\">0.560</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.15\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.7.7.15.1\">PURE-PRM-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.15.2\">0.690</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.15.3\">0.665</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.15.4\">0.484</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.15.5\">0.459</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.15.6\">0.575</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.7.7.7.1\">Qwen2.5-Math-PRM-7B<sup class=\"ltx_sup\" id=\"S5.T1.7.7.7.1.1\">\u22c4</sup>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.7.2\">0.824</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.7.3\">0.776</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.7.4\">0.675</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.7.5\">0.663</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.7.6\">0.735</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.16\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.7.7.16.1\">Universal-PRM</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.16.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.16.2.1\">0.858</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.16.3\">0.777</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.16.4\">0.676</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.16.5\">0.664</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.16.6\">0.743</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.17\" style=\"background-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.7.7.17.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T1.7.7.17.1.1\">\\hdashline</span>\n<span class=\"ltx_text\" id=\"S5.T1.7.7.17.1.2\" style=\"background-color:#E6E6E6;\">\n<span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T1.7.7.17.1.2.1\">ActPRM</span> (ours)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.17.2\"><span class=\"ltx_text\" id=\"S5.T1.7.7.17.2.1\" style=\"background-color:#E6E6E6;\">0.816</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.17.3\"><span class=\"ltx_text\" id=\"S5.T1.7.7.17.3.1\" style=\"background-color:#E6E6E6;\">0.798</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.17.4\"><span class=\"ltx_text\" id=\"S5.T1.7.7.17.4.1\" style=\"background-color:#E6E6E6;\">0.714</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.17.5\"><span class=\"ltx_text\" id=\"S5.T1.7.7.17.5.1\" style=\"background-color:#E6E6E6;\">0.670</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.7.7.17.6\"><span class=\"ltx_text\" id=\"S5.T1.7.7.17.6.1\" style=\"background-color:#E6E6E6;\">0.750</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.7.7.18\" style=\"background-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T1.7.7.18.1\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T1.7.7.18.1.1\" style=\"background-color:#E6E6E6;\">ActPRM-X<span class=\"ltx_text ltx_font_upright\" id=\"S5.T1.7.7.18.1.1.1\"> (ours)</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.7.7.18.2\"><span class=\"ltx_text\" id=\"S5.T1.7.7.18.2.1\" style=\"background-color:#E6E6E6;\">0.827</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.7.7.18.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.18.3.1\" style=\"background-color:#E6E6E6;\">0.820</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.7.7.18.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.18.4.1\" style=\"background-color:#E6E6E6;\">0.720</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.7.7.18.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.18.5.1\" style=\"background-color:#E6E6E6;\">0.673</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.7.7.18.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.7.7.18.6.1\" style=\"background-color:#E6E6E6;\">0.760</span></td>\n</tr>\n</table>", "caption": "Table 1: Performance comparison on ProcessBench.\nWe report the results in the same calculation method with ProcessBench. \u22c4 denotes the results are from Qwen PRM\u2019s report\u00a0(Zhang et\u00a0al., 2025).", "description": "This table presents a performance comparison of various models on the ProcessBench benchmark.  The models are categorized into LLMs (Large Language Models) and Process Reward Models (PRMs), with further subcategories indicating model size or specific training techniques.  Performance is measured using the F1 score, calculated using the same method as in the ProcessBench evaluation. A symbol (\u22c4) is used to mark results that were taken from a separate publication by Zhang et al. (2025), providing context for data not directly obtained by the authors of this current paper.", "section": "5.2 Experimental Results"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T2.1.1\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.1.1\">#</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.2.1\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.3.1\">Simlicity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.4.1\">Soundness</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.5.1\">Sensitivity</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.1.1.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.1.6.1\">Average</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.2\">\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T2.1.1.2.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"5\" id=\"S5.T2.1.1.2.2\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S5.T2.1.1.2.2.1\">LLM-as-judge</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.3.1\">1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.3.2\">Gemini-2.0-thinking-exp-1219</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.3\">0.662</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.4\">0.718</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.5\">0.753</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.3.6\">0.688</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.4.1\">1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.4.2\">o1-mini</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.3\">0.646</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.4\">0.721</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.5\">0.755</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.4.6\">0.688</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.5.1\">4</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.5.2\">GPT-4o</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.3\">0.597</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.4\">0.709</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.5\">0.758</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.5.6\">0.668</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.6.1\">6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.6.2\">Gemini-2.0-flash-exp</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.3\">0.627</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.4\">0.673</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.5\">0.754</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.6.6\">0.660</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.7\">\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T2.1.1.7.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"5\" id=\"S5.T2.1.1.7.2\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S5.T2.1.1.7.2.1\">Process Reward Models (72B)</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.8\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.8.1\">3</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.8.2\">Qwen-2.5-Math-PRM-72B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.8.3\">0.546</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.8.4\">0.739</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.8.5\">0.770</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.8.6\">0.682</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.9\">\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T2.1.1.9.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" colspan=\"5\" id=\"S5.T2.1.1.9.2\"><em class=\"ltx_emph ltx_font_bold ltx_font_italic\" id=\"S5.T2.1.1.9.2.1\">Process Reward Models (7B+)</em></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.10\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.10.1\">7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.10.2\">Qwen2.5-Math-PRM-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.10.3\">0.521</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.10.4\">0.710</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.10.5\">0.755</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.10.6\">0.655</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.11\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.11.1\">9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.11.2\">Pure-PRM-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.11.3\">0.522</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.11.4\">0.702</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.11.5.1\">0.758</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.11.6\">0.653</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.12\" style=\"background-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.12.1\">\n<span class=\"ltx_ERROR undefined\" id=\"S5.T2.1.1.12.1.1\">\\hdashline</span>\n<span class=\"ltx_text\" id=\"S5.T2.1.1.12.1.2\" style=\"background-color:#E6E6E6;\">\n7</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.1.12.2\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T2.1.1.12.2.1\" style=\"background-color:#E6E6E6;\">ActPRM<span class=\"ltx_text ltx_font_upright\" id=\"S5.T2.1.1.12.2.1.1\"> (ours)</span></span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.12.3\"><span class=\"ltx_text\" id=\"S5.T2.1.1.12.3.1\" style=\"background-color:#E6E6E6;\">0.536</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.12.4\"><span class=\"ltx_text\" id=\"S5.T2.1.1.12.4.1\" style=\"background-color:#E6E6E6;\">0.713</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.12.5\"><span class=\"ltx_text\" id=\"S5.T2.1.1.12.5.1\" style=\"background-color:#E6E6E6;\">0.752</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.1.12.6\"><span class=\"ltx_text\" id=\"S5.T2.1.1.12.6.1\" style=\"background-color:#E6E6E6;\">0.655</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.13\" style=\"background-color:#E6E6E6;\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.1.13.1\"><span class=\"ltx_text\" id=\"S5.T2.1.1.13.1.1\" style=\"background-color:#E6E6E6;\">5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.1.13.2\"><span class=\"ltx_text ltx_font_smallcaps\" id=\"S5.T2.1.1.13.2.1\" style=\"background-color:#E6E6E6;\">ActPRM-X<span class=\"ltx_text ltx_font_upright\" id=\"S5.T2.1.1.13.2.1.1\"> (ours)</span></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.1.13.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.13.3.1\" style=\"background-color:#E6E6E6;\">0.545</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.1.13.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.13.4.1\" style=\"background-color:#E6E6E6;\">0.727</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.1.13.5\"><span class=\"ltx_text\" id=\"S5.T2.1.1.13.5.1\" style=\"background-color:#E6E6E6;\">0.756</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.1.13.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.1.13.6.1\" style=\"background-color:#E6E6E6;\">0.667</span></td>\n</tr>\n</table>", "caption": "Table 2: Performance comparison on PRMBench.\nAll results of the other models are from the official leaderboard. # denotes the ranking.", "description": "This table presents a performance comparison of different Process Reward Models (PRMs) on the PRMBench benchmark.  It shows the average F1 score, simplicity, soundness, and sensitivity of each model.  The models include various LLM-based approaches and process reward models, highlighting the performance differences on this specific benchmark.  The ranking (#) indicates the relative performance compared to other models on the leaderboard.", "section": "5.2 Experimental Results"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A2.T3.1.1\">\n<tr class=\"ltx_tr\" id=\"A2.T3.1.1.1\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A2.T3.1.1.1.1\"></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T3.1.1.1.2.1\"># Problem set</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T3.1.1.1.3\"># <span class=\"ltx_text ltx_font_bold\" id=\"A2.T3.1.1.1.3.1\">CoT Trajectories</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A2.T3.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T3.1.1.1.4.1\">ProcessBench F1 score</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T3.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T3.1.1.2.1.1\">PRM800K</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.1.2.2\">7,500</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.1.2.3\">460,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T3.1.1.2.4\">0.575</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T3.1.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T3.1.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T3.1.1.3.1.1\">NuminaMath (Random Selected)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T3.1.1.3.2\">100,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T3.1.1.3.3\">100,000</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T3.1.1.3.4\">0.673</td>\n</tr>\n</table>", "caption": "Table 3: Comparison between PRM800K and 100K data collected from NuminaMath labeled by Qwen-QwQ.", "description": "This table compares the performance of Process Reward Models (PRMs) trained on two different datasets: PRM800K, a widely used dataset with 800K step-level labels, and a newly collected dataset of 100K samples from the NuminaMath dataset, labeled using the Qwen-QwQ model.  The comparison focuses on the ProcessBench F1 score, demonstrating the impact of dataset size and labeling method on the model's performance.", "section": "5.1 Pool-Based Active Learning"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A3.T4.3.3\">\n<tr class=\"ltx_tr\" id=\"A3.T4.3.3.4\">\n<td class=\"ltx_td ltx_border_tt\" id=\"A3.T4.3.3.4.1\"></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A3.T4.3.3.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T4.3.3.4.2.1\">Value</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"A3.T4.3.3.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T4.3.3.4.3.1\">Source</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T4.1.1.1.1.1\"># Reasoning Steps (<math alttext=\"S\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.1.1.1.1.1.m1.1\"><semantics id=\"A3.T4.1.1.1.1.1.m1.1a\"><mi id=\"A3.T4.1.1.1.1.1.m1.1.1\" xref=\"A3.T4.1.1.1.1.1.m1.1.1.cmml\">S</mi><annotation-xml encoding=\"MathML-Content\" id=\"A3.T4.1.1.1.1.1.m1.1b\"><ci id=\"A3.T4.1.1.1.1.1.m1.1.1.cmml\" xref=\"A3.T4.1.1.1.1.1.m1.1.1\">\ud835\udc46</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T4.1.1.1.1.1.m1.1c\">S</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.T4.1.1.1.1.1.m1.1d\">italic_S</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T4.1.1.1.2\">8.845</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A3.T4.1.1.1.3\">Qwen Models\u2019 rollouts</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.2.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T4.2.2.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T4.2.2.2.1.1\"># Tokens per Rollout (<math alttext=\"R\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.2.2.2.1.1.m1.1\"><semantics id=\"A3.T4.2.2.2.1.1.m1.1a\"><mi id=\"A3.T4.2.2.2.1.1.m1.1.1\" xref=\"A3.T4.2.2.2.1.1.m1.1.1.cmml\">R</mi><annotation-xml encoding=\"MathML-Content\" id=\"A3.T4.2.2.2.1.1.m1.1b\"><ci id=\"A3.T4.2.2.2.1.1.m1.1.1.cmml\" xref=\"A3.T4.2.2.2.1.1.m1.1.1\">\ud835\udc45</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T4.2.2.2.1.1.m1.1c\">R</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.T4.2.2.2.1.1.m1.1d\">italic_R</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T4.2.2.2.2\">625.098</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A3.T4.2.2.2.3\">Qwen Models\u2019 rollouts</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T4.3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T4.3.3.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T4.3.3.3.1.1\"># Tokens per Critic Response from Judge (<math alttext=\"C\" class=\"ltx_Math\" display=\"inline\" id=\"A3.T4.3.3.3.1.1.m1.1\"><semantics id=\"A3.T4.3.3.3.1.1.m1.1a\"><mi id=\"A3.T4.3.3.3.1.1.m1.1.1\" xref=\"A3.T4.3.3.3.1.1.m1.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"A3.T4.3.3.3.1.1.m1.1b\"><ci id=\"A3.T4.3.3.3.1.1.m1.1.1.cmml\" xref=\"A3.T4.3.3.3.1.1.m1.1.1\">\ud835\udc36</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T4.3.3.3.1.1.m1.1c\">C</annotation><annotation encoding=\"application/x-llamapun\" id=\"A3.T4.3.3.3.1.1.m1.1d\">italic_C</annotation></semantics></math>)</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T4.3.3.3.2\">1,919.860</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A3.T4.3.3.3.3\">Qwen-QwQ\u2019s responses as LLM-as-Judge</td>\n</tr>\n</table>", "caption": "Table 4: Statistics of 1M NuminaMath CoT Trajectories collected by Qwen2.5-Math Models.", "description": "Table 4 presents a statistical overview of 1 million Chain-of-Thought (CoT) trajectories generated using the Qwen2.5-Math language models. These statistics are crucial for estimating the annotation costs associated with the active learning approach.  The table shows the average number of reasoning steps per trajectory, the average number of tokens required per complete rollout, and the average number of tokens per critic response from the judge, which are all key factors influencing the computational cost of labeling.", "section": "5.2 Achieving New SOTA Performance on ProcessBench (75.0%) with Solely 6% Annotation Cost"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"A3.T5.1.1\">\n<tr class=\"ltx_tr\" id=\"A3.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A3.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T5.1.1.1.1.1\">Dataset</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A3.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T5.1.1.1.2.1\"># Labeled Data</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T5.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T5.1.1.2.1\"><span class=\"ltx_text ltx_font_bold ltx_font_smallcaps\" id=\"A3.T5.1.1.2.1.1\">ActPRM</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A3.T5.1.1.2.2\">624,000 (labeled in two stages)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T5.1.1.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.1.3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T5.1.1.3.1.1\">Qwen2.5-Math-PRM-Math-shepherd</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.1.3.2\">860,000</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T5.1.1.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.1.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T5.1.1.4.1.1\">Qwen2.5-Math-PRM</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"A3.T5.1.1.4.2\">860,000</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A3.T5.1.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T5.1.1.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A3.T5.1.1.5.1.1\">UniversalPRM</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A3.T5.1.1.5.2\">690,000</td>\n</tr>\n</table>", "caption": "Table 5: Data number of datasets.", "description": "This table lists the number of labeled data points used to train different process reward models (PRMs).  The datasets include those used in the ACTPRM model (the authors' model), Qwen2.5-Math-PRM-Math-shepherd, Qwen2.5-Math-PRM, and UniversalPRM. Note that ACTPRM's data was labeled in two stages, resulting in a higher number of labeled data points.", "section": "5.2 Achieving New SOTA Performance on ProcessBench (75.0%) with Solely 6% Annotation Cost"}]