[{"figure_path": "https://arxiv.org/html/2504.11042/x1.png", "caption": "Figure 1: Illustration of lazy thinking in ARR-22 reviews sourced from NLPeer\u00a0Dycke et\u00a0al. (2023). The first review segment belongs to the class \u2018The results are not novel.\u2019 The last segment pertains to, \u2018The approach is tested only on [not English], so unclear if it will generalize to other languages.\u2019 as per ARR-22 guidelines.", "description": "The figure showcases examples of \"lazy thinking\" in peer reviews, a phenomenon where reviewers dismiss papers based on superficial heuristics rather than thorough analysis.  It presents two review segments from the ACL Rolling Review (ARR) dataset (sourced from the NLPeer dataset). The first segment exemplifies the \"results are not novel\" category of lazy thinking, while the second highlights the \"approach is only tested on non-English data\" category.  These examples illustrate how reviewers might use shortcuts in their evaluations, potentially leading to unfair or inaccurate assessments.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.11042/x2.png", "caption": "Figure 2: Distribution of lazy thinking labels in our dataset, LazyReview.", "description": "This bar chart presents the frequency distribution of the 18 different lazy thinking categories in the LAZYREVIEW dataset.  The x-axis lists each lazy thinking category and the y-axis shows their counts.  It provides a visual representation of the class imbalance within the dataset, highlighting which lazy thinking categories are most and least prevalent in the NLP peer review sentences.", "section": "LAZYREVIEW: A Dataset for detecting Lazy Thinking in Peer Reviews"}, {"figure_path": "https://arxiv.org/html/2504.11042/x4.png", "caption": "(a) Gemma 7B", "description": "This figure is a part of the experimental results. It shows the performance of Gemma 7B model, one of the LLMs used in the study, during the in-context learning experiment of round 3. The x-axis represents the number of exemplars (1, 2, or 3), and the y-axis represents the accuracy. Different colors represent different methods used for selecting exemplars (BM25, Random, TopK, VoteK, Static). Error bars indicate the use of only the target segment (T) as the information source, and accuracy refers to the use of GPT-based accuracy.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x5.png", "caption": "(b) LLaMa 7B", "description": "The figure shows the performance of the LLaMa 7B model in detecting lazy thinking in NLP peer reviews.  The x-axis represents different methods for selecting in-context learning (ICL) examples, while the y-axis shows the accuracy. The graph displays the performance for both fine-grained and coarse-grained classification tasks. Error bars represent the accuracy using only the target segment (T) as the information source.  The figure demonstrates the impact of different ICL example selection techniques on the model's ability to correctly identify instances of lazy thinking, comparing methods like BM25, Random, TopK, VoteK, and Static selection.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x6.png", "caption": "(c) LLaMa 13B", "description": "This figure displays the performance of the LLaMa 13B model across various experimental setups.  It showcases the model's accuracy in detecting lazy thinking in peer reviews, broken down by fine-grained and coarse-grained classification. Different data mixes and prompting methods (target segment only (T) or target segment and full review (RT)) are used, highlighting the model's behavior across varied conditions and showing the impact of in-context learning, positive examples, and instruction tuning.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x7.png", "caption": "(d) Mistral 7B", "description": "The figure shows the performance of Mistral 7B, a large language model, on the task of identifying \"lazy thinking\" in peer reviews.  It displays the model's accuracy across different rounds of annotation, which involved refining the guidelines used to label lazy thinking behaviors in reviews.  Specifically, it highlights the impact of improved guidelines on the model's ability to accurately classify instances of lazy thinking.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x8.png", "caption": "(e) SciT\u00fclu 7B", "description": "This figure displays the performance of the SciT\u00fclu 7B language model on a specific task.  It likely shows the model's accuracy or other relevant metrics across different experimental conditions, such as different input methods or data sets.  The specific metrics and conditions would need to be determined from the surrounding text within the paper.  The graph's visual representation (bars, lines, etc.) would also aid in interpreting the results more precisely.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x9.png", "caption": "(f) Qwen 7B", "description": "This figure shows the performance of the Qwen 7B model in the experiment on detecting lazy thinking in peer reviews.  Specifically, it displays the model's accuracy in a multi-class classification task, where each class represents a different type of lazy thinking.  The x-axis likely represents different methods or variations used for in-context learning, and the y-axis depicts the model's performance (likely accuracy).  Different colored bars may show performance results based on using different numbers of examples in in-context learning. This data helps to assess the effectiveness of different in-context learning strategies for improving the model's ability to identify various forms of lazy thinking.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x11.png", "caption": "Figure 3: Performance of LLMs on using different In Context learning (ICL) methods for Round 3 of our annotation study for fine-grained classification. Error bars indicate using only the target segment (T) as the information source. Acc refers to using GPT-based accuracy", "description": "This figure displays the performance of various Large Language Models (LLMs) on a fine-grained classification task during the third round of the annotation process for the LAZYREVIEW dataset.  The models were evaluated using different in-context learning (ICL) methods.  The results are shown as accuracy scores using GPT-based evaluation. Error bars represent the performance when only the target segment (T), not the full review context, was used as input.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x12.png", "caption": "(a) Gemma 7B", "description": "This figure displays the performance of the Gemma 7B large language model (LLM) in the context of in-context learning (ICL) using different methods for Round 3 of an annotation study.  The graph shows accuracy across different methods of selecting in-context examples for both fine-grained and coarse-grained classification tasks. Error bars represent using only the target segment ('T') as information, while the accuracy scores are based on GPT evaluations.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x13.png", "caption": "(b) LLaMa 7B", "description": "The figure shows the performance of the LLaMa 7B model on the fine-grained classification task during Round 3 of the annotation study.  The x-axis represents the number of in-context learning (ICL) examples used (1, 2, or 3), and the y-axis shows the accuracy. Different ICL selection methods are compared: BM25, Random, TopK, Votek, and Static. The error bars indicate the accuracy when only the target segment (T) is used as the information source.  GPT-based accuracy is reported.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x14.png", "caption": "(c) LLaMa 13B", "description": "The figure displays the performance of the LLaMa 13B model across different experimental setups for fine-grained classification.  The x-axis represents the percentage of dataset mixes used in training, and the y-axis represents the accuracy achieved.  Different colored lines show the performance for various combinations of in-context learning techniques and prompt types (target segment only or target segment combined with full review). The graph visualizes how the model's accuracy changes in response to different training data proportions and input variations.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x15.png", "caption": "(d) Mistral 7B", "description": "The figure shows the performance of Mistral 7B model in detecting lazy thinking in NLP peer reviews. It presents the accuracy of the model across different annotation rounds, comparing its performance in fine-grained and coarse-grained classification tasks, with and without using the combination of review and target segment (RT).", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x16.png", "caption": "(e) SciT\u00fclu 7B", "description": "This figure presents the performance of the SciT\u00fclu 7B language model on a fine-grained classification task, specifically for identifying lazy thinking in peer reviews.  The results are part of a study evaluating various Large Language Models (LLMs) on their ability to detect and classify instances of lazy thinking, a phenomenon where reviewers dismiss or criticize papers based on superficial heuristics or preconceived notions rather than thorough analysis.  This specific figure likely shows the model's performance across different experimental setups or parameters, possibly comparing its performance with and without in-context learning or different datasets.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x17.png", "caption": "(f) Qwen 7B", "description": "The figure shows the performance of the Qwen 7B model in detecting lazy thinking in peer reviews.  The x-axis represents different in-context learning (ICL) methods used to enhance model comprehension of lazy thinking, including BM25, Random, TopK, Votek, and Static. The y-axis represents the accuracy of the model in classifying review segments. Error bars show the standard deviation across multiple runs.  The plot demonstrates how different ICL approaches affect the model's accuracy in this task.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x18.png", "caption": "Figure 4: Performance of LLMs on using different In Context learning (ICL) methods for Round 3 of our annotation study for the coarse classification task. Error bars indicate using only the target segment (T) as the information source. Acc refers to using GPT-based evaluator accuracy.", "description": "This figure displays the performance of various Large Language Models (LLMs) on a coarse-grained classification task during the third round of the annotation study.  The models were evaluated using different in-context learning (ICL) methods to determine the presence of lazy thinking in peer reviews.  The accuracy of the LLMs was measured using a GPT-based evaluator. Error bars in the chart illustrate the performance variations when only the target segment (T) of the review text is used as input, rather than the full review text.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x19.png", "caption": "Figure 5: Prompt for GPT based review segment extraction", "description": "This figure shows the prompt used to instruct GPT-4 to extract review segments from the \"Summary of Weaknesses\" section of NLP peer reviews that are relevant to the task of identifying lazy thinking.  The prompt provides a definition of lazy thinking and lists examples of different categories of lazy thinking.  It instructs the model to only extract segments relevant to those categories.", "section": "2 LAZYREVIEW: A Dataset for detecting Lazy Thinking in Peer Reviews"}, {"figure_path": "https://arxiv.org/html/2504.11042/x20.png", "caption": "Figure 6: Fixed Prompt for defining lazy thinking", "description": "This figure shows the prompt template used for defining lazy thinking in the LAZYREVIEW dataset.  The prompt begins by establishing the context of peer review, its growing workload, and the phenomenon of 'lazy thinking.' It then presents a dictionary defining several categories of lazy thinking, each with a detailed explanation. This comprehensive prompt ensures annotators have a clear understanding of the different types of lazy thinking before they begin their annotation task.  Each lazy thinking category provided serves as a guide for identifying and labeling instances within the review dataset.", "section": "LAZYREVIEW: A Dataset for detecting Lazy Thinking in Peer Reviews"}, {"figure_path": "https://arxiv.org/html/2504.11042/x21.png", "caption": "Figure 7: Prompt for fine-grained classification", "description": "This figure shows the prompt template used for the fine-grained classification task in the LAZYREVIEW dataset experiment.  The prompt instructs the language model to classify a given target sentence from a review into one of several predefined lazy thinking categories. The prompt includes the full review text and the target segment to provide context for the classification.", "section": "2 LAZYREVIEW: A Dataset for detecting Lazy Thinking in Peer Reviews"}, {"figure_path": "https://arxiv.org/html/2504.11042/x22.png", "caption": "Figure 8: Prompt for coarse-grained classification", "description": "This figure shows the prompt template used for the coarse-grained classification task in the LAZYREVIEW dataset creation.  The task is to classify a given review segment as either containing lazy thinking or not. The prompt provides context by defining lazy thinking in the context of NLP peer review, and includes instructions on how to perform the classification task.  The model is given both the full review and the target review segment for context.", "section": "2 LAZYREVIEW: A Dataset for detecting Lazy Thinking in Peer Reviews"}, {"figure_path": "https://arxiv.org/html/2504.11042/x23.png", "caption": "Figure 9: Prompt for fine-grained classification based on In-Context Learning (ICL) as used in Round 3 of our study", "description": "This figure displays the prompt template used for the fine-grained classification task in Round 3 of the LAZYREVIEW dataset creation.  The prompt provides instructions to the annotators, including a full review text, a highlighted target segment from the review, and a request to classify the target segment into one of the predefined 'lazy thinking' categories. An example is shown to illustrate the annotation process. This approach uses in-context learning, where the model learns to classify based on examples shown within the prompt.", "section": "2 LAZYREVIEW: A Dataset for detecting Lazy Thinking in Peer Reviews"}, {"figure_path": "https://arxiv.org/html/2504.11042/x24.png", "caption": "Figure 10: Prompt for coarse-grained classification based on In-Context Learning (ICL) as used in Round 3 of our study", "description": "This figure displays the prompt template used for the coarse-grained classification task in Round 3 of the LAZYREVIEW dataset creation.  The prompt guides the language model to classify review segments as either exhibiting 'lazy thinking' or not.  It provides context, instructions, and an example to aid the model's comprehension of the task. The prompt leverages in-context learning by including an example of a review segment and its corresponding classification, helping the model learn the task more effectively.", "section": "2 LAZYREVIEW: A Dataset for detecting Lazy Thinking in Peer Reviews"}, {"figure_path": "https://arxiv.org/html/2504.11042/x25.png", "caption": "(a) Gemma 7B", "description": "This figure displays the performance of the Gemma 7B language model across different in-context learning methods used in Round 3 of the annotation study for fine-grained classification.  The x-axis represents the number of exemplars (1, 2, or 3), and the y-axis represents accuracy.  Each bar represents the accuracy achieved using a specific in-context learning method (BM25, Random, TopK, VoteK, Static). Error bars are included to show variability, and only the target segment (T) is used as the information source.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x26.png", "caption": "(b) LLaMa 7B", "description": "The figure shows the performance of the LLaMa 7B model in detecting lazy thinking in peer reviews.  Specifically, it illustrates the model's performance across different annotation rounds.  The x-axis likely represents the annotation round (e.g., indicating improvements in guidelines over time), and the y-axis shows performance metrics, likely accuracy.  Different colored bars may represent different variations of the task (e.g., using only the target segment, or using the full review and the target segment).  The figure likely aims to demonstrate how model performance changes as the annotation guidelines and training data improve.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x27.png", "caption": "(c) LLaMa 13B", "description": "The figure shows the performance of the LLaMa 13B model in the fine-grained classification task.  The x-axis represents the different data mixes used for training, ranging from \u2018No Mix\u2019 (using only the LAZYREVIEW dataset) to \u2018T\u00fclu Mix\u2019, \u2018SciRiFF Mix\u2019, and \u2018Full Mix\u2019 (combinations of LAZYREVIEW with other datasets). The y-axis represents the accuracy of the model.  Different colored lines and shaded areas represent different input types (target sentence only (T), target sentence and review (RT), etc.) and different annotation rounds. The figure illustrates how the model's performance changes with different amounts and types of training data.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x28.png", "caption": "(d) Mistral 7B", "description": "This figure shows the performance of Mistral 7B, a large language model, in detecting lazy thinking in peer reviews.  The x-axis represents different methods used for in-context learning (ICL) during the third round of annotation, and the y-axis shows the accuracy.  The results indicate how effective each ICL method is in improving the model's ability to identify lazy thinking instances.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x29.png", "caption": "(e) SciT\u00fclu 7B", "description": "This figure presents the performance of the SciT\u00fclu 7B language model on the task of detecting lazy thinking in peer reviews.  The x-axis represents different in-context learning methods (BM25, Random, TopK, VoteK, Static) used for selecting examples to aid the model's understanding. The y-axis shows the accuracy achieved.  The graph helps evaluate how different example selection techniques impact the model's ability to classify review segments correctly.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x30.png", "caption": "(f) Qwen 7B", "description": "The figure shows the performance of the Qwen 7B model in the in-context learning experiment for the fine-grained classification task of the LAZYREVIEW dataset.  It compares different methods for selecting in-context examples (BM25, Random, TopK, VoteK, Static) and shows the accuracy achieved using only the target segment (T) as the information source. Error bars represent variations in accuracy across multiple runs.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x31.png", "caption": "(g) Yi 6B", "description": "This figure (Figure 12(g)) presents the performance of the instruction-tuned Yi 6B model on the fine-grained classification task.  It showcases the model's accuracy across various datasets (No Mix, T\u00fclu Mix, SciRiFF Mix, Full Mix) and different proportions of those datasets. The x-axis represents the percentage of the dataset used for training, and the y-axis shows the accuracy achieved by the model. Error bars illustrate the variability in performance. The graph allows for a visual comparison of how the model's performance changes with different dataset sizes and compositions.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x32.png", "caption": "Figure 11: Performance of instruction-tuned LLMs for fine-grained classification on the dev set with multiple percentages of dataset mixes using target segment (T) as the source of information in the prompt.", "description": "This figure displays the performance of various instruction-tuned Large Language Models (LLMs) on a fine-grained classification task. The models were evaluated using different proportions of dataset mixes, with only the target segment (T) of each review used as input to the model.  The performance is measured and shown for each LLM across different dataset mixes.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x33.png", "caption": "(a) Gemma 7B", "description": "This figure presents the performance of the Gemma 7B language model on a fine-grained classification task, as part of an experiment to detect lazy thinking in NLP peer reviews.  It shows the accuracy (vertical axis) achieved using different in-context learning methods for selecting examples (BM25, Random, TopK, VoteK, and Static)  across three annotation rounds (horizontal axis). The results illustrate how the choice of example selection and the annotation round impact the model's performance in identifying subtle nuances in text indicating lazy thinking.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x34.png", "caption": "(b) LLaMa 7B", "description": "The figure shows the performance of the LLaMa 7B model on the fine-grained classification task during Round 3 of the annotation study.  Different in-context learning (ICL) methods were used, and the performance is shown for different numbers of examples.  Error bars indicate whether the target segment alone (T) or a combination of the review and the target segment (RT) was used as the information source.  Accuracy is assessed using GPT-based methods.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x35.png", "caption": "(c) LLaMa 13B", "description": "This figure shows the performance of the LLaMa 13B model on the fine-grained classification task during Round 3 of the annotation study.  The graph displays the model's accuracy using various in-context learning methods, specifically: BM25, Random, TopK, VoteK, and Static. Different color bars represent different methods, with error bars indicating the use of only the target segment (T) as the information source.  The x-axis represents the number of exemplars used (1, 2, or 3), and the y-axis represents accuracy.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x36.png", "caption": "(d) Mistral 7B", "description": "This figure shows the performance of Mistral 7B, a 7-billion parameter instruction-tuned large language model, in the task of detecting lazy thinking in NLP peer reviews.  The x-axis represents the different methods used for selecting in-context learning examples, while the y-axis represents the accuracy.  The different colored bars represent the accuracy achieved using different methods for example selection. The figure illustrates how the choice of in-context learning example selection impacts the performance of the model in this specific NLP task. ", "section": "Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2504.11042/x37.png", "caption": "(e) SciT\u00fclu 7B", "description": "This figure shows the performance of the SciT\u00fclu 7B model on the fine-grained classification task.  It displays the model's accuracy using different in-context learning (ICL) methods.  Specifically, it illustrates the impact of using BM25, Random, TopK, Votek, and Static methods for selecting examples during the annotation process of Round 3. Error bars indicate using only the target segment (T) as the information source; accuracy is determined using GPT-based evaluation.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x38.png", "caption": "(f) Qwen 7B", "description": "The figure shows the performance of the Qwen 7B model on the fine-grained classification task during Round 3 of the annotation study.  It illustrates the model's accuracy using different in-context learning (ICL) methods. The accuracy is shown as a function of the number of exemplars used, and error bars are included to show variability.  The information source for the model's input is only the target segment (T).", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x39.png", "caption": "(g) Yi 6B", "description": "The figure shows the performance of the Yi 6B instruction-tuned LLM on the fine-grained classification task.  It displays the accuracy scores across multiple data mixes (No Mix, T\u00fclu Mix, SciRiFF Mix, Full Mix) and varying percentages of the data used for training.  The x-axis represents the percentage of data, and the y-axis represents the accuracy. Error bars show the variation in performance across different runs. The model's accuracy is evaluated using two different metrics: string matching (S.A.) and a GPT-based semantic evaluation (G.A.). Both metrics are displayed on the graph, illustrating the differences between exact string matches and semantic similarity.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x40.png", "caption": "Figure 12: Performance of instruction-tuned LLMs for fine-grained classification on the dev set with multiple percentages of dataset mixes using the combination of review and target segment (RT) as the source of information in the prompt.", "description": "This figure displays the performance of various instruction-tuned Large Language Models (LLMs) on a fine-grained classification task.  The models were evaluated on a development set using different proportions of training data mixes. The input to the models consisted of both the review text and the target segment (RT).  The graph likely shows the accuracy or F1-score of the models across varying dataset mix percentages, allowing for an analysis of how effective the instruction tuning was and how the model's performance changes with the amount of training data provided.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x41.png", "caption": "(a) Gemma 7B", "description": "This figure presents the performance of the Gemma 7B language model in detecting lazy thinking in peer reviews.  It showcases the model's accuracy across different methods for selecting in-context learning examples (BM25, Random, TopK, VoteK, Static), during Round 3 of the annotation study for the fine-grained classification task.  Error bars represent the use of only the target segment (T) as the information source, and accuracy is measured using a GPT-based approach.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x42.png", "caption": "(b) LLaMa 7B", "description": "This figure presents the results of the instruction-tuned LLaMa 7B model for the fine-grained classification task. It displays the model's performance across different data mixes (No Mix, T\u00fclu Mix, SciRiFF Mix, Full Mix), varying the proportion of the data used for training.  The x-axis represents the percentage of the data mix used, and the y-axis represents the accuracy. Error bars indicate using only the target segment (T) as the information source.", "section": "Experimental Setup"}, {"figure_path": "https://arxiv.org/html/2504.11042/x43.png", "caption": "(c) LLaMa 13B", "description": "This figure displays the performance of the LLaMa 13B model across different experimental setups.  The x-axis shows different methods of in-context learning (BM25, Random, TopK, VoteK, Static), and the y-axis shows accuracy.  The different colored bars represent the performance on different rounds of annotation in the LAZYREVIEW dataset.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x44.png", "caption": "(d) Mistral 7B", "description": "This figure is a bar chart showing the performance of Mistral 7B model in detecting lazy thinking in peer reviews. The chart displays the model's performance on different tasks, including fine-grained classification and coarse-grained classification using two types of input: target segment (T) and a combination of review and target segment (RT). The results are presented for three rounds of annotation in the study.  The performance is measured in terms of accuracy and F1 scores calculated using string matching (SA) and GPT-based methods (GA).", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x45.png", "caption": "(e) SciT\u00fclu 7B", "description": "This figure displays the performance of the SciT\u00fclu 7B language model on a fine-grained classification task.  It shows the model's accuracy using different in-context learning (ICL) methods during Round 3 of the annotation study.  The chart visualizes the accuracy using GPT-based evaluation, considering only the target segment (T) as the information source. Error bars represent the variance.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x46.png", "caption": "(f) Qwen 7B", "description": "This figure displays the performance of the Qwen 7B model across different in-context learning methods in round 3 of the annotation study.  The x-axis shows the number of exemplars (1, 2, or 3), while the y-axis represents accuracy.  The various bars represent different methods for selecting examples (BM25, Random, TopK, Votek, Static). The top half of the figure shows results for fine-grained classification, and the bottom half displays results for coarse-grained classification. Error bars indicate the variation based on using only the target segment (T) as the information source.  Accuracy is calculated using the GPT-based method.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x47.png", "caption": "(g) Yi 6B", "description": "The figure shows the performance of the Yi 6B instruction-tuned large language model (LLM) on a fine-grained classification task.  The x-axis represents the percentage of different data mixes used for training the model (No Mix, T\u00fclu Mix, SciRiFF Mix, Full Mix), while the y-axis represents the accuracy of the model.  The graph displays accuracy values for three rounds of annotation (R1, R2, R3) and two input methods: target sentence (T) and review + target sentence (RT).  The graph shows how the model's performance changes based on different data training scenarios and annotation rounds.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x48.png", "caption": "Figure 13: Performance of instruction-tuned LLMs for coarse-grained classification on the dev set with multiple percentages of dataset mixes using target segment (T) as the source of information in the prompt.", "description": "This figure displays the performance of various instruction-tuned Large Language Models (LLMs) on a coarse-grained classification task. The models were evaluated on a development set, using different proportions of data mixes.  Each LLM's accuracy is shown for various data mix percentages. The 'target segment (T)' method, which uses only the target sentence as input, was employed in this experiment.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x49.png", "caption": "(a) Gemma 7B", "description": "This figure displays the performance of the Gemma 7B language model on the task of detecting lazy thinking in peer reviews.  It shows the model's accuracy using different in-context learning methods (BM25, Random, TopK, Votek, Static) for different numbers of examples. The x-axis represents the number of examples used, and the y-axis represents the accuracy. The figure helps evaluate how effectively different strategies for providing example sentences influence the model's performance.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x50.png", "caption": "(b) LLaMa 7B", "description": "This figure shows the performance of the LLaMa 7B model during Round 3 of the annotation study.  Specifically, it illustrates the model's performance in fine-grained classification using different in-context learning (ICL) methods.  The x-axis likely represents the number of in-context examples used, while the y-axis displays accuracy. Different colored bars show results from various ICL strategies (BM25, Random, TopK, VoteK, Static).  The figure helps to analyze the impact of these different ICL methods on the LLaMa 7B model's ability to accurately classify lazy thinking instances.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x51.png", "caption": "(c) LLaMa 13B", "description": "The figure shows the performance of the LLaMa 13B model on the fine-grained classification task in Round 3 of the annotation study.  It displays the accuracy of the model using different in-context learning (ICL) methods,  with error bars indicating the usage of only the target segment (T) as the information source.  The accuracy is measured using GPT-based accuracy.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.11042/x52.png", "caption": "(d) Mistral 7B", "description": "The figure shows the performance of Mistral 7B model in detecting lazy thinking in NLP peer reviews.  It presents results for two tasks: coarse-grained (binary classification of lazy thinking vs. not lazy thinking) and fine-grained (classifying into specific lazy thinking categories).  The results are shown for various experimental setups (different annotation rounds, different input types\u2014target segment only vs. review + target segment), illustrating how model performance varies with the quality and context of the input data and the granularity of the classification task.", "section": "Experiments"}]