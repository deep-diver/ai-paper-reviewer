{"references": [{"fullname_first_author": "Greg Brockman", "paper_title": "Openai gym", "publication_date": "2016-06-01", "reason": "This paper introduces OpenAI Gym, a widely adopted platform for reinforcement learning, which serves as a foundational inspiration for TextArena's framework."}, {"fullname_first_author": "Ralf Herbrich", "paper_title": "TrueskillTM: a bayesian skill rating system", "publication_date": "2006-01-01", "reason": "This paper describes the TrueSkill system, which TextArena uses for ranking and tracking model performance on its leaderboard, making it central to the platform's evaluation methodology."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-01", "reason": "This paper discusses the MMLU benchmark, a significant evaluation tool for language models; TextArena aims to address the limitations of such static benchmarks by providing a dynamic, game-based environment."}, {"fullname_first_author": "Wei-Lin Chiang", "paper_title": "Chatbot arena: An open platform for evaluating llms by human preference", "publication_date": "2024-03-01", "reason": "This paper presents Chatbot Arena, a related platform that evaluates language models through human preference, which TextArena builds upon by adding competitive gameplay and removing human bias."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-01", "reason": "This paper discusses HumanEval, an important benchmark that TextArena aims to supplement by offering a dynamic evaluation environment and addressing skills such as theory of mind and deception."}]}