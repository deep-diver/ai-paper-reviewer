[{"figure_path": "https://arxiv.org/html/2503.01183/x1.png", "caption": "Figure 1: Architecture of DiffRhythm. The style and lyrics are used as external control signals, which are preprocessed to get the style embedding and lyrics token, input to DiT to generate latent, and subsequently go through the VAE decoder to generate the audio.", "description": "The DiffRhythm model uses both style and lyrics as input. The style prompt is processed through an LSTM to generate a style embedding. The lyrics are processed to generate a sequence of phoneme tokens.  Both the style embedding and phoneme tokens, along with the timestep, are fed into a diffusion transformer (DiT) which generates a latent representation. This latent representation is decoded using a Variational Autoencoder (VAE) to produce the final audio output containing both vocals and accompaniment. ", "section": "3 DiffRhythm"}, {"figure_path": "https://arxiv.org/html/2503.01183/x2.png", "caption": "Figure 2: The data preprocessing pipeline of DiffRhythm. Lyrics go through G2P and are placed at the positions corresponding to their timestamps", "description": "This figure illustrates the preprocessing steps involved in preparing the lyrics data for use in the DiffRhythm model.  The process begins with the lyrics, which are converted into phonemes using a grapheme-to-phoneme (G2P) converter. These phonemes are then aligned temporally with the corresponding timestamps of the music waveform. This alignment ensures that the generated vocals align with the music during song synthesis. The figure highlights the crucial role of this preprocessing step in ensuring that the model accurately produces synchronized vocals and accompaniment.", "section": "3.2 Variational Autoencoder"}, {"figure_path": "https://arxiv.org/html/2503.01183/extracted/6222909/figs/logit-norm.png", "caption": "Figure 3: Logit-normal timestep distribution.", "description": "This figure shows the probability density function (PDF) of the logit-normal distribution used for sampling timesteps in the diffusion process. The x-axis represents the timestep t, ranging from 0 to 1, and the y-axis shows the probability density. The curve demonstrates the distribution's characteristics: concentrated around the middle timesteps (around t = 0.5), indicating that the model focuses more on the challenging intermediate regions during training. The parameters m and s control the distribution's mean and standard deviation respectively, enabling bias toward either data or noise regions.", "section": "3.3 Diffusion Transformer"}, {"figure_path": "https://arxiv.org/html/2503.01183/extracted/6222909/figs/flac.png", "caption": "(a) GT (Lossless)", "description": "This figure shows a spectrogram visualization comparing the proposed VAE with baseline models. (a) displays a spectrogram of lossless ground truth audio.  The figure highlights the effects of MP3 compression on audio and how the proposed VAE model is able to reconstruct the audio from the compressed version more accurately than a baseline model.", "section": "4.3 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2503.01183/extracted/6222909/figs/mp3.png", "caption": "(b) GT (MP3 Compressed)", "description": "This figure shows a spectrogram of a ground truth song after being compressed using the MP3 format.  This visualization helps illustrate the effect of MP3 compression on audio quality, particularly highlighting the loss of high-frequency components and the introduction of artifacts, which is used to evaluate the VAE's ability to reconstruct high-quality audio even from compressed inputs.", "section": "4.3 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2503.01183/extracted/6222909/figs/vae.png", "caption": "(c) Proposed VAE", "description": "Spectrogram of an MP3-compressed audio clip reconstructed by the proposed Variational Autoencoder (VAE). The figure shows the effectiveness of the VAE in reconstructing audio from compressed data by visually comparing it to the ground truth (lossless and MP3-compressed versions). Boxed regions highlight specific areas for detailed analysis.", "section": "4.3 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2503.01183/extracted/6222909/figs/stable-audio.png", "caption": "(d) Stable Audio 2 VAE", "description": "The figure displays a spectrogram generated by Stable Audio 2 VAE model after reconstructing an MP3 compressed audio. This allows for visual comparison with other spectrograms, including the ground truth (lossless and MP3 compressed) and the spectrogram generated by the proposed VAE model in the paper.  The comparison highlights the performance of different models in reconstructing audio from MP3 compressed versions, focusing on high-frequency attenuation and mid-frequency hollowing effects.", "section": "4.3 Evaluation Metrics"}]