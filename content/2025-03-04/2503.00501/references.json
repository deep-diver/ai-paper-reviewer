{"references": [{"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "publication_date": "2018-10-05", "reason": "This is an important reference because it introduces BERT, a foundational model used for various NLP tasks, including retrieval and language understanding."}, {"fullname_first_author": "Yunfan Gao", "paper_title": "Retrieval-augmented generation for large language models: A survey", "publication_date": "2023-12-19", "reason": "This is an important reference because it surveys retrieval-augmented generation (RAG), which enhances language model performance through external knowledge retrieval, and is central to modern information retrieval."}, {"fullname_first_author": "Yixing Fan", "paper_title": "Pre-training methods in information retrieval", "publication_date": "2022-01-01", "reason": "This paper is important because it reviews pre-training methods in information retrieval, which helps establish the foundation in designing the system."}, {"fullname_first_author": "Tri Nguyen", "paper_title": "MS MARCO: A human-generated machine reading comprehension dataset", "publication_date": "2016-01-01", "reason": "This paper is important because it references MS MARCO, a widely used dataset for machine reading comprehension and information retrieval tasks."}, {"fullname_first_author": "Xiaohui Xie", "paper_title": "T2ranking: A large-scale chinese benchmark for passage ranking", "publication_date": "2023-01-01", "reason": "This paper is important because it introduces a large-scale Chinese benchmark for passage ranking, related to search-based tasks."}]}