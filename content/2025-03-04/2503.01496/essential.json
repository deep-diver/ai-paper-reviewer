{"importance": "This paper presents **Liger, a novel method for efficiently linearizing LLMs into gated recurrent structures**, reducing computational costs, and enabling faster, memory-efficient deployment, opening avenues for practical application in resource-constrained environments and further research into efficient LLM architectures.", "summary": "Liger: LLMs linearized to gated recurrent models, enabling efficient deployment via key matrix repurposing and LoRA fine-tuning.", "takeaways": ["Liger linearizes LLMs into gated recurrent structures by repurposing pretrained key matrix weights, avoiding extra parameters and training.", "Liger Attention, a hybrid attention mechanism, enhances the linearization process while maintaining pre-trained LLM capabilities.", "Liger achieves competitive performance on benchmarks with minimal pre-training cost and can scale across model sizes, offering benefits of constant-memory inference."], "tldr": "Large Language Models (LLMs) face challenges of high computational cost and memory demands due to the Transformer architecture's quadratic complexity, limiting their use for long sequences. Linear recurrent models offer linear-time training and constant-memory inference, but pretraining them from scratch is costly. Existing linearization methods introduce extra modules that require fine-tuning and overlook gating mechanisms crucial for memory retention in these models.\n\nTo address these issues, **Liger repurposes pretrained key matrix weights to construct gating mechanisms, creating gated recurrent structures without additional parameters**. By fine-tuning these models with Low-Rank Adaptation (LoRA), Liger recovers the performance of original LLMs. The method introduces Liger Attention, a hybrid attention mechanism that improves performance, validated on models from 1B to 8B parameters. The results validate that **Liger outperforms other methods**.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.01496/podcast.wav"}