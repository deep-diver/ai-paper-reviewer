[{"figure_path": "https://arxiv.org/html/2503.01103/x3.png", "caption": "Figure 1:  Toy example illustrating DDO. (a) Models pretrained via maximum likelihood estimation (MLE) exhibit dispersed density, while DDO imposes contrastive forces toward the data distribution. (b) The finetuned model concentrates better on the main mode.", "description": "This figure provides a visual illustration of Direct Discriminative Optimization (DDO).  Panel (a) shows a comparison of models trained with maximum likelihood estimation (MLE) versus those trained with DDO. The MLE models show a dispersed density, indicating that they struggle to capture the important modes of the data distribution.  In contrast, the DDO method demonstrates a contrastive force, pulling the model's distribution closer to the true data distribution. Panel (b) highlights how the DDO fine-tuned model significantly improves, concentrating its density more effectively on the main mode of the data, leading to superior sample quality.", "section": "3 Direct Discriminative Optimization"}, {"figure_path": "https://arxiv.org/html/2503.01103/x4.png", "caption": "Figure 2:  Illustration of DDO. (1) Models. \u03b8refsubscript\ud835\udf03ref\\theta_{\\text{ref}}italic_\u03b8 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT is the (pretrained) reference model frozen during training. \u03b8\ud835\udf03\\thetaitalic_\u03b8 is the learnable model initialized as \u03b8refsubscript\ud835\udf03ref\\theta_{\\text{ref}}italic_\u03b8 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT. (2) Data. Samples from pdatasubscript\ud835\udc5ddatap_{\\text{data}}italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT are drawn from the training dataset. Samples from p\u03b8refsubscript\ud835\udc5dsubscript\ud835\udf03refp_{\\theta_{\\text{ref}}}italic_p start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT end_POSTSUBSCRIPT are generated by the reference model, either offline or online. (3) Objective. The target model \u03b8\ud835\udf03\\thetaitalic_\u03b8 is optimized by applying the GAN discriminator loss with the implicitly parameterized discriminator d\u03b8subscript\ud835\udc51\ud835\udf03d_{\\theta}italic_d start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT to distinguish between real samples from pdatasubscript\ud835\udc5ddatap_{\\text{data}}italic_p start_POSTSUBSCRIPT data end_POSTSUBSCRIPT and fake samples from p\u03b8refsubscript\ud835\udc5dsubscript\ud835\udf03refp_{\\theta_{\\text{ref}}}italic_p start_POSTSUBSCRIPT italic_\u03b8 start_POSTSUBSCRIPT ref end_POSTSUBSCRIPT end_POSTSUBSCRIPT.", "description": "This figure illustrates the Direct Discriminative Optimization (DDO) framework.  It shows three key components: (1) Two models are involved: a pretrained reference model (\u03b8ref) that remains frozen during training, and a target model (\u03b8) that is initially the same as the reference model but will be updated.  (2) Data sources consist of real data samples (from pdata) and synthetic samples generated by the reference model (from p\u03b8ref). (3) The training objective is a GAN loss where the discriminator is implicitly defined by the likelihood ratio of the target and reference models. The goal is to train the target model (\u03b8) to better match the true data distribution (pdata) by discriminating between real and fake samples.", "section": "3. Direct Discriminative Optimization"}, {"figure_path": "https://arxiv.org/html/2503.01103/x5.png", "caption": "Figure 3:  Comparison with DPO.", "description": "This figure compares Direct Discriminative Optimization (DDO) with Direct Preference Optimization (DPO).  It illustrates the key differences in their methodologies. DPO uses paired human preference data to align a language model with human preferences by expressing the reward model as a likelihood ratio. DDO, in contrast, uses unpaired data from the data distribution and the pretrained model to improve the target model by implicitly parameterizing a discriminator using the likelihood ratio. This allows for direct and efficient finetuning of the pretrained model without the need for additional networks or alternating optimization processes. The figure visually represents these differences and highlights how DDO achieves model alignment using an implicitly defined discriminator.", "section": "4. Comparison with Existing Methods"}, {"figure_path": "https://arxiv.org/html/2503.01103/extracted/6246558/figures/cifar10-32x32-uncond.jpg", "caption": "Figure 4:  Comparison of model parameter counts and inference time across different guidance methods and DDO. For DG, we measure the statistics on class-conditional CIFAR-10. For AG, we measure the statistics on ImageNet-64.", "description": "Figure 4 compares the number of parameters and the inference time required for different image generation methods.  The methods compared include several guidance techniques (Classifier-Free Guidance, Discriminator Guidance, and Autoguidance) as well as the proposed Direct Discriminative Optimization (DDO) method.  The baseline model is also shown.  The evaluation is performed on two different datasets: CIFAR-10 and ImageNet-64, with Discriminator Guidance evaluated on class-conditional CIFAR-10 and Autoguidance on ImageNet-64. This allows for a direct comparison of the computational efficiency and model complexity associated with various approaches to improving image generation quality.", "section": "4.2. Guidance Methods"}, {"figure_path": "https://arxiv.org/html/2503.01103/extracted/6246558/figures/cifar10-32x32-uncond-r12.jpg", "caption": "Figure 5:  Illustrations on diffusion models. (a) Multi-round refinement and (b)(c)\ntraining curves under different \u03b1,\u03b2\ud835\udefc\ud835\udefd\\alpha,\\betaitalic_\u03b1 , italic_\u03b2.", "description": "This figure shows the results of applying Direct Discriminative Optimization (DDO) to diffusion models. Specifically, it illustrates (a) the impact of iterative refinement (self-play) on model performance across multiple rounds, and training curves under different hyperparameter settings (b) \u03b1 and (c) \u03b2. The plots in (b) and (c) show how the FID (Fr\u00e9chet Inception Distance) score changes with the number of iterations during training for different values of \u03b1 and \u03b2, demonstrating the effect of these hyperparameters on model performance.", "section": "3.3. Practical Implementation"}, {"figure_path": "https://arxiv.org/html/2503.01103/extracted/6246558/figures/cifar10-32x32-cond.jpg", "caption": "Figure 6:  Illustrations on autoregressive models. (a)(b) FID-IS trade-off curves and (c) the impact of \u03b1\ud835\udefc\\alphaitalic_\u03b1 under \u03b2=0.02\ud835\udefd0.02\\beta=0.02italic_\u03b2 = 0.02.", "description": "Figure 6 presents an analysis of autoregressive models, specifically focusing on the impact of hyperparameters on model performance. Subfigures (a) and (b) show FID (Fr\u00e9chet Inception Distance) and IS (Inception Score) trade-off curves for different autoregressive models (VAR-d16 and VAR-d30). These curves illustrate the balance between image diversity (IS) and image quality (FID) at varying guidance scales. Subfigure (c) explores the effect of hyperparameter \u03b1 on model performance, while keeping \u03b2 constant at 0.02. This analysis reveals how adjustments to \u03b1 influence the FID and IS, offering insights into optimizing the generation process.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.01103/extracted/6246558/figures/cifar10-32x32-cond-r16.jpg", "caption": "Figure 7: Random samples of EDM (CIFAR-10, Unconditional), FID 1.97.", "description": "This figure displays several randomly generated images from the Enhanced Diffusion Model (EDM) trained on the CIFAR-10 dataset.  The generation process is unconditional, meaning that no specific class label or other guidance was provided to the model during generation.  The FID score (Fr\u00e9chet Inception Distance) of 1.97 indicates the model's performance, with lower scores suggesting better image quality and similarity to real images from the CIFAR-10 dataset.", "section": "5.2. Results on Diffusion Models"}, {"figure_path": "https://arxiv.org/html/2503.01103/extracted/6246558/figures/img64-s-base-64x64.jpg", "caption": "Figure 8: Random samples of EDM + DDO (CIFAR-10, Unconditional), FID 1.38.", "description": "This figure displays 100 randomly generated images from the EDM (Energy-based Diffusion Model) model after being fine-tuned using the Direct Discriminative Optimization (DDO) method.  The model was trained on the CIFAR-10 dataset in an unconditional setting (meaning no class labels were provided during training, allowing the model to generate images from all classes).  The FID (Fr\u00e9chet Inception Distance) score of 1.38 indicates a high level of image quality and realism, suggesting that the DDO fine-tuning significantly improved the model's generation capabilities compared to the original EDM model.", "section": "5.2 Results on Diffusion Models"}, {"figure_path": "https://arxiv.org/html/2503.01103/extracted/6246558/figures/img64-s-r24-64x64.jpg", "caption": "Figure 9: Random samples of EDM (CIFAR-10, Class-conditional), FID 1.85.", "description": "This figure displays random image samples generated by the Enhanced Diffusion Model (EDM) when trained on the CIFAR-10 dataset in a class-conditional setting.  The Fr\u00e9chet Inception Distance (FID) score achieved by this model is 1.85, indicating its performance in generating realistic and diverse images.  Each image is an example of the model's output for a given class label.", "section": "5.2 Results on Diffusion Models"}, {"figure_path": "https://arxiv.org/html/2503.01103/extracted/6246558/figures/image_grid.jpg", "caption": "Figure 10: Random samples of EDM + DDO (CIFAR-10, Class-conditional), FID 1.30.", "description": "This figure displays several randomly generated images from a class-conditional generative model trained on the CIFAR-10 dataset. The model is a diffusion model enhanced with Direct Discriminative Optimization (DDO), which aims to improve its performance beyond the limitations of maximum likelihood estimation. The FID (Fr\u00e9chet Inception Distance) score for these samples is 1.30, indicating high visual quality and diversity.", "section": "5.2 Results on Diffusion Models"}]