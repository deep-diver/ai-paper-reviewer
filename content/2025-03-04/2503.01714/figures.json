[{"figure_path": "https://arxiv.org/html/2503.01714/x1.png", "caption": "Figure 1: Relationship between \u0394\u0394\\Deltaroman_\u0394SR and Average NegCorrScore across LLaMA models of different scales. The increasing trend of NegCorrScore with \u0394\u0394\\Deltaroman_\u0394SR validates SemRecScore as a reliable measure of semantic reconstruction.", "description": "This figure illustrates the correlation between the increase in scrambling ratio (\u0394SR) and the average negative correlation rate (NegCorrRate) across various LLaMA models.  The x-axis represents the change in scrambling ratio (\u0394SR), indicating the extent of character scrambling within words. The y-axis shows the average NegCorrRate, a metric reflecting the consistency of the model's completion probability across different scrambling levels. The graph reveals a strong positive correlation: as the scrambling ratio increases, the NegCorrRate also increases. This demonstrates that SemRecScore (a metric used to quantify semantic reconstruction) is reliably associated with completion probability consistency and is a valid measure for semantic reconstruction. The different colored lines represent the results for different sized LLAMA models, indicating the trend holds across a range of model sizes.", "section": "5 How to Measure the Degree of Semantic Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.01714/x2.png", "caption": "Figure 2: Semantic reconstruction performance across different Scramble Ratios (SR) and Context Integrity (CI) levels. The top row (a-d) presents SemRecScore trends under varying SR values for 1B, 3B, and 70B models. The bottom row (e-h) illustrates SemRecScore evolution for fixed SR values while varying CI. Across all models, word form plays a dominant role, with context integrity having minimal impact on reconstruction performance.", "description": "Figure 2 visualizes the performance of semantic reconstruction by LLMs (1B, 3B, and 70B parameter models) under varying levels of word scrambling (Scramble Ratio, SR) and contextual information (Context Integrity, CI). The top row displays the SemRecScore (a metric quantifying semantic reconstruction) across different SR levels for each model, holding CI constant at its highest value (CI=1).  The bottom row shows the impact of varying CI levels on SemRecScore, while keeping SR constant (SR=0.25 and SR=1). This figure demonstrates that across all three models and varying CI conditions, word form is the primary driver of successful semantic reconstruction in the face of scrambled letters, with context playing a minimal role.", "section": "6 How Word Form and Contextual Information Influence LLMs\u2019 Semantic Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.01714/x3.png", "caption": "Figure 3: Attention allocation to word form under varying Scramble Ratios (SR).\nSubplots (a-c) show AttentionSelf trends for 1B, 3B, and 70B models with full context (CI=1), while (d) presents the 3B model without context (CI=0). Higher SR values consistently elicit stronger attention to word form, and the cyclic attention pattern remains unchanged even without context, suggesting that LLMs process word form independently of contextual information.", "description": "This figure displays the average AttentionSelf across all samples at each layer for different Scramble Ratios (SR) and model sizes.  It shows how much attention the final token of a scrambled word gives to all other tokens in the same word, across different layers of the LLM.  Subplots (a), (b), and (c) show the results for 1B, 3B, and 70B parameter models, respectively, all with complete contextual information (CI=1). Subplot (d) shows results for the 3B parameter model with no contextual information (CI=0). The key observation is that as the SR increases (meaning more scrambling), the AttentionSelf consistently increases across all layers and model sizes.  Furthermore, a cyclic pattern in attention allocation is observed across layers. Critically, the similar cyclic pattern even in the absence of context (subplot (d)) indicates that LLMs focus on word form independently of context.", "section": "7.1 Attention Allocation to Word Form"}, {"figure_path": "https://arxiv.org/html/2503.01714/x4.png", "caption": "Figure 4: Heatmaps of attention allocation to word form in the LLaMA-1B-Instruct across Scramble Ratios (SR). The x-axis denotes attention heads, and the y-axis denotes layers. Specific heads consistently focus on word form, with higher SR activating more form-sensitive heads, indicating a structured and stable processing mechanism.", "description": "This figure visualizes how attention is allocated to word forms within the LLaMA-1B-Instruct language model under varying degrees of word scrambling (Scramble Ratio or SR).  Each heatmap represents a different SR level (0, 0.5, and 1, indicating no scrambling, moderate scrambling, and extreme scrambling, respectively). The x-axis shows the different attention heads in the model, and the y-axis represents the layers of the model's architecture. The color intensity of each cell in the heatmap indicates the strength of attention paid to the word form by a specific attention head at a particular layer. Brighter colors signify stronger attention. The figure demonstrates that certain attention heads consistently focus on word form regardless of the SR level, but a higher SR activates more of these specialized attention heads. This suggests a structured and stable mechanism within the model for processing word-form information, even when the words are significantly scrambled.", "section": "7.2 Form-Sensitive Attention Heads"}, {"figure_path": "https://arxiv.org/html/2503.01714/x5.png", "caption": "Figure 5: Semantic Reconstruction Performance across Different LLM Scales and Context Integrity Levels.\nThe plots illustrate the layer-wise Semantic Reconstruction Score (SemRecScore) for various SR values across different LLaMA models (1B, 3B, and 70B). The top row represents CI = 0, while the bottom row represents CI = 0.25. The legend indicates different SR conditions, including the \u201cCompletely Scrambled\u201d setting. The similarity of the curves across different CI values suggests that Context Integrity (CI) has minimal impact on semantic reconstruction performance.", "description": "Figure 5 displays the layer-wise semantic reconstruction scores (SemRecScore) for different LLaMA models (1B, 3B, and 70B parameters) under varying levels of word scrambling (SR).  The scores are shown for two levels of context integrity (CI): 0 and 0.25. Each plot shows how well the model reconstructs the meaning of a scrambled word across the different layers of the model.  The consistent performance across different levels of CI, regardless of scrambling level, demonstrates that the amount of surrounding context has minimal effect on the model's ability to reconstruct the meaning of a scrambled word.", "section": "6.2 Impact of Contextual Information on Semantic Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.01714/x6.png", "caption": "Figure 6: Semantic Reconstruction Performance across Different LLM Scales and Context Integrity Levels.\nThe plots illustrate the layer-wise Semantic Reconstruction Score (SemRecScore) for various SR values across different LLaMA models (1B, 3B, and 70B). The top row represents CI = 0.25, while the bottom row represents CI = 0.75. The legend indicates different SR conditions, including the \u201cCompletely Scrambled\u201d setting. The similarity of the curves across different CI values suggests that Context Integrity (CI) has minimal impact on semantic reconstruction performance.", "description": "Figure 6 shows the impact of context on semantic reconstruction performance across different LLaMA model sizes (1B, 3B, and 70B parameters).  It presents layer-wise Semantic Reconstruction Scores (SemRecScores) for various levels of word scrambling (Scramble Ratio, SR).  Two rows show results for different levels of context integrity (CI): the top row for CI=0.25 and the bottom for CI=0.75.  Across different levels of scrambling, the consistency of SemRecScore curves for various CI levels suggests that the amount of context has very little effect on the models' ability to reconstruct the meaning of scrambled words.  Word form is the primary determinant of performance.", "section": "6.2 Impact of Contextual Information on Semantic Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.01714/x7.png", "caption": "Figure 7: Semantic Reconstruction Performance across Different LLM Scales and Scramble Ratio Levels.\nThe plots illustrate the layer-wise Semantic Reconstruction Score (SemRecScore) for various CI values across different LLaMA models (1B, 3B, and 70B). The top row represents SR = 0, while the bottom row represents CI = 0.5. The legend indicates different CI conditions.The close alignment of curves across different CI values suggests that Context Integrity has a limited impact on semantic reconstruction.", "description": "Figure 7 displays the semantic reconstruction performance of LLMs (LLaMA 1B, 3B, and 70B) across different context integrity (CI) levels and scramble ratios (SR).  The plots show the layer-wise SemRecScore for each model, illustrating how well the models reconstruct the original word meaning from scrambled versions. The top row shows the performance when there is no word scrambling (SR=0), and the bottom row demonstrates performance when CI is 0.5. The legend clarifies the different CI levels used in the experiment. The results indicate that variations in CI have minimal effect on semantic reconstruction, implying that word form plays a more significant role than context in LLMs' ability to process scrambled text.", "section": "6 How Word Form and Contextual Information Influence LLMs\u2019 Semantic Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.01714/x8.png", "caption": "Figure 8: Semantic Reconstruction Performance across Different LLM Scales and Scramble Ratio Levels.\nThe plots illustrate the layer-wise Semantic Reconstruction Score (SemRecScore) for various CI values across different LLaMA models (1B, 3B, and 70B). The top row represents SR = 0.75, while the bottom row represents CI = 1. The legend indicates different CI conditions.The close alignment of curves across different CI values suggests that Context Integrity has a limited impact on semantic reconstruction. In the rows with higher SR, all curves are noticeably lower, confirming that Word Form plays a crucial role in semantic reconstruction.", "description": "Figure 8 displays the results of an experiment evaluating the impact of word form and context on semantic reconstruction using three different sizes of the LLaMA language model.  The experiment manipulated the Scramble Ratio (SR), representing the degree of letter scrambling within words, and Context Integrity (CI), indicating the amount of surrounding context.  The figure plots the Semantic Reconstruction Score (SemRecScore) across different layers of each model.  The top row shows results with a Scramble Ratio of 0.75, while the bottom row presents results when the context is complete (CI=1).  The close similarity of lines within each row (different CI levels) demonstrates the minor effect of context on semantic reconstruction.  The noticeably lower SemRecScores in the higher SR rows highlight the crucial role of word form in the model's ability to reconstruct the meaning of scrambled words.", "section": "6 How Word Form and Contextual Information Influence LLMs\u2019 Semantic Reconstruction"}, {"figure_path": "https://arxiv.org/html/2503.01714/x9.png", "caption": "Figure 9: Heatmaps of attention allocation to word form in the LLaMA-3.2-3B-Instruct across Scramble Ratios (SR). The x-axis denotes attention heads, and the y-axis denotes layers. Specific heads consistently focus on word form, with higher SR activating more form-sensitive heads, indicating a structured and stable processing mechanism.", "description": "This figure displays heatmaps visualizing how attention is distributed across different attention heads within the LLaMA-3.2-3B-Instruct model when processing words with varying degrees of character scrambling (Scramble Ratio or SR).  Each heatmap represents a different SR level (0, 0.5, and 1, indicating no scrambling, moderate scrambling, and extreme scrambling, respectively). The x-axis represents the attention heads, while the y-axis shows the layers of the model.  The color intensity of each cell indicates the magnitude of attention allocated to word-form features by that specific attention head at that layer. The heatmaps show that certain attention heads consistently focus on word-form information across all SR levels. Furthermore, as the SR increases (more scrambling), more attention heads become dedicated to processing word-form information, suggesting a structured and consistent mechanism for handling word-form processing even under significant disruption.", "section": "7.2 Form-Sensitive Attention Heads"}, {"figure_path": "https://arxiv.org/html/2503.01714/x10.png", "caption": "Figure 10: Heatmaps of attention allocation to word form in the LLaMA-3.3-70B-Instruct across Scramble Ratios (SR). The x-axis denotes attention heads, and the y-axis denotes layers. Specific heads consistently focus on word form, with higher SR activating more form-sensitive heads, indicating a structured and stable processing mechanism.", "description": "This figure visualizes how attention is distributed across different attention heads within the LLaMA-3.3-70B-Instruct language model when processing words with varying degrees of character scrambling (Scramble Ratio, SR).  The heatmaps show the attention weight assigned by each attention head to each token in a scrambled word across different layers of the model. Warmer colors indicate higher attention weights.  The consistent patterns observed across different SR levels demonstrate that specific attention heads consistently focus on word form information, even as the level of scrambling increases. This illustrates a structured and stable processing mechanism within the LLM where certain heads are specialized for handling word form regardless of the degree of perturbation.", "section": "7.2 Form-Sensitive Attention Heads"}]