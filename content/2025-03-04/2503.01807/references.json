{"references": [{"fullname_first_author": "Zhou", "paper_title": "LIMA: Less Is More for Alignment", "publication_date": "2023-01-01", "reason": "This reference is important because it highlights the effectiveness of carefully curated datasets in instruction tuning, demonstrating that smaller, high-quality datasets can outperform larger, noisier ones."}, {"fullname_first_author": "Wang", "paper_title": "How far can camels go? exploring the state of instruction tuning on open resources", "publication_date": "2023-01-01", "reason": "This reference is crucial as it represents one of the families of open-source state-of-the-art dataset and model, T\u00dcLU 2, that this paper extends its experimental design off, highlighting its relevance for modern instruction-tuned language models."}, {"fullname_first_author": "Lambert", "paper_title": "T\u00fclu 3: Pushing frontiers in open language model post-training", "publication_date": "2024-11-15", "reason": "This reference is crucial as it represents a state-of-the-art model and the dataset, T\u00dcLU 3, that this paper uses for comparison, providing insights into performance with different data selection methods."}, {"fullname_first_author": "Xia", "paper_title": "LESS: Selecting influential data for targeted instruction tuning", "publication_date": "2024-01-01", "reason": "This reference is significant as it discusses a gradient-based influence method for data selection, which this paper compares against and finds to be outperformed by simpler embedding methods at larger scales."}, {"fullname_first_author": "Yin", "paper_title": "Compute-constrained data selection", "publication_date": "2024-01-01", "reason": "This reference is important because it studies how well data selection techniques work for instruction tuning when compute is constrained, a key aspect explored in the current paper regarding the scaling properties of these methods."}]}