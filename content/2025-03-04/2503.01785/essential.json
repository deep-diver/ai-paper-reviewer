{"importance": "This paper introduces Visual-RFT, a novel approach to fine-tuning LVLMs that significantly improves performance with limited data. **It offers a data-efficient, reward-driven method for enhancing reasoning and adaptability**, opening new avenues for research in visual perception and multi-modal learning.", "summary": "Visual-RFT: Enhance LVLMs' visual reasoning via reinforcement learning with verifiable rewards, achieving strong performance with limited data.", "takeaways": ["Visual-RFT enhances LVLMs in visual tasks through reinforcement learning.", "Verifiable reward functions improve data efficiency in visual perception tasks.", "Visual-RFT demonstrates strong generalization in few-shot and open-vocabulary settings."], "tldr": "Large Reasoning Models(LRMs) need Reinforcement Fine-Tuning(RFT) to learn. RFT is useful when fine-tuning data is scarse. But the application in multi-modal domains remains under-explored. Thus, this paper introduces Visual Reinforcement Fine-Tuning. It extends the application areas of RFT on visual tasks. It uses Large Vision-Language Models(LVLMs) to generate multiple responses and uses visual perception verifiable reward functions to update the model. \n\nVisual-RFT offers data-efficient, reward-driven approach that enhances reasoning and adaptability for domain-specific tasks. It achieves better performance than Supervised Fine-Tuning on tasks such as Open Vocabulary/Few-shot Detection, Reasoning Grounding, and Fine-grained Classification. Visual-RFT improves accuracy by 24.3% in one-shot fine-grained image classification with around 100 samples. In few-shot object detection, Visual-RFT also exceeds the baseline.", "affiliation": "Shanghai Jiaotong University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.01785/podcast.wav"}