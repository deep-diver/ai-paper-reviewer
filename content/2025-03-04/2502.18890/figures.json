[{"figure_path": "https://arxiv.org/html/2502.18890/x1.png", "caption": "Figure 1: Comparison of the time taken to generate 100K tokens using autoregressive (AR) and TokenSwift with prefix length of 4096 on LLaMA3.1-8b. As seen, TokenSwift accelerates the AR process from nearly 5 hours to just 90 minutes.", "description": "This figure compares the time taken to generate 100,000 tokens using two different methods: autoregressive (AR) generation and the TokenSwift method.  The experiment was conducted using the LLaMA3.1-8b language model with a prefix length of 4096 tokens. The graph clearly shows that TokenSwift significantly reduces the generation time, from nearly 5 hours (around 300 minutes) for the AR method to just 90 minutes for the TokenSwift method. This demonstrates the substantial acceleration achieved by TokenSwift in generating ultra-long sequences.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.18890/x2.png", "caption": "Figure 2: Illustration of TokenSwift Framework. First, target model (LLM) with partial KV cache and three linear layers outputs 4 logits in a single forward pass. Tree-based attention is then applied to construct candidate tokens. Secondly, top-k\ud835\udc58kitalic_k candidate 4444-grams are retrieved accordingly. These candidates compose draft tokens, which are fed into the LLM with full KV cache to generate target tokens. The verification is performed by checking if draft tokens match exactly with target tokens (Algorithm\u00a01). Finally, we randomly select one of the longest valid draft tokens, and update n\ud835\udc5bnitalic_n-gram table and KV cache accordingly.", "description": "The TokenSwift framework accelerates ultra-long sequence generation by using a draft-then-verify approach.  First, a target language model (LLM) with a limited key-value (KV) cache and three linear layers generates multiple prediction logits simultaneously. Tree-based attention selects the top-k 4-grams as candidate tokens. These are combined to form draft tokens, which are then passed to the full LLM (with complete KV cache) to generate target tokens.  The draft and target tokens are compared; if they match exactly, the generation is deemed successful.  Finally, the longest valid draft token is randomly chosen, and the n-gram table and KV cache are updated for the next iteration. This process minimizes frequent model reloads, streamlines KV management, and improves generation speed while preserving accuracy.", "section": "3. TOKENSWIFT"}, {"figure_path": "https://arxiv.org/html/2502.18890/x3.png", "caption": "Figure 3: Upper: The acceptance rate \u03b1\ud835\udefc\\alphaitalic_\u03b1 for k=20\ud835\udc5820k=20italic_k = 20 and k=0\ud835\udc580k=0italic_k = 0, along with the n\ud835\udc5bnitalic_n-gram acceptance rate \u03b2\ud835\udefd\\betaitalic_\u03b2 for k=20\ud835\udc5820k=20italic_k = 20, plotted against varying generation lengths. Lower: The speedup \u00d7\\times\u00d7 achieved at different generation lengths.", "description": "This figure compares the performance of TOKENSWIFT with different n-gram candidates (k=0 and k=20) across varying generation lengths. The upper panel shows the acceptance rate (\u03b1) for both k=0 and k=20, and the n-gram acceptance rate (\u03b2) for k=20.  The lower panel displays the speedup (\u00d7) achieved by TOKENSWIFT compared to the autoregressive baseline (AR) for different generation lengths.  This visualization helps demonstrate how the acceptance rate and speedup of TOKENSWIFT change with varying generation length and number of n-gram candidates.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.18890/x4.png", "caption": "Table 5: The ablation experiment results on KV management.", "description": "This table presents the ablation study results focusing on the impact of different KV (key-value) cache management strategies on the performance of the TOKENSWIFT model.  It shows the acceptance rate (\u03b1) and speedup (\u00d7) achieved by three different KV management methods: Full Cache, Partial Cache, and Dynamic Partial Cache, for generating sequences of varying lengths (20K, 40K, 60K, 80K, and 100K tokens). The results illustrate the trade-offs between the efficiency of the KV management methods and the model's overall performance in terms of acceptance rate and speedup.", "section": "4.3.2. Dynamic KV Updates"}, {"figure_path": "https://arxiv.org/html/2502.18890/x5.png", "caption": "Table 6: The ablation experiment results on contextual penalty using different sampling methods. Light cell represents the settings adopted by TokenSwift. We take \u03b8=1.2,W=1024formulae-sequence\ud835\udf031.2\ud835\udc4a1024\\theta=1.2,W=1024italic_\u03b8 = 1.2 , italic_W = 1024.", "description": "This table presents an ablation study on the impact of contextual penalty and different sampling methods on the performance of the TokenSwift model.  The experiment varied the contextual penalty parameter (\u03b8) and penalty window size (W), while using top-p, min-p, and n-sampling methods. The table shows the Distinct-n scores (measuring text diversity) for each combination of these parameters. The highlighted cells indicate the parameter settings used in the main TokenSwift model (\u03b8=1.2 and W=1024). The results demonstrate the effect of contextual penalty on reducing repetition in generated text, and its interaction with different sampling techniques.", "section": "4.3. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.18890/x6.png", "caption": "Figure 4: Upper: The acceptance rate \u03b1\ud835\udefc\\alphaitalic_\u03b1 and n\ud835\udc5bnitalic_n-gram acceptance rate \u03b2\ud835\udefd\\betaitalic_\u03b2 versus varying k\ud835\udc58kitalic_k. Lower: The speedup \u00d7\\times\u00d7 versus varying k\ud835\udc58kitalic_k.", "description": "This figure shows the effects of varying the number of n-gram candidates (k) on the performance of the TOKENSWIFT model. The top panel displays the acceptance rate (\u03b1) and n-gram acceptance rate (\u03b2) as functions of k.  The bottom panel shows the speedup (\u00d7) achieved by TOKENSWIFT relative to a standard autoregressive (AR) generation method, also as a function of k.  The plots reveal the trade-off between using a larger number of candidates (improving acceptance rates) and the computational cost of evaluating more candidates (decreasing speedup).", "section": "4.3.1. TOKEN REUTILIZATION"}, {"figure_path": "https://arxiv.org/html/2502.18890/x7.png", "caption": "Table 7: Acceptance rate \u03b1\ud835\udefc\\alphaitalic_\u03b1 (k=0\ud835\udc580k=0italic_k = 0) and speedup \u00d7\\times\u00d7 across different tree configurations. Each configuration is represented by a 4-digit array: they represent the number of candidates for different decoding heads in\u00a0\u00a73.2.", "description": "Table 7 presents the results of an ablation study investigating the impact of different tree configurations on the performance of the TOKENSWIFT model in accelerating ultra-long sequence generation. The table shows the acceptance rate (\u03b1) and speedup (\u00d7) achieved with different tree configurations, where each configuration is represented by a 4-digit array. Each digit in the array specifies the number of candidate tokens considered for a given decoding head during the tree-based attention mechanism described in Section 3.2 of the paper. The study evaluates various configurations, comparing them to assess the balance between computational efficiency and accuracy.  The results are useful in selecting the optimal tree structure for the model.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.18890/x8.png", "caption": "Table 8: Distinct-n\ud835\udc5bnitalic_n score across different penalty value \u03b8\ud835\udf03\\thetaitalic_\u03b8. 1.01.01.01.0 indicate that no penalty is applied. We take W=1024\ud835\udc4a1024W=1024italic_W = 1024 (See Section\u00a0F.3 for ablation on W\ud835\udc4aWitalic_W).", "description": "This table presents the Distinct-n scores for different values of the contextual penalty parameter (\u03b8).  Distinct-n measures the diversity of generated text; higher scores indicate greater diversity and less repetition.  The experiment uses a penalty window size (W) of 1024 tokens.  A value of \u03b8 = 1.0 indicates that no contextual penalty is applied.  This table helps demonstrate the impact of the contextual penalty on the diversity of generated text when generating ultra-long sequences, as discussed in Section 4.3.4 of the paper.  The effect of different penalty window sizes (W) is explored further in Section F.3.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.18890/x9.png", "caption": "Figure 5: Case Study on LLaMA3.1-8b. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. The blue text is repetition part. See Appendix\u00a0G for more cases.", "description": "This figure presents a case study to illustrate the impact of the contextual penalty on the generated text quality of LLaMA3.1-8b model. The left side shows the generated text without applying the contextual penalty, while the right side presents the text generated with the contextual penalty. The repeated sentences in both examples are highlighted in blue for easy identification.  The figure demonstrates how the contextual penalty helps reduce the repetition problem often encountered in long text generation. More examples can be found in Appendix G.", "section": "4.4.4. Case Study"}, {"figure_path": "https://arxiv.org/html/2502.18890/x10.png", "caption": "Figure 6: Case Study on YaRN-LLaMA2-7b-128k. Left: fragments of generated text without Contextual Penalty. Right: fragments of generated text with Contextual Penalty. The blue text is repetition part.", "description": "This figure showcases a comparative analysis of text generation using the YaRN-LLaMA2-7b-128k model with and without the contextual penalty mechanism. The left side displays text generated without the penalty, highlighting instances of repetitive phrases marked in blue. In contrast, the right side presents text generated with the contextual penalty applied, demonstrating a significant reduction in repetitive content. This comparison visually illustrates the effectiveness of the contextual penalty in enhancing the diversity and fluency of ultra-long text generation.", "section": "4.4.4. CASE STUDY"}]