[{"figure_path": "https://arxiv.org/html/2502.18965/x1.png", "caption": "Figure 1. (a) Our proposed unified architecture for end-to-end generation. (b) A typical cascade ranking system, which includes three stages from the bottom to the top: Retrieval, Pre-ranking, and Ranking.", "description": "Figure 1 illustrates two different recommendation system architectures. (a) shows the proposed OneRec model, which is a unified end-to-end architecture for generating recommendations. This model directly generates a list of recommended items, unlike traditional systems.  (b) depicts a typical cascade ranking system, which uses a three-stage pipeline: Retrieval (identifying a large set of candidates), Pre-ranking (filtering the candidates to a smaller subset), and Ranking (ordering the remaining candidates). This figure visually contrasts the simplicity and directness of OneRec with the complexity of the traditional approach.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.18965/x2.png", "caption": "Figure 2. The overall framework of OneRec, consists of two stages: (i) the session training stage which train OneRec with session-wise data; (ii) the IPA stage which utilizes iterative direct preference optimization with self-hard negatives.", "description": "This figure illustrates the two-stage training process of the OneRec model. The first stage focuses on training OneRec using session-wise data, which means that the model learns to generate relevant sequences of videos for each user session. The second stage employs an Iterative Preference Alignment (IPA) module which leverages iterative direct preference optimization using self-hard negatives to improve the quality of generated recommendations.  Self-hard negatives are generated from the beam search results, ensuring high-quality preference pairs are used to refine the model's preferences. This iterative refinement process aims to align the model's generated recommendations more closely with actual user preferences.", "section": "3 Methods"}, {"figure_path": "https://arxiv.org/html/2502.18965/x3.png", "caption": "Figure 3. Framework of Online Deployment of OneRec.", "description": "This figure illustrates the online deployment architecture of the OneRec model. It shows how the trained model parameters are synchronized to both an online inference model and a DPO sample server.  The online inference model serves user requests in real-time, while the DPO sample server provides preference data for model updates. The system also includes modules for log collection, preprocessing, and distributed training. The architecture is optimized for efficiency and stability, using techniques like key-value caching, float16 quantization, and beam search.", "section": "System Deployment"}, {"figure_path": "https://arxiv.org/html/2502.18965/x4.png", "caption": "Figure 4. The ablation study on DPO sample ratio rDPOsubscript\ud835\udc5fDPOr_{\\rm DPO}italic_r start_POSTSUBSCRIPT roman_DPO end_POSTSUBSCRIPT. The results indicate that a 1% ratio of DPO training leads to significant gains but further increase the sample ratio results in limited improvements.", "description": "This ablation study investigates the impact of varying the DPO (Direct Preference Optimization) sample ratio on model performance. The x-axis represents the DPO sample ratio, ranging from 1% to 5%.  The y-axis displays the resulting performance metrics for different aspects of the recommendation system (e.g., session watch time, view probability, follow probability, like probability).  The results demonstrate that increasing the sample ratio from 1% yields only marginal performance improvements, indicating a diminishing return.  A 1% sample ratio is identified as the optimal balance between performance gain and computational efficiency.  Beyond this point, the additional computational cost outweighs any minor performance increases.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2502.18965/x5.png", "caption": "Figure 5. The visualization of the probability distribution of the softmax output for each layer of the semantic ID. The red star represents the sematic ID of item which has the highest reward value.", "description": "Figure 5 presents a detailed visualization of the probability distributions generated by the softmax layer for each level of semantic IDs within the OneRec model.  The probability distributions show how the model assigns probabilities to different semantic IDs at various stages of processing. Each plot displays the distribution for a particular layer, illustrating how the model's confidence in certain semantic IDs evolves as it processes the data across different layers. The red star highlights the specific semantic ID that receives the highest reward value from the reward model, indicating the model's top choice at that layer. This visualization effectively demonstrates the hierarchical refinement process within the model as it progresses towards a final prediction, providing insight into how the uncertainty and confidence of the model change as more context is considered.", "section": "5.4 Prediction Dynamics of OneRec"}, {"figure_path": "https://arxiv.org/html/2502.18965/extracted/6233922/figs/fig5.jpg", "caption": "Figure 6. Scalability of OneRec on model scaling. The results show that OneRec constantly benefits from performance improvement when the parameters are scaled up.", "description": "Figure 6 demonstrates the impact of model size on OneRec's performance.  Multiple lines graph the performance against increasing model parameters (x-axis) for various metrics, including accuracy on different layers (Layer 1, Layer 2, Layer 3) and training loss.  The results show a consistent positive correlation between model size and performance across all metrics, indicating OneRec effectively leverages increased model capacity to improve accuracy and reduce loss.", "section": "5 Experiment"}]