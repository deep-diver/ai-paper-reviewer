[{"figure_path": "https://arxiv.org/html/2503.01370/x1.png", "caption": "Figure 1: A 3D Harry Potter scene built with Kiss3DGen. Our proposed framework, KISS3DGen, is a unified 3D generation framework that facilitates various 3D generation tasks, including text-to-3D, image-to-3D, 3D enhancement, editing and more. Specifically, most of the assets in the figure is generated from text (captioned with abbreviated text prompts) or image (marked by dash lines) conditions, while the main characters (Hermoine, Ron and Potter) are created using a hybrid pipeline that combines image-to-3D and text-guided mesh editing. Please zoom in for details and refer to our main paper for a more introduction.", "description": "This figure showcases a 3D scene from the Harry Potter universe, created using the Kiss3DGen framework.  Kiss3DGen is a versatile 3D generation tool capable of producing 3D models from text descriptions, images, or a combination of both.  The scene demonstrates various features of the framework.  Many objects were generated from short text prompts (e.g., \"A red sofa\"), while some assets were created by converting images into 3D models (indicated by dashed lines).  The main characters, Hermione, Ron, and Harry Potter, were created using a more complex process:  an image was converted to a 3D model and further refined with text-based mesh editing. This highlights Kiss3DGen's ability to handle different generation approaches and tasks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.01370/x2.png", "caption": "Figure 2: The overview of our text-to-3D training and generation framework.\nIn this work, we curate a high-quality text-3D dataset, then train a LoRA\u00a0[15] layer for text to 3D bundle image (Sec.\u00a03) generation upon a pretrained text-to-image diffusion transformer model with flow matching\u00a0[25].\nAfter training, our framework generates 3D assets with text condition in two stages: the 3D-Bundle-Image (Sec.\u00a03) generation (Stage I) and the 3D reconstruction (Stage II). In Stage I, we generate 3D bundle image with our Kiss3DGen base model guided by text prompts. In Stage II, we reconstruct the geometry and texture of the 3D asset via LRM\u00a0[60, 14] or sphere initialization followed by optimization-based mesh refinement and texture projection approach, i.e., ISOMER\u00a0[56]. Please zoom in for details.", "description": "This figure illustrates the two-stage process of the Kiss3DGen framework for text-to-3D generation.  First, a high-quality text-3D dataset is created and used to train a LoRA (Low-Rank Adaptation) layer on a pre-trained text-to-image diffusion transformer model. This model is enhanced using flow matching.  The resulting model, Kiss3DGen-base, generates a 3D Bundle Image (a multi-view representation combining RGB and normal maps) from text prompts (Stage I).  In Stage II, this 2D image is processed to reconstruct a 3D mesh and texture. Reconstruction uses either LRM (Large Reconstruction Model) or sphere initialization, followed by mesh refinement and texture projection with ISOMER.  The output is a complete 3D model. The figure visually depicts each step of the process, including data preparation, model training, and the two-stage generation.", "section": "3. Proposed Method"}, {"figure_path": "https://arxiv.org/html/2503.01370/x3.png", "caption": "Figure 3: 3D enhancement and editing with Kiss3DGen. In order to achieve high-quality image-to-3D generation, we incorporate the existing image-to-3D pipeline\u00a0[60] with our general 3D enhancement pipeline. Please zoom in for details.", "description": "Figure 3 demonstrates Kiss3DGen's capabilities in 3D enhancement and editing.  It showcases how a low-quality input mesh (or a coarse mesh from a 2D image) can be significantly improved using ControlNet, a technique that integrates additional information to guide the generation process.  Specifically, the figure illustrates 3D enhancement by refining a mesh with blurry textures and geometry lacking detail (subfigures (a) and (b)).  It also shows 3D editing, where attributes like color, shape, and expression are modified from a base mesh through caption-based editing (subfigures (c) and (d)). This highlights Kiss3DGen's ability to handle both enhancement and targeted edits, leveraging a combination of its base 3D generation model and the ControlNet framework.", "section": "3.2 Kiss3DGen-ControlNet"}, {"figure_path": "https://arxiv.org/html/2503.01370/x4.png", "caption": "Figure 4: Qualitative comparisons with MVDream\u00a0[48] for text-to-multiview generation. Within the context of text-conditioned multi-view generation, our method produces significantly better results in both text-image alignment and geometric coherence.", "description": "Figure 4 presents a qualitative comparison of multi-view image generation results between the proposed method, Kiss3DGen, and the existing MVDream method [48].  The comparison focuses on text-to-image generation, where a textual description is used to generate multiple views of an object. The figure visually demonstrates that Kiss3DGen achieves significantly superior results in two key aspects:  First, the generated images from Kiss3DGen show stronger alignment with the input text descriptions, meaning the generated images more accurately reflect the content described in the text.  Second, Kiss3DGen produces images with improved geometric coherence.  This means the generated views of the object are more consistent and realistic across different viewpoints, leading to a more unified and believable 3D representation.  The figure uses example text prompts and corresponding generated images from both methods to visually illustrate the differences in image quality and consistency.", "section": "4.4. Comparison with State-of-the-Art Methods"}, {"figure_path": "https://arxiv.org/html/2503.01370/x5.png", "caption": "Figure 5: Qualitative comparisons with state-of-the-art methods for text-to-3D generation. It demonstrates that Kiss3DGen achieves the highest quality 3D mesh, delivering more accurate texture generation from the input prompts compared to others.", "description": "Figure 5 presents a qualitative comparison of 3D models generated by Kiss3DGen and other state-of-the-art text-to-3D generation methods.  For each method, several example 3D objects are shown, illustrating the quality of mesh generation and the accuracy of texture mapping from text prompts. The figure visually demonstrates that Kiss3DGen outperforms other approaches in generating high-quality 3D meshes and accurately reflecting the textual descriptions in the generated textures.", "section": "4.4 Comparison with State-of-the-Art Methods"}, {"figure_path": "https://arxiv.org/html/2503.01370/x6.png", "caption": "Figure 6: Text-to-3D generation comparison between our Base and Doll models (Sec.\u00a04.1). Each model generates different results with different seeds. All images are rendered from 3D mesh.", "description": "This figure compares the results of text-to-3D generation using two different models: the base model and a specialized doll model.  The same text prompt (\"A realistic photo of a cute cat, orange fur, smiling, sitting with body straight up, rich details\") was used for both models.  Multiple generations were produced using each model with different random seeds to illustrate the variability in the outputs.  All images shown are renderings of the generated 3D mesh.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.01370/x7.png", "caption": "Figure 7: Qualitative comparison on 3D mesh Enhancement and Editing with MVEdit\u00a0[3]. Our results (line 2) maintain significantly better consistency with the input mesh in enhancement and better align with the given text condition in editing.", "description": "This figure compares the results of 3D mesh enhancement and editing using the proposed Kiss3DGen model and the baseline MVEdit [3] method.  The top row shows the input image and the target edits specified using text prompts. The second row presents the results generated by Kiss3DGen.  The third row displays the results from MVEdit.  The comparison highlights that Kiss3DGen produces significantly better results in terms of maintaining consistency with the input mesh during the enhancement process. Moreover, Kiss3DGen achieves better alignment with the text conditions provided for editing tasks, leading to more accurate and targeted modifications to the 3D mesh.", "section": "3D Enhancement and Editing"}, {"figure_path": "https://arxiv.org/html/2503.01370/x8.png", "caption": "Figure 8: Qualitative comparisons with state-of-the-art methods for image-to-3D generation. Our framework achieves the highest quality 3D mesh, delivering more accurate and realistic texture generation from input images compared to other models.", "description": "Figure 8 presents a qualitative comparison of image-to-3D generation results produced by the proposed Kiss3DGen model and several state-of-the-art methods.  The figure showcases input images alongside their corresponding 3D reconstructions generated by each method. The comparison highlights Kiss3DGen's superior ability to generate high-quality 3D meshes with accurate and realistic textures.  The differences in detail, texture quality, and overall fidelity of the 3D models are clearly illustrated, demonstrating the effectiveness of the proposed approach compared to existing techniques.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.01370/x9.png", "caption": "Figure 9: Ablating the mechanisms of generating multiview RGB and normal maps. Both our \u201c3D Bundle Image\u201d and \u201cSwitcher\u201d\u00a0[30, 20] are built upon Flux.1-dev\u00a0[1] model.", "description": "This figure compares two different methods for generating multi-view RGB and normal maps for 3D object representation, both based on the Flux.1-dev model.  The first method, '3D Bundle Image', combines RGB and normal map information into a single image, while the second method, 'Switcher', generates them separately and combines them later.  The figure visually demonstrates the advantages of the '3D Bundle Image' approach by showcasing its superior consistency and coherence in the resulting multi-view representations compared to the 'Switcher' method. This highlights the importance of the proposed 3D Bundle Image approach for effective 3D object generation.", "section": "3. Proposed Method"}]