[{"figure_path": "https://arxiv.org/html/2501.10021/x1.png", "caption": "Figure 1: \nWe leverage a pretrained diffusion UNet backbone for controlled human image animation, enabling expressive dynamic details and precise motion control. Specifically, we introduce a dynamics adapter D\ud835\udc37Ditalic_D that seamlessly integrates the reference image context as a trainable residual to the spatial attention, in parallel with the denoising process, while preserving the original spatial and temporal attention mechanisms within the UNet. In addition to body pose control via a ControlNet CPsubscript\ud835\udc36\ud835\udc43C_{P}italic_C start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT , we introduce a local face control module CFsubscript\ud835\udc36\ud835\udc39C_{F}italic_C start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT that implicitly learns facial expression control from a synthesized cross-identity face patch. We train our model on a diverse dataset of human motion videos and natural scene videos simultaneously. Our model achieves remarkable transfer of body poses and facial expressions, as well as highly vivid and detailed dynamics for both the human and the scene.", "description": "This figure illustrates the X-Dyna architecture, a diffusion-based model for human image animation. It uses a pretrained diffusion UNet, enhanced with a Dynamics Adapter (D) to integrate reference image context into spatial attention, improving dynamic detail generation without sacrificing pose control.  A ControlNet (CP) handles body pose, while a local face control module (CF) manages facial expressions using synthesized cross-identity face patches. The model is trained on a diverse dataset of human motion and natural scene videos, resulting in realistic animations with accurate pose and expression transfer.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.10021/x2.png", "caption": "Figure 2:  a) IP-Adapter\u00a0[50] can generate vivid texture from the reference image but fails to preserve the appearance. b) Though ReferenceNet\u00a0[16] can preserve the identity from the human reference, it generates a static background without any dynamics. c) Dynamics-Adapter provides both expressive details and consistent identities.", "description": "Figure 2 illustrates a comparison of three different methods for incorporating reference images into a diffusion model for human image animation. Panel (a) shows the results using IP-Adapter, which generates vivid textures but struggles to preserve the appearance of the reference image. Panel (b) displays the results obtained using ReferenceNet, which successfully maintains identity but generates static backgrounds and lacks dynamic elements. Finally, panel (c) presents the results achieved using the proposed Dynamics-Adapter, demonstrating its ability to produce both expressive details and consistent identities.", "section": "3.1 Preliminary"}, {"figure_path": "https://arxiv.org/html/2501.10021/x3.png", "caption": "Figure 3: a) IP-Adapter\u00a0[50] encodes the reference image as an image CLIP embedding and injects the information into the cross-attention layers in SD as the residual. b) ReferenceNet\u00a0[16] is a trainable parallel UNet and feeds the semantic information into SD via concatenation of self-attention features. c) Dynamics-Adapter encodes the reference image with a partially shared-weight UNet. The appearance control is realized by learning a residual in the self-attention with trainable query and output linear layers. All other components share the same frozen weight with SD.", "description": "Figure 3 illustrates three different approaches to integrating reference image information into a Stable Diffusion (SD) model for controlled image generation.  (a) shows the IP-Adapter method, where the reference image is encoded as a CLIP embedding and added as a residual to the cross-attention layers of the SD UNet. (b) depicts ReferenceNet, which uses a parallel, trainable UNet to process the reference image and concatenate its self-attention features with those of the SD UNet. (c) presents the Dynamics-Adapter, a more efficient method that uses a partially shared-weight UNet to process the reference image and learns a residual within the self-attention mechanism of the SD UNet using trainable query and output layers, while keeping the rest of the SD UNet's weights frozen.", "section": "3.1. Preliminary"}, {"figure_path": "https://arxiv.org/html/2501.10021/x4.png", "caption": "Figure 4: Qualitative Comparison on Human in Dynamic Scene. While existing SOTA methods struggle to generate consistent and realistic scene dynamics involving humans, our method successfully produces dynamic human-scene interactions while preserving the structure of the reference image.", "description": "This figure shows a qualitative comparison of human image animation results in dynamic scenes.  It compares the output of the proposed X-Dyna method against several state-of-the-art (SOTA) techniques. The goal is to highlight X-Dyna's ability to generate realistic human and scene dynamics while maintaining the original image structure. The comparison demonstrates that unlike the existing methods, X-Dyna successfully generates consistent and lifelike interactions between the animated human and their environment.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10021/x5.png", "caption": "Figure 5: Qualitative Comparison on Poses and Face Expressions Control. We show each method on test cases using the same reference image and pose skeleton. For improved visualization, a zoomed-in view of the face area is also provided. Our method produces results that most closely match the ground truth and best preserve face identity.", "description": "This figure compares the results of different human image animation methods on pose and facial expression control.  Each method is tested using the same reference image and pose skeleton.  Zoomed-in views of the face are included for detailed comparison. The results show that the proposed method (X-Dyna) most accurately matches the ground truth and best preserves the identity of the subject.", "section": "4. Experiments"}]