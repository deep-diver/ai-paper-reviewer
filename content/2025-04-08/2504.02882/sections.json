[{"heading_title": "DiaTool-DPO", "details": {"summary": "DiaTool-DPO centers around **improving tool-augmented language models (TA-LLMs)**, with a focus on dialogue capabilities. It introduces a novel method using **Direct Preference Optimization** to address challenges like incomplete queries. A key idea is modeling TA-LLM interactions as a **Markov Decision Process** with distinct dialogue states and query categorization based on transition trajectories. Paired trajectory datasets of correct and incorrect dialogue flows are automatically constructed. The effort includes a specialized objective loss function for dialogue control. Evaluations show the approach improves performance on tasks like information gathering and tool call rejection, suggesting a method to handle diverse scenarios without expert demonstrations. The **DiaTool-DPO dataset and algorithm** enhance conversational abilities by controlling dialogue flow through contrasts between chosen and rejected trajectories."}}, {"heading_title": "TA-LLMs States", "details": {"summary": "**TA-LLMs internal states are formulated as Markov Decision Process(MDP)**. There are 5 internal states defined in this work. The **TA-LLMs move from initial state to tool selected with complete slots, and finally complete the task**. The states are traversed based on the query type as not all states are visited in every interaction. The initial state starts without any context, whereas complete occurs after receiving output. There also exists some intermediate states such as tool selected without complete slots, and wait for tool response."}}, {"heading_title": "Data Creation", "details": {"summary": "The paper introduces a novel data creation methodology for training tool-augmented language models using Direct Preference Optimization. They construct the DiaTool-DPO dataset, which consists of paired trajectories of correct and incorrect dialogue flows. **A key aspect is the automatic generation of rejected trajectories by pairing user queries with mismatched conversation trajectories.**  The dataset is designed to improve the model's ability to handle incomplete queries and out-of-scope requests by learning which conversation flow to choose in specific situations, rather than relying solely on expert trajectories. **The data is stratified by difficulty, with Easy and Hard subsets targeting different aspects of conversational abilities.** The process involves modifying existing dialogues and creating new ones to reflect various types of interactions, including those requiring slot-filling and tool call rejection."}}, {"heading_title": "Objective Loss", "details": {"summary": "The objective loss function, denoted as L_align, is designed to optimize the alignment between a reference model (\u03c0_ref) and a trained model (\u03c0_\u03b8) using Direct Preference Optimization (DPO). **It aims to control the conversation flow in TA-LLMs.** This loss function is formulated based on paired trajectories, contrasting chosen trajectories (T_c) with rejected trajectories (T_r), weighted by \u03b2(t,T_c) and \u03b2(t,T_r).  Key innovations include the normalization term \u03c8(T) which accounts for disparities in turn counts between chosen and rejected trajectories, addressing the inherent bias towards longer chosen rewards in slot-filling scenarios and larger rejected rewards for relevant cases. Additionally, a reward gap margin subtraction strategy using the arbitrary margin p, seeks to more effectively facilitate the learning by focusing more on the harder examples. **The objective function provides a structured way to tune TA-LLMs, balancing preferences for different dialogue management strategies by considering trajectory data.**"}}, {"heading_title": "Ablation Study", "details": {"summary": "The ablation study meticulously examines the impact of each component in DiaTool-DPO on performance metrics. **Removing individual components reveals their specific contributions to the overall system performance.** The study contrasts DiaTool-DPO with SFT models, demonstrating the benefit of contrastive learning for contextual differences between actions like slot-filling and tool calls. Key findings include that normalization improves slot scores by reducing artificially inflated reward gaps. However, reward scaling shows minimal impact, potentially due to the teacher-forcing evaluation methodology that mitigates error accumulation. Further investigation shows **improved slot performance by combining both easy and hard datasets** and demonstrates the need for pretraining to establish basic LLM tool calling abilities."}}]