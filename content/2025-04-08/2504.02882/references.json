{"references": [{"fullname_first_author": "Richard Bellman", "paper_title": "A markovian decision process", "publication_date": "1957-01-01", "reason": "This foundational work on Markov Decision Processes provides the theoretical framework for modeling TA-LLM interactions."}, {"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper introduces Direct Preference Optimization, which is leveraged in DiaTool-DPO to improve conversational abilities."}, {"fullname_first_author": "Shuofei Qiao", "paper_title": "Making language models better tool learners with execution feedback", "publication_date": "2024-01-01", "reason": "This work addresses how to learn effective tool usage with language models, a core problem tackled by DiaTool-DPO."}, {"fullname_first_author": "Nisan Stiennon", "paper_title": "Learning to summarize from human feedback", "publication_date": "2020-01-01", "reason": "This work is important as it describes learning to summarize from human feedback, which demonstrates how training with human preference information can be used to improve the performance of models and inspires this work."}, {"fullname_first_author": "Lee, Shinbok", "paper_title": "FunctionChat-Bench: Comprehensive evaluation of language models' generative capabilities in korean tool-use dialogs", "publication_date": "2024-01-01", "reason": "This paper is important because the evaluation methodology is based on the open source FunctionChat-Bench benchmark."}]}