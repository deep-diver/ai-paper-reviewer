[{"figure_path": "https://arxiv.org/html/2504.05298/x2.png", "caption": "Figure 1: TTT layers enable a pre-trained Diffusion Transformer to generate one-minute videos from text storyboards.\nWe use Tom and Jerry cartoons as a proof of concept.\nThe videos tell complex stories with coherent scenes composed of dynamic motion.\nEvery video is produced directly by the model in a single shot, without editing, stitching, or post-processing.\nEvery story is newly created.", "description": "This figure showcases the capability of adding Test-Time Training (TTT) layers to a pre-trained Diffusion Transformer model for generating one-minute long videos from textual storyboards.  Using Tom and Jerry cartoons as an example, the model produces videos that narrate complex, multi-scene stories. The generated videos feature coherent scenes with dynamic movement, and importantly, are created in a single, continuous shot without any manual editing, stitching, or post-processing steps. Each generated video tells a completely novel story.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.05298/x3.png", "caption": "Figure 2: All RNN layers can be expressed as a hidden state that transitions according to an update rule.\nThe key idea in [43] is to make the hidden state itself a model f\ud835\udc53fitalic_f with weights W\ud835\udc4aWitalic_W, and the update rule a gradient step on the self-supervised loss \u2113\u2113\\ellroman_\u2113.\nTherefore, updating the hidden state on a test sequence is equivalent to training the model f\ud835\udc53fitalic_f at test time.\nThis process, known as Test-Time Training (TTT), is programmed into TTT layers.\nFigure and caption taken from [43].", "description": "This figure illustrates the core concept of Test-Time Training (TTT) layers.  RNN layers maintain a hidden state that changes based on an update rule.  The TTT approach modifies this by making the hidden state itself a trainable model (f) with its own weights (W). The update rule then becomes a gradient descent step to minimize a self-supervised loss (\u2113). Consequently, updating the hidden state during testing is akin to training the model f on that test data. This test-time training process is integrated into the TTT layers.", "section": "2. Test-Time Training Layers"}, {"figure_path": "https://arxiv.org/html/2504.05298/x4.png", "caption": "Figure 3: Overview of our approach.\nLeft: Our modified architecture adds a TTT layer with a learnable gate after each attention layer. See Subsection\u00a03.1.\nRight: Our overall pipeline creates input sequences composed of 3-second segments.\nThis structure enables us to apply self-attention layers locally over segments and TTT layers globally over the entire sequence.\nSee Subsection\u00a03.2.", "description": "Figure 3 illustrates the architecture and pipeline of the proposed approach for generating long videos. The left panel shows the modified architecture where a test-time training (TTT) layer with a learnable gate is added after each self-attention layer. This gating mechanism helps to control the influence of the TTT layer on the output, avoiding potential disruptions during the training process. The right panel details the overall pipeline that processes input sequences composed of 3-second segments.  This segmented approach allows for local self-attention within each segment and global TTT across the entire sequence, balancing computational efficiency with the ability to handle long-range dependencies in the video.", "section": "3. Approach"}, {"figure_path": "https://arxiv.org/html/2504.05298/x5.png", "caption": "Figure 4: On-chip Tensor Parallel, discussed in Subsection\u00a03.5.\nLeft: To reduce the memory required on each SM for TTT-MLP, we shard the hidden state W(1)superscript\ud835\udc4a1W^{(1)}italic_W start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT and W(2)superscript\ud835\udc4a2W^{(2)}italic_W start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT across SMs, transferring them between HBM and SMEM only during initial loading and final output.\nRight: We update the hidden state entirely on-chip and use the DSMEM feature on the NVIDIA Hopper GPU architecture to AllReduce intermediate activations among SMs.", "description": "Figure 4 illustrates the on-chip tensor parallelism strategy used to optimize the TTT-MLP model's efficiency on NVIDIA Hopper GPUs.  The left panel shows how the large hidden state (W<sup>(1)</sup> and W<sup>(2)</sup>) is split across multiple Streaming Multiprocessors (SMs) to reduce memory demands on each SM. Data transfer between high-bandwidth memory (HBM) and the smaller, faster SMEM occurs only at the start and end of processing, improving efficiency. The right panel depicts the on-chip computation: the hidden states are updated entirely within the SMs, and the DSMEM feature facilitates efficient AllReduce operations for aggregating intermediate activations across the different SMs.", "section": "3.5 On-Chip Tensor Parallel"}, {"figure_path": "https://arxiv.org/html/2504.05298/x7.png", "caption": "Figure 5: Video frames comparing TTT-MLP against Gated DeltaNet and sliding-window attention, the leading baselines in our human evaluation.\nTTT-MLP demonstrates better scene consistency by preserving details across transitions and better motion naturalness by accurately depicting complex actions.", "description": "Figure 5 showcases a comparison of video frames generated by three different video generation models: TTT-MLP, Gated DeltaNet, and Sliding-window attention.  These models were compared in a human evaluation, with TTT-MLP emerging as a top performer.  The figure highlights the superior performance of TTT-MLP in preserving consistent scenes and achieving more natural motion.  Each row represents a different model, and the frames depict a scene where a cat is eating a pie while a mouse attempts to steal it. The visuals clearly illustrate TTT-MLP's strength in maintaining scene consistency across transitions (such as the cat's movement) and its ability to produce smooth, realistic actions, in contrast to the artifacts present in the other models' output.", "section": "4. Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.05298/x8.png", "caption": "Figure 6: \nFor 63-second videos, inference with full attention (over 300k tokens) would have taken 11\u00d711\\times11 \u00d7 longer than local attention, and training 12\u00d712\\times12 \u00d7 longer, as discussed in Section\u00a01.\nTTT-MLP takes 2.5\u00d72.5\\times2.5 \u00d7 and 3.8\u00d73.8\\times3.8 \u00d7 respectively \u2013 significantly more efficient than full attention, but still less efficient than, for example, Gated DeltaNet, which takes 1.8\u00d71.8\\times1.8 \u00d7 longer than local attention in both inference and training.", "description": "This figure shows a comparison of inference and training times for different video generation models on 63-second videos.  The model using full attention (considering the entire video sequence at once) is shown to be drastically slower (11 times slower for inference and 12 times slower for training) compared to the model using local attention (only focusing on short segments).  The proposed TTT-MLP model shows a significant improvement over full attention (2.5 times faster for inference and 3.8 times faster for training) but is still slower than Gated DeltaNet. This comparison highlights the efficiency gains of using the TTT-MLP method for generating long videos.", "section": "4. Evaluation"}]