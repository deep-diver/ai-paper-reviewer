[{"figure_path": "https://arxiv.org/html/2504.03770/x1.png", "caption": "Figure 1: JailDAM overview (see \u00a73).\n(A)\u00a0Training: We encode safe text and images with CLIP, computing attention scores against a policy-driven unsafe memory bank. An autoencoder learns to reconstruct these features, linking benign inputs to unsafe concepts\u2014without explicit harmful data.\n(B)\u00a0Inference: For each new input, we compute attention scores and measure the autoencoder\u2019s reconstruction error; high error indicates potential harm. If similarity to the memory bank is low, JailDAM updates the least-used concept with a residual representation, adapting to new attacks over time.", "description": "Figure 1 illustrates the JailDAM framework's two main stages: training and inference.  During training (A), safe text and image data are encoded using CLIP, and attention scores are calculated against a memory bank representing unsafe concepts (derived from safety guidelines, not explicit harmful data). An autoencoder learns to reconstruct these features, associating safe inputs with the unsafe concepts in the memory bank. In the inference stage (B), a new input undergoes the same CLIP encoding and attention score calculation. The autoencoder reconstructs the input features, and a high reconstruction error suggests potential harm. If the input's similarity to the memory bank is low, the least-frequently used unsafe concept in the memory bank is updated with a residual representation of the input, allowing the system to adapt to previously unseen attack strategies.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2504.03770/x2.png", "caption": "Figure 2: The pipeline of concepts and memory bank generation by GPT-4o.", "description": "This figure illustrates the process of creating a memory bank of unsafe concepts, which is a crucial component of the JAILDAM framework.  The process begins with utilizing GPT-40, a large language model, to interpret and generate structured representations of unsafe knowledge based on provided harmful content policies.  These policies define various categories of harmful content (e.g., hate speech, violence, illegal activities). GPT-40 extracts key concepts within each category, focusing on crucial features, broader categories (superclasses), and typical contexts where such harmful content might appear. These extracted concepts form the memory bank, which serves as a reference point during the detection of potential jailbreaks.", "section": "3.3 Memory Bank Generation"}, {"figure_path": "https://arxiv.org/html/2504.03770/x3.png", "caption": "Figure 3: JailDAM-D (see \u00a73.6), an end-to-end jailbreak defense framework", "description": "JAILDAM-D is a defense framework composed of two stages. First, the input is evaluated by JAILDAM to determine if it contains harmful content or attack patterns. If harmful content is detected, a defense prompt is automatically added before the original query to warn the target VLM of the potential risks and instruct it to refuse to respond to the harmful request. If the input is determined to be benign, it proceeds to the target VLM without any modifications, allowing the model to respond normally.", "section": "3.6 Application: An End-to-End Attack Defense Framework"}, {"figure_path": "https://arxiv.org/html/2504.03770/x4.png", "caption": "Figure 4: The radar diagrams about F1-score of 4 attack defense methods on 4 VLMs. JailDAM-D outperforms other methods in most settings, except on CogVLM-chat with JailBreakV-28K, where Adashield-A\u00a0(Wang et\u00a0al., 2024) achieves a perfect score.", "description": "This radar chart visualizes the performance of four attack defense methods (Vanilla, FSD, AdaShield-S, AdaShield-A, and JailDAM-D) across four different Vision-Language Models (VLMs): LLaVA-1.5-7B, LLaVA-1.5-13B, CogVLM-chat, and GPT40-mini.  Each axis represents a different dataset (MM-SafetyBench, FigStep, JailBreakV-28K) and shows the F1-score achieved by each defense method on that dataset.  The chart effectively compares the overall effectiveness of the defense methods in mitigating jailbreak attacks, revealing that JailDAM-D generally outperforms the others, with the exception of the CogVLM-chat model tested on the JailBreakV-28K dataset where AdaShield-A achieves a perfect score.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.03770/x5.png", "caption": "Figure 5: Time cost on model training, detection task inference, and defense task inference. In here, A-S: Adashield-S, A-A: Adashield-A, JG: JailGuard, HD: HiddenDetect, GS: GradSafe,LG: LlavaGuard, VG: VLGuard", "description": "This figure presents a comparison of the computational time required for model training, detection task inference, and defense task inference across various methods.  The methods compared are Adashield-S (A-S), Adashield-A (A-A), JailGuard (JG), HiddenDetect (HD), GradSafe (GS), LlavaGuard (LG), and VLGuard (VG).  The figure uses a logarithmic scale to visualize the wide range of time costs, effectively showing how some methods are orders of magnitude more efficient than others in specific tasks.  The bars visually represent the time spent on each stage for each method, providing a direct comparison of their computational efficiency.", "section": "4.3 Time Cost Analysis"}, {"figure_path": "https://arxiv.org/html/2504.03770/x6.png", "caption": "Figure 6: AUROC of detection task on different concept sizes", "description": "This figure shows the Area Under the Receiver Operating Characteristic curve (AUROC) for the jailbreak detection task using different concept pool sizes.  The x-axis represents the number of concepts per harmful category in the memory bank used by the model, ranging from 5 to 100. The y-axis shows the AUROC, a metric that measures the model's ability to distinguish between safe and harmful inputs. The results are shown separately for three datasets: MM-SafetyBench, FigStep, and JailBreakV-28K, illustrating how the performance varies with different concept pool sizes and datasets.  The figure suggests an optimal size of concepts per harmful category balancing model performance and efficiency.", "section": "4.3.2 Ablation on Concepts Size of Each Category"}, {"figure_path": "https://arxiv.org/html/2504.03770/x7.png", "caption": "Figure 7: Concepts Generation Prompt", "description": "This prompt instructs GPT-40 to generate a list of 40-50 key concepts for a given harmful content category.  These concepts should be concise (1-4 words) and fall into three categories: Important Features (key characteristics), Superclasses (higher-level categories the issue belongs to), and Commonly Seen Contexts (typical scenarios where this issue appears).  The prompt provides an example output format to ensure consistency and clarity.", "section": "3.3 Memory Bank Generation"}]