[{"heading_title": "API Audit Problem", "details": {"summary": "The 'API Audit Problem' in the context of LLMs accessed through black-box APIs presents a significant challenge to trust and reliability. Users pay for specific model capabilities (size, performance), but **providers could covertly substitute lower-quality alternatives to reduce costs**. This lack of transparency undermines fairness, complicates benchmarking, and erodes trust. Detecting such substitutions is difficult due to the black-box nature, typically limiting interaction to input-output queries. The problem requires formalization and systematic evaluation of existing verification techniques, like statistical tests, benchmark evaluations, and log probability analysis, under realistic attack scenarios. **Quantization, randomized substitution, benchmark evasion, and limiting information disclosure** are practical attacks that need to be considered. Existing methods relying solely on text outputs face limitations against subtle or adaptive attacks. Stronger guarantees may be available with log probability analysis, but its accessibility is often limited. Hardware-based solutions like Trusted Execution Environments (TEEs) are promising for provable model integrity, but present trade-offs between security, performance, and provider adoption. The problem requires increased transparency, robust auditing techniques, standardized protocols, and user awareness to foster trust in LLM API services."}}, {"heading_title": "Model Sub. Attacks", "details": {"summary": "Model substitution attacks in the context of LLM APIs present a significant threat to users who rely on advertised model capabilities. **Providers may covertly replace a specified model with a cheaper, lower-quality alternative to reduce operational costs**, leading to unfair pricing, eroded trust, and unreliable benchmarking. Detection is challenging due to the black-box nature of APIs, which typically limits interaction to input-output queries. Practical attack scenarios include **quantization substitution**, where a full-precision model is replaced with a quantized version, and **randomized model substitution**, where queries are routed to either the original or a cheaper substitute with a certain probability. These attacks exploit distributional differences or introduce noise to evade detection. **Countermeasures like benchmark evasion**, where providers detect and serve genuine responses to audit queries, and **limiting information disclosure**, where providers restrict access to metadata like log probabilities, further complicate the task of auditing model substitution."}}, {"heading_title": "Text Output Fails", "details": {"summary": "**Text-output-based verification methods face inherent limitations.** Relying solely on generated text proves insufficient for detecting subtle model substitutions like quantization, as classifiers struggle to differentiate outputs. Identity prompting is also easily circumvented via system prompt overrides, highlighting vulnerabilities. Model equality testing, while promising, suffers from reduced statistical power under randomized substitution attacks and exhibits sensitivity to undisclosed decoding parameters and variations in implementation by different API providers. In essence, relying solely on text outputs is **insufficient** due to the inherent randomness, the possibility of adversarial manipulation, and the lack of transparency from providers. The **lack of accuracy** and **robustness** of existing algorithms makes them potentially unable to guarantee which model the user is querying."}}, {"heading_title": "LogProb: Best Signal", "details": {"summary": "The idea of using log probabilities (**logprobs**) as the \"best signal\" for auditing model substitution hinges on their sensitivity to the underlying model's weights. Slight alterations, such as quantization or version changes, will manifest as discernible differences in logprob distributions. This method assumes access to logprobs, which isn't universally offered by API providers. It provides strong guarantee but its applicability is limited by provider's policy(**Table 1**). Practical challenges arise even in controlled environments, where subtle variations in software, hardware, and API configurations can introduce instability in logprob patterns. The potential impact of provider-side optimization techniques (e.g., continuous batching) and the need for user-side reference computations add layers of complexity. However, the reliance on API features makes it susceptible to provider countermeasures, such as selectively disclosing logprobs or employing benchmark evasion tactics."}}, {"heading_title": "TEE: Full Integrity", "details": {"summary": "**Trusted Execution Environments (TEEs) offer a hardware-backed solution for ensuring the integrity of LLM APIs**, a critical aspect in mitigating model substitution attacks. By providing a secure and isolated environment, TEEs guarantee that the code being executed and the data being processed remain confidential and tamper-proof. **This approach shifts the trust anchor from the API provider's operational honesty to the integrity of the underlying hardware and attestation process.** Verifiable measurements of the inference stack and model weights, coupled with open-source software components, establish trust. While TEEs present a robust defense, challenges remain in terms of infrastructure dependency and provider adoption. Establishing trust requires verifiable measurements. Overcoming these hurdles is crucial for realizing the full potential of TEEs in securing LLM APIs and fostering user confidence."}}]