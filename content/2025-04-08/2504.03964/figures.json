[{"figure_path": "https://arxiv.org/html/2504.03964/extracted/6337549/icd.png", "caption": "Figure 1: Medical Code Ontologies Construction: An illustration of structured ontology construction across multiple ICD code versions. Each row represents a distinct medical concept identified by its version-specific code and description, which is then converted into a standardized, descriptive natural language representation. This process facilitates alignment and interoperability across evolving coding schemes. This setup is inspired by methods like (Hegselmann et\u00a0al., 2023; Ono and Lee, 2024) which use text templates to serialize tabular data.", "description": "This figure illustrates how the model handles multiple versions of ICD codes. Each row shows a medical concept with its code and description from different ICD versions. These are then converted into a unified natural language representation to ensure consistency across evolving coding systems.  This approach is inspired by methods that use text templates to format tabular data, facilitating alignment and interoperability across different code versions.", "section": "3.2 Pretraining Data Sources"}, {"figure_path": "https://arxiv.org/html/2504.03964/extracted/6337549/final-tsne-side-by-side2.png", "caption": "Figure 2: ICD-9 tSNE Latent Space Visualization: A tSNE visualization of the ICD 9 Diagnoses codes using modernBERT versus Clinical ModernBERT. This visualization provides the added use of adding the medical code ontologies as a pre-training source to encode coded language seen frequently in clinical practice.", "description": "This figure compares the t-distributed Stochastic Neighbor Embedding (t-SNE) visualizations of ICD-9 diagnosis code embeddings generated by two different models: ModernBERT and Clinical ModernBERT.  The visualizations project the high-dimensional embeddings into a 2D space, allowing for visual inspection of the semantic relationships between codes.  Each point represents an ICD-9 code, and the color of the point corresponds to its high-level ICD-9 category.  The visualization shows that Clinical ModernBERT, which was pre-trained on a dataset that included both free text and structured medical ontologies (including ICD-9 codes and their descriptions), produces a more distinct and organized clustering of ICD-9 codes according to their high-level categories. This improved clustering demonstrates that the inclusion of structured medical ontologies during pre-training helps the model learn more meaningful semantic relationships between the codes. In contrast, the ModernBERT model, which lacks this additional pretraining data, shows less distinct clusters.", "section": "5.4 Latent Space Visualizations for medical codes"}, {"figure_path": "https://arxiv.org/html/2504.03964/extracted/6337549/time.png", "caption": "Figure 3: Comparative Performance Analysis of BERT Models:\nThis figure demonstrates the processing time requirements across three BERT variants (Distil-BERT, BioClinicalBERT, and Clinical ModernBERT) as data volume increases from 10,000 to 100,000 points. BioClinicalBERT consistently shows the highest computational demand, requiring approximately 1.4x the processing time of Distil-BERT and 1.6x that of Clinical ModernBERT at maximum load. Clinical ModernBERT demonstrates superior efficiency, maintaining the lowest processing times across all data volumes, making it optimal for resource-constrained environments.", "description": "This figure compares the processing time of three different BERT models: Distil-BERT, BioClinicalBERT, and Clinical ModernBERT.  The x-axis represents the number of data points (from 10,000 to 100,000), and the y-axis shows the processing time in seconds.  The graph clearly shows that Clinical ModernBERT is the most efficient, consistently having the lowest processing time. BioClinicalBERT is the least efficient, taking significantly longer than the other two models. This demonstrates that Clinical ModernBERT is a superior choice for applications with limited computational resources.", "section": "5 Results"}]