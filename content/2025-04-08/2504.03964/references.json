{"references": [{"fullname_first_author": "Devlin, Jacob", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-01-01", "reason": "This paper introduced BERT, a foundational model in NLP and a key component in many subsequent architectures."}, {"fullname_first_author": "Warner, Benjamin", "paper_title": "Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference", "publication_date": "2024-12-01", "reason": "This paper introduces ModernBERT, which serves as the base architecture upon which Clinical ModernBERT is built, incorporating key architectural upgrades."}, {"fullname_first_author": "Lee, Jinhyuk", "paper_title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining", "publication_date": "2020-01-01", "reason": "This paper introduced BioBERT, an early and influential domain-adapted BERT model for biomedical text, serving as a direct predecessor to Clinical ModernBERT."}, {"fullname_first_author": "Alsentzer, Emily", "paper_title": "Publicly available clinical bert embeddings", "publication_date": "2019-04-01", "reason": "This paper introduced BioClinicalBERT, an extension of BioBERT that includes clinical notes, a valuable part of the pre-training process."}, {"fullname_first_author": "Dao, Tri", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-01-01", "reason": "This paper introduced FlashAttention, a key architectural improvement used in both ModernBERT and Clinical ModernBERT for memory efficiency."}]}