{"importance": "This paper introduces **VAPO, which outperforms previous methods** in long-CoT reasoning tasks. It offers researchers insights into addressing challenges like value model bias and reward sparsity, paving the way for more effective reinforcement learning in complex reasoning tasks.", "summary": "VAPO: Efficient RL for reasoning tasks, achieving state-of-the-art with stable training and enhanced performance in long-CoT scenarios.", "takeaways": ["VAPO is the first value-based RL framework to surpass value-free methods in long-CoT tasks.", "Length-adaptive GAE is proposed to address the distinct bias-variance trade-off requirements with highly variable lengths.", "Techniques from prior works such as Clip-Higher from DAPO, Value-Pretraining from VC-PPO, self-imitation learning from SIL, and Group-Sampling from GRPO were integrated."], "tldr": "Reinforcement learning(RL) plays a pivotal role in enhancing model performance for complex tasks, however, value-free methods have gained traction due to the difficulty in training reliable value models. Value-free methods do not involve learning a value model. Despite successes, value-based approaches have a higher ceiling if value model training challenges can be addressed, enabling precise credit assignment and enhanced training stability. Training a perfect value model in Long COT tasks faces challenges. First is non-trivial low-bias value model learning due to trajectory length and instability. Second is handling short and long responses simultaneously with distinct preferences. Last is the sparsity of the reward signal. \n\nTo fully unleash value-based methods in reasoning tasks, this paper presents Value Augmented Proximal Policy Optimization (**VAPO**), a value-based RL training framework. **VAPO not only demonstrates remarkable superiority in terms of performance but also showcases enhanced training efficiency**. It introduces **Length-adaptive GAE, which adjusts the A parameter in GAE computation based on response lengths. Techniques from prior work such as Clip-Higher and Token-level Loss from DAPO, Value-Pretraining and Decoupled-GAE from VC-PPO, self-imitation learning from SIL, and Group-Sampling from GRPO were integrated.**", "affiliation": "ByteDance Seed", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2504.05118/podcast.wav"}