{"references": [{"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This work is pivotal because it details training language models with human feedback (RLHF), a core technique used to align models and is cited twice."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-01-01", "reason": "This paper introduces Proximal Policy Optimization (PPO), a widely used reinforcement learning algorithm and is thus fundamental to the work."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This work introduces the Chain-of-Thought (CoT) prompting, an approach that is the core of this paper."}, {"fullname_first_author": "Zhihong Shao", "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models", "publication_date": "2024-01-01", "reason": "This paper describes the DeepSeekMath model, representing a significant advancement in mathematical reasoning with language models and is directly comparable."}, {"fullname_first_author": "John Schulman", "paper_title": "High-dimensional continuous control using generalized advantage estimation", "publication_date": "2015-01-01", "reason": "This paper introduces Generalized Advantage Estimation (GAE), a technique used to improve policy gradient methods."}]}