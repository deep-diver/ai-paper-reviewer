[{"figure_path": "https://arxiv.org/html/2504.03193/extracted/6335383/figures/VFM_vs_VLM.png", "caption": "Figure 1: Comparative analysis of the VFM and the VLM features. VFM: Visualization of PCA-computed features from DINOv2 (the first three components of PCA, computed on the image features, serve as color channels), displaying fine-grained details but lacking text alignment. VLM: Image-text similarity map from EVA02-CLIP using the query \u2018car\u2019, demonstrating good alignment with text but insufficient localization of queried objects. MFuser: Our proposed fusion framework integrates VFM and VLM, resulting in unified features that exhibit both precise locality and robust text alignment. Quantitative results on synthetic-to-real DGSS benchmarks further validate our approach, with MFuser consistently achieving the highest mIoU scores across all tasks.", "description": "Figure 1 compares feature visualizations from a Vision Foundation Model (VFM), a Vision-Language Model (VLM), and the proposed MFuser model. The VFM visualization (DINOv2) uses PCA to project features into RGB channels, revealing detailed spatial information but lacking semantic context. The VLM visualization (EVA02-CLIP) shows a similarity map for the query 'car', demonstrating strong text alignment but poor localization. MFuser combines both models, resulting in features with both precise localization and strong text alignment, as shown quantitatively in the graph.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.03193/extracted/6335383/figures/main_fig.png", "caption": "Figure 2: Overall architecture of MFuser. MFuser takes inputs through both VFM and VLM visual encoders. Features from each encoder layer are concatenated and refined in MVFuser, which captures sequential and spatial dependencies in parallel. The refined features are then added back to the original features and passed to the next layer. MTEnhancer strengthens text embeddings of each class by integrating visual features through a hybrid attention-Mamba mechanism. The enhanced text embeddings serve as object queries for the Mask2Former decoder, alongside multi-scale visual features. During training, only MVFusers, MTEnhancers, and the segmentation decoder are trainable while the VFM and VLM remain frozen, preserving their generalization ability and enabling efficient training. Note that skip connections between each block of MTEnhancer are omitted for clarity.", "description": "The figure illustrates the MFuser architecture, a framework that combines Vision Foundation Models (VFMs) and Vision Language Models (VLMs) for domain-generalized semantic segmentation.  It shows how features from each layer of both VFM and VLM encoders are concatenated and processed by the MVFuser module to capture both sequential and spatial dependencies.  The MVFuser output is then refined and fed back to the original feature streams. Additionally, the MTEnhancer module enhances the text embeddings by incorporating visual features using a hybrid attention-Mamba mechanism. These refined text embeddings serve as object queries for the Mask2Former decoder, which uses both multi-scale features and enhanced text embeddings to generate the final segmentation mask. A key aspect is that during training, only the MVFuser, MTEnhancer, and Mask2Former decoder are fine-tuned, while the VFMs and VLMs remain frozen to preserve their pre-trained knowledge and improve training efficiency.", "section": "4. Proposed Method"}, {"figure_path": "https://arxiv.org/html/2504.03193/extracted/6335383/figures/fea_PCA.png", "caption": "Figure 3: Qualitative results on unseen target domains under the G\u2192\u2192\\rightarrow\u2192{C, B, M} setting. MFuser is compared with Rein\u00a0[55] and tqdm\u00a0[40].", "description": "This figure shows a qualitative comparison of semantic segmentation results on unseen target domains (Cityscapes, BDD100k, Mapillary) using three different methods: MFuser (the proposed method), Rein [55], and tqdm [40].  The 'G\u2192{C,B,M}' notation indicates that the models were trained on synthetic data (GTA V) and tested on the three real-world datasets. Each row represents a different real-world dataset, with several sample images.  The ground truth (GT) segmentation masks are provided for comparison, allowing visual assessment of the accuracy and detail captured by each method in diverse and challenging scenarios.", "section": "5. Experiments"}]