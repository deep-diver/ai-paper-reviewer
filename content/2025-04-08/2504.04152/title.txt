Rethinking Multilingual Continual Pretraining: Data Mixing for Adapting LLMs Across Languages and Resources