{"references": [{"fullname_first_author": "Fei Yuan", "paper_title": "Lego-MT: Learning detachable models for massively multilingual machine translation", "publication_date": "2023-07-01", "reason": "This paper is a key source for the Lego-MT dataset, which is used as the basis for the CPT setup in the evaluated study."}, {"fullname_first_author": "Ji, Shaoxiong", "paper_title": "EMMA-500: Enhancing massively multilingual adaptation of large language models.", "publication_date": "2024-09-01", "reason": "This paper outlines the pre-processing strategy that this paper follows to incorporate code data."}, {"fullname_first_author": "Costa-juss\u00e0, Marta R", "paper_title": "No language left behind: Scaling human-centered machine translation.", "publication_date": "2022-07-01", "reason": "This paper introduces the NLLB dataset, a source of parallel bilingual data for the evaluation setup."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need.", "publication_date": "2017-01-01", "reason": "This is a fundamental paper that introduced the Transformer architecture, upon which LLMs are based."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models.", "publication_date": "2023-07-01", "reason": "This paper introduces the Llama 2 models, which are used as base models for some of the CPT configurations evaluated in the study."}]}