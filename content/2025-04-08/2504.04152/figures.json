[{"figure_path": "https://arxiv.org/html/2504.04152/x2.png", "caption": "Figure 1: FLORES-200 X-Eng BLEU score comparing bilingual and monolingual CPT across high-, mid-, and low-resource languages.", "description": "This figure displays the BLEU scores achieved on the FLORES-200 benchmark's English-to-other-language translation task.  It compares the performance of models that underwent monolingual continual pretraining (CPT) versus those using bilingual CPT, categorized by high, mid, and low-resource language groups.  Lower BLEU scores indicate poorer translation quality. The purpose is to show the effect of bilingual vs. monolingual CPT on translation performance, and how that effect varies depending on the language resource level.", "section": "3.2 Effect of Monolingual and Bilingual Continual Pretraining"}, {"figure_path": "https://arxiv.org/html/2504.04152/x3.png", "caption": "Figure 2: SIB-200 classification accuracy comparing monolingual and bilingual CPT across high-, mid-, and low-resource languages.", "description": "This figure compares the performance of monolingual and bilingual continual pretraining (CPT) strategies on a multilingual news topic classification task (SIB-200).  It displays average classification accuracy across three resource levels (high, mid, and low) for each of the three base language models (Llama-3.1-8B, Llama-2-7B, Viking-7B). The results show how each CPT approach affects different language resource levels and base models.  Specifically, you can analyze how bilingual CPT influences performance compared to the monolingual approach for high, mid, and low-resource language categories and across the different base models.", "section": "3.2 Effect of Monolingual and Bilingual Continual Pretraining"}, {"figure_path": "https://arxiv.org/html/2504.04152/x4.png", "caption": "Figure 3: SIB-200 classification accuracy comparing monolingual and monolingual+code CPT across high-, mid-, and low-resource languages.", "description": "This figure compares the classification accuracy of monolingual continual pretraining (CPT) models with and without the inclusion of code data.  The comparison is shown across three resource levels (high, mid, and low) for languages.  It displays the average accuracy for each model type across the different resource levels, allowing for a direct visual assessment of the impact of incorporating code data on the model's performance for languages with varying resource availability. ", "section": "3.3 Effect of Including Code Data"}, {"figure_path": "https://arxiv.org/html/2504.04152/x5.png", "caption": "Figure 4: FLORES-200 X-Eng BLEU score comparing monolingual and monolingual+code CPT across high-, mid-, and low-resource languages.", "description": "This figure displays the results of comparing monolingual continual pretraining (CPT) with and without code data on the FLORES-200 X-Eng machine translation benchmark.  It shows the BLEU scores achieved across different resource levels (high, mid, low) for three different base multilingual language models (Llama-3.1-8B, Llama-2-7B, Viking-7B). The graph allows for a visual comparison of the impact of adding code data to the monolingual CPT approach on translation quality across various language resource levels and models.  Higher BLEU scores indicate better translation performance.", "section": "3.3 Effect of Including Code Data"}, {"figure_path": "https://arxiv.org/html/2504.04152/x6.png", "caption": "Figure 5: SIB-200 classification accuracy comparing bilingual and bilingual+code CPT across high-, mid-, and low-resource languages.", "description": "This figure compares the performance of bilingual continual pretraining (CPT) models with and without code data on the SIB-200 benchmark, a multilingual news topic classification task.  The results are broken down by language resource levels (high, mid, low). It illustrates how adding code data to bilingual CPT affects classification accuracy for different language resource levels and helps understand the impact of code integration on multilingual model performance within the context of bilingual continual pretraining.  Higher accuracy indicates better performance.", "section": "3.2 Effect of Monolingual and Bilingual Continual Pretraining"}, {"figure_path": "https://arxiv.org/html/2504.04152/x7.png", "caption": "Figure 6: Examples of language mixing in bilingual CPT (L3-Bi-) compared to monolingual CPT (L3-Mono-).", "description": "This figure showcases examples highlighting the issue of language mixing that arises in bilingual continual pretraining (CPT) models, specifically comparing the results from a Llama-3.1-8B model trained using bilingual CPT (L3-Bi-) against those trained with monolingual CPT (L3-Mono-).  The examples demonstrate that models trained with bilingual data frequently insert fragments of text from both source and target languages into the generated output.  Each example includes the original prompt, the expected translation, the output generated by the bilingual model, the output generated by the monolingual model, and the BLEU score for each output.  The comparison reveals a noticeable degradation in translation quality and coherence in the bilingual models, characterized by the insertion of inappropriate language tokens.  The monolingual models, on the other hand, generate translations that are more accurate and fluent.", "section": "3.2 Effect of Monolingual and Bilingual Continual Pretraining"}]