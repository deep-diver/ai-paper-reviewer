{"importance": "This paper is important for researchers because it **uncovers the complexities of multilingual continual pretraining**, challenging existing assumptions about language categorization and data strategies. The nuanced findings provide **valuable insights for developing more adaptive and effective CPT methods**, paving the way for future research to bridge language disparities in large language models.", "summary": "Multilingual continual pretraining: Data mixing strategies significantly impact LLM performance across languages!", "takeaways": ["Bilingual CPT enhances classification accuracy but may cause language mixing during generation.", "Including code data during CPT improves classification accuracy, especially for low-resource languages, but can slightly reduce generation quality.", "Current language classifications based on cross-lingual transfer abilities may not generalize across different CPT conditions."], "tldr": "Large Language Models (LLMs) show performance disparities across languages. Continual Pretraining (CPT) is promising for fixing this, but it's unclear how monolingual, bilingual, and code-augmented data compare. This study evaluates 36 CPT setups with three multilingual models and 30+ languages. Languages are categorized as altruistic, selfish, and stagnant. The research uncovers data strategies with substantial effect on language representation. \n\nThe study reveals key insights: Bilingual CPT boosts classification but can mix languages in generation. Code data consistently improves classification, especially for low-resource languages, but may slightly hurt generation. The language categorization based on cross-lingual transfer doesn't always hold true. This nuanced interaction highlights the complexity of multilingual representation learning. It is essential to study language classifications for generalizable CPT strategies.", "affiliation": "University of Helsinki", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.04152/podcast.wav"}