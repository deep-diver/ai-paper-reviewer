[{"figure_path": "https://arxiv.org/html/2504.05305/x2.png", "caption": "Figure 1: Unique Region Caption Anything. We introduce URECA dataset, a novel region-level captioning dataset designed to ensure caption uniqueness and support multi-granularity regions. Each caption in our benchmark is uniquely mapped to its corresponding region, capturing distinctive attributes that differentiate it from surrounding areas. Moreover, we show that our proposed model trained on our dataset effectively generates unique captions for regions at any level of granularity.", "description": "The figure showcases the URECA dataset, a new dataset for region-level image captioning.  It's designed to produce unique captions for each region, regardless of size (multi-granularity).  The dataset ensures that every caption precisely describes only its associated region and highlights the features that set it apart from its surroundings. The authors also demonstrate that their model, trained on this dataset, successfully generates these unique, multi-granularity captions.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.05305/x3.png", "caption": "Figure 2: Automated data curation pipeline of URECA dataset. Our pipeline consists of four key stages to generate unique captions for multi-granularity regions. In Stage 1, we construct a mask tree that captures hierarchical relationships between regions. Stage 2 generates short captions based on the parent node. Stage 3 aggregates captions from child nodes, and Stage 4 ensures that each node is assigned a unique caption. Best viewed in zoomed-in.", "description": "This figure illustrates the four-stage automated data curation pipeline used to create the URECA dataset.  The pipeline focuses on generating unique captions for image regions at multiple levels of granularity. Stage 1 constructs a hierarchical \"mask tree\" representing the relationships between different regions within an image. Stage 2 creates short, initial captions for each region based on its parent node in the tree. Stage 3 refines these captions by incorporating information from child nodes in the tree, leading to more detailed and comprehensive descriptions. Finally, Stage 4 ensures that each region's caption is unique by comparing it to the captions of visually similar regions using image features. This iterative process results in a dataset with unique, descriptive captions for a wide variety of regions at different granularities.", "section": "3. URECA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.05305/x4.png", "caption": "Figure 3: URECA architecture. URECA enables users to generate unique captions that describe distinctive attributes of any region. The mask encoder effectively encodes multi-granularity regions while preserving their identity. The mask token serves as a localizer, guiding the LLM to generate region-specific captions based on the image and query token.", "description": "The figure illustrates the architecture of the URECA model, which is designed for generating unique captions describing image regions at various granularities.  The model takes an image and a target region (specified as a mask) as input. A mask encoder processes the mask, creating a representation that preserves the region's identity and multi-granularity information. This mask representation is then combined with image features and a query token (containing descriptive information about what to caption) within a Large Language Model (LLM). The LLM uses this combined information to generate a unique caption that specifically describes the designated region, distinguishing it from other regions in the image. The mask token acts as a crucial localizer, guiding the LLM to focus its attention on the correct image area.", "section": "4. URECA"}, {"figure_path": "https://arxiv.org/html/2504.05305/x5.png", "caption": "Figure 4: Qualitative results of the URECA and comparison models\u00a0[36, 65]. Our model generates unique caption conditioned on multi-granularity regions.", "description": "Figure 4 presents a qualitative comparison of captioning results between the proposed URECA model and two existing models (KOSMOS-2 and OMG-LLaVA).  The figure showcases examples where URECA successfully generates unique and descriptive captions for image regions at multiple granularities, highlighting its ability to handle nuanced details and contextual information that the comparison models struggle with.  Each example includes the captions generated by URECA and the other models, demonstrating URECA's superior performance in producing distinctive and accurate descriptions of varied image regions. The captions for each region are compared visually. ", "section": "5. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2504.05305/x6.png", "caption": "Figure A: Qualitative results of the URECA and comparison models\u00a0[4, 65]. Our model generates unique caption conditioned on multi-granularity regions.", "description": "Figure A presents a qualitative comparison of captioning results between the URECA model and two other models ([4] and [65]) on various image regions with varying levels of granularity. Each example shows the captions generated by each model, along with an indication of whether or not the model produced a unique caption. The figure highlights the superior ability of the URECA model to generate distinctive and accurate captions for granular image regions, demonstrating its effectiveness in handling multi-granularity.", "section": "5.2 Qualitative Results"}]