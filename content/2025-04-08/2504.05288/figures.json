[{"figure_path": "https://arxiv.org/html/2504.05288/x2.png", "caption": "Figure 1: LiveVQA comprises 14 different News categories, containing 1233 News and 3602 question-answer pairs. Each instance includes a representative image, QA pair for basic image for understanding, and two multimodal multi-hop QA pairs for deeper reasoning.", "description": "Figure 1 presents an overview of the LIVEVQA dataset.  It visually displays the dataset's organization across 14 diverse news categories, highlighting the composition of images and question-answer pairs.  Each data point in LIVEVQA consists of an image depicting a news story, a basic question-answer pair designed for foundational image understanding, and two more complex multimodal question-answer pairs that require deeper reasoning involving multiple steps and a combination of visual and textual information. The figure provides a visual representation of the dataset's structure and content variety.", "section": "2. LIVEVQA: The Dataset"}, {"figure_path": "https://arxiv.org/html/2504.05288/x3.png", "caption": "Figure 2: Pipline of LiveVQA data engine. Our pipeline consists of three modules: news collector, data filter, and Q&A pairs builder. It collects illustrated news from mainstream media, performs multi-level data filtering, and generates foundational and detailed Q&A pairs for training multimodal question-answering models.", "description": "This figure illustrates the LiveVQA data engine pipeline, which comprises three main modules: news collector, data filter, and Q&A pairs builder.  The news collector gathers illustrated news articles from various mainstream media sources. The data filter then rigorously cleanses this data through multi-level filtering, ensuring only high-quality and relevant information is retained. This filtering includes checks for format compliance, plagiarism, and duplicate removal. Finally, the Q&A pairs builder generates both basic and detailed question-answer pairs, using the filtered news and images, to create a comprehensive dataset for training multimodal question-answering models. These question-answer pairs include foundational questions for basic image understanding and more complex multi-hop questions that require deeper reasoning.", "section": "2. LIVEVQA: The Dataset"}, {"figure_path": "https://arxiv.org/html/2504.05288/x4.png", "caption": "Figure 3: Large visual reasoning model QvQ-72B-Preview perform best on cross-modality multi-hop pproblems.", "description": "Figure 3 showcases the superior performance of the QvQ-72B-Preview large language model on complex cross-modality multi-hop questions.  The figure likely presents example questions and answers, illustrating how the model effectively integrates visual and textual information to arrive at accurate answers for questions requiring multiple reasoning steps.  This highlights the model's advanced visual reasoning capabilities and its ability to outperform other models on such complex tasks.", "section": "3.2. Experiment Results"}]