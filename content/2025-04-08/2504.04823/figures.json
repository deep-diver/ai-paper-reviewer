[{"figure_path": "https://arxiv.org/html/2504.04823/x1.png", "caption": "Figure 1: Performance of the quantized DeepSeek-R1-Distill-Qwen models on five benchmarks: AIME-120, MATH-500, GSM8K, GPQA-Diamond, and LiveCodeBench.\nEach chart presents the performance of different quantization strategies compared to the BF16 baseline.", "description": "This figure displays the performance comparison of various quantization techniques applied to the DeepSeek-R1-Distilled-Qwen language models across five distinct benchmarks: AIME-120, MATH-500, GSM8K, GPQA-Diamond, and LiveCodeBench. Each sub-figure represents a specific benchmark, showing how different quantization strategies (e.g., different bit-widths for weights, KV cache, and activations) affect the model's performance.  The performance is compared against a baseline model using BF16 precision. This allows for a clear visualization of the trade-offs between quantization levels and accuracy across diverse reasoning tasks.", "section": "Introduction"}]