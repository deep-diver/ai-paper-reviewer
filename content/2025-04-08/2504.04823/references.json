{"references": [{"fullname_first_author": "Frantar", "paper_title": "Optq: Accurate quantization for generative pre-trained transformers.", "publication_date": "2022-01-01", "reason": "This paper introduces a widely used weight quantization approach for transformers."}, {"fullname_first_author": "Xiao", "paper_title": "Smoothquant: Accurate and efficient post-training quantization for large language models.", "publication_date": "2023-01-01", "reason": "This paper presents a post-training quantization technique to address activation outliers in large language models."}, {"fullname_first_author": "Lin", "paper_title": "Awq: Activation-aware weight quantization for llm compression and acceleration.", "publication_date": "2023-01-01", "reason": "This paper proposes an activation-aware weight quantization method that focuses on protecting salient weights during quantization."}, {"fullname_first_author": "Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.", "publication_date": "2022-01-01", "reason": "This paper introduces chain-of-thought prompting, a technique to elicit reasoning in large language models."}, {"fullname_first_author": "Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.", "publication_date": "2025-01-01", "reason": "This paper presents a reasoning model trained with reinforcement learning, showing strong performance."}]}