[{"figure_path": "https://arxiv.org/html/2501.13452/extracted/6150209/figures/0.jpg", "caption": "Figure 1: Sampling results of EchoVideo. (a) Facial feature preservation. (b) Full-body feature preservation. EchoVideo is capable of not only extracting human features but also resolving semantic conflicts between these features and the prompt, thereby generating coherent and consistent videos.", "description": "This figure showcases the capability of the EchoVideo model to generate high-quality, consistent videos while preserving human identity.  Panel (a) demonstrates facial feature preservation where the generated video accurately reflects the facial features from the input identity image, even while following a complex prompt. Panel (b) shows that this capability extends to full-body feature preservation. Even with a detailed description, the model correctly generates a video where both facial and body features align with the input image and the prompt's specifications, demonstrating EchoVideo's capacity to resolve potential semantic conflicts between the identity and the video prompt. This is a key advantage, as it avoids issues with incoherent or inconsistent video generation that are commonly found in existing identity-preserving video generation models.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.13452/extracted/6150209/figures/1.jpg", "caption": "Figure 2: Issues in IP character generation. (a) Semantic conflict. The input image depicts a child\u2019s face, while the prompt specifies an adult male. Insufficient information interaction leads to inconsistent character traits in the model\u2019s output. (b) Copy-paste. During training, the model overly relies on visual information from facial images, directly using the Variational Autoencoder(VAE)-encoded [1] face as the output for the generated face.", "description": "Figure 2 illustrates two common problems in identity-preserving video generation.  In (a), a semantic conflict occurs: the input image shows a child, but the text prompt describes an adult male.  The model struggles to reconcile this mismatch, resulting in inconsistencies in the generated character's features. In (b), the \"copy-paste\" issue is shown, where the model over-relies on the input facial image, directly copying its features without proper integration with the textual prompt, leading to unnatural and unrealistic results. This demonstrates the need for a more sophisticated approach that harmoniously integrates textual and visual information.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2501.13452/extracted/6150209/figures/2.jpg", "caption": "Figure 3: Overall architecture of EchoVideo. By employing a meticulously designed IITF module and mitigating the over-reliance on input images, our model effectively unifies the semantic information between the input facial image and the textual prompt. This integration enables the generation of consistent characters with multi-view facial coherence, ensuring that the synthesized outputs maintain both visual and semantic fidelity across diverse perspectives.", "description": "EchoVideo's architecture centers around a diffusion transformer (DiT) model.  It leverages an Identity Image-Text Fusion (IITF) module to combine high-level semantic features from text with facial identity information from an input image. The IITF module is designed to extract clean identity representations and to resolve potential semantic conflicts between text and image information. The system avoids over-reliance on the input image, preventing 'copy-paste' artifacts and generating more consistent and coherent characters with multi-view facial coherence across different perspectives. The overall process ensures that generated videos maintain both visual and semantic fidelity.", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.13452/extracted/6150209/figures/3.jpg", "caption": "Figure 4: Illustration of facial information injection methods. (a) Dual branch. Facial and textual information are independently injected through Cross Attention mechanisms, providing separate guidance for the generation process. (b) IITF. Facial and textual information are fused to ensure consistent guidance throughout the generation process.", "description": "Figure 4 illustrates two approaches for incorporating facial and textual information into a video generation model.  (a) shows a 'dual branch' method where facial and textual features are processed independently through cross-attention mechanisms before influencing the video generation process. This approach can lead to inconsistencies because the features are not directly integrated. (b) showcases the Identity Image-Text Fusion (IITF) module, which fuses facial and textual information, ensuring consistent guidance throughout the generation process. This fusion aims to resolve potential conflicts between facial appearance and the text description and produce more coherent and accurate results.", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.13452/extracted/6150209/figures/4.jpg", "caption": "Figure 5: Qualitative results. (a) Ours. (b) ConsisID [10]. (c) ID-Animator [8]. Our model can effectively overcome semantic conflicts and copy-paste phenomena while maintaining the face IP.", "description": "Figure 5 presents a qualitative comparison of video generation results from three different methods: the proposed EchoVideo model, ConsisID [10], and ID-Animator [8].  Each column shows example videos generated using the respective methods. The figure highlights EchoVideo's ability to effectively avoid common issues such as semantic conflicts (inconsistencies between the text prompt and generated visuals) and the 'copy-paste' artifact (where parts of the input image are directly replicated in the output without proper integration).  EchoVideo consistently maintains high facial identity preservation (IP).", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.13452/extracted/6150209/figures/5.jpg", "caption": "Figure 6: Effect of the IITF module. (a) Without IITF. (b) With IITF. IITF can effectively extract facial semantic information and resolve conflicts with text information, generating consistent characters while maintaining the face IP.", "description": "This figure demonstrates the impact of the Identity Image-Text Fusion (IITF) module on video generation.  Subfigure (a) shows the results without IITF, illustrating inconsistencies between the generated face and the text prompt.  The generated faces often do not match the prompt's description of the person, demonstrating limitations in semantic understanding and identity preservation. Subfigure (b) shows how IITF addresses these issues by effectively extracting facial semantics from images, resolving conflicts between text and image data. The result is consistent character traits in generated videos that accurately reflect both the facial image and textual description, thus successfully preserving the identity of the person depicted. ", "section": "4.2 Identity Image-Text Fusion Module"}, {"figure_path": "https://arxiv.org/html/2501.13452/extracted/6150209/figures/6.jpg", "caption": "Figure 7: Effect of using facial visual features encoded by VAE. (a) Without face visual features. (b) With face visual features. By using the facial visual information , the facial details in the generated video can be effectively supplemented.", "description": "This figure demonstrates the impact of incorporating facial visual features, encoded by a Variational Autoencoder (VAE), on the quality of generated videos.  Subfigure (a) shows the results when these features are omitted; the generated faces lack detail and appear blurry. Subfigure (b) presents the results when the VAE-encoded facial features are included. The addition of this information significantly enhances the level of facial detail in the generated videos, leading to more realistic and visually appealing results.  This highlights the importance of incorporating the additional low-level facial information in order to complement the high-level semantic information for improved video generation.", "section": "4.3 Data and Training"}]