[{"figure_path": "https://arxiv.org/html/2501.13629/x1.png", "caption": "Figure 1: Overview of our proposed method for differential rescaling of QKV, compared alongside Multi-Head Attention (MHA), Multi-Query Attention (MQA), and Grouped Query Attention (GQA). Specifically, our method involves: (1) differentially compressed KV: applying more aggressive compression on the number of K heads and their dimensions than on the V components, which more significantly reduces the size of K cache. We can also optionally adopt selective V cache fetching for V compression; (2) augmented Q: adopting a higher dimension for the Q head compared to the KV heads.", "description": "Figure 1 illustrates four different attention mechanisms: Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and the proposed DiffQKV attention.  The figure uses diagrams to highlight the key differences in how queries (Q), keys (K), and values (V) are handled in each method.  Specifically, it shows that DiffQKV employs (1) differentially compressed KV: a more aggressive compression of K's number of heads and dimensions compared to V to reduce K cache size, with optional selective V cache fetching for further compression, and (2) augmented Q: a higher dimension for Q heads than K or V heads to improve representation capability.  The visual comparison allows readers to quickly grasp the architectural innovations introduced by DiffQKV and understand how it improves efficiency.", "section": "2 DIFFQKV ATTENTION"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kernel_cost_split.png", "caption": "(a) KET of the split kernel.", "description": "This figure shows the Kernel Execution Time (KET) for the split kernel of the FlexHeadFA attention mechanism. The KET is measured in nanoseconds (ns) and plotted against the prefix length, which represents the length of the input sequence.  Two models, SIGMA and STD, are compared. The split kernel is one part of the Flash Attention 2 algorithm, responsible for dividing the key and value matrices into smaller chunks for efficient computation. This plot illustrates how the time taken by the split kernel changes as the input sequence length increases, revealing potential performance differences between the SIGMA and STD models in their attention mechanism.", "section": "3.3 Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kernel_cost_combine.png", "caption": "(b) KET of the combine kernel.", "description": "This figure shows the Kernel Execution Time (KET) for the combine kernel in the FlexHeadFA attention mechanism.  The combine kernel is responsible for assembling smaller output chunks into a complete output matrix during attention calculations. The graph likely plots KET (in nanoseconds) on the y-axis against prefix length (the length of the input sequence) on the x-axis. Separate lines probably represent the performance of the SIGMA model and the standard model (STD) for comparison. The purpose is to demonstrate the relative efficiency of SIGMA's attention mechanism, especially as the input sequence grows longer, with a focus on the combine kernel's contribution to the overall computation time. ", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kernel_cost_tot.png", "caption": "(c) Total KET.", "description": "The figure shows the total kernel execution time (KET) for both the standard model (STD) and SIGMA 1.5B.  The KET is the sum of the execution times for the 'split' and 'combine' kernels within the flash attention mechanism.  The x-axis represents the prefix length, indicating the length of the input sequence processed before the current token. The y-axis represents the total kernel execution time in nanoseconds.  The plot illustrates the impact of SIGMA's DiffQKV attention mechanism on inference speed, particularly at longer prefix lengths, where SIGMA shows significantly improved performance compared to the standard model.  The results are broken down to show the contribution of split and combine separately in order to demonstrate where the speed up comes from.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_2k.png", "caption": "Figure 2: KET comparison of FlexHeadFA between Standard model(Std) and Sigma.", "description": "This figure compares the kernel execution time (KET) of the FlexHeadFA (Flexible Head Flash Attention) between the Standard model and the SIGMA model.  The KET is broken down into the split and combine kernels. The x-axis represents the prefix length, and the y-axis shows the KET in nanoseconds.  The figure shows how the efficiency of SIGMA improves, particularly as the prefix length increases. Different subplots focus on different components of the computation.", "section": "3.3 Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_4k.png", "caption": "(a) Output Length\u00a0=2\u2062kOutput Length\u00a02\ud835\udc58\\text{Output Length }=2kOutput Length = 2 italic_k.", "description": "The figure shows the CUDA Event Elapsed Time (CEET) results for different prefix lengths when the output length is fixed at 2k tokens. It compares the performance of SIGMA and the standard model (STD). The x-axis represents the prefix length, and the y-axis shows the CEET (in milliseconds). Different colored lines represent the CEET results for SIGMA (in blue) and STD (in red). The plot shows how the execution time of both models changes with the increasing prefix length.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_8k.png", "caption": "(b) Output Length =4\u2062kabsent4\ud835\udc58=4k= 4 italic_k.", "description": "This figure shows the Kernel Execution Time (KET) results for the combine kernel in the Flash Attention mechanism.  The x-axis represents the prefix length of the input sequence, and the y-axis represents the execution time of the combine kernel.  There are two lines in the graph, one for the standard model and one for the SIGMA model. The output length is fixed at 4k tokens. This figure demonstrates the efficiency gains of the SIGMA model over the standard model for the combine kernel operation when generating text with longer contexts, and shows that SIGMA's gains become increasingly pronounced as the prefix length increases.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_16k.png", "caption": "(c) Output Length =8\u2062kabsent8\ud835\udc58=8k= 8 italic_k.", "description": "The figure shows the result of Kernel Execution Time (KET) test for the combine kernel in the flash attention mechanism. The x-axis represents the prefix length, and the y-axis represents the execution time of the combine kernel in nanoseconds. Two models are compared: the standard model (STD) and SIGMA. The output length is fixed at 8k tokens.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_32k.png", "caption": "(d) Output Length =16\u2062kabsent16\ud835\udc58=16k= 16 italic_k.", "description": "This figure displays the Cuda Event Elapsed Time (CEET) results for various components of the attention mechanism (KV Cache, Attention Computation, and Augmented Q) in the SIGMA and Standard models. The output length is fixed at 16k tokens, while the prefix length varies.  The plot visually shows the time (in milliseconds) taken by each module for different prefix lengths, illustrating the performance difference between SIGMA (with DiffQKV attention) and the Standard model (with conventional grouped-query attention). The goal is to compare how the efficiency of these components changes in both models under different context lengths.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_64k.png", "caption": "(e) Output Length =32\u2062kabsent32\ud835\udc58=32k= 32 italic_k.", "description": "This figure displays the results from the Cuda Event Elapsed Time (CEET) test, specifically focusing on the scenario with an output length of 32,000 tokens.  The CEET measures the end-to-end time of the attention computation. The figure likely shows how the total time for attention computation changes as the prefix length varies. This would help demonstrate SIGMA's efficiency gain, particularly in longer sequences, where it surpasses other state-of-the-art models. The x-axis represents different prefix lengths, and the y-axis shows the corresponding CEET, allowing for a comparison between SIGMA and a standard model across various prefix lengths.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_delta.png", "caption": "(f) Output Length =64\u2062kabsent64\ud835\udc58=64k= 64 italic_k.", "description": "This figure displays the results of the Cuda Event Elapsed Time (CEET) test when the output length is 64k tokens.  The CEET test measures the time taken to perform specific operations within the model, providing insights into efficiency and performance at varying prefix lengths.  Different colored lines in the plot likely represent the performance of the model (SIGMA) compared to a baseline or standard model (STD) under different conditions. The x-axis represents the prefix length, and the y-axis displays the CEET in milliseconds.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/augq_cost_redelta.png", "caption": "(g) Sigma\u2019s absolute CEET improvment vs Std.", "description": "The figure shows the absolute improvement in CUDA Event Elapsed Time (CEET) of SIGMA compared to the standard model (STD).  The x-axis represents the prefix length, and the y-axis represents the absolute difference in milliseconds (ms) between the CEET of SIGMA and STD. Positive values indicate that SIGMA is faster than STD, while negative values indicate that SIGMA is slower.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_2k.png", "caption": "(h) Sigma\u2019s relative CEET improvment vs Std.", "description": "This figure shows the relative improvement in Cuda Event Elapsed Time (CEET) achieved by SIGMA compared to the standard model (STD).  It displays the percentage difference in computation time between SIGMA and STD across various prefix lengths and output lengths. A positive value indicates that SIGMA is faster, while a negative value means STD is faster.  The figure provides a visual representation of SIGMA's efficiency gains in handling long sequences, showing how its performance advantage grows with increasing context size.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_4k.png", "caption": "Figure 3: CEET comparison of augmented Q between Standard model(Std) and Sigma. From (a) to (f), the output length increases progressively from 2k to 64k tokens.", "description": "This figure compares the computational cost of the augmented Q module in the SIGMA model against a standard model (Std). The experiment is conducted across a range of output sequence lengths, from 2k tokens to 64k tokens. For each output length, the figure displays the computational time taken by both models. The x-axis represents the length of the input sequence (prefix length), and the y-axis represents the computational cost in milliseconds (CEET, Cuda Event Elapsed Time). This visualization helps to analyze the impact of the augmented Q module on the overall efficiency of the SIGMA model.", "section": "3.3 Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_8k.png", "caption": "(a) Output Length =2\u2062kabsent2\ud835\udc58=2k= 2 italic_k.", "description": "This figure shows the results of the Kernel Execution Time (KET) test.  Specifically, it displays a comparison of the time taken to execute the split kernel and combine kernel of the FlexHeadFA attention mechanism in the SIGMA model versus the standard model (STD).  The x-axis represents the length of the input prefix, and the y-axis shows the time in nanoseconds. This test is designed to isolate and measure the efficiency of different parts of the attention computation.  Multiple sub-figures are presented (a-c) showing the breakdown of the total kernel execution time into its component parts, allowing for a detailed analysis of the impact of SIGMA's architecture changes on efficiency at varying sequence lengths.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_16k.png", "caption": "(b) Output Length =4\u2062kabsent4\ud835\udc58=4k= 4 italic_k.", "description": "The figure shows the Kernel Execution Time (KET) results for the combine kernel in the context of FlexHeadFA, comparing the performance between the standard model and SIGMA. The x-axis represents the prefix length (in tokens), and the y-axis represents the KET (in nanoseconds).  The plot shows results for an output length of 4k tokens. This helps illustrate how the performance of the combine kernel changes with varying input sequence lengths for a specific output length. The goal is to assess the efficiency gains of SIGMA's DiffQKV attention over the standard model.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_32k.png", "caption": "(c) Output Length =8\u2062kabsent8\ud835\udc58=8k= 8 italic_k.", "description": "This figure shows the results of measuring CUDA Event Elapsed Time (CEET) for different components of the attention mechanism in the SIGMA and Standard models. The x-axis represents the prefix length, and the y-axis represents the CEET in milliseconds. The output length is fixed at 8k tokens. The figure illustrates the differences in performance between SIGMA and Standard models across various prefix lengths.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_64k.png", "caption": "(d) Output Length =16\u2062kabsent16\ud835\udc58=16k= 16 italic_k.", "description": "This figure presents the results of the Cuda Event Elapsed Time (CEET) test, specifically focusing on scenarios with an output length of 16k tokens. It displays how the total computation time varies across different prefix lengths (the amount of input text preceding the generated text).  The x-axis represents the prefix length, showing how computation time changes as more input is processed.  The y-axis shows the CEET in milliseconds. The results likely compare the performance of the SIGMA model against a baseline model, showcasing the efficiency gains of SIGMA in long-context scenarios.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_delta.png", "caption": "(e) Output Length =32\u2062kabsent32\ud835\udc58=32k= 32 italic_k.", "description": "This figure shows the results of measuring CUDA event elapsed time (CEET) for different components of the SIGMA model. Specifically, it focuses on the scenario where the output length is 32k tokens. The x-axis represents the prefix length, and the y-axis represents the measured CEET. Different lines represent different modules within the model, such as KV cache, attention computation, and augmented Q. By comparing the CEET values for SIGMA and a standard model across various prefix lengths, the figure helps to illustrate the efficiency gains achieved by SIGMA, particularly in long-context scenarios.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/kvcache_cost_redelta.png", "caption": "(f) Output Length =64\u2062kabsent64\ud835\udc58=64k= 64 italic_k.", "description": "This figure shows the results of the Cuda Event Elapsed Time (CEET) test with an output length of 64k tokens. The CEET test measures the total time elapsed for specific operations in the model. It is used to assess the efficiency of SIGMA and compare its performance against the standard model (STD). The x-axis represents the prefix length, and the y-axis represents the CEET in milliseconds. The figure displays the CEET for SIGMA and STD, allowing for a comparison of their efficiency across different prefix lengths. The results highlight the improvements in inference speed achieved by SIGMA, especially for longer sequences.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_2k.png", "caption": "(g) Sigma\u2019s absolute CEET improvment vs Std.", "description": "The figure shows the absolute improvement in Cuda Event Elapsed Time (CEET) achieved by SIGMA compared to the standard model (STD).  It illustrates how much faster SIGMA is than the standard model in terms of milliseconds (ms). The improvement is shown across varying prefix and output lengths, revealing how the advantage of SIGMA changes across different text lengths.", "section": "3.3 Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_4k.png", "caption": "(h) Sigma\u2019s relative CEET improvment vs Std.", "description": "This figure shows the relative improvement in Cuda Event Elapsed Time (CEET) of SIGMA compared to the standard model (STD).  The relative improvement is calculated as ((STD_CEET - SIGMA_CEET) / STD_CEET) * 100%.  The x-axis represents different prefix lengths, and each subplot represents a different output length.  The plot visually demonstrates how SIGMA's efficiency gains increase as both prefix and output lengths increase.  Specifically, it shows how much faster SIGMA is than STD in terms of time cost.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_8k.png", "caption": "Figure 4: CEET comparison of KV cache between Standard model(Std) and Sigma. From (a) to (f), the output length increases progressively from 2k to 64k tokens.", "description": "This figure compares the computational cost of loading and storing the key-value (KV) cache between the standard model (Std) and SIGMA, a novel language model introduced in the paper. The comparison is performed across varying output lengths, ranging from 2k to 64k tokens, and across different prefix lengths. The results are presented in a series of subplots, each showing the cost for a specific output length. This figure helps to illustrate the efficiency gains achieved by SIGMA through a reduction in the number of key heads, which directly impacts the size of the KV cache.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_16k.png", "caption": "(a) Output Length =2\u2062kabsent2\ud835\udc58=2k= 2 italic_k.", "description": "The figure shows the results of Kernel Execution Time (KET) measurements for the split and combine kernels of the FlexHeadFA attention mechanism.  It compares the performance of the SIGMA model (with reduced key heads) against a standard model (STD) with balanced key and value heads. The x-axis represents the prefix length, and the y-axis displays the KET in nanoseconds.  Separate graphs are presented for (a) the split kernel, (b) the combine kernel, and (c) the total KET which is sum of split and combine kernel.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_32k.png", "caption": "(b) Output Length =4\u2062kabsent4\ud835\udc58=4k= 4 italic_k.", "description": "The figure shows the result of Kernel Execution Time (KET) test, measuring the execution time of two kernels in the flash attention mechanism: the split kernel and the combine kernel. The test compares the performance between the standard model (STD) and SIGMA model, varying the prefix length while keeping the output length fixed at 4k tokens. The results illustrate the efficiency improvement of SIGMA over STD.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_64k.png", "caption": "(c) Output Length =8\u2062kabsent8\ud835\udc58=8k= 8 italic_k.", "description": "This figure shows the results of Kernel Execution Time (KET) test for the combine kernel in the DiffQKV attention mechanism. The x-axis represents the prefix length, and the y-axis represents the execution time in nanoseconds. There are two lines in the plot, one for SIGMA and another one for the standard model (STD).  The output length is fixed at 8k tokens. The plot displays the execution time of the combine kernel as the prefix length varies. It's used to evaluate the efficiency improvements brought by SIGMA, especially in long-sequence scenarios, by comparing the execution time with a standard model.", "section": "3.3 Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_delta.png", "caption": "(d) Output Length =16\u2062kabsent16\ud835\udc58=16k= 16 italic_k.", "description": "This figure is a part of the empirical analysis on efficiency. Specifically, it presents the Cuda Event Elapsed Time (CEET) results when the output length is 16k tokens. The x-axis represents the prefix length, and the y-axis represents the CEET in milliseconds.  The plot shows the CEET for both SIGMA and the standard model (STD) to illustrate the efficiency improvement achieved by SIGMA. The results show that SIGMA is significantly more efficient than the STD in terms of inference time, especially for longer prefixes and sequences.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/attn_cost_redelta.png", "caption": "(e) Output Length =32\u2062kabsent32\ud835\udc58=32k= 32 italic_k.", "description": "This figure presents the results of Cuda Event Elapsed Time (CEET) measurements for the 'Attention Computation' module in the SIGMA and STD models when the output length is 32k tokens.  The x-axis represents the prefix length, and the y-axis displays the CEET in milliseconds.  Separate lines show the results for the SIGMA and STD models, allowing comparison of their efficiency in processing different prefix lengths. The plot helps demonstrate the effect of the SIGMA model's architecture, particularly its DiffQKV attention mechanism, on reducing computational time as prefix and sequence length increases.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_2k.png", "caption": "(f) Output Length =64\u2062kabsent64\ud835\udc58=64k= 64 italic_k.", "description": "This figure shows the results of the Cuda Event Elapsed Time (CEET) test when the output length is 64k tokens.  The CEET test measures the total time taken for different operations within the attention mechanism (KV cache, attention computation, and augmented Q).  By showing results for different prefix lengths (the amount of input text preceding the generation task), the plot helps to analyze how these operations' time changes with both increasing input and output length.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_4k.png", "caption": "(g) Sigma\u2019s absolute CEET improvment vs Std.", "description": "The figure shows the absolute improvement in Cuda Event Elapsed Time (CEET) achieved by SIGMA compared to the standard model (STD).  The x-axis represents the prefix length, while the y-axis shows the absolute difference in CEET (in milliseconds).  Positive values indicate that SIGMA outperforms STD; and negative values indicate that STD outperforms SIGMA. The plot is further divided into subplots with different output lengths to illustrate how the performance gap changes as both the context and generated sequence length increase.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_8k.png", "caption": "(h) Sigma\u2019s relative CEET improvment vs Std.", "description": "This figure shows the relative improvement in Cuda Event Elapsed Time (CEET) of SIGMA compared to the standard model (STD).  CEET measures the end-to-end time of specific operations. The relative improvement is calculated as the percentage decrease in CEET from STD to SIGMA. The figure likely displays this relative improvement across various conditions, such as different prefix and output sequence lengths, to illustrate how the performance gain changes under various scenarios.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_16k.png", "caption": "Figure 5: CEET comparison of attention computation between Standard model(Std) and Sigma. From (a) to (f), the output length increases progressively from 2k to 64k tokens.", "description": "Figure 5 presents a comparison of the Cuda Event Elapsed Time (CEET) for the attention computation module between the Standard model (STD) and SIGMA across different output lengths.  The x-axis represents the prefix length, while the y-axis shows the CEET in milliseconds. The figure is composed of six subfigures (a) through (f), each corresponding to a different output length, ranging from 2k to 64k tokens. Each subfigure contains two lines, one for SIGMA and one for STD, illustrating their respective computation times for the attention module. The results demonstrate that SIGMA exhibits superior efficiency compared to the STD, especially as the output and prefix lengths increase, which shows the effectiveness of the proposed DiffQKV attention module.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_32k.png", "caption": "(a) Output Length =2\u2062kabsent2\ud835\udc58=2k= 2 italic_k.", "description": "This figure shows the comparison of CUDA Event Elapsed Time (CEET) between SIGMA and the standard model (STD).  The plots display the CEET for different prefix lengths (0k, 2k, 4k, 16k, 32k, 64k) when generating sequences of length 2k. This illustrates the relative performance of SIGMA in terms of inference speed across various sequence lengths and demonstrates the efficiency gains achieved by SIGMA, especially in longer sequences.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_64k.png", "caption": "(b) Output Length =4\u2062kabsent4\ud835\udc58=4k= 4 italic_k.", "description": "This figure shows the results of the Kernel Execution Time (KET) test for the combine kernel when the output sequence length is 4k tokens. The combine kernel combines output chunks generated during the flash attention calculation.  The x-axis represents the prefix length (input sequence length), and the y-axis represents the execution time of the combine kernel. Separate lines show the results for the standard model (STD) and the SIGMA model.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_delta.png", "caption": "(c) Output Length =8\u2062kabsent8\ud835\udc58=8k= 8 italic_k.", "description": "The figure shows the results of the CUDA Event Elapsed Time (CEET) test for the attention computation.  The CEET measures the total time spent on attention computation. The x-axis represents the prefix length, and the y-axis represents the CEET in milliseconds. The figure shows that the time spent on attention computation increases with the increase of the prefix length. This is expected as the attention mechanism needs to process more tokens as the prefix length increases. The figure also shows that the time spent on attention computation is longer when the output length is 8k compared to other output lengths. This is because the attention mechanism needs to compute attention scores between the query and key vectors for all the tokens, including those in the output sequence. The longer the output sequence, the more computations required, and thus the longer the computation time.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/extracted/6151052/Figures/total_cost_redelta.png", "caption": "(d) Output Length =16\u2062kabsent16\ud835\udc58=16k= 16 italic_k.", "description": "This figure shows the results of measuring CUDA event elapsed time (CEET).  Specifically, it displays the CEET for different prefix lengths when the output length is fixed at 16k tokens.  The results are relevant to the efficiency analysis of the SIGMA model and highlight the impact of varying prefix lengths on overall processing time.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/x2.png", "caption": "(e) Output Length =32\u2062kabsent32\ud835\udc58=32k= 32 italic_k.", "description": "This figure shows the results from the Cuda Event Elapsed Time (CEET) test, specifically focusing on the scenario where the output length is 32k tokens.  The test measures the time taken for different stages of the attention mechanism: KV Cache (loading and storing key and value vectors), Attention Computation (calculating attention scores and weighted sums), and Augmented Q (applying a modified query component). By comparing the time taken for these stages between the SIGMA and Standard models across varying prefix lengths, the figure illustrates the efficiency gains achieved by SIGMA, particularly in long-context scenarios.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/x3.png", "caption": "(f) Output Length =64\u2062kabsent64\ud835\udc58=64k= 64 italic_k.", "description": "This figure shows the results of measuring the total cost of CUDA events elapsed time (CEET) for different prefix and output lengths when the output length is set to 64k tokens. The x-axis represents the prefix length, and the y-axis shows the CEET in milliseconds. The plot contains two lines: one for the standard model (STD) and one for SIGMA. The figure illustrates the efficiency gains of SIGMA compared to the standard model, especially as the context length increases.", "section": "3.3 EMPIRICAL ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2501.13629/x4.png", "caption": "(g) Sigma\u2019s absolute CEET improvment vs Std.", "description": "The figure shows the absolute improvement in CUDA Event Elapsed Time (CEET) of SIGMA compared to the standard model (STD).  The improvement is calculated by subtracting the CEET of STD from that of SIGMA for different combinations of prefix and output lengths.  Positive values indicate that SIGMA is faster; negative values indicate that SIGMA is slower.  The x-axis represents the prefix length, and the y-axis shows the difference in CEET.", "section": "3.3.2 EMPIRICAL RESULTS"}, {"figure_path": "https://arxiv.org/html/2501.13629/x5.png", "caption": "(h) Sigma\u2019s relative CEET improvment vs Std.", "description": "This figure shows the relative improvement in CUDA Event Elapsed Time (CEET) achieved by SIGMA compared to the standard model (STD).  The relative improvement is calculated as ((STD CEET - SIGMA CEET) / STD CEET) * 100.  The figure likely displays this relative improvement across various experimental conditions, such as different sequence lengths of the input and output texts. It helps to visualize the efficiency gains of SIGMA, particularly in scenarios with longer sequences, where the improvements are expected to be more substantial.  The y-axis represents the relative improvement in percentage, and the x-axis likely represents the different experimental conditions.", "section": "3.3.2 Empirical Results"}, {"figure_path": "https://arxiv.org/html/2501.13629/x6.png", "caption": "Figure 6: Comparison of total CEET cost between Standard model(Std) and Sigma. From (a) to (f), the output length increases progressively from 2k to 64k tokens. The gray dashed line indicates where the inference costs of both models are equal. As the output length increases, this intersection point moves progressively earlier. When generating 64k tokens, Sigma achieves up to a 33% reduction in inference cost compared to Std.", "description": "Figure 6 presents a comparison of the total computational cost (measured using CUDA Event Elapsed Time, or CEET) between the standard model (Std) and the SIGMA model for varying output lengths.  The x-axis represents the prefix length, and the y-axis displays the total CEET.  Subplots (a) through (f) show results for output lengths increasing from 2k to 64k tokens. A gray dashed line in each plot indicates the point where the computational costs of both models are equal. As the output length increases, this intersection point shifts left, indicating that SIGMA's efficiency advantage increases significantly for longer outputs.  For an output length of 64k tokens, SIGMA demonstrates a cost reduction of up to 33% when compared to the standard model. This illustrates the substantial efficiency gains of the SIGMA model, particularly in long-context scenarios, and validates the theoretical analysis presented earlier in the paper.", "section": "3 Efficiency Analysis"}]