[{"figure_path": "https://arxiv.org/html/2502.12054/x1.png", "caption": "Figure 1: An illustration of example from our PhysReason benchmark. Due to space constraints, only key components are shown. Please refer to Appendix D for complete annotations.", "description": "Figure 1 shows an example problem from the PhysReason benchmark dataset.  The problem involves a ball suspended from a point O by a string, colliding with an identical ball on a table below.  The figure displays the diagram of the problem setup, the context which describes the setup, and sub-questions that need to be answered. The solution is not entirely shown in the figure, but it does show an example of a 'Step Analysis' in order to demonstrate the structure of the answer. The full solution with annotations for this problem, along with annotations for all problems in the benchmark dataset, is available in Appendix D.  PhysReason focuses on multi-step physics reasoning problems that often involve diagrams and require application of multiple physics theorems.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.12054/extracted/6210222/fig/analysis_subplots_v2.png", "caption": "Figure 2: Analysis of solution theorems, solution steps, and solution tokens across different problem categories, with comparisons from SciBench, GPQA, and OlympiadBench.", "description": "This figure compares the number of theorems, steps, and tokens used in solving problems across different difficulty levels (Knowledge, Easy, Medium, Hard) in the PhysReason benchmark.  It also includes a comparison with three other physics-based reasoning benchmarks: SciBench, GPQA, and OlympiadBench, showing how PhysReason problems are more complex, requiring more steps and tokens to solve, especially at the harder levels.  The purpose is to illustrate the increased difficulty and complexity of physics-based reasoning problems in PhysReason compared to existing benchmarks.", "section": "3 Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.12054/x2.png", "caption": "Figure 3: Step-level evaluation example obtained from PSAS-S framework.", "description": "This figure showcases a step-level evaluation example generated using the Physics Solution Auto-Scoring Framework (PSAS-S). PSAS-S is an automated evaluation method designed to assess the accuracy of reasoning steps within a physics problem solution.  The example demonstrates how PSAS-S extracts relevant information from a language model's response, compares it against the correct solution step-by-step, and provides an error analysis if necessary. The figure highlights the detailed breakdown of the evaluation process, showing specific steps identified as correct or incorrect and indicating the error type. This illustrative example helps to clarify how PSAS-S achieves a comprehensive step-level evaluation accuracy, exceeding 98% in the experimental results.", "section": "4 Evaluation Framework"}, {"figure_path": "https://arxiv.org/html/2502.12054/extracted/6210222/fig/error_types_count_selected.png", "caption": "Figure 4: Error statistics with PSAS-S framwork in PhysReason-mini, where Gemini-T-1206 and Gemini-T-0121 denote Gemini-2.0-Flash-Thinking-1206 and Gemini-2.0-Flash-Thinking-0121.", "description": "Figure 4 presents a bar chart visualizing the distribution of various error types identified by the Physics Solution Auto-Scoring Framework - Step Level (PSAS-S) within the PhysReason-mini benchmark.  The error types are: Diagram Analysis Errors, Physics Theorem Application Errors, Physics Process Understanding Errors, Physics Condition Analysis Errors, Calculation Process Errors, Boundary Condition Analysis Errors, and Variable Relationship Errors. The chart shows the frequency or percentage of each error type for several different language models. These models include Deepseek-R1, Gemini 2.0 Flash-Thinking-0121, Gemini 2.0 Flash-Thinking-1206, GLM-Zero, QwQ-32B, and 01-mini.  The figure highlights the types of errors that each model tends to make more frequently, offering insights into the strengths and weaknesses of each model's physics-based reasoning capabilities.", "section": "5.5 Error Kind Distribution Analysis"}, {"figure_path": "https://arxiv.org/html/2502.12054/extracted/6210222/fig/model_performance_analysis_v2.png", "caption": "Figure 5: Performance with PSAS-S framework in the hard problems from PhysReason-mini.", "description": "Figure 5 is a bar chart showing the performance of different large language models (LLMs) on the hard problems within the PhysReason-mini benchmark, using the Physics Solution Auto-Scoring Framework Step Level (PSAS-S) evaluation method.  The chart displays the cumulative scores for each model, highlighting their ability to accurately solve the complex, multi-step reasoning tasks presented in these challenging physics problems.  Models are ranked by performance, allowing for easy comparison of their strengths and weaknesses in physics-based reasoning.", "section": "5.5 Error Kind Distribution Analysis"}, {"figure_path": "https://arxiv.org/html/2502.12054/x3.png", "caption": "Figure 6: Illustration of the data collection pipeline.", "description": "The figure illustrates the multi-stage data collection pipeline used to build the PhysReason benchmark dataset.  Starting with the acquisition of raw data from multiple sources (international physics competitions, college entrance exams, etc.), the pipeline progresses through standardization, translation (to English), search prevention (removing problems easily solvable with internet searches), and finally, difficulty classification (categorizing problems into knowledge-based, easy, medium, and hard reasoning-based).  The end result is a comprehensive and rigorously curated dataset ready for use in benchmarking large language models' physics reasoning abilities.", "section": "3 Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.12054/x4.png", "caption": "Figure 7: A knowledge example in our benchmark.", "description": "This figure displays a sample question from the PhysReason benchmark categorized as a 'knowledge' level problem.  It presents a physics problem involving a circular metal ring within a changing magnetic field. The question requires calculating the induced electromotive force (emf) in the ring, the current flowing through it, and the resulting Joule heating power. The solution steps are shown and make use of Faraday's law of induction, Ohm's law, and Joule's law. The annotation shows the theorems used for each step. This example showcases how the benchmark assesses a fundamental understanding of physics principles and basic formula applications without complex reasoning processes.", "section": "3 Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.12054/x5.png", "caption": "Figure 8: An easy example in our benchmark.", "description": "This figure shows an example of an 'easy' problem from the PhysReason benchmark.  The problem involves a simple collision scenario between two balls: one suspended by a string and another on a frictionless surface. The solution requires applying fundamental physics principles such as the conservation of energy and momentum, with a relatively small number of steps to arrive at the answer. The diagram clearly illustrates the physical setup and the annotation details the solution steps.", "section": "3 Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.12054/x6.png", "caption": "Figure 9: A medium example in our benchmark.", "description": "This medium-difficulty example presents a thermally conductive cylindrical container with a piston, containing an ideal gas.  The problem involves analyzing the gas's behavior under different orientations (vertical inverted, vertical suspended, horizontal) and temperature changes.  It requires applying Boyle's Law, the Ideal Gas Law, and force equilibrium principles to determine gas volume and temperature under various conditions. The multi-step solution involves calculating pressure under different orientations and utilizing the gas laws to connect volume, pressure, and temperature.", "section": "3 Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.12054/x7.png", "caption": "Figure 10: A hard example in our benchmark.", "description": "This figure depicts a complex physics problem involving a small slider moving down a ramp, colliding with a ball, and the ball subsequently traversing a circular track and colliding with a prism. The problem requires multiple steps and the application of several physics principles including conservation of energy and momentum, and is designed to assess the model's ability to handle multi-step reasoning and complex scenarios.", "section": "3 Benchmark"}]