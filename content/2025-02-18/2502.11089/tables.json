[{"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.1\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MMLU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MMLU-PRO</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">CMMLU</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">BBH</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">GSM8K</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MATH</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">DROP</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MBPP</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">HumanEval</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1.11\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1.11.1\">Avg.</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.1.1\" style=\"font-size:70%;\">Acc. 5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.2.1\" style=\"font-size:70%;\">Acc. 5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.3.1\" style=\"font-size:70%;\">Acc. 5-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.4.1\" style=\"font-size:70%;\">Acc. 3-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.5.1\" style=\"font-size:70%;\">Acc. 8-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.6.1\" style=\"font-size:70%;\">Acc. 4-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.7.1\" style=\"font-size:70%;\">F1 1-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.8.1\" style=\"font-size:70%;\">Pass@1 3-shot</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.1.2.2.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T1.1.1.2.2.9.1\" style=\"font-size:70%;\">Pass@1 0-shot</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Full Attn</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.3.3.2.1\">0.567</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.279</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.576</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.497</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.486</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.263</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.503</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.3.3.9.1\">0.482</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.335</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.3.3.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.443</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.1.1.4.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">NSA</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.565</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.3.1\">0.286</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.4.1\">0.587</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.5.1\">0.521</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.6.1\">0.520</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.7.1\">0.264</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.8.1\">0.545</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.466</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.1.4.4.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.10.1\">0.348</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.1.1.4.4.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.4.11.1\">0.456</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Pretraining performance comparison between the full attention baseline and NSA on general benchmarks, across knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding (MBPP, HumanEval) tasks. NSA achieves superior average performance on most benchmarks despite high sparsity.", "description": "This table presents a comparison of the pretraining performance between a model using full attention and a model using the proposed Native Sparse Attention (NSA) mechanism.  The comparison is made across a range of benchmark tasks categorized into three areas: knowledge, reasoning, and coding.  Each area includes several specific benchmarks, providing a comprehensive evaluation of model performance across diverse task types. The results show NSA's performance, despite its sparsity, is comparable to or better than the full attention model on most benchmarks.", "section": "4. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T2.1.1.1.1.1\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T2.1.1.1.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">SQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T2.1.1.1.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MQA</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S4.T2.1.1.1.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Synthetic</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.1.1.1.1.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Code</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.1.1.1.1.6\" rowspan=\"2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text\" id=\"S4.T2.1.1.1.1.6.1\">Avg.</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MFQA-en</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MFQA-zh</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Qasper</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">HPQ</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">2Wiki</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">GovRpt</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Dur</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PassR-en</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">PassR-zh</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T2.1.1.2.2.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">LCC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.1.1.3.1.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">H2O</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.428</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.429</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.308</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.112</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.101</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.231</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.208</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.704</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.421</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.092</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.1.3.1.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.303</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.1.4.2.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">InfLLM</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.474</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.517</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.356</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.306</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.250</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.277</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.257</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.766</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.486</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.143</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.4.2.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.383</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.1.5.3.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Quest</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.495</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.561</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.365</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.295</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.245</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.293</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.257</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.792</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.478</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.135</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.5.3.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.392</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.1.6.4.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Exact-Top</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.502</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.605</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.397</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.321</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.288</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.6.4.7.1\">0.316</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.291</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.810</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.548</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.156</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.6.4.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.423</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.1.1.7.5.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">Full Attn</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.7.5.2.1\">0.512</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.3.1\">0.623</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.4.1\">0.409</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.5.1\">0.350</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.6.1\">0.305</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.7.5.7.1\">0.324</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.8.1\">0.294</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.9.1\">0.830</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.7.5.10.1\">0.560</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.11.1\">0.163</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.1.7.5.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.7.5.12.1\">0.437</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.1\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">NSA</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.2\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.8.6.2.1\">0.503</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.3\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.3.1\">0.624</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.4\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.4.1\">0.432</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.5\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.5.1\">0.437</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.6\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.6.1\">0.356</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.7\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.307</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.8\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.8.1\">0.341</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.9\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.9.1\">0.905</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.10\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T2.1.1.8.6.10.1\">0.550</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.11\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.11.1\">0.232</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T2.1.1.8.6.12\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.8.6.12.1\">0.469</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: Performance comparison between our NSA and baselines on LongBench, including subsets in single document QA, multi-document QA, synthetic and code task categories. NSA outperformed most of the baselines including Full Attention.", "description": "This table presents a performance comparison of different sparse attention methods, including the proposed NSA (Natively trainable Sparse Attention) model and several baseline methods, on the LongBench benchmark. LongBench is a comprehensive benchmark dataset that evaluates long-context understanding performance across diverse tasks such as single-document question answering, multi-document question answering, synthetic tasks, and code-related tasks. The table shows the performance of each model on each of these tasks, indicating that NSA outperforms most other methods, including the Full Attention model which serves as a strong baseline.", "section": "4.3. Performance Comparison"}, {"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T3.1.1.1.1.1\">Generation Token Limit</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.2\">\u00a0\u00a0 8192</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.1.1.1.1.3\">\u00a0\u00a0 16384</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.1.1.2.1.1\">Full Attention-R</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.2\">0.046</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.1.2.1.3\">0.092</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T3.1.1.3.2.1\">NSA-R</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.3.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.3.2.2.1\">0.121</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.1.3.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.3.2.3.1\">0.146</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: AIME Instruction-based Evaluating after supervised fine-tuning. Our NSA-R demonstrates better performance than Full Attention-R at both 8k and 16k sequence lengths", "description": "This table presents the results of evaluating two models, NSA-R (Natively trainable Sparse Attention) and Full Attention-R, on the American Invitational Mathematics Examination (AIME) after supervised fine-tuning.  Both models underwent fine-tuning using 10B tokens of 32k-length mathematical reasoning traces. The evaluation focused on instruction-based reasoning tasks and assessed performance at both 8k and 16k sequence lengths.  The key metric is the accuracy of the models in solving the AIME problems.  The table aims to showcase the superior performance of NSA-R compared to the Full Attention-R model for instruction-based reasoning tasks, specifically highlighting its effectiveness at different sequence lengths. This demonstrates that the NSA-R's architectural design and training approach are beneficial for long-context, complex reasoning tasks.", "section": "4. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.4.4\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.4.4.5.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T4.4.4.5.1.1\">Context Length</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.4.5.1.2\">\u00a0\u00a08192</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.4.5.1.3\">\u00a0\u00a0 16384</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.4.5.1.4\">32768</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.4.5.1.5\">\u00a0\u00a065536</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.4.6.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T4.4.4.6.2.1\">Full Attention</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.4.4.6.2.2\">\u00a0\u00a08192</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.4.4.6.2.3\">16384</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.4.4.6.2.4\">32768</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.4.4.6.2.5\">\u00a0\u00a065536</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.4.7.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T4.4.4.7.3.1\">NSA</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.7.3.2\">\u00a0\u00a02048</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.7.3.3\">2560</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.7.3.4\">3584</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.4.7.3.5\">\u00a0\u00a05632</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S5.T4.4.4.4.5\">Expected Speedup</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.1.1.1.1.1\">4<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.1.1.1.1.1.m1.1\"><semantics id=\"S5.T4.1.1.1.1.1.m1.1a\"><mo id=\"S5.T4.1.1.1.1.1.m1.1.1\" xref=\"S5.T4.1.1.1.1.1.m1.1.1.cmml\">\u00d7</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.1.1.1.1.1.m1.1b\"><times id=\"S5.T4.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.1.1.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.1.1.1.1.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T4.1.1.1.1.1.m1.1d\">\u00d7</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.2.2.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.2.2.2.2.1\">\u00a0\u00a06.4<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.2.2.2.2.1.m1.1\"><semantics id=\"S5.T4.2.2.2.2.1.m1.1a\"><mo id=\"S5.T4.2.2.2.2.1.m1.1.1\" xref=\"S5.T4.2.2.2.2.1.m1.1.1.cmml\">\u00d7</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.2.2.2.2.1.m1.1b\"><times id=\"S5.T4.2.2.2.2.1.m1.1.1.cmml\" xref=\"S5.T4.2.2.2.2.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.2.2.2.2.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T4.2.2.2.2.1.m1.1d\">\u00d7</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.3.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.3.3.3.3.1\">\u00a0\u00a09.1<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.3.3.3.3.1.m1.1\"><semantics id=\"S5.T4.3.3.3.3.1.m1.1a\"><mo id=\"S5.T4.3.3.3.3.1.m1.1.1\" xref=\"S5.T4.3.3.3.3.1.m1.1.1.cmml\">\u00d7</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.3.3.3.3.1.m1.1b\"><times id=\"S5.T4.3.3.3.3.1.m1.1.1.cmml\" xref=\"S5.T4.3.3.3.3.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.3.3.3.3.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T4.3.3.3.3.1.m1.1d\">\u00d7</annotation></semantics></math></span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.4.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.4.4.4.1\">\u00a0\u00a011.6<math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T4.4.4.4.4.1.m1.1\"><semantics id=\"S5.T4.4.4.4.4.1.m1.1a\"><mo id=\"S5.T4.4.4.4.4.1.m1.1.1\" xref=\"S5.T4.4.4.4.4.1.m1.1.1.cmml\">\u00d7</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.4.4.4.4.1.m1.1b\"><times id=\"S5.T4.4.4.4.4.1.m1.1.1.cmml\" xref=\"S5.T4.4.4.4.4.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.4.4.4.4.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T4.4.4.4.4.1.m1.1d\">\u00d7</annotation></semantics></math></span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 4: Memory access volume (in equivalent number of tokens) per attention operation during decoding. Due to the low arithmetic intensity and memory-bound nature of decoding, the expected speedup is approximately linear with the volume of memory access.", "description": "This table presents the memory access volume during the decoding phase for both the Full Attention model and the NSA model, measured in equivalent numbers of tokens.  The low arithmetic intensity and memory-bound nature of the decoding process lead to a nearly linear relationship between memory access volume and speedup.  Specifically, it shows how many tokens are accessed in each model during decoding for different sequence lengths (8192, 16384, 32768, and 65536 tokens). The expected speedup of the NSA model is also calculated and presented, demonstrating the significant efficiency gains achieved by the model.", "section": "5. Efficiency Analysis"}]