[{"figure_path": "https://arxiv.org/html/2502.11775/x1.png", "caption": "Figure 1: video-SALMONN-o1 model structure. The input video is processed by the visual and audio branches, generating encodings from the visual and audio frame sequences respectively. Two encoding streams are combined in an interleaved fashion to synchronize across time before sending to LLM.", "description": "The figure illustrates the architecture of the video-SALMONN-01 model.  It shows how input video data is processed through two parallel branches: one for visual information and one for audio.  Each branch uses an encoder (visual encoder and audio encoder) to extract relevant features from the respective input sequences (frames and audio segments).  These feature representations are then combined in an interleaved manner to align temporal information and create a unified representation suitable for processing by a Large Language Model (LLM). This unified representation is then inputted to the LLM for further analysis and task completion.", "section": "3. video-SALMONN-01"}, {"figure_path": "https://arxiv.org/html/2502.11775/x2.png", "caption": "Figure 2: Acquisition pipeline of reasoning-intensive SFT data. The question, answer and reasoning paths are generated by Gemini-1.5-pro taking the video with paired audio as inputs. GPT4o is employed for quality checks to ensure the QA-pair and the reasoning steps are valid and require logical thinking.", "description": "This figure illustrates the process of creating a dataset for fine-tuning (SFT) a large language model (LLM).  The process starts with a video and its corresponding audio. Gemini-1.5-pro, a large language model, is used to generate a question-answer pair related to the video content, along with a step-by-step reasoning path that explains how to arrive at the answer.  GPT-40, another LLM, then acts as a quality control mechanism, evaluating the validity and logical coherence of both the question-answer pair and the reasoning steps.  Only those QA pairs and reasoning paths that pass GPT-40's quality check are included in the final SFT dataset.  This ensures that the model learns from high-quality, logically sound reasoning examples.", "section": "3.2. Reasoning-intensive SFT Data"}, {"figure_path": "https://arxiv.org/html/2502.11775/x3.png", "caption": "Figure 3: Illustration of the contrastive step selection (top) and pairwise rollout (bottom) to construct per-step expected correctness score for pDPO. Contrastive step selection: Top 2 steps, s2subscript\ud835\udc602s_{2}italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and s5subscript\ud835\udc605s_{5}italic_s start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT are selected in this example, and for s2subscript\ud835\udc602s_{2}italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, an alternative step, s2\u2032subscriptsuperscript\ud835\udc60\u20322s^{\\prime}_{2}italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT, is sampled to form the preference pair. Pairwise rollout: Three rollouts are shown for each step and s2subscript\ud835\udc602s_{2}italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and s2\u2032subscriptsuperscript\ud835\udc60\u20322s^{\\prime}_{2}italic_s start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT are step pairs with the same prefix solution. The answer correctness is checked using GPT-4o by comparing it against the reference answer.", "description": "Figure 3 illustrates the process of contrastive step selection and pairwise rollout used in the process direct preference optimization (pDPO) method.  The top panel shows how the top two most influential steps (s2 and s5) are selected for optimization. For each step, an alternative step is sampled to create a pair used for comparison.  The bottom panel demonstrates the pairwise rollout process.  For each step (like s2), multiple possible next steps are simulated (rollouts).  These rollouts, starting from the same prefix, are then evaluated by GPT-40 to determine which is closer to the correct solution and thus which step was more effective.", "section": "4. Process DPO"}, {"figure_path": "https://arxiv.org/html/2502.11775/x4.png", "caption": "Figure 4: Distributions of the numbers of reasoning steps in SFT data. Left: Distribution of the entire SFT data. Right: Distribution on the reasoning-intensive subset of SFT data. Due to the difficulty of the reasoning-intensive subset, more reasoning steps are required in general for samples in this set.", "description": "This figure shows the distribution of the number of reasoning steps in the supervised fine-tuning (SFT) data. The left panel displays the distribution for the entire SFT dataset, while the right panel focuses on a subset of the data specifically designed for reasoning-intensive tasks.  The figure highlights that the reasoning-intensive subset, which contains more challenging questions, necessitates a greater number of reasoning steps for successful problem-solving, compared to the overall SFT dataset.", "section": "6.2. Data"}, {"figure_path": "https://arxiv.org/html/2502.11775/x5.png", "caption": "Figure 5: Comparison between different top T steps selected for pDPO. Pairs of full solution paths are always used in addition to pairs of intermediate steps.", "description": "This figure compares the performance of the process direct preference optimization (pDPO) algorithm using different numbers of top steps selected for pairwise training.  The x-axis represents the number of top steps selected, and the y-axis represents the accuracy.  The results show that including intermediate steps in addition to full solution paths improves the overall accuracy of pDPO, suggesting that focusing on specific, error-prone steps enhances the model's learning and reasoning capabilities. Note that pairs of full solution paths are always included in the training.", "section": "4. Training to Enhance Reasoning Abilities"}, {"figure_path": "https://arxiv.org/html/2502.11775/x6.png", "caption": "Figure 6: Example of reasoning SFT data", "description": "This figure shows an example from the reasoning-intensive SFT (Supervised Fine-Tuning) dataset. It illustrates a question-answer pair along with a step-by-step reasoning process. The visual input is a short video showing items made of glass and plastic. The question asks which material is better when considering frequent handling and minimal dropping risk. The provided answer is 'the glass one', and the figure details a six-step reasoning path leading to that conclusion. This showcases the multimodal nature of the dataset, combining visual information from a video with text-based questions, answers, and reasoning steps.", "section": "3.2. Reasoning-intensive SFT Data"}, {"figure_path": "https://arxiv.org/html/2502.11775/x7.png", "caption": "Figure 7: Example of StandUp part of the RivaBench.", "description": "This figure shows an example from the StandUp comedy section of the RivaBench dataset.  The example includes a short video clip, a question related to understanding the humor in the video, the correct answer, and an explanation clarifying why that answer is correct. The question requires the model to understand the interplay between the audio (comedian's words), visual (comedian's actions and facial expressions), and the audience's reaction (laughter) to determine the humor's source.  It highlights the multi-modal nature of the reasoning required within RivaBench.", "section": "5. Audio-visual Reasoning Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.11775/x8.png", "caption": "Figure 8: Example of StandUp part of the RivaBench.", "description": "Figure 8 shows an example from the StandUp comedy subset of the RivaBench dataset.  It displays a still image from a stand-up comedy video, the corresponding audio transcription, a multiple-choice question about the comedic effect of a particular line, the correct answer, and a detailed explanation justifying the answer. This exemplifies the type of high-quality, expert-annotated audio-visual reasoning data included in the RivaBench benchmark.", "section": "B. StandUp Data Examples"}, {"figure_path": "https://arxiv.org/html/2502.11775/x9.png", "caption": "Figure 9: Example of Academic part of the RivaBench.", "description": "Figure 9 shows an example from the Academic portion of the RivaBench dataset.  It displays a slide from an academic presentation about a twin study investigating the relationship between traumatic brain injury (TBI) and dementia. The figure also includes a portion of the accompanying audio transcript and the question and answer related to this video segment. The question assesses the study's method for isolating the impact of TBI on dementia risk, while the answer explains how the twin study design controls for genetic and early life factors by analyzing twins with differing TBI and dementia onset.", "section": "5. Audio-visual Reasoning Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.11775/x10.png", "caption": "Figure 10: Example of Academic part of the RivaBench.", "description": "Figure 10 shows an example from the Academic portion of the RivaBench dataset.  The figure contains a screenshot of a video showing students working on a hands-on electronics project using either a Zoom-based remote learning environment or a RobotAR augmented reality environment.  Accompanying the video screenshot is a textual description of the key learning competencies tested (conceptual understanding of voltage and current, series and parallel circuits, use of a breadboard, multimeter, and building a working circuit). The results show that students in the RobotAR condition achieved greater competency in more of the learning objectives than their peers using Zoom. The caption references the improved performance with RobotAR.", "section": "5. Audio-visual Reasoning Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.11775/x11.png", "caption": "Figure 11: Example video clip of the SynthDec part of RivaBench.", "description": "This figure shows an example video clip from the RivaBench benchmark's SynthDec (synthetic video detection) partition. The SynthDec partition contains videos generated using AI, making it challenging to distinguish between real and synthetic content. This particular clip is likely intended to showcase a particularly difficult example or characteristic of synthetic videos for which visual reasoning is needed to accurately classify.", "section": "5. Audio-visual Reasoning Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.11775/x12.png", "caption": "Figure 12: Example video clip of the SynthDec part of RivaBench.", "description": "This figure shows a short video clip from the RivaBench dataset's SynthDec (synthetic video detection) subset. The video clip is an example of a synthetic video, meaning it was artificially generated rather than recorded from real life.  SynthDec is designed to test the ability of large language models to differentiate between real and synthetic videos. This specific clip likely contains visual anomalies or inconsistencies that are characteristic of AI-generated videos and would be used to train or evaluate such a model. ", "section": "5. Audio-visual Reasoning Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.11775/x13.png", "caption": "Figure 13: Example video and solutions from the StandUp test set.", "description": "This figure shows a side-by-side comparison of how two different models, video-SALMONN-01 SFT and video-SALMONN-01 Process DPO, approached the same question from the StandUp subset of the RivaBench benchmark dataset.  The StandUp subset contains comedic video clips. The video shows a comedian performing on stage.  The question asks what the speaker implies by saying \"I didn't need to know that\" at the end of the video. Both models provide a step-by-step reasoning process leading to their respective answers. The figure highlights the differences in the reasoning process, showing how the enhanced reasoning capabilities of pDPO lead to a more accurate and nuanced understanding of the context compared to the simpler SFT model.", "section": "F. Case Studies: Solution with Reasoning Examples"}, {"figure_path": "https://arxiv.org/html/2502.11775/x14.png", "caption": "Figure 14: Example video and solutions from videoMME test set.", "description": "This figure showcases an example video frame from the VideoMME test set, accompanied by solution approaches from two distinct model versions: video-SALMONN-01 using standard supervised fine-tuning (SFT) and video-SALMONN-01 employing the proposed Process Direct Preference Optimization (pDPO).  The video depicts a scene involving a character in a game being struck by a turret.  The solutions highlight the different reasoning steps each model undertakes to arrive at its answer, and how pDPO leads to a more accurate response by leveraging its audio-visual reasoning capabilities.", "section": "7. Results"}, {"figure_path": "https://arxiv.org/html/2502.11775/x15.png", "caption": "Figure 15: Example video and solutions from videoMME test set.", "description": "This figure showcases a video from the VideoMME test set and presents two different solution approaches generated by the model.  The top solution demonstrates the model's initial attempt using supervised fine-tuning (SFT), revealing a flawed reasoning process due to misinterpreting visual information (mistaking moonlit sky for the moon). The bottom solution illustrates the enhanced performance achieved through the proposed process direct preference optimization (pDPO) method. Using pDPO, the model correctly identifies the missing element in the video.", "section": "7. Results"}, {"figure_path": "https://arxiv.org/html/2502.11775/x16.png", "caption": "Figure 16: Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection.", "description": "This figure showcases a comparative analysis of three different large language models (LLMs) - video-SALMONN-01, GPT-4, and Gemini-1.5-pro - in their ability to detect synthetic videos.  Each model is presented with the same video clip and asked to determine if it is synthetically generated or real. The figure displays the response of each model, highlighting the reasoning steps and the final conclusion reached by each model. This comparison underscores the varying capabilities of these LLMs in handling this nuanced task, demonstrating the strengths and weaknesses in their ability to process and interpret visual cues to identify artificial or synthetic video content.", "section": "G. Case Studies: Zero-shot Synthetic Video Detection"}, {"figure_path": "https://arxiv.org/html/2502.11775/x17.png", "caption": "Figure 17: Example output from video-SALMONN-o1, GPT-4o and Gemini-1.5-pro for synthetic video detection.", "description": "Figure 17 shows a comparison of the outputs of three different large language models (LLMs) \u2013 video-SALMONN-01, GPT-4, and Gemini-1.5-pro \u2013 when tasked with detecting whether a video is synthetically generated or real.  The models analyze the same video and provide a Yes/No answer, along with a step-by-step reasoning process justifying their conclusions. The figure highlights the differences in their reasoning abilities and the types of visual cues each model focuses on to arrive at its decision. This showcases the varying capabilities of these LLMs in identifying subtle visual artifacts characteristic of AI-generated videos.", "section": "G. Case Studies: Zero-shot Synthetic Video Detection"}, {"figure_path": "https://arxiv.org/html/2502.11775/x18.png", "caption": "Figure 18: Example of the contrastive step selection process where two sampled paths are shown and the scores dsksubscript\ud835\udc51subscript\ud835\udc60\ud835\udc58d_{s_{k}}italic_d start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT are given for each reasoning steps. The 3rd step in the first solution is wrong due to visual hallucination, and as a result, a very high score is assigned to that step and that step will be used to perform rollout.", "description": "Figure 18 illustrates the contrastive step selection process used in the process direct preference optimization (pDPO) method. Two example reasoning paths for answering a question about a video are shown, highlighting the step-by-step reasoning process.  Each step in the reasoning path has an associated score (d<sub>sk</sub>), representing the sensitivity of that step to small perturbations in the input video. A higher d<sub>sk</sub> score indicates a greater sensitivity, suggesting that the model is more likely to make an error at that step.  The figure demonstrates how the algorithm selects the most crucial steps for further optimization by focusing on steps with high d<sub>sk</sub> scores, particularly those where errors due to visual misinterpretations or hallucinations might occur. In this example, the third step in the first solution path demonstrates a visual hallucination, and hence it receives a very high d<sub>sk</sub> score and is selected for optimization via rollout.", "section": "4.3. Contrastive Step Selection"}]