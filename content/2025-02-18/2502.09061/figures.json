[{"figure_path": "https://arxiv.org/html/2502.09061/extracted/6200562/figures/example.png", "caption": "Figure 1: An example from the GSM-symbolic dataset (variables in blue) where unconstrained generation produces syntactically incorrect output, while constrained generation provides a syntactically valid but incorrect answer. CRANE, however, generates a correct answer.", "description": "The figure illustrates a comparison of different LLM decoding methods on a symbolic math problem from the GSM-symbolic dataset.  The problem involves calculating a cost based on rental hours, free hours, and cost per hour. The unconstrained method produces a syntactically incorrect answer (a Python expression with a syntax error). A constrained method produces a syntactically correct but numerically incorrect answer.  CRANE, the proposed method, generates both a syntactically correct and numerically correct answer, demonstrating its ability to balance correctness and reasoning capability.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09061/extracted/6200562/figures/crane.png", "caption": "Figure 2: \nCRANE adaptively switches between constrained LLM generation and unconstrained LLM generation based on start and end delimiters (in this example << and >>). Using these delimiters, CRANE dynamically tracks which windows (highlighted in the figure) of the LLM generation constraints should be applied to.", "description": "The figure illustrates how CRANE, a novel decoding algorithm, dynamically switches between constrained and unconstrained large language model (LLM) generation.  The transition between modes is controlled by start and end delimiters (e.g., << and >>).  When the model encounters a start delimiter, CRANE enters constrained generation mode, ensuring the output adheres to specific grammatical constraints. The constrained generation is highlighted visually.  When the end delimiter is encountered, the model returns to unconstrained generation, allowing for more flexible reasoning.  This adaptive approach enables CRANE to balance the benefits of both constrained and unconstrained decoding.", "section": "4. CRANE Algorithm"}, {"figure_path": "https://arxiv.org/html/2502.09061/extracted/6200562/figures/k_accuracy_Qwen2.5-Math-7B-Instruct.png", "caption": "Figure 3: Accuracy (%) of Qwen2.5-Math-7B-Instruct By Method and Number of Shots on GSM-Symbolic", "description": "This figure shows the accuracy of the Qwen2.5-Math-7B-Instruct language model on the GSM-Symbolic benchmark across different methods (unconstrained, constrained, unconstrained with Chain-of-Thought prompting, and CRANE) and varying numbers of few-shot examples provided during training.  It visualizes the impact of both the decoding method and the amount of training data on the model's performance in solving symbolic math word problems.", "section": "5. Evaluation"}]