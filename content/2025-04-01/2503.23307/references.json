{"references": [{"fullname_first_author": "Alexei Baevski", "paper_title": "wav2vec 2.0: A framework for self-supervised learning of speech representations", "publication_date": "2020-01-01", "reason": "This paper is crucial because MoCha leverages Wav2Vec2 for deriving audio conditions from raw waveforms, essential for speech-driven character animation."}, {"fullname_first_author": "Adam Polyak", "paper_title": "Movie gen: A cast of media foundation models", "publication_date": "2024-10-01", "reason": "This paper is important because the MoCha model follows a similar design to MovieGen, utilizing a DiT architecture for video generation."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "MoCha builds upon the Diffusion Transformer (DiT) architecture introduced in this paper, which is key to achieving high-quality, movie-grade talking character generation."}, {"fullname_first_author": "Wenxuan Zhang", "paper_title": "Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation", "publication_date": "2023-01-01", "reason": "This paper serves as a key baseline for comparison, representing a prominent approach in audio-driven talking face generation, which MoCha aims to surpass in realism and expressiveness."}, {"fullname_first_author": "Yuwei Guo", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2023-01-01", "reason": "This work is critical because MoCha extends diffusion models, as demonstrated in AnimateDiff, to jointly condition on speech and text, enabling lifelike character animations."}]}