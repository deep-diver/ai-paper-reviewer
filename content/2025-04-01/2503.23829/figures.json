[{"figure_path": "https://arxiv.org/html/2503.23829/x1.png", "caption": "Figure 1: Overview paradigm of RLVR with our cross-domain verifier.", "description": "This figure illustrates the process of Reinforcement Learning with Verifiable Rewards (RLVR) using a cross-domain verifier.  It shows three steps. Step 1 involves using a base language model and reasoning data (prompt and expert-written reference answer) to generate exploration data that includes the model's response and a correctness label from a teacher grader. Step 2 uses this exploration data to train a reward model. Step 3 employs the trained reward model within the RLVR framework to fine-tune the base model, resulting in a final policy that can generate more accurate and reliable responses across diverse domains.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.23829/x2.png", "caption": "Figure 2: Distribution of subject occurrences in the test set of ExamQA (excluding unclassified).", "description": "This bar chart visualizes the distribution of subjects within the ExamQA test set, excluding unclassified entries.  Each bar represents a subject category, and its height corresponds to the percentage of questions belonging to that subject.  The chart helps illustrate the diversity of subject matter within the dataset, showing which subjects are more prevalent than others in the test set used for evaluating the models.", "section": "4.1 Data"}, {"figure_path": "https://arxiv.org/html/2503.23829/x3.png", "caption": "Figure 3: Agreement between GPT-4o and Majority Vote with m Graders, measured by Cohen\u2019s Kappa.", "description": "This figure displays the level of agreement between using GPT-4 as a single evaluator versus using majority voting from multiple (m) Qwen2.5-72B-Instruct evaluators. The agreement is measured using Cohen's Kappa, a statistical measure of inter-rater reliability. The x-axis represents the number of Qwen2.5-72B-Instruct evaluators used in the majority voting (m), and the y-axis shows the corresponding Cohen's Kappa score.  Separate lines are plotted for math problems and multi-subject problems, illustrating how agreement varies depending on the type of problem and the number of evaluators. The chart shows a high level of agreement (approaching perfect agreement at 0.81 or higher) across various values of m, regardless of the problem type. This indicates robustness of the evaluation method.", "section": "4.3 Evaluation"}]