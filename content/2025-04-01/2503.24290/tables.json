[{"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S2.T1.1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S2.T1.1.1.1.1.1\">A conversation between User and Assistant. The user asks a question, and the Assistant solves it.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.1.2.2.1\">The assistant first thinks about the reasoning process in the mind and then provides the user</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.3.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.1.3.3.1\">with the answer.\nThe reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.4.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.1.4.4.1\">&lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt;</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.5.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.1.5.5.1\">&lt;answer&gt; answer here &lt;/answer&gt;.\nUser: <span class=\"ltx_text\" id=\"S2.T1.1.1.5.5.1.1\" style=\"color:#0000FF;\">You must put your answer inside &lt;answer&gt; &lt;/answer&gt; tags, i.e.,</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.6.6\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.1.6.6.1\"><span class=\"ltx_text\" id=\"S2.T1.1.1.6.6.1.1\" style=\"color:#0000FF;\">&lt;answer&gt; answer here &lt;/answer&gt;. And your final answer will be extracted automatically by the \\boxed{} tag.</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.7.7\">\n<td class=\"ltx_td ltx_align_left\" id=\"S2.T1.1.1.7.7.1\"><span class=\"ltx_text\" id=\"S2.T1.1.1.7.7.1.1\" style=\"color:#FF0000;\">{{prompt}}</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.8.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S2.T1.1.1.8.8.1\">Assistant: &lt;think&gt;</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Template for Open-Reasoner-Zero. prompt will be replaced with the specific reasoning question during training.", "description": "This table shows the template used for creating prompts in the Open-Reasoner-Zero training process.  The template includes placeholders for the user's question and the assistant's response, clearly defining how the model should format its answer.  The key element is the use of `<think>` and `<answer>` tags to structure the reasoning process and the final answer, enabling automated extraction of the correct response for evaluating the model's performance.", "section": "2.1. Basic Settings"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.2.1\">AIME 2024</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.3.1\">AIME 2025</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.4.1\">MATH500</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.5.1\">GPQA Diamond</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.2.1.1\">QwQ-32B-preview</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.2.1.2\">50.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.2.1.3\">33.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.2.1.4\">90.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.2.1.5\">54.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T2.1.3.2.1\">DeepSeek-R1-Zero-Qwen-32B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.3.2.2\">47.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.3.2.3\">-</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.3.2.4\">91.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.3.2.5\">55.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S3.T2.1.4.3.1\">Open-Reasoner-Zero-32B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.4.3.2\">48.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.4.3.3\">36.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.4.3.4\">92.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T2.1.4.3.5\">55.5</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: \nComparison of Open-Reasoner-Zero-32B with DeepSeek-R1-Zero-Qwen-32B and QwQ-32B-Preview on reasoning-related benchmarks. DeepSeek-R1-Zero-Qwen-32B results are from \u00a0[2], and no AIME2025 results are provided.", "description": "This table compares the performance of three different large language models (LLMs) on several reasoning benchmarks.  The models compared are Open-Reasoner-Zero-32B (the model introduced in this paper), DeepSeek-R1-Zero-Qwen-32B (a prior model from another research paper), and QwQ-32B-Preview (another baseline model). The benchmarks used assess the models' reasoning abilities across various tasks, and the results are presented as accuracy scores. Note that DeepSeek-R1-Zero-Qwen-32B's results are taken from a separate publication, and that model did not report results for the AIME2025 benchmark.", "section": "3. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S3.T3.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.2.1\">MMLU</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T3.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.1.1.3.1\">MMLU_PRO</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T3.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T3.1.2.1.1\">Qwen2.5-32B-Base</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.2.1.2\">83.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T3.1.2.1.3\">55.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S3.T3.1.3.2.1\">Qwen2.5-32B-Instruct</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.3.2.2\">83.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T3.1.3.2.3\">69.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T3.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S3.T3.1.4.3.1\">Open-Reasoner-Zero-32B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.4.3.2.1\">84.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T3.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T3.1.4.3.3.1\">74.4</span></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: Generalization performance of Open-Reasoner-Zero models on MMLU and MMLU_PRO benchmarks.\nThrough solely scaling up RL training on reasoning-oriented tasks, Open-Reasoner-Zero achieves superior performance on both benchmarks,\nsurpassing Qwen2.5-Instruct without any additional instruction tuning.\nThis demonstrates the remarkable effectiveness of our training pipeline in enhancing model generalization capabilities.", "description": "Table 3 presents the generalization performance of the Open-Reasoner-Zero models on the MMLU and MMLU_PRO benchmarks.  It demonstrates that by solely scaling up reinforcement learning (RL) on reasoning-oriented tasks, Open-Reasoner-Zero surpasses the Qwen2.5-Instruct model's performance without any extra instruction tuning. This highlights the significant improvement in model generalization capabilities achieved through the training pipeline used in Open-Reasoner-Zero.", "section": "3. Experiments"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A2.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"A2.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A2.T4.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T4.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.1.2.1\">AIME 2024</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T4.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.1.3.1\">AIME 2025</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T4.1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.1.4.1\">MATH500</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"A2.T4.1.1.1.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A2.T4.1.1.1.5.1\">GPQA Diamond</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"A2.T4.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A2.T4.1.2.1.1\">Open-Reasoner-Zero-0.5B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.2.1.2\">1.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.2.1.3\">0.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.2.1.4\">31.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"A2.T4.1.2.1.5\">12.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.1.3.2.1\">Open-Reasoner-Zero-1.5B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.3.2.2\">3.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.3.2.3\">1.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.3.2.4\">58.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.3.2.5\">16.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"A2.T4.1.4.3.1\">Open-Reasoner-Zero-7B</th>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.4.3.2\">17.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.4.3.3\">15.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.4.3.4\">81.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"A2.T4.1.4.3.5\">36.6</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"A2.T4.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"A2.T4.1.5.4.1\">Open-Reasoner-Zero-32B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.1.5.4.2\">48.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.1.5.4.3\">36.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.1.5.4.4\">92.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"A2.T4.1.5.4.5\">55.5</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 4: \nDetailed benchmark performance across Open-Reasoner-Zero model sizes.\nPerformance metrics are provided for four reasoning-oriented benchmarks:\nAIME 2024, AIME 2025, MATH500, and GPQA Diamond.\nAs model size increases from 0.5B to 32B parameters, we observe consistent and substantial improvements, underscoring the scalability and effectiveness of our minimalist reinforcement learning approach.", "description": "This table presents a detailed breakdown of the performance achieved by the Open-Reasoner-Zero models across four different reasoning benchmarks (AIME 2024, AIME 2025, MATH500, and GPQA Diamond).  It shows the accuracy of models with varying parameter sizes (0.5B, 1.5B, 7B, and 32B).  The results demonstrate a clear improvement in performance as the model size scales up, highlighting the effectiveness and scalability of the minimalist reinforcement learning approach used.", "section": "3.4 Evaluation Results"}]