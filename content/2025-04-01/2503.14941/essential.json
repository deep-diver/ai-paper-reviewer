{"importance": "This paper introduces an innovative framework for evaluating MLLMs, addressing key limitations of existing methods such as annotation costs and biases. It offers a promising direction for automating and improving the objectivity of multimodal model assessments and opens new avenues for future research.", "summary": "UPME: Peer review for MLLMs, minus human bias!", "takeaways": ["UPME framework effectively reduces reliance on human workload in MLLM evaluation by using unsupervised peer review.", "The vision-language scoring system of the UPME achieves high consistency with human evaluations, reducing verbosity and self-preference issues.", "Experimental results demonstrate that UPME aligns closely with human-designed benchmarks, showing potential for reliable automation."], "tldr": "Existing methods for evaluating Multimodal Large Language Models (MLLMs) face challenges due to high human annotation costs and biases. While automated approaches attempt to reduce workload, they often introduce biases.  To resolve these limitations, this paper introduces an Unsupervised Peer review MLLM Evaluation (**UPME**) framework which focuses on conducting objective evaluation.\n\n The **UPME** framework utilizes only image data, enabling models to generate questions automatically and conduct peer review assessments. To mitigate bias, a vision-language scoring system assesses response correctness, visual understanding, and image-text correlation. Results showed that UPME aligns with human-designed benchmarks and inherent preferences. ", "affiliation": "Peking University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.14941/podcast.wav"}