[{"figure_path": "https://arxiv.org/html/2504.17761/x1.png", "caption": "Figure 1: Overview of Step1X-Edit. Step1X-Edit is an open-source general editing model that achieves proprietary-level performance with comprehensive editing capabilities.", "description": "Figure 1 showcases the capabilities of Step1X-Edit, a novel open-source image editing model.  It demonstrates the model's ability to perform a wide range of editing tasks, achieving results comparable to closed-source, proprietary models.  The figure displays diverse before-and-after image pairs, each illustrating a different editing operation. These operations include motion changes, material modifications, subject additions and removals, background alterations, style transfers, text modifications, portrait beautifications, and color adjustments.  The variety and quality of the edits highlight Step1X-Edit's comprehensive and powerful editing capabilities.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.17761/x2.png", "caption": "Figure 2: Data Volume Comparison.", "description": "This bar chart compares the sizes of various image editing datasets.  It visually represents the number of data points (images and their corresponding editing instructions) in each dataset. This allows for a quick comparison of the scale of different publicly available and proprietary datasets used in image editing research, highlighting the relative size of the Step1X-Edit dataset.", "section": "3.1 Data Creation"}, {"figure_path": "https://arxiv.org/html/2504.17761/x3.png", "caption": "Figure 3: Data Construction Pipeline and Sub-Task Distribution.", "description": "This figure provides a visual representation of the data creation pipeline and sub-task distribution used in the Step1X-Edit model.  The left side details the data creation pipeline, showing the steps involved in creating high-quality image editing data for each of the eleven sub-tasks. This includes processes such as annotation, segmentation, image generation, filtering, and human verification. Each sub-task (e.g., subject addition, background change, style transfer) has a dedicated pipeline illustrated in the figure. The right side shows a pie chart summarizing the proportion of the training data allocated to each sub-task, offering a clear view of data distribution among different editing operations.", "section": "3 Step1X-Edit"}, {"figure_path": "https://arxiv.org/html/2504.17761/x4.png", "caption": "Figure 4: Framework of Step1X-Edit. Step1X-Edit leverages the image understanding capabilities of MLLMs to parse editing instructions and generate editing tokens, which are then decoded into images using a DiT-based network.", "description": "This figure illustrates the architecture of the Step1X-Edit model.  The model takes both an image and editing instructions as input. A Multimodal Large Language Model (MLLM) processes the image and instructions to generate a set of 'editing tokens' that represent the desired changes.  These tokens are then passed through a connector module to a Diffusion in Transformer (DiT)-based network, which decodes the tokens and generates the final edited image.", "section": "3 Step1X-Edit"}, {"figure_path": "https://arxiv.org/html/2504.17761/x5.png", "caption": "Figure 5: De-Identification Process.", "description": "The figure illustrates the de-identification process used to protect user privacy in the GEdit-Bench dataset.  The process involves a multi-faceted reverse image search across multiple public search engines to find publicly accessible alternative images similar to the original images. If no suitable alternatives are found, the editing instructions are carefully modified to maintain the original intent while ensuring anonymity. This ensures ethical data usage and preserves the benchmark's integrity.", "section": "4.1 GEdit(Genuine Edit)-Bench"}, {"figure_path": "https://arxiv.org/html/2504.17761/x10.png", "caption": "(a) VIEScore for the Intersection-subset.", "description": "This figure displays the VIEScore results for a subset of the GEdit-Bench benchmark. The 'Intersection-subset' refers to the set of editing tasks where all the evaluated models (open-source and closed-source) successfully produced results. The VIEScore metric provides a comprehensive assessment of the image editing quality, encompassing semantic consistency (how well the edit adheres to the instructions), perceptual quality (how visually natural the edited image appears), and an overall score.  Each point on the radar chart represents a specific editing task category, and the distance of the point from the center indicates the model's performance in that category.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2504.17761/x11.png", "caption": "(b) VIEScore for the Full set.", "description": "This figure shows the VIEScore (Visual Instruction Evaluation Score) for all editing tasks in the GEdit-Bench benchmark.  VIEScore assesses the quality of image edits by considering semantic consistency, perceptual quality, and an overall score.  The full set includes all test samples, not just the subset where all models produced valid results. The radar chart visualizes the performance of each image editing model across different editing tasks.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2504.17761/x12.png", "caption": "(c) VIEScore for the Intersection-subset.", "description": "This figure shows the VIEScore (Visual Instruction Editing Score) for a subset of the GEdit-Bench benchmark.  The Intersection-subset includes only the results where all tested models successfully produced outputs for given instructions.  The VIEScore is composed of three metrics: semantic consistency, perceptual quality and an overall score, providing a comprehensive evaluation of the image editing results in various sub-tasks within the benchmark.", "section": "4.2 Experimental Results"}, {"figure_path": "https://arxiv.org/html/2504.17761/x13.png", "caption": "(d) VIEScore for the Full set.", "description": "This figure shows the VIEScore (Visual Instruction Editing Score) for all sub-tasks in the GEdit-Bench benchmark.  It's a radar chart comparing different image editing models, including open-source options and proprietary models such as GPT-40.  Each axis represents a specific editing task, and the distance from the center reflects the model's performance on that task, according to the VIEScore metric, which incorporates semantic consistency and perceptual quality assessments.  The chart visualizes a comprehensive comparison across multiple tasks for all tested models, illustrating their strengths and weaknesses.", "section": "4.2 Experimental Results"}]