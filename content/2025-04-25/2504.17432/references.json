{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-01", "reason": "This paper introduces the CLIP model, which is a widely used approach for multimodal representation learning and a baseline for comparison in this work."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This paper introduces LLaVA, a significant MLLM that influenced the development of this paper."}, {"fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "publication_date": "2023-01-01", "reason": "This paper introduces BLIP-2, a model built on frozen image encoders and large language models that is related to multimodal understanding in UniME."}, {"fullname_first_author": "Ziyan Jiang", "paper_title": "Vlm2vec: Training vision-language models for massive multimodal embedding tasks", "publication_date": "2025-01-01", "reason": "This paper introduces VLM2Vec and the MMEB benchmark, both directly used in UniME's development and evaluation."}, {"fullname_first_author": "Tianyu Gao", "paper_title": "Simcse: Simple contrastive learning of sentence embeddings", "publication_date": "2021-04-01", "reason": "This paper introduces SimCSE, a self supervised learning method for sentence embeddings, and has influence in the knowledge distillation stage of this paper."}]}