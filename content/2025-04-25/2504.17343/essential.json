{"importance": "This paper introduces **TimeChat-Online**, a novel streaming video understanding framework. By addressing the high visual redundancy inherent in streaming videos, this research significantly advances efficient video processing, opening new avenues for real-time video analysis and interaction.", "summary": "TimeChat-Online: Redundant Visual Tokens", "takeaways": ["Differential Token Dropping (DTD) reduces video tokens by 82.8% while maintaining accuracy.", "TimeChat-Online enables real-time video interaction with proactive response capabilities.", "The new TimeChat-Online-139K dataset facilitates more flexible streaming VideoQA interactions."], "tldr": "Online video platforms need real-time video understanding, but current Video Large Language Models (VideoLLMs) struggle with streaming due to dense, redundant frames and the need for instant user interaction. Short-context models lose context, while long-context models are slow. Existing compression methods are inflexible and language-guided approaches are inefficient. The key is addressing the fundamental challenge of visual redundancy. \n\nThis paper introduces **TimeChat-Online**, featuring Differential Token Drop (DTD), inspired by human visual perception. DTD preserves meaningful temporal changes, filtering static content and reducing video tokens by 82.8% without user queries. **TimeChat-Online-139K**, a new dataset, supports backward tracing, current perception, and future-responding scenarios. This achieves superior performance on streaming benchmarks and enhances long-form video tasks, revealing natural video redundancy for future VideoLLMs.", "affiliation": "Peking University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.17343/podcast.wav"}