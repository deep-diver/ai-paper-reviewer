{"references": [{"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This seminal paper introduced the Transformer architecture, which is the foundation for the autoregressive models used in the paper."}, {"fullname_first_author": "Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-01-01", "reason": "This paper introduces VQGAN, used in this study to encode the images, decoder, and codebook of 1024 tokens."}, {"fullname_first_author": "Van Den Oord", "paper_title": "Conditional image generation with pixelcnn decoders", "publication_date": "2016-01-01", "reason": "This paper describes a common technique for image generation, that is improved upon by this paper."}, {"fullname_first_author": "OpenAI", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper describes a common technique for language generation, that is improved upon by this paper."}, {"fullname_first_author": "Uria", "paper_title": "Neural autoregressive distribution estimation", "publication_date": "2016-01-01", "reason": "This work introduces NADE, a deep feed-forward neural network that is trained to compute conditional distributions for any variable given any subset of others."}]}