---
title: "Distilling semantically aware orders for autoregressive image generation"
summary: "Smarter Image Generation: This paper distills semantic knowledge to optimize patch order in autoregressive models, improving image quality."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Computer Vision", "Image Generation", "üè¢ Stony Brook University, NY, USA",]
showSummary: true
date: 2025-04-23
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.17069 {{< /keyword >}}
{{< keyword icon="writer" >}} Rishav Pramanik et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-04-25 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.17069" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.17069" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.17069/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

AR models require a patch-generation order, but there's no inherent order for images. The traditional raster-scan order is suboptimal because it ignores image content causality. For example, in an image of a sunset, clouds are generated before the sun, even though cloud color depends on the sun. Therefore, the paper argues that learning a semantic generation order makes more sense for images. 



This paper introduces a content-dependent and semantically aware generation of orders for AR patch-based image generation to improve overall generation quality. First, the model is trained with any given order, followed by self-supervised finetuning using the semantically aware orders generated by the any-order model. Experiments on two datasets show that this method produces better images than the traditional raster-scan approach, with similar training costs and no extra annotations.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Autoregressive image generation benefits from semantically aware patch ordering. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} A two-stage training process, involving order discovery and fine-tuning, enhances image quality. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} The proposed method outperforms traditional raster-scan approaches with similar training costs. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper is important for researchers because it **introduces a novel approach to autoregressive image generation** that **enhances image quality** and **semantic understanding**. It **challenges the traditional raster-scan order** and opens new avenues for **content-aware image synthesis**, potentially impacting various applications in computer vision and generative modeling.

------
#### Visual Insights



![](https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_1/fig1_celeba_fashion-row.png)

> üîº This figure visualizes the generation process of the Ordered Autoregressive (OAR) model on two datasets: Fashion Product and Multimodal CelebA-HQ.  The left panel shows the Fashion Product dataset, where the model starts by generating the simple white background, then progresses to more complex areas like the objects. The right panel shows the CelebA-HQ dataset, where generation begins with basic facial features (cheeks, chin) and gradually moves to more detailed elements. Color intensity represents the generation order, with yellow indicating early patches and violet representing later patches. The figure demonstrates that the OAR model's learned generation order prioritizes simpler image regions, making the generation process more efficient and intuitive.
> <details>
> <summary>read the caption</summary>
> Figure 1: Generation with our distilled order on the Fashion Product dataset (Left) and the Multimodal CelebA-HQ dataset (Right) with the corresponding generation order produced by our Ordered Autoregressive (OAR) model. The generation order is visualized through color intensity, progressing from yellow (early patches) to violet (later patches). Our learned order typically starts with simpler regions of the image before moving to more complex ones. For the Fashion Product dataset, this often means generating the white background first, while in the CelebA-HQ dataset, the model tends to begin with facial regions like the cheeks and chin, which are generally easier to generate.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.3.3.4">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.3.5">Train</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.3.3.6">Generation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.1.1.1">FID <math alttext="(\downarrow)" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.m1.1a"><mrow id="S5.T1.1.1.1.m1.1.2.2"><mo id="S5.T1.1.1.1.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T1.1.1.1.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T1.1.1.1.m1.1.1.cmml">‚Üì</mo><mo id="S5.T1.1.1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">(\downarrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.m1.1d">( ‚Üì )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.2.2.2">IS <math alttext="(\uparrow)" class="ltx_Math" display="inline" id="S5.T1.2.2.2.m1.1"><semantics id="S5.T1.2.2.2.m1.1a"><mrow id="S5.T1.2.2.2.m1.1.2.2"><mo id="S5.T1.2.2.2.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T1.2.2.2.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T1.2.2.2.m1.1.1.cmml">‚Üë</mo><mo id="S5.T1.2.2.2.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m1.1b"><ci id="S5.T1.2.2.2.m1.1.1.cmml" xref="S5.T1.2.2.2.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m1.1c">(\uparrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.m1.1d">( ‚Üë )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T1.3.3.3">KID <math alttext="(\downarrow)" class="ltx_Math" display="inline" id="S5.T1.3.3.3.m1.1"><semantics id="S5.T1.3.3.3.m1.1a"><mrow id="S5.T1.3.3.3.m1.1.2.2"><mo id="S5.T1.3.3.3.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T1.3.3.3.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T1.3.3.3.m1.1.1.cmml">‚Üì</mo><mo id="S5.T1.3.3.3.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><ci id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">(\downarrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.m1.1d">( ‚Üì )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T1.3.3.7">d</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.3.4.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T1.3.4.1.1">Raster</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.4.1.2">Raster</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.3.4.1.3">Raster</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.4.1.4">4.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.4.1.5">1.106</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.3.4.1.6">0.0031</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T1.3.4.1.7">1.83</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.5.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.3.5.2.1">Random-Raster</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.5.2.2">Random</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.5.2.3">Raster</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.5.2.4">4.38</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.5.2.5">1.102</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.5.2.6">0.0031</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.5.2.7">1.83</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.6.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.3.6.3.1">Random</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.6.3.2">Random</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.6.3.3">Random</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.6.3.4">4.07</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.6.3.5">1.103</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.6.3.6">0.0028</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.6.3.7">8.19</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.7.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T1.3.7.4.1">Ordered (Ours)</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.7.4.2">Random</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.7.4.3">Ordered</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.7.4.4">3.02</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.7.4.5">1.108</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T1.3.7.4.6">0.0019</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.7.4.7">4.34</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.8.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T1.3.8.5.1">Fine-tuned Ordered (Ours)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.8.5.2">Ordered</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.3.8.5.3">Ordered</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.8.5.4">2.56</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.8.5.5">1.111</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T1.3.8.5.6">0.0015</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T1.3.8.5.7">3.99</td>
</tr>
</tbody>
</table>{{< /table-caption >}}

> üîº This table presents a comparison of image generation results using different patch generation orders in an autoregressive model.  The methods compared are: raster-scan (traditional top-to-bottom left-to-right order), random order, an 'ordered' method proposed by the authors, and a 'fine-tuned ordered' version. The evaluation metrics used are FID (Fr√©chet Inception Distance), IS (Inception Score), KID (Kernel Inception Distance), and the average distance ('d') between consecutively generated patches. Lower FID, KID, and higher IS values indicate better image quality. The 'd' metric reflects the spatial coherence of the generation process; lower values suggest more localized patch generation.
> <details>
> <summary>read the caption</summary>
> Table 1: Generation with different orders. Our ordered generation improves over the standard raster-scan order and random order generation similar to¬†Li et¬†al. (2024). FID is the Fr√©chet inception distance, IS is the Inception Score and KID Kernel Inception Distance. d denotes the average distance between subsequently generated patches.
> </details>





### In-depth insights


#### Order Matters
The notion of "order matters" is pivotal in understanding the mechanics of autoregressive models, particularly in image generation. While sequence is inherently defined in text generation, where a left-to-right or similar structure mirrors the temporal flow of language, images lack such an intuitive order. This absence of a natural order presents both a challenge and an opportunity. The traditional raster-scan approach, while simple, is often suboptimal as it disregards the intrinsic causality and semantic relationships within an image. For instance, generating background elements before foreground objects can lead to inconsistencies. However, by training models to generate patches in any order and then distilling the knowledge to infer the optimal generation sequence, we can significantly improve image quality. This approach allows the model to prioritize semantically important regions, respecting the dependencies between different parts of the image and ultimately leading to more coherent and visually appealing results. The **order in which an image is constructed can greatly influence the final output, highlighting the need for semantically aware and content-dependent generation strategies**.

#### Any-Order AR
**Any-Order Autoregressive (AR) models** represent a paradigm shift in sequential data generation, offering flexibility by removing the constraint of a fixed generation order. Traditional AR models process data in a predetermined sequence, such as raster-scan for images, which can be suboptimal as it fails to capture semantic dependencies or inherent causality. **Any-Order AR** models, on the other hand, learn to generate data in any order, allowing the model to discover the most efficient and semantically meaningful sequence. This approach typically involves training a model to predict the next element given any subset of previously generated elements, effectively learning a joint distribution without imposing a specific factorization order. This flexibility enables the model to adapt to the specific characteristics of the data, potentially leading to improved generation quality and efficiency. However, training **Any-Order AR** models can be computationally challenging, requiring sophisticated techniques to manage the vast space of possible generation orders and to ensure convergence. Strategies such as uniform sampling of orders, order-agnostic training, and distillation methods are often employed to mitigate these challenges. Further enhancing the **Any-Order AR** paradigm lies in developing novel training methodologies and architectural designs that can efficiently capture long-range dependencies and contextual information, enabling the generation of high-quality and semantically coherent sequences.

#### Distilling Order
**Distilling order** in autoregressive image generation refers to extracting and refining the optimal sequence for generating image patches. Instead of relying on a fixed raster-scan order, which may not align with the image's semantic structure, the idea is to learn an order that prioritizes semantically important regions or those that are easier to generate first. The process typically involves training a model to generate images with arbitrary orders, then analyzing the learned representations to infer the most effective order. This inferred order can then be used to fine-tune the model, improving image quality and generation efficiency by focusing on a more natural, content-aware sequence. It's an effort to inject semantic understanding into the generation process, leading to more coherent and visually pleasing results.

#### Relative Pos.Enc.
Relative positional encoding moves beyond simply embedding the absolute location of each patch, instead focusing on the spatial relationships *between* patches. This is crucial for autoregressive image generation, as the model needs to understand how each newly generated patch relates to the existing context. By encoding the relative distances and orientations, the model can better capture local dependencies and ensure a more coherent and structured image. This approach promotes local context and is useful for better generation. Models use **absolute and relative position encoding**. The *relative* one helps to *capture local dependencies*. 

#### KV Caching Impact
KV caching is a crucial optimization, especially for large language models (LLMs) and autoregressive models, significantly reducing latency. It works by storing key and value projections of previously generated tokens, avoiding redundant re-computation, this is particularly beneficial in causal masking scenarios. Without KV caching, there is performance overhead as the generation process requires to compute the possible locations in parallel. The optimization enables inference efficiency to be comparable to traditional AR models, balancing the computational load and model performance.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.17069/x1.png)

> üîº Figure 2 illustrates three different autoregressive (AR) image generation approaches. The top panel shows the traditional raster-scan method, where patches are generated sequentially from top-left to bottom-right. Each input token includes the patch's content and its position. The middle panel depicts an 'any-given-order' approach, allowing patches to be generated in any order but requiring the position of the next patch as additional input.  The bottom panel presents the 'Ordered Autoregressive' method proposed in the paper. This method starts with an any-given-order model, but instead of receiving the next patch position as input, it generates all possible patch positions and selects the one with the highest probability (represented by darker yellow shading), resulting in a semantically aware generation order.
> <details>
> <summary>read the caption</summary>
> Figure 2: Different Autoregressive (AR) models. (Top) A raster scan is the normal approach for autoregressive generation from top left to bottom-right. The input token contains the content xisubscriptùë•ùëñx_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and the position lisubscriptùëôùëñl_{i}italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT. (Middle) Any-given-order learns to generate tokens at any possible location. However, the position of the next token should be given as input in an additional positional embedding. (Bottom) Our method, Ordered Autoregressive, uses the any-given-order model but generates all possible positions and selects the most likely one (darker yellow) as the next generated token.
> </details>



![](https://arxiv.org/html/2504.17069/extracted/6384630/Images/fig_samples/merged.png)

> üîº Figure 3 demonstrates the impact of using different generation orders on image quality for the Fashion Products dataset. The top row displays images generated using a standard raster scan autoregressive (AR) model, progressing from top-left to bottom-right. The middle row shows images generated with the proposed ordered AR model, which learns a semantically aware order instead of using a fixed raster scan. The bottom row visualizes the learned generation order for each image using a color gradient from yellow (early patches) to violet (later patches). The color intensity shows the sequence in which the patches were generated. This visualization reveals that the ordered AR model tends to generate simpler regions first (like the background), before progressing to more complex image components.  By generating patches in an order highly correlated with the image content, the ordered AR model achieves improved image quality, as evident by comparing the middle row (ordered AR) with the top row (raster AR).
> <details>
> <summary>read the caption</summary>
> Figure 3: Examples of generation on the Fashion Products dataset. (Top) Generated images with raster AR mode. (Middle) Generated images with ordered AR model. (Bottom) Generation order, from yellow to violet. From these images, we see that our approach finds an order highly correlated with the image content, often resulting in better image quality.
> </details>



![](https://arxiv.org/html/2504.17069/extracted/6384630/Images/celeba.png)

> üîº Figure 4 presents a comparison of image generation results on the CelebA-HQ dataset using two different autoregressive (AR) models: a raster-scan AR model and the proposed ordered AR model.  The top row shows images generated by the raster-scan model, which processes image patches sequentially from left-to-right and top-to-bottom. The middle row displays images generated by the ordered AR model, which generates patches in a semantically meaningful order determined by the model itself. This order prioritizes generating salient facial features (such as eyes, nose, and mouth) before generating less important details like hair and the background. The bottom row visualizes this learned generation order using a color gradient, where yellow indicates earlier-generated patches and violet indicates later-generated patches.  The ordered AR model demonstrates improved image quality, with generated images exhibiting greater smoothness, richer contextual details, and better alignment with the text prompts compared to the images produced by the raster-scan AR model.
> <details>
> <summary>read the caption</summary>
> Figure 4: Examples of generation on the CelebA dataset. (Top) Generated images with raster AR mode. (Middle) Generated images with ordered AR model. (Bottom) Generation order, from yellow to violet. On this dataset our model generates first the salient parts of a face, leaving hair and background at the end. Our model produces images with greater smoothness, rich context and more aligned with the text
> </details>



![](https://arxiv.org/html/2504.17069/extracted/6384630/Images/avg.png)

> üîº This figure visualizes the impact of absolute versus relative positional encodings on the generation order in an autoregressive image generation model.  The top panel shows the generation sequence with absolute positional encoding; the patches are scattered across the image.  In contrast, the bottom panel, using relative positional encoding, demonstrates a more localized generation process with patches clustered together.  Quantitatively, the average Euclidean distance between consecutively generated patches is significantly smaller (4.34) with relative encoding compared to absolute encoding (5.78), highlighting the effectiveness of relative encoding in generating spatially coherent images.
> <details>
> <summary>read the caption</summary>
> Figure 5: Generation order with absolute and relative positioning encoding. (Top) With absolute encoding the generation is very scattered. (Bottom) With relative positioning the generation is more localized. The average euclidean distance between the subsequently generated patches in case of absolute encoding is 5.78 whereas in case of relative encoding it is 4.34
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.4.4.5">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T2.1.1.1"><math alttext="l_{i+1}" class="ltx_Math" display="inline" id="S5.T2.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.m1.1a"><msub id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.m1.1.1.2" xref="S5.T2.1.1.1.m1.1.1.2.cmml">l</mi><mrow id="S5.T2.1.1.1.m1.1.1.3" xref="S5.T2.1.1.1.m1.1.1.3.cmml"><mi id="S5.T2.1.1.1.m1.1.1.3.2" xref="S5.T2.1.1.1.m1.1.1.3.2.cmml">i</mi><mo id="S5.T2.1.1.1.m1.1.1.3.1" xref="S5.T2.1.1.1.m1.1.1.3.1.cmml">+</mo><mn id="S5.T2.1.1.1.m1.1.1.3.3" xref="S5.T2.1.1.1.m1.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T2.1.1.1.m1.1.1.2.cmml" xref="S5.T2.1.1.1.m1.1.1.2">ùëô</ci><apply id="S5.T2.1.1.1.m1.1.1.3.cmml" xref="S5.T2.1.1.1.m1.1.1.3"><plus id="S5.T2.1.1.1.m1.1.1.3.1.cmml" xref="S5.T2.1.1.1.m1.1.1.3.1"></plus><ci id="S5.T2.1.1.1.m1.1.1.3.2.cmml" xref="S5.T2.1.1.1.m1.1.1.3.2">ùëñ</ci><cn id="S5.T2.1.1.1.m1.1.1.3.3.cmml" type="integer" xref="S5.T2.1.1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">l_{i+1}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.m1.1d">italic_l start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.2.2.2">FID <math alttext="(\downarrow)" class="ltx_Math" display="inline" id="S5.T2.2.2.2.m1.1"><semantics id="S5.T2.2.2.2.m1.1a"><mrow id="S5.T2.2.2.2.m1.1.2.2"><mo id="S5.T2.2.2.2.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T2.2.2.2.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T2.2.2.2.m1.1.1.cmml">‚Üì</mo><mo id="S5.T2.2.2.2.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><ci id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">(\downarrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.m1.1d">( ‚Üì )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.3.3">IS <math alttext="(\uparrow)" class="ltx_Math" display="inline" id="S5.T2.3.3.3.m1.1"><semantics id="S5.T2.3.3.3.m1.1a"><mrow id="S5.T2.3.3.3.m1.1.2.2"><mo id="S5.T2.3.3.3.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T2.3.3.3.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T2.3.3.3.m1.1.1.cmml">‚Üë</mo><mo id="S5.T2.3.3.3.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><ci id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">(\uparrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.m1.1d">( ‚Üë )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T2.4.4.4">KID <math alttext="(\downarrow)" class="ltx_Math" display="inline" id="S5.T2.4.4.4.m1.1"><semantics id="S5.T2.4.4.4.m1.1a"><mrow id="S5.T2.4.4.4.m1.1.2.2"><mo id="S5.T2.4.4.4.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T2.4.4.4.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T2.4.4.4.m1.1.1.cmml">‚Üì</mo><mo id="S5.T2.4.4.4.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.m1.1b"><ci id="S5.T2.4.4.4.m1.1.1.cmml" xref="S5.T2.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.m1.1c">(\downarrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.m1.1d">( ‚Üì )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.4.6">d</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.4.5.1.1">Raster</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.4.5.1.2">-</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.5.1.3">4.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.5.1.4">1.106</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.4.5.1.5">0.0031</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.5.1.6">1.83</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.4.6.2.1">Ordered</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T2.4.6.2.2">Abs.</th>
<td class="ltx_td ltx_align_center" id="S5.T2.4.6.2.3">3.96</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.6.2.4">1.102</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.4.6.2.5">0.0024</td>
<td class="ltx_td ltx_align_center" id="S5.T2.4.6.2.6">5.78</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.4.7.3.1">Ordered</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T2.4.7.3.2">Rel.</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.7.3.3">3.02</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.7.3.4">1.108</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.4.7.3.5">0.0019</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.7.3.6">4.34</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents a comparison of the performance of an autoregressive (AR) image generation model using two different positional encoding methods: absolute and relative.  The model's performance is evaluated across several metrics, including Fr√©chet Inception Distance (FID), Inception Score (IS), Kernel Inception Distance (KID), and the average distance between consecutively generated patches (d).  The results show how the choice of positional encoding affects the model's ability to learn and generate high-quality images.
> <details>
> <summary>read the caption</summary>
> Table 2: Generation with absolute and relative positional encoding for the next token.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.1.1.1"><math alttext="\lambda" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.m1.1a"><mi id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.m1.1d">italic_Œª</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.2.2.2">FID <math alttext="(\downarrow)" class="ltx_Math" display="inline" id="S5.T3.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.m1.1a"><mrow id="S5.T3.2.2.2.m1.1.2.2"><mo id="S5.T3.2.2.2.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T3.2.2.2.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T3.2.2.2.m1.1.1.cmml">‚Üì</mo><mo id="S5.T3.2.2.2.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">(\downarrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.m1.1d">( ‚Üì )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.3.3.3">IS <math alttext="(\uparrow)" class="ltx_Math" display="inline" id="S5.T3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.m1.1a"><mrow id="S5.T3.3.3.3.m1.1.2.2"><mo id="S5.T3.3.3.3.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T3.3.3.3.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T3.3.3.3.m1.1.1.cmml">‚Üë</mo><mo id="S5.T3.3.3.3.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">(\uparrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.m1.1d">( ‚Üë )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T3.4.4.4">KID <math alttext="(\downarrow)" class="ltx_Math" display="inline" id="S5.T3.4.4.4.m1.1"><semantics id="S5.T3.4.4.4.m1.1a"><mrow id="S5.T3.4.4.4.m1.1.2.2"><mo id="S5.T3.4.4.4.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T3.4.4.4.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T3.4.4.4.m1.1.1.cmml">‚Üì</mo><mo id="S5.T3.4.4.4.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.m1.1b"><ci id="S5.T3.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.m1.1c">(\downarrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.m1.1d">( ‚Üì )</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.5">d</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.4.5.1.1">0.0</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.5.1.2">3.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.5.1.3">1.106</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.4.5.1.4">0.0023</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.4.5.1.5">5.34</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.4.6.2.1">0.3</th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.6.2.2">3.11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.6.2.3">1.106</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.6.2.4">0.0021</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.6.2.5">4.65</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.4.7.3.1">0.5</th>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.3.2">3.02</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.3.3">1.108</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.4.7.3.4">0.0019</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.7.3.5">4.34</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.4.8.4.1">0.7</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.8.4.2">3.05</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.8.4.3">1.106</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.4.8.4.4">0.0019</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.4.8.4.5">4.09</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents the results of experiments evaluating the effect of different distance penalty regularization strengths (Œª) on the quality of image generation using an autoregressive model.  The regularization encourages the model to generate spatially close patches during inference. The table shows how varying Œª impacts the FID, IS, KID scores and the average distance between consecutively generated patches (d). Lower FID and KID scores, and higher IS score indicate improved image quality.
> <details>
> <summary>read the caption</summary>
> Table 3: Generation with different penalty regularization.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.3.1.3">Dist. Reg.</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.1.1">FID <math alttext="(\downarrow)" class="ltx_Math" display="inline" id="S5.T4.3.1.1.m1.1"><semantics id="S5.T4.3.1.1.m1.1a"><mrow id="S5.T4.3.1.1.m1.1.2.2"><mo id="S5.T4.3.1.1.m1.1.2.2.1" stretchy="false">(</mo><mo id="S5.T4.3.1.1.m1.1.1" lspace="0em" rspace="0em" stretchy="false" xref="S5.T4.3.1.1.m1.1.1.cmml">‚Üì</mo><mo id="S5.T4.3.1.1.m1.1.2.2.2" stretchy="false">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.T4.3.1.1.m1.1b"><ci id="S5.T4.3.1.1.m1.1.1.cmml" xref="S5.T4.3.1.1.m1.1.1">‚Üì</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.1.1.m1.1c">(\downarrow)</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.1.1.m1.1d">( ‚Üì )</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.2.1.1">Raster-Scan</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.2.1.2">‚úò</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.2.1.3">1.94</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.2">
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.2.1">Ordered</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.2.2">‚úò</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.3.2.3">1.68</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.4.3">
<td class="ltx_td ltx_align_center" id="S5.T4.3.4.3.1">Ordered</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.4.3.2">‚úì</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.4.3.3">1.52</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.5.4.1">Fine Tuned Ordered</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.5.4.2">‚úì</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.5.4.3">1.41</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table presents a comparison of image generation results on the Multi-Modal CelebA-HQ dataset using different generation orders: raster scan (the standard approach), random order, and the proposed ordered generation method.  It shows the Fr√©chet Inception Distance (FID), a metric evaluating the quality of generated images.  Lower FID values indicate better image quality.  The table also includes results obtained with and without distance regularization (a technique to encourage local patch generation), denoted by Dist. Reg.  The parameter Œª controls the strength of this regularization, and its effect on image quality is examined by comparing different Œª values. The results show that the proposed ordered generation method consistently improves FID scores over the raster-scan approach, highlighting the benefit of learning a semantically-aware order.
> <details>
> <summary>read the caption</summary>
> Table 4: Generation with Different Orders on the Multi-Modal CelebA-HQ Dataset. Our ordered generation improves over the standard raster-scan order. Dist. Reg. is the distance regularization parameter (ŒªùúÜ\lambdaitalic_Œª, see Equation: 8)
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A4.F5.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.F5.2.2">
<td class="ltx_td ltx_align_center" id="A4.F5.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="269" id="A4.F5.1.1.1.g1" src="extracted/6384630/Images/fig_3/abs.png" width="269"/></td>
<td class="ltx_td ltx_align_center" id="A4.F5.2.2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="269" id="A4.F5.2.2.2.g1" src="extracted/6384630/Images/fig_3/abs_arrow.png" width="269"/></td>
</tr>
<tr class="ltx_tr" id="A4.F5.4.4">
<td class="ltx_td ltx_align_center" id="A4.F5.3.3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="269" id="A4.F5.3.3.1.g1" src="extracted/6384630/Images/fig_3/rel.png" width="269"/></td>
<td class="ltx_td ltx_align_center" id="A4.F5.4.4.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="269" id="A4.F5.4.4.2.g1" src="extracted/6384630/Images/fig_3/rel_arrow.png" width="269"/></td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> üîº This table compares the inference time for generating a single image across five different model configurations.  The configurations include a standard raster-scan approach, a raster scan with key-value caching, a naive ordered autoregressive (OAR) model, an OAR model with parallel evaluation, and finally, an OAR model that incorporates both parallel evaluation and optimized key-value caching. The table quantifies the impact of these different methods and optimizations on the inference speed.
> <details>
> <summary>read the caption</summary>
> Table 5: Inference Time Comparison Across Different Model Configurations
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.17069/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17069/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}