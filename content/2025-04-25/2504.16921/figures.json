[{"figure_path": "https://arxiv.org/html/2504.16921/x2.png", "caption": "(a) Iberian Peninsula languages", "description": "This figure shows a map of the Iberian Peninsula and parts of Ibero-America highlighting the languages included in the IberBench benchmark.  Panel (a) displays the five languages spoken on the Iberian Peninsula: Spanish, Portuguese, Catalan, Galician, and Basque. Panel (b) shows the different varieties of Spanish spoken across Ibero-America that are included in the evaluation: Mexican, Uruguayan, Peruvian, Costa Rican, and Cuban.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.16921/x3.png", "caption": "(b) Spanish varieties in Ibero-America", "description": "This figure shows a map highlighting the different varieties of Spanish spoken in Ibero-America.  It visually represents the geographic distribution of these varieties, providing a geographical context to the linguistic diversity examined in the IberBench study.  These varieties of Spanish are included in the IberBench benchmark to evaluate LLMs' performance across different regional dialects, expanding beyond the standard Castilian Spanish.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.16921/extracted/6363510/images/iberbench-diagram.png", "caption": "Figure 1: Maps showing the Iberian languages considered in IberBench.", "description": "This figure showcases the geographic distribution of the Iberian languages included in the IberBench benchmark.  It features two maps: one highlighting the languages spoken on the Iberian Peninsula (Spain and Portugal), and a second map focusing on the Spanish-speaking regions of Ibero-America where variations of Spanish are prevalent. The maps visually represent the linguistic diversity encompassed within the benchmark, emphasizing its comprehensive nature.", "section": "3. Iberbench"}, {"figure_path": "https://arxiv.org/html/2504.16921/x4.png", "caption": "Figure 2:  IberBench overview. Users can view rankings, plots, and reports; request LLMs for evaluation through the UI; and propose new datasets to the organization. The organization reviews these proposals for possible inclusion in the leaderboard. Once approved, datasets and models are prepared, evaluated, and hosted to be displayed in the UI.", "description": "This figure illustrates the workflow of the IberBench LLM evaluation benchmark.  Users interact with a user interface (UI) to view evaluation results, request the evaluation of new LLMs, and propose new datasets.  The IberBench organization reviews these requests and, upon approval, adds the models and datasets to the benchmark. After preparation and evaluation, the results are hosted and displayed on the UI.", "section": "3. Iberbench"}, {"figure_path": "https://arxiv.org/html/2504.16921/x5.png", "caption": "Figure 3:  Performance per model size (number of parameters), averaged across all the languages and tasks. Legend shows model families, e.g., the Llama-3.2 family includes Llama-3.2-1B-Instruct and Llama-3.2-3b-Instruct, which are plotted with the same marker and color. The random baseline is shown as horizontal line.", "description": "This figure displays the average performance of various Large Language Models (LLMs) across multiple tasks and languages, categorized by the number of parameters (model size).  The x-axis represents the model size, and the y-axis represents the average performance score across all tasks and languages. Each point represents a specific LLM, and models belonging to the same family (e.g., Llama-3.2 family including Llama-3.2-1B-Instruct and Llama-3.2-3B-Instruct) are marked with the same symbol and color for easy comparison. A horizontal line indicates the performance of a random baseline, providing a reference point for evaluating model performance. The chart allows for analysis of how LLM performance scales with model size, and helps identify models that outperform the random baseline across all tasks and languages.", "section": "4. Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.16921/x6.png", "caption": "Figure 4:  Averaged performance per model type.", "description": "This box plot displays the average performance of different Large Language Models (LLMs) categorized by their type: base models and chat models.  The y-axis represents the average performance score, and the x-axis shows the model type.  The box plot visualizes the median, quartiles, and range of performance scores for each model type, allowing for a comparison of their relative performance.", "section": "4. Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.16921/x7.png", "caption": "Figure 5:  Performance per task category across all models and languages. The random baseline for each category is marked with a black cross.", "description": "This figure presents a box plot visualizing the performance of various Large Language Models (LLMs) across different Natural Language Processing (NLP) tasks. Each box represents a task category, showcasing the median performance, interquartile range, and data distribution of the LLMs' performance scores within each category.  The horizontal line indicates the performance of a random baseline model for each task. This visualization allows for a comparison of LLM performance across various tasks and helps identify categories where model performance is significantly better or worse than random chance.", "section": "4.2.2. Performance per Task"}, {"figure_path": "https://arxiv.org/html/2504.16921/x8.png", "caption": "Figure 6:  Performance in fundamental and industry-relevant tasks, averaged across all the languages and models.", "description": "This boxplot displays the average performance of various LLMs on different tasks categorized by their relevance (fundamental vs. industry-relevant).  The data is aggregated across all languages included in the IberBench benchmark.  It highlights the performance disparity between LLMs on fundamental tasks (generally easier for LLMs) compared to those in industry-relevant settings (which often involve more complex, nuanced challenges).", "section": "4.2. Results"}, {"figure_path": "https://arxiv.org/html/2504.16921/x9.png", "caption": "Figure 7:  Performance per language and Spanish variety averaged across LLMs and tasks. For clarity, we remove the \u201cambiguous\u201d Spanish variety.", "description": "This figure displays box plots illustrating the performance of various Large Language Models (LLMs) across different Iberian languages and Spanish varieties.  The performance is averaged across all tasks included in the IberBench benchmark.  The box plots show the median, interquartile range (IQR), and outliers for each language, giving a visual representation of the central tendency and variability of LLM performance. Note that the 'ambiguous' Spanish variety is excluded for clarity.  This allows for a clear comparison of LLM performance across different dialects and languages.", "section": "4. Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.16921/x10.png", "caption": "Figure 8:  LLM performances in Iberian languages, averaged across tasks. Vertical lines denote the random baseline.", "description": "This figure displays box plots illustrating the performance of various Large Language Models (LLMs) across six Iberian languages (Spanish, Portuguese, Galician, Basque, Catalan, and English) and five Spanish varieties (Spain, Mexico, Cuba, Peru, and Uruguay). The box plots represent the distribution of LLM performance scores averaged across all tasks in IberBench.  The performance is measured relative to a random baseline.  The vertical lines within each box plot indicate the random baseline performance for that particular language. The figure aims to compare LLM performance across different languages and to reveal which languages pose more challenges for the LLMs.", "section": "4.2.3. Performance per Iberian language"}, {"figure_path": "https://arxiv.org/html/2504.16921/x11.png", "caption": "Figure 9:  Model performance in Spanish varieties. Vertical lines denote the random baseline.", "description": "This figure presents a comparison of Large Language Model (LLM) performance across different varieties of Spanish.  Each box plot represents the performance distribution of various LLMs on a specific Spanish variety (e.g., Mexican Spanish, Cuban Spanish, etc.).  The models are evaluated across multiple tasks in the IberBench benchmark.  Vertical lines indicate the performance of a random baseline for each variety; models above this line surpass random prediction.", "section": "4.2.3. Performance per Iberian language"}, {"figure_path": "https://arxiv.org/html/2504.16921/x12.png", "caption": "Figure 10:  Ranking of models per task category.", "description": "This figure presents a heatmap visualizing the ranking of different Large Language Models (LLMs) across various Natural Language Processing (NLP) tasks.  Each cell in the heatmap represents an LLM's performance on a specific task, with the color intensity indicating the rank (darker colors representing higher ranks). This allows for a comprehensive comparison of the relative strengths and weaknesses of each LLM across a range of tasks.", "section": "4.2 Results"}]