[{"heading_title": "IberBench: Aims", "details": {"summary": "IberBench aims to address the shortcomings of existing LLM benchmarks, **particularly for languages other than English.** Existing benchmarks often fail to capture the linguistic diversity, focusing mainly on fundamental NLP tasks rather than industry-relevant applications. Moreover, **IberBench aims for continual evaluation** by enabling updates, community contributions, expert moderation to address the static nature of existing benchmarks, making it comprehensive. The benchmark focuses on **Iberian languages and their regional varieties**, a historically underrepresented aspect in LLM evaluations, addressing the limitations of evaluations in both coverage and task selection. IberBench also aims to establish a long-term, reliable, and methodologically rigorous benchmark that includes a diverse set of data sources, while promoting collaboration and innovation."}}, {"heading_title": "LLM Variety Impact", "details": {"summary": "The research indicates that LLM performance varies significantly across different architectures and training methodologies.  **European LLMs, despite being tailored for Iberian languages, often underperform compared to broader multilingual models like Qwen and Llama.** Instruction tuning appears crucial, as base models lag behind. Fine-tuning for a specific language can lead to **catastrophic forgetting**, harming performance in others. Model size matters, but larger models like Phi-4 do not guarantee superior results. The paper also points out a *surprising consistency in model rankings across languages and varieties*, suggesting that if a model excels in one, it tends to do well in others. This points to the importance of a solid foundation and that further research is needed to understand how to best leverage specific knowledge without compromising overall abilities. In summary, LLM variety affects performance and further analysis is needed."}}, {"heading_title": "Task Category Gap", "details": {"summary": "The Task Category Gap highlights a crucial disparity: **LLMs excel in fundamental NLP tasks** (commonsense reasoning, reading comprehension) but **underperform in industry-relevant applications** (intent classification, sentiment analysis). This divergence suggests that while LLMs grasp core linguistic principles, they struggle with the nuances, context-specificity, and practical challenges of real-world scenarios. The benchmark results reveal that models are better on knowledge tasks than industry driven and more complex tasks like summarization, mental health detection. In industry specific the baseline is often exceeded which highlight the challenge for LLMs in the industry. This gap underscores the need for benchmark development to concentrate on real-world applications to practically assess capability."}}, {"heading_title": "Basque Challenge", "details": {"summary": "The paper highlights the **unique challenges** that Basque presents, noting that the top models only slightly surpass the random baseline. This is attributed to a combination of factors, including **imbalances in task difficulty**, **distinct linguistic features**, and **limited resources** for LLM training. Unlike other Iberian languages, Basque's isolated nature and lack of related languages impact LLM adaptation. Furthermore, Basque is still an under-represented language when it comes to the available resources for LLM training, and thus the generalizability might be an issue."}}, {"heading_title": "Future IberBench", "details": {"summary": "IberBench's future likely involves **expanding language coverage**, notably to Brazilian Portuguese and Argentinian Spanish, and **balancing task diversity** by incorporating more text generation and sequence labeling datasets. A key focus will be on **industry-relevant tasks**, especially in areas like mental health detection and fake news detection, potentially involving collaboration with industry partners to acquire more data. Future iterations also should prioritize establishing and refining techniques for detecting complex, low-resource languages such as the Basque language as well as ensuring that future iterations of IberBench will have more attention devoted towards how such languages can be accurately tested within this framework. Community contributions and new evaluation campaigns could introduce fresh datasets and tasks as well in terms of determining exactly what type of languages would be used to test these models for evaluation to allow for LLMs to provide proper assessment. Furthermore, continuous assessment of emerging LLMs to refine zero-shot prompt engineering to create better and more accurate answers could greatly improve evaluations."}}]