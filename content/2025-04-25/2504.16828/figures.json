[{"figure_path": "https://arxiv.org/html/2504.16828/x1.png", "caption": "Figure 1: \nLeft: ThinkPRM-14B, trained on 8K process labels or 1K synthetic examples, outperforms discriminative PRMs trained on about 100x more data on ProcessBench\u00a0(Zheng et\u00a0al., 2024).\nRight: ThinkPRM-1.5B, trained using the same 8K labels, outperforms LLM-as-a-judge and discriminative verifiers in reward-guided search on MATH-500. The LLM-as-a-judge in both figures uses the same base model as ThinkPRM.", "description": "This figure demonstrates ThinkPRM's data efficiency and performance compared to other methods. The left panel shows ThinkPRM-14B achieving higher accuracy on ProcessBench with only 8K process labels or 1K synthetic examples, outperforming discriminative PRMs trained on significantly more data (approximately 100 times more).  The right panel illustrates ThinkPRM-1.5B's superior performance on MATH-500 when using reward-guided search, surpassing both LLM-as-a-judge and discriminative methods while using the same limited training data.  Both panels highlight ThinkPRM's ability to achieve high accuracy with minimal training data, showcasing its data efficiency advantage.", "section": "3 THINKPRM"}, {"figure_path": "https://arxiv.org/html/2504.16828/x2.png", "caption": "Figure 2: ThinkPRM supports scaling verification compute by thinking longer.", "description": "The figure illustrates how THINKPRM, a generative process reward model, scales verification compute more effectively than LLM-as-a-judge and discriminative PRMs.  It shows that increasing the number of tokens considered during verification (allowing the model to \"think longer\") leads to improved F1-score on the ProcessBench benchmark. THINKPRM demonstrates better scaling performance, achieving higher accuracy with increased compute compared to other models.", "section": "3.1 RQ1: LLM-as-a-judge PRMs are suboptimal"}, {"figure_path": "https://arxiv.org/html/2504.16828/x3.png", "caption": "Figure 3: Example output by ThinkPRM-14B, where it verifies and labels every step in the provided prefix via a long verification CoT. We omit the problem and solution for brevity.", "description": "ThinkPRM-14B is a large language model fine-tuned to perform step-by-step verification of solution steps.  The figure shows an example of its output. For a given problem and solution, ThinkPRM-14B generates a chain-of-thought (CoT) that analyzes and verifies each step, providing a 'correct' or 'incorrect' label for each.  The CoT provides reasoning for each label. The full problem and solution are omitted for brevity, but the example demonstrates ThinkPRM-14B's ability to reason through a solution, step by step, in a detailed and interpretable way.", "section": "THINKPRM"}, {"figure_path": "https://arxiv.org/html/2504.16828/x4.png", "caption": "Figure 4: \nVerifier performance on ProcessBench in light of CoT lengths. On the left, LLM-as-a-judge produces excessively long chains including repetition, infinite looping, and overthinking, leading to worse verifier performance since the output never terminates. Training on collected syntehtic data substantially reduces these issues as shown in the ThinkPRM plot on the right.", "description": "This figure compares the performance of LLMs used as verifiers on the ProcessBench dataset, focusing on the length of their chain-of-thought (CoT). The left panel shows the distribution of CoT lengths generated by an LLM-as-a-judge, which exhibits issues like excessive length, repetition, infinite looping, and overthinking, leading to poor performance.  The right panel displays the improved distribution of CoT lengths after fine-tuning the same LLM with synthetic data to create ThinkPRM, showcasing a substantial reduction in these negative behaviors and consequently better performance.", "section": "3.1 RQ1: LLM-as-a-judge PRMs are suboptimal"}, {"figure_path": "https://arxiv.org/html/2504.16828/x7.png", "caption": "Figure 5: Collecting verification chains for finetuning. First, we prompt a reasoning model, in our case QwQ-32B-Preview to critique a given solution to a problem. Then, we sample multiple verification chains, which we judge against gold process labels from PRM800K, only keeping chains that match the gold process labels.", "description": "This figure illustrates the process of creating high-quality synthetic data for fine-tuning a process reward model (PRM).  First, a large language model (LLM), specifically QwQ-32B-Preview, is prompted to critique a given solution to a problem. The LLM generates multiple verification chains, which are chains of reasoning explaining why each step in the solution is correct or incorrect. These generated chains are then compared to the gold standard process labels from the PRM800K dataset. Only the chains that perfectly match the gold standard labels are kept, forming a high-quality dataset for finetuning the PRM. This ensures the fine-tuned PRM learns from accurate and consistent reasoning.", "section": "3 THINKPRM"}, {"figure_path": "https://arxiv.org/html/2504.16828/x10.png", "caption": "Figure 6: LLM-as-a-judge suffers from a significant ratio of verification CoTs that do not terminate with a parsable label i.e., \\boxed{yes} or \\boxed{no}. Our finetuning process that yields ThinkPRM, substantially mitigates this issue. Both verifiers are based on R1-Distill-Qwen-14B.", "description": "Figure 6 is a bar chart comparing the performance of LLMs used as-a-judge and ThinkPRMs in terms of generating valid verification chains. LLMs-as-a-judge produce a significant number of chains that do not have a conclusive label (\"yes\" or \"no\"), indicating issues with their reliability. The ThinkPRM, a model fine-tuned by the authors, shows a substantial reduction in this issue, highlighting the efficacy of their proposed approach. The comparison uses the same base model (R1-Distill-Qwen-14B), allowing for a fair assessment of the impact of finetuning.", "section": "3.1 RQ1: LLM-as-a-judge PRMs are suboptimal"}, {"figure_path": "https://arxiv.org/html/2504.16828/x11.png", "caption": "Figure 7: Verification accuracy on 2K question-solution pairs from two most challenging subsets of ProcessBench: OlympiadBench and OmniMath. ThinkPRM obtained by finetuning the correponding model over only 1K verification chains performs better.", "description": "This figure compares the performance of THINKPRM, a newly proposed process reward model, against several baseline models on two difficult subsets of the ProcessBench benchmark: OlympiadBench and OmniMath.  Each subset contains 1000 question-solution pairs.  The results demonstrate that THINKPRM, despite being trained on only 1000 verification chains (a fraction of the data used to train the baselines), achieves higher accuracy than the baseline models (LLM-as-a-Judge and discriminative PRMs) on both OlympiadBench and OmniMath. This highlights the model's data efficiency and superior performance.", "section": "RQ3: How does a finetuned verbalized PRM (THINKPRM) compare to discriminative PRMs and LLM-as-a-Judge baselines under different test-time scaling scenarios?"}]