{"importance": "This paper introduces THINKPRM, a data-efficient approach to building PRMs by leveraging long CoT models. It **demonstrates superior performance with minimal supervision**, offering new avenues for scaling verification in complex reasoning tasks and guiding future research in generative PRM development.", "summary": "Generative PRMs scale verification compute with less data!", "takeaways": ["THINKPRM utilizes long CoT models for step-by-step verification.", "It achieves high performance with significantly less training data.", "It outperforms existing PRMs in various reasoning tasks."], "tldr": "Process reward models (PRMs) are vital for test-time scaling but require costly step-level supervision. This work addresses this by introducing verbalized step-wise reward models for verification via chain-of-thought (CoT). It capitalizes on long CoT models, creating THINKPRM, which requires only 1% of the process labels needed by discriminative PRMs. It verifies solutions by generating a CoT, capitalizing on inherent reasoning abilities.\n\nTHINKPRM surpasses LLM-as-a-Judge and discriminative verifiers on benchmarks like ProcessBench, MATH-500, and AIME '24. Out-of-domain, it beats discriminative verifiers by 8% on GPQA-Diamond and 4.5% on LiveCodeBench. THINKPRM scales verification compute more effectively, outperforming LLM-as-a-Judge by 7.2% on ProcessBench. It uses generative, long CoT PRMs to scale test-time compute while minimizing supervision.", "affiliation": "University of Michigan", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.16828/podcast.wav"}