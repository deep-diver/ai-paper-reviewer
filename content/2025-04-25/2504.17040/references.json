{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning Transferable Visual Models From Natural Language Supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, which is used as the visual encoder in this research."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Improved Baselines with Visual Instruction Tuning", "publication_date": "2023-01-01", "reason": "This paper establishes strong baselines for visual instruction tuning, and is used to create the dataset used in this research."}, {"fullname_first_author": "Jianlin Su", "paper_title": "Roformer: Enhanced transformer with rotary position embedding", "publication_date": "2024-01-01", "reason": "This paper introduces Rotary Position Embedding (RoPE) which is a key component of this research's Virtual Token Unmerging (VTU) method."}, {"fullname_first_author": "Daniel Bolya", "paper_title": "Token merging: Your vit but faster", "publication_date": "2022-01-01", "reason": "This paper introduces ToMe, which this research extends to create their proposed Dynamic Token Merging (DToMe) method."}, {"fullname_first_author": "Drew A Hudson", "paper_title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering", "publication_date": "2019-01-01", "reason": "This paper introduces GQA, one of the benchmark datasets on which the method is evaluated in this research."}]}