{"importance": "This paper introduces DYMU, a method for **dynamically adjusting visual token counts** in VLMs, enhancing efficiency and opening new avenues for adaptive, content-aware processing in multimodal AI.", "summary": "DYMU: Efficient VLMs via Dynamic Token Management", "takeaways": ["DYMU dynamically adjusts visual token lengths based on image complexity, improving efficiency.", "The method maintains competitive performance while significantly reducing computational costs.", "DYMU is training-free and applicable to various VLM architectures."], "tldr": "Large Vision-Language Models (VLMs) often process images with a fixed number of visual tokens, regardless of image complexity, leading to computational inefficiency. To address this, the paper introduces Dynamic Merging and Virtual Unmerging (DYMU). DYMU dynamically reduces visual token lengths based on image complexity; simpler images are represented using fewer tokens, while more complex images retain more tokens. This training-free framework aims to reduce the computational burden of VLMs while maintaining high task performance. \n\nDYMU comprises two key components: Dynamic Token Merging (DToMe), which reduces the number of visual token embeddings by merging similar tokens, and Virtual Token Unmerging (VTU), which simulates the expected token sequence for LLMs by efficiently reconstructing the attention dynamics of a full sequence. Experimental results demonstrate that DYMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures. **DYMU offers a more efficient plug-and-play alternative to VLMs**.", "affiliation": "University of Illinois Urbana-Champaign", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.17040/podcast.wav"}