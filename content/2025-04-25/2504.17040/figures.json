[{"figure_path": "https://arxiv.org/html/2504.17040/extracted/6384571/assets/method.png", "caption": "Figure 1: Dynamic Merging and Virtual Unmerging (DyMU) adaptively reduces visual token lengths based on image complexity, as shown on the left where simpler images are represented using fewer tokens. In contrast, existing representations (like CLIP) always use the same number of tokens regardless of image content. DyMU applied to recent VLMs (right) maintains competitive performance across different token compression levels. This training-free approach preserves key semantic information, offering a more efficient plug-and-play alternative to VLMs with fixed-length visual tokens.", "description": "The figure illustrates the core idea of DyMU, a method for efficient processing of visual information in Vision-Language Models (VLMs).  The left side shows how DyMU dynamically adjusts the number of visual tokens based on image complexity: simpler images use fewer tokens, while complex images use more.  This contrasts with existing methods (like CLIP) that always use a fixed number of tokens. The right side demonstrates that using DyMU with various recent VLMs achieves competitive performance even with significant reductions in the number of tokens, highlighting its efficiency and plug-and-play nature. The method is training-free, preserving important semantic information.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.17040/x1.png", "caption": "Figure 2: Method Overview. \\ours, is composed of two key ideas: Dynamic Token Merging (DToMe) and Virtual Token Unmerging (VTU).\nDToMe first determines per\u2010layer thresholds (left) by feeding a large batch of images into the vision transformer and computing bipartite token similarities. We rank these edges across the entire batch and choose the top-B\u2062r\ud835\udc35\ud835\udc5fBritalic_B italic_r (r=\ud835\udc5fabsentr=italic_r = desired average number of tokens, batch size B\ud835\udc35Bitalic_B). This leads to more edges from simpler images (with more redundancy) being chosen, while complex images remain less merged. During inference, DToMe merges tokens on a per\u2010image basis using these pre-computed thresholds. We then apply VTU (right) in the self\u2010attention layers of the pretrained VLM to efficiently expand the attention matrices to the standard token count\u2014ensuring the model\u2019s original weights and outputs remain compatible\u2014before re\u2010merging the tokens for the next layer. The overall process is training\u2010free and utilizes crucial image information by allocating the token budget more effectively for both simple and complex images.", "description": "This figure illustrates the two-stage process of DYMU (Dynamic Merging and Virtual Unmerging).  First, Dynamic Token Merging (DToMe) pre-computes thresholds for merging similar visual tokens in a vision transformer.  This is done by feeding a large batch of images into the model and identifying redundant tokens based on similarity scores. The algorithm prioritizes merging tokens in simpler images, leaving complex images relatively unchanged. During inference, DToMe dynamically merges tokens on a per-image basis using these pre-computed thresholds, leading to variable-length visual token sequences. Second, Virtual Token Unmerging (VTU) reconstructs the attention dynamics of a full-length token sequence for the language model, efficiently simulating the attention interactions that would have occurred with the complete sequence. This simulates the attention process that the language model expects, preserving its original weights and avoiding any performance degradation, despite the reduced token number.  This process is completely training-free.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.17040/x2.png", "caption": "Table 2: Comparison with state-of-the-art methods for improving efficiency on LLaVA 1.5\u00a0[25]. \\ours-low achieves 97.7% of the original full-length LLaVA baseline\u2019s performance while using only \u223csimilar-to\\sim\u223c15% of the tokens. Importantly, \\oursis entirely training-free and generally outperforms previous fixed-length, training-free methods such as [3, 5, 46], while also enabling variable-length outputs.", "description": "Table 2 compares the performance of DYMU with other state-of-the-art methods designed to improve the efficiency of the LLaVA 1.5 vision-language model.  It demonstrates that DYMU-low achieves nearly the same performance as the original full-length model while using only about 15% of the visual tokens.  The table highlights that DYMU is unique because it's training-free and outperforms other training-free methods that use fixed-length token compression.  DYMU also offers the advantage of variable-length visual token outputs, adapting to the complexity of each image.", "section": "4.2. Quantitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.17040/x3.png", "caption": "Figure 3: Image Complexity vs Token Count and Accuracy \u00a0 The scatter plot (left) demonstrates a strong correlation between DyMU\u2019s token count and image complexity score\u2014more complex images naturally receive more tokens. On the right, MME accuracy at varying complexity levels is compared between ToMe (fixed-length) and DyMU (dynamic-length), highlighting the benefit of assigning additional tokens to complex images.", "description": "Figure 3 visually analyzes the relationship between image complexity and the number of tokens generated by the proposed Dynamic Token Merging (DToMe) method, part of the DYMU framework. The left panel shows a scatter plot illustrating a strong positive correlation: more complex images lead to a higher token count.  The right panel compares the performance (MME accuracy) of ToMe (fixed-length token approach) and DToMe (dynamic-length token approach) across various image complexity levels. It highlights how DToMe's dynamic approach yields better accuracy, particularly for complex images, by adaptively assigning more tokens as needed.", "section": "4.3. Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2504.17040/extracted/6384571/assets/qualitative_v2.png", "caption": "Figure 4: Importance of Virtual Token Unmerging (VTU). We ablate the performance of LLaVA\u00a01.5 with two token reduction methods applied to the visual encoder\u2014ToMe (fixed\u2010length) and DToMe (variable\u2010length). We observe that applying VTU significantly improves performance on 8 out of 9 benchmarks, demonstrating robustness to varied token reduction methods.", "description": "This figure demonstrates the importance of Virtual Token Unmerging (VTU) in improving the performance of vision-language models (VLMs).  Two token reduction methods, ToMe (fixed-length token reduction) and DToMe (variable-length token reduction), were applied to the visual encoder of the LLaVA 1.5 model. The results show that adding VTU significantly enhances performance across various benchmarks. This improvement highlights VTU's ability to effectively maintain performance even with different token reduction methods and varying levels of token compression.", "section": "4.2. Quantitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.17040/x4.png", "caption": "Figure 5: Comparing thresholds using LLaVA Instruct Data vs Pixmo-Cap. Although both methods use the same per\u2010layer merging hyperparameter (risubscript\ud835\udc5f\ud835\udc56r_{i}italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), the Pixmo\u2010based thresholds lead to fewer tokens (top)\u2014likely due to domain differences.\nHowever, performance across a range of benchmarks shows minimal drop (bottom),\nindicating the robustness of our threshold estimation.", "description": "Figure 5 is a comparison of two different methods for finding optimal thresholds in the Dynamic Token Merging (DToMe) algorithm.  The top panel shows the number of tokens generated using thresholds derived from the LLaVA Instruct dataset versus thresholds derived from the Pixmo-Cap dataset.  Even though both methods used the same per-layer merging hyperparameter, the Pixmo-Cap dataset yielded fewer tokens, likely due to differences in image content and characteristics between the two datasets. The bottom panel displays the performance (accuracy) on multiple benchmark datasets when using thresholds derived from each of the two datasets.  Despite the differing number of tokens produced, performance remains nearly identical. This demonstrates the robustness of the DToMe threshold estimation technique across diverse datasets.", "section": "4.2. Quantitative Evaluation"}]