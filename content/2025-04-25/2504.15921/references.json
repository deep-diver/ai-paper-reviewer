{"references": [{"fullname_first_author": "Kristen Grauman", "paper_title": "Ego4d: Around the world in 3,000 hours of egocentric video", "publication_date": "2022-01-01", "reason": "This paper is important because it introduces the Ego4D dataset, which is used extensively in the current paper for evaluation and pre-training."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, a visual-language model that is a key component used for comparison and concept of large model foundation within this work."}, {"fullname_first_author": "Yue Zhao", "paper_title": "Learning video representations from large language models", "publication_date": "2023-01-01", "reason": "This paper introduces LaViLa, a key video captioning model, that is used as a benchmark and component in the current work."}, {"fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "publication_date": "2023-01-01", "reason": "This paper outlines GPT-4's technical specifications as it is used to assist with tasks like summarization in current work."}, {"fullname_first_author": "Md Mohaiminul Islam", "paper_title": "Video ReCap: Recursive Captioning of Hour-Long Videos", "publication_date": "2024-01-01", "reason": "Video ReCap, the only work specifically designed for hour-long video summaries and is the main comparision model against ViSMaP."}]}