{"importance": "ViSMap tackles the challenge of hour-long video summarization without extensive labeled data. This impacts the video understanding field by offering a scalable, domain-adaptable method, bridging gaps between short-form and long-form video analysis and opening doors for unsupervised learning.", "summary": "ViSMap: Summarizing hours of video made easy with Meta-Prompting!", "takeaways": ["Meta-prompting can generate high-quality pseudo-summaries from short video segments for training long-form video summarization models.", "ViSMap achieves comparable performance to supervised methods while being unsupervised and domain adaptable.", "The iterative LLM-based approach optimizes semantic alignment and bridges the gap between short and long videos."], "tldr": "Existing video understanding models struggle with long videos due to sparse events and lack of annotations. ViSMap addresses this by leveraging Large Language Models (LLMs) to create pseudo-summaries of long videos using segment descriptions from short ones. This bypasses the need for extensive annotations. A meta-prompting strategy is adopted, using three LLMs: one to generate the pseudo-summary, one to evaluate it, and one to optimize the generator's prompt. This closes the gap between short videos with plentiful data and long ones with scarce data. \n\nViSMap uses minute-long annotated videos to construct a short-form video summary model. Unsupervised hour-long videos are split into segments, generating pseudo-captions using the earlier model. These refine pseudo-summaries using meta-prompting with LLMs, generating training data for long-form summaries. The method reduces the domain gap caused by video length and semantic distribution. Experiments show that **ViSMap achieves performance comparable to state-of-the-art models while generalising across domains**.", "affiliation": "Queen Mary University of London", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2504.15921/podcast.wav"}