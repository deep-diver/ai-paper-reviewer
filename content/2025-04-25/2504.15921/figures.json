[{"figure_path": "https://arxiv.org/html/2504.15921/x1.png", "caption": "(a) Verb frequency: Segments vs Summaries. Illustration of the semantic gap between segment descriptions and video summaries in Ego4D-HCap (Islam et\u00a0al., 2024) dataset.", "description": "This figure visually demonstrates the semantic difference between short video segment descriptions and longer video summaries.  The bar charts compare the frequency of verbs used in descriptions of short segments (4 seconds) from the Ego4D-HCap dataset with the verbs used in summaries of the entire 3-minute segments. The significant difference in verb frequencies highlights the challenge of bridging the semantic gap between local actions (in segments) and higher-level narrative (in summaries) when creating video summaries. This gap makes it challenging to directly generate long video summaries from short segment descriptions.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.15921/x2.png", "caption": "(b) t-SNE visualization between Ego4D-HCap and Youcook2 datasets. \nIllustration of the distributional shift between these two dataset.", "description": "This figure visualizes the distributional shift between the Ego4D-HCap and YouCook2 datasets using t-SNE.  The t-SNE plots show how the embeddings of videos from the two datasets cluster differently in the reduced-dimensional space. This demonstrates the differing semantic distributions between the datasets, highlighting the domain adaptation challenge addressed by the ViSMaP model.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.15921/x3.png", "caption": "Figure 1. Two main challenges we address with our approach: (a) Bridging the semantic gap between short-form segment descriptions and hour-long video summaries descriptions (b) Overcoming the domain shift between the source domain and the target domain.", "description": "Figure 1 illustrates the two key challenges addressed in the ViSMaP approach.  (a) shows the \"semantic gap\" \u2013 the difference in language used to describe short video segments versus the language needed for a comprehensive hour-long video summary.  (b) highlights the \"domain shift\" \u2013 the differences between the characteristics of short videos used for training (source domain) and the characteristics of long, unsegmented videos the system aims to summarize (target domain).  This visual representation underscores the difficulties in adapting models trained on readily available short videos to the task of summarizing longer, more complex videos where labeled data is scarce.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.15921/x4.png", "caption": "Figure 2.  Motivation of our VisMaP. Most existing video summarisation models focus on minute-level short-form videos, while hour-long videos, which are more common in real-world scenarios, are often overlooked due to their length, content complexity, and the prohibitively high cost of manual annotation. We propose a cross-domain unsupervised approach for hour-long video summarisation. It leverages the inductive power of multiple LLMs to generate high-quality pseudo-summaries from short video segments via meta-prompting. These pseudo-summaries are then used to train a model, enabling effective summarisation of long videos without costly human annotations.", "description": "Existing video summarization models primarily focus on short videos (around a minute long), neglecting the abundance of hour-long videos in real-world applications.  These longer videos are difficult to process due to their length, intricate content, and the high cost of manual annotation for training data.  ViSMaP is an unsupervised cross-domain approach designed to address this limitation. It leverages the strengths of large language models (LLMs) to create high-quality \"pseudo-summaries\" for hour-long videos. This is achieved by using meta-prompting techniques on short video segments, generating summaries without needing manual annotations for long videos. These pseudo-summaries then serve as training data for a model that effectively summarizes hour-long videos.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.15921/x5.png", "caption": "Figure 3. An overview of our VisMaP. (a) First stage: we use\n180-second source video vssuperscript\ud835\udc63\ud835\udc60v^{s}italic_v start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT for supervised pretraining to establish basic summary capabilities. (b) Second stage: we split hour-long target videos vtsuperscript\ud835\udc63\ud835\udc61v^{t}italic_v start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT\ninto 3-minute segments set \ud835\udc15itsuperscriptsubscript\ud835\udc15\ud835\udc56\ud835\udc61\\mathbf{V}_{i}^{t}bold_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT and process them through the first-stage summary model to generate pseudo captions C^tsuperscript^\ud835\udc36\ud835\udc61\\widehat{C}^{t}over^ start_ARG italic_C end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. C^tsuperscript^\ud835\udc36\ud835\udc61\\widehat{C}^{t}over^ start_ARG italic_C end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT are then refined through a meta-prompting process with K\ud835\udc3eKitalic_K iterations, using Gemini as the evaluator and GPT-3.5\nas the optimiser and the generator, to create more tailored prompts P\u2062rt\ud835\udc43superscript\ud835\udc5f\ud835\udc61{Pr}^{t}italic_P italic_r start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT and summaries Y^tsuperscript^\ud835\udc4c\ud835\udc61\\widehat{Y}^{t}over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. (c) Third stage: Refined Y^tsuperscript^\ud835\udc4c\ud835\udc61\\widehat{Y}^{t}over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT pseudo-summaries are utilised to fine-tune the summary model for effective hour-long video summary.", "description": "This figure illustrates the three-stage ViSMaP framework for unsupervised hour-long video summarization. Stage 1 uses short videos for supervised pretraining of a summary model. Stage 2 splits hour-long videos into segments, generates pseudo-captions using the pretrained model, and refines them iteratively via meta-prompting (using three LLMs: generator, evaluator, and optimizer) to create tailored prompts and summaries. Stage 3 uses the refined summaries to fine-tune the summary model for effective hour-long video summarization.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2504.15921/x6.png", "caption": "Figure 4. An example of summaries from ViSMaP on the Ego4D-HCap dataset.", "description": "Figure 4 presents a visual illustration of ViSMaP's summarization capabilities using a sample from the Ego4D-HCap dataset. It showcases how the model processes a long (63-minute) video by segmenting it into smaller parts (around 30 seconds each), generating descriptions for each segment, and then producing a concise summary that captures the video's essence. The example highlights ViSMaP's ability to identify and summarize key events from a large amount of redundant information. The ground truth summary is also shown for comparison.", "section": "Qualitative Results on the Ego4D-HCap dataset"}]