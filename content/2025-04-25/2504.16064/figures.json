[{"figure_path": "https://arxiv.org/html/2504.16064/x1.png", "caption": "Figure 1: ReDi: Our generative image modeling framework\nbridges the gap between generative modeling and representation learning by leveraging a diffusion model that jointly captures low-level image details (via VAE latents) and high-level semantic features (via DINOv2). Trained to generate coherent image\u2013feature pairs from pure noise, this\nunified latent-semantic dual-space\ndiffusion approach significantly boosts both generative quality and training convergence speed.", "description": "This figure illustrates the ReDi framework, a novel generative image modeling approach.  ReDi overcomes limitations of existing methods by integrating both low-level image details (represented by Variational Autoencoder, or VAE, latents) and high-level semantic features (extracted using DINOv2, a self-supervised vision transformer). The diffusion model in ReDi is trained to generate matching image-feature pairs directly from noise, thus simultaneously learning and generating both image and feature spaces. This unified approach leads to significantly improved image generation quality and faster model training.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.16064/x2.png", "caption": "Figure 2: \nAccelerated Training Training curves (without Classifier-Free Guidance) for DiT-XL/2, SiT-XL/2 and SiT-XL/2+REPA, showing that our ReDi accelerates convergence by \u00d723absent23\\times 23\u00d7 23 and \u00d76absent6\\times 6\u00d7 6 (compared to DiT-XL/2 and SiT-XL/2+REPA, respectively).", "description": "Figure 2 presents a comparison of training curves for three different generative image models: DiT-XL/2, SiT-XL/2, and SiT-XL/2 enhanced with REPA (Representation Enhancement via Pretrained Autoencoders).  The plot shows the FID (Fr\u00e9chet Inception Distance) score over training iterations.  The key finding is that the ReDi (Representation-aware Diffusion model) approach significantly accelerates the convergence of the SiT-XL/2 model.  Specifically, ReDi achieves the same FID score approximately 23 times faster than DiT-XL/2 and 6 times faster than SiT-XL/2+REPA. This highlights ReDi's efficiency in learning to generate high-quality images.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.16064/x3.png", "caption": "Figure 3: Given an input image, the VAE latent and the principal components of DINOv2 are extracted. Then both modalities are noised and fused into a joint token sequence which is given as input to DiT or SiT.", "description": "This figure illustrates the ReDi model's architecture.  An input image is processed by a Variational Autoencoder (VAE) to generate a low-level latent representation and by a DINOv2 model to extract high-level semantic features.  Both representations are then corrupted with noise, and their principal components are combined into a unified token sequence. This sequence serves as the input for either a Diffusion Transformer (DiT) or a Sinusoidal Transformer (SiT) model, which jointly predicts the denoised versions of both the latent representation and semantic features.", "section": "3.2 Joint Image-Representation Generation"}]