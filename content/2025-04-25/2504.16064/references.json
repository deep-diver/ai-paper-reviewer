{"references": [{"fullname_first_author": "Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This paper introduces Latent Diffusion Models (LDMs), which the current paper aims to improve upon, making it a foundational reference."}, {"fullname_first_author": "Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "This paper introduces Diffusion Transformers (DiT), the architecture used in the current paper's implementation, making it a core component of their work."}, {"fullname_first_author": "Oquab", "paper_title": "DINOv2: Learning robust visual features without supervision", "publication_date": "2024-01-01", "reason": "This paper introduces DINOv2, the self-supervised encoder the current paper utilizes to extract high-level semantic features, making it essential to their approach."}, {"fullname_first_author": "Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This paper introduces Denoising Diffusion Probabilistic Models (DDPM), which is the basis of diffusion models the paper builds upon."}, {"fullname_first_author": "Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, a contrastive vision-language pretraining approach that relates to the representation learning aspect of the presented work."}]}