[{"figure_path": "https://arxiv.org/html/2504.17502/x1.png", "caption": "Figure 1: Illustration of RefVNLI: Given a reference image of a subject, a prompt referring to the subject, and a target image, RefVNLI assesses both subject consistency and textual alignment.\nFor subject consistency, it distinguishes identity-preserving variations, like dew on a flower (top image), from identity-altering changes, such as color change (middle image).\nFor textual alignment, it assesses whether the target image reflects all details from the prompt, such as the fence\u2019s position relative to the flower (bottom image).", "description": "RefVNLI is a metric that evaluates subject-driven text-to-image generation models. Given a reference image, a text prompt, and a generated image, RefVNLI assesses two aspects: subject consistency and textual alignment. Subject consistency measures how well the generated image preserves the identity of the subject from the reference image, differentiating between acceptable variations (e.g., adding dew to a flower) and unacceptable changes (e.g., altering the flower's color). Textual alignment evaluates how accurately the generated image matches the details described in the text prompt (e.g., the position of a fence relative to a flower).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.17502/x2.png", "caption": "Figure 2: Qualitative Comparison: We compare RefVNLI with DreamBench++ and CLIP, which score both Subject Consistency (SC) and Textual Alignment (TA), using examples from the Animal, Object, and Human categories. DreamBench++ scores (0-4) are scaled to 0-100 for better readability.\nRefVNLI demonstrates superior robustness to identity-agnostic changes (SC), such as the zoomed-out parrot (top-middle) and the zoomed-out man with different attire (bottom-middle). It is also more sensitive to identity-defining attributes, assigning low scores to the altered-faced man (bottom-left) and the balloon with mismatched patterns (middle-left).\nAdditionally, RefVNLI excels at detecting text-image mismatches (TA), as seen in its penalization of the top-left image for lacking a waterfall.", "description": "Figure 2 presents a qualitative comparison of RefVNLI's performance against DreamBench++ and CLIP on subject consistency (SC) and textual alignment (TA).  Three example image triplets, one each from the Animal, Object, and Human categories, are shown with their respective scores.  RefVNLI's superior robustness to identity-agnostic changes is highlighted by comparing its scores for images with only minor alterations to the subject (e.g., zoomed-out parrot, zoomed-out man with different clothing) versus images with changes that affect the subject's identity (e.g., altered face, balloon with mismatched patterns).  It also demonstrates superior sensitivity to details, as shown in its penalty for images where important textual elements are missing (e.g., the waterfall in the top left image). DreamBench++ scores are scaled from 0-4 to 0-100 for easier comparison.", "section": "2. Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2504.17502/x3.png", "caption": "Figure 3: Generating subject consistency classification training instances from video frames. Given two pairs of frames, each extracted from distinct video scenes featuring the same entity (e.g., a dog), where both frames within each pair depict the same subject (e.g., the same dog), we construct training {imageref, imagetgt} pairs for subject consistency classification.\nPositive pairs are formed by pairing a cropped subject from one frame (e.g., dog from left frame in Scene 1) with the full frame from the same scene (right frame in Scene 1). In contrast, negative pairs are created by pairing the cropped subject with the other scene\u2019s full frames (e.g., Scene 2). This process is applied to all four frames, with each taking turns as the cropped reference image (imageref), while the corresponding full-frame counterparts serve as imagetgt, yielding a total of 4 positive and 8 negative training pairs.", "description": "This figure illustrates the process of creating training data for subject consistency in a text-to-image model.  It uses video frames featuring the same entity (e.g., a dog) across different scenes. Positive pairs are created by pairing a cropped version of the subject from one frame with the full frame from the same scene. Negative pairs are made by combining the cropped subject with full frames from a different scene. This process is repeated with all frames, resulting in four positive and eight negative pairs per set of four images.", "section": "2. Training Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.17502/x4.png", "caption": "Figure 4: Creating identity-sensitive {imageref, imagetgt} pairs.\nStarting with an image and a mask of a subject (e.g., a briefcase), we randomly keep 5 patches within the masked area ([1]) and use them to create 5 inpainted versions ([2]). The version with the highest MSE between the altered and original areas (e.g., bottom image, MSE = 3983) is paired with the unmodified crop to form a negative pair, while the original image and the same crop create a positive pair, with the crop acting as imageref in both cases.", "description": "This figure illustrates the process of creating training data pairs for evaluating subject consistency in image generation.  It starts with an image and its corresponding mask identifying the subject. Five random patches within the masked area are selected and inpainted to create altered versions of the image. The inpainted version showing the largest mean squared error (MSE) between the altered and original regions is paired with the original unmodified subject crop. This pair forms a negative example. Meanwhile, the original image and the same crop constitute a positive example, where the crop serves as the reference image.", "section": "2. Training Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.17502/x5.png", "caption": "Figure 5: Example of prompt-imagetgt pairs. Given an image with a specific subject (e.g., a dog), we generate a positive prompt by placing a bounding box around the subject and instructing Gemini to describe it (top prompts). Negative prompts are created by swapping positive prompts between images of the same entity (middle prompts). For additional hard negatives, we instruct Gemini to modify a single non-subject detail in the positive prompt while keeping the rest unchanged (bottom prompts).", "description": "Figure 5 illustrates the process of generating image-prompt pairs for training the REFVNLI model.  Starting with an image containing a specific subject (e.g., a dog), three types of prompts are generated: 1) Positive prompts: Created by using Gemini to describe the image subject within a bounding box, ensuring accurate focus.  2) Negative prompts: Generated by swapping positive prompts between images featuring the same type of entity (e.g., different dog images). 3) Hard negative prompts: Generated by instructing Gemini to modify a single, non-subject detail in a positive prompt while keeping the remaining details unchanged. This method creates prompts that are subtly misaligned, further improving the model's robustness to subtle misalignments.", "section": "2. Training Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.17502/x6.png", "caption": "Figure 6: ImageRAG Rare Entities Examples: We compare RefVNLI with CLIP and DreamBench++ in aligning with human preferences (top rows of each example) across Textual Alignment (TA), Image Quality (IQ), and Overall Preference (OP).\nDreamBench++ scores (0\u20134) are rescaled to 0\u2013100 for readability. The higher of the two criterion-wise scores is emphasized unless both are equal.\nRefVNLI consistently aligns with human judgments across all three criteria. Notably, in the bottom example, it is the only metric to correctly identify the higher-quality image based on IQ, albeit by a small margin. This case is particularly challenging as it is out-of-distribution for RefVNLI, being that the preferred image is inspired by the reference rather than being of the same identity.", "description": "Figure 6 presents a qualitative comparison of RefVNLI, CLIP, and DreamBench++ on the ImageRAG benchmark, focusing on rare or uncommon entities.  Each row shows a triplet: a reference image, a generated image from each of the three methods, and the human preference scores. The evaluation considers three aspects: Textual Alignment (TA), Image Quality (IQ), and Overall Preference (OP).  DreamBench++ scores were rescaled from 0-4 to 0-100 for easier comparison.  In most cases, RefVNLI's results closely match human judgments across all three evaluation criteria.  The figure highlights RefVNLI's superiority in handling out-of-distribution scenarios, where the preferred image is inspired by the reference image, not a direct representation of the same entity.", "section": "5.2. Applicability to Rare Entities"}, {"figure_path": "https://arxiv.org/html/2504.17502/x7.png", "caption": "Figure 7: Hard Negative Caption Generation. This figure illustrates the prompting strategy used to generate hard negative captions, containing a single, plausible but factually incorrect visual detail, for enhanced misalignment detection.", "description": "Figure 7 details the method for creating 'hard negative' captions used in training the REFVNLI model.  Hard negative captions are designed to be plausible but subtly incorrect, containing a single factual error. This allows the model to improve its ability to distinguish between correct and incorrect image-text pairings, enhancing its sensitivity to even small misalignments. The figure shows the steps involved: understanding the original caption, identifying a single detail to modify, replacing it with an alternative detail using specific tags, and finally reviewing the edited caption for natural language flow and accuracy.", "section": "2. Training Dataset Construction"}, {"figure_path": "https://arxiv.org/html/2504.17502/x8.png", "caption": "Figure 8: Qualitative Examples of Subject Preservation Evaluation for the Animal Category. DreamBench++ scores (0-4) are scaled to 0-100 for better readability.", "description": "This figure displays qualitative examples showcasing the evaluation of subject preservation in the Animal category, comparing REFVNLI's performance against several baselines, including DreamBench++, CLIP, and DINO.  Each row shows a reference image, a generated image, and the corresponding scores for each method, illustrating how well the generated images maintain the subject's identity from the reference image. The scores reflect both subject consistency and textual alignment, providing a comprehensive evaluation of the generated images.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2504.17502/x9.png", "caption": "Figure 9: Qualitative Examples of Subject Preservation Evaluation for the Human Category. DreamBench++ scores (0-4) are scaled to 0-100 for better readability.", "description": "Figure 9 presents qualitative examples illustrating the evaluation of subject preservation in the Human category.  Each example shows a reference image, a generated image, and the scores from REFVNLI, DreamBench++, CLIP, and DINO.  The scores indicate how well the generated image maintains the visual identity of the subject (person) from the reference image. DreamBench++ scores, originally ranging from 0-4, have been scaled to 0-100 for easier comparison.", "section": "3.2. Meta-evaluation and Benchmarks"}, {"figure_path": "https://arxiv.org/html/2504.17502/x10.png", "caption": "Figure 10: Qualitative Examples of Subject Preservation Evaluation for the Object Category. DreamBench++ scores (0-4) are scaled to 0-100 for better readability.", "description": "This figure displays qualitative examples showcasing the evaluation of subject preservation for the 'Object' category within the REFVNLI framework.  It features multiple triplets, each consisting of a reference image, a generated image, and scores from REFVNLI and Dreambench++ for subject consistency and textual alignment. The scores are scaled to a 0-100 range to facilitate easy comparison. Each triplet illustrates how well the generated image maintains the visual identity of the object from the reference image.", "section": "3.2. Meta-evaluation and Benchmarks"}]