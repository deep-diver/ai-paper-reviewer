{"references": [{"fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "publication_date": "2021-08-07", "reason": "This paper is a foundational work on using large language models for program synthesis."}, {"fullname_first_author": "Jinheon Baek", "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models", "publication_date": "2024-04-07", "reason": "This paper presents an iterative approach to research idea generation using large language models."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-08", "reason": "This paper assesses the effectiveness of large language models trained on code."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "publication_date": "2024-07-21", "reason": "This paper introduces Llama 3, a high-performing open-source language model."}, {"fullname_first_author": "Tianyang Liu", "paper_title": "Repobench: Benchmarking repository-level code auto-completion systems", "publication_date": "2024-05-07", "reason": "This paper introduces RepoBench, a benchmark for repository-level code auto-completion systems."}]}