{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models.", "publication_date": "2023-02-01", "reason": "This paper introduces LLaMA, which serves as the foundational language model architecture used in the presented work."}, {"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models.", "publication_date": "2022-01-01", "reason": "This paper introduces Latent Diffusion Models (LDMs), a significant approach in image generation that the presented work compares against."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis.", "publication_date": "2021-01-01", "reason": "This paper introduces VQGANs, used for discretizing the image space, which is essential for the Token-Shuffle method."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models.", "publication_date": "2020-01-01", "reason": "This paper is one of the earliest explorations of diffusion models, and its findings serve as a baseline the work builds upon."}, {"fullname_first_author": "Aditya Ramesh", "paper_title": "Learning transferable visual models from natural language supervision.", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, which incorporates continuous features like VAE or CLIP features of visual data into LLMs for improved multimodal understanding and generation."}]}