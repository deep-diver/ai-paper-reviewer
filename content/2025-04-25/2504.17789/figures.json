[{"figure_path": "https://arxiv.org/html/2504.17789/x1.png", "caption": "Figure 1: \nHigh-resolution images generated by our 2.7B AR model with Token-Shuffle (shuffle window size = 2).", "description": "This figure showcases high-resolution images generated using a 2.7B parameter autoregressive (AR) model.  The key innovation is the application of the 'Token-Shuffle' technique, with a shuffle window size of 2. This method efficiently reduces the number of image tokens processed by the model, allowing for high-resolution generation while maintaining computational efficiency. The images represent a variety of scenes and subjects, demonstrating the model's capability to generate diverse and detailed imagery.", "section": "High-resolution images generated by our 2.7B AR model with Token-Shuffle (shuffle window size = 2)."}, {"figure_path": "https://arxiv.org/html/2504.17789/x2.png", "caption": "Figure 2: \nToken-Shuffle Pipeline: a plug-and-play operation pair for reducing visual token number in MLLMs, comprising a token-shuffle operation to merge spatially local visual tokens for Transformer input and a token-unshuffle operation to disentangle inferred visual tokens.", "description": "The Token-Shuffle pipeline is a two-stage process designed to improve efficiency in Multimodal Large Language Models (MLLMs) by reducing the number of image tokens.  First, the token-shuffle operation merges spatially adjacent tokens in the input image, reducing redundancy and the number of tokens sent to the Transformer. Then, after processing by the Transformer, the token-unshuffle operation reverses this process, separating the processed tokens to reconstruct the original image spatial arrangement.  This method enables more efficient training and inference for high-resolution image generation.", "section": "3 Token-Shuffle"}, {"figure_path": "https://arxiv.org/html/2504.17789/x5.png", "caption": "Figure 3: \nIllustration of visual vocabulary dimensional redundancy. Left: Two MLPs reduce visual token rank by a factor of r\ud835\udc5fritalic_r. Right: Pre-training loss (log-scaled perplexity) for different r\ud835\udc5fritalic_r values, showing substantial dimension reduction with minimal performance impact.", "description": "This figure demonstrates the redundancy in the dimensionality of visual tokens used in multimodal large language models (MLLMs).  The left panel shows the architecture where two Multilayer Perceptrons (MLPs) are used to reduce the dimensionality of visual tokens by a factor of 'r'. The right panel presents a graph showing the pre-training loss (measured as log-scaled perplexity) for different values of 'r'.  The graph shows that even with substantial dimensionality reduction, the pre-training loss does not increase significantly, indicating the presence of redundant information within the high-dimensional visual vocabulary.", "section": "Visual Dimensional Redundancy"}, {"figure_path": "https://arxiv.org/html/2504.17789/x6.png", "caption": "Figure 4: Token-Shuffle can enhance efficiency quadratically. For instance, with a shuffle window size s=2\ud835\udc602s=2italic_s = 2, we achieve approximately a 4\u00d74\\times4 \u00d7 reduction in both training FLOPs and token number. Considering the use of KV-cache during inference, inference time scales roughly linearly with the token number.", "description": "Figure 4 illustrates the impact of the Token-Shuffle technique on computational efficiency.  The left and right panels show the results for 1024x1024 and 2048x2048 resolution image generation, respectively.  Each panel presents a bar graph comparing the baseline (no Token-Shuffle) and three variations with shuffle window sizes of 2, 4, and 8. The y-axis shows the token number (in thousands) and the FLOPs (in trillions). The figure demonstrates that using Token-Shuffle reduces both the number of tokens and FLOPs, with a quadratic reduction in computation cost. For instance, a shuffle window size of 2 reduces the number of tokens and FLOPs by approximately 4 times compared to the baseline.  In the context of inference, where KV-cache is typically used, inference time is largely dependent on the number of tokens; therefore, Token-Shuffle leads to roughly linear improvements in inference speed.", "section": "3 Token-Shuffle"}, {"figure_path": "https://arxiv.org/html/2504.17789/x7.png", "caption": "Figure 5: Comparison of different CFG schedulers with a monotonic increase in CFG scale from 1 to 7.5. Right: CFG-scheduler improves both visual aesthetics and text alignment, compared to the baseline of a consistent CFG value of 7.5 across all visual tokens.", "description": "This figure compares different Classifier-Free Guidance (CFG) schedulers for image generation.  The x-axis represents the inference step (or index of the next token generated), and the y-axis represents the CFG scale.  Five different CFG schedulers are shown: Baseline (constant CFG scale), Linear (linearly increasing CFG scale), Half-Linear (linearly increasing to a point, then constant), Drop-First (initial CFG scale of 1, then constant CFG scale), Sin (sinusoidal increase), and Sigmoid (sigmoid increase). The right panel showcases sample images demonstrating the impact of different schedulers on both visual aesthetics (image quality) and text alignment (how well the image matches the text prompt).  The figure shows that using a dynamic CFG schedule, as opposed to a constant CFG scale, generally improves both the quality and text-alignment of the generated images.", "section": "3.5 Token-Shuffle Implementation Details"}, {"figure_path": "https://arxiv.org/html/2504.17789/x8.png", "caption": "Figure 6: Human evaluation comparing Token-Shuffle with LlamaGen\u00a0Sun et\u00a0al. (2024a) (AR-based model without text), Lumina-mGPT\u00a0Liu et\u00a0al. (2024) (AR-based model with text) and LDM\u00a0Rombach et\u00a0al. (2022) (diffusion-based model) on text alignment, visual flaws, and visual appearance.", "description": "This figure presents the results of a human evaluation comparing the performance of Token-Shuffle against three other image generation models: LlamaGen (an autoregressive model without text input), Lumina-mGPT (an autoregressive model with text input), and LDM (a diffusion-based model).  The evaluation focuses on three key aspects of image quality:  text alignment (how well the generated image matches the textual description), visual flaws (presence of errors or inconsistencies in the image), and visual appearance (overall aesthetic quality).  The bar charts show the percentage of times each model won, tied, or lost for each of these three metrics in a pairwise comparison.", "section": "4.3 Human Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.17789/x9.png", "caption": "Figure 7: Visual comparison with other open-source diffusion-based and AR-based models (zoom in for details).", "description": "This figure presents a visual comparison of images generated by the Token-Shuffle model against those from other open-source models, including both diffusion-based and autoregressive (AR) models.  The comparison highlights the visual differences in image quality, detail, and adherence to text prompts.  By zooming in, finer details and subtle distinctions between the different models become apparent, allowing for a more thorough qualitative assessment of the Token-Shuffle method's performance.", "section": "Visual Comparison"}, {"figure_path": "https://arxiv.org/html/2504.17789/x10.png", "caption": "Figure 8: Visual comparison of different Token-Shuffle window sizes. We tested each prompt with fixed random seeds and reported the VQAScore\u00a0Lin et\u00a0al. (2024) in the bottom-right corner.", "description": "This figure presents a comparison of image generation results using different window sizes in the Token-Shuffle method.  The Token-Shuffle method reduces the number of visual tokens in a transformer model by merging spatially local tokens.  Different window sizes correspond to different levels of token merging and therefore affect computational efficiency and image quality.  Three different window sizes (1, 2, and 4) are shown, each generating an image for the same prompt.  The VQAScore (Lin et al., 2024) is provided for each generated image, offering a quantitative measure of the image quality and text-image alignment.  Fixed random seeds ensure consistency between images of the same size and prompt, making the comparison reliable.", "section": "4.5.2 Comparison of different shuffle sizes"}, {"figure_path": "https://arxiv.org/html/2504.17789/x11.png", "caption": "(a) More MLP blocks", "description": "This ablation study investigates the impact of increasing the number of Multi-Layer Perceptrons (MLPs) within the Token-Shuffle operation on model performance.  The experiment compares three configurations: the default setting with 2 MLP blocks, and variations with 4 and 6 MLP blocks.  The x-axis represents training iterations, and the y-axis represents the training loss (log-scaled perplexity). The graph visually demonstrates how changing the number of MLPs affects the training process and model convergence.", "section": "4.5 Ablation study"}, {"figure_path": "https://arxiv.org/html/2504.17789/x12.png", "caption": "(b) Drop tokens", "description": "This ablation study compares the performance of the standard Token-Shuffle method against a modified version where, within each local window of tokens, all but the last token are dropped.  The graph likely shows the training loss (or a related metric) over training iterations for both methods. This comparison helps determine the impact of each token's contribution within the local window to the overall generation performance. A significant difference would suggest that certain tokens within the window are more crucial than others, implying that simply retaining a single token isn't enough to capture the complete information needed for high-quality image generation.", "section": "4.5 Ablation study"}, {"figure_path": "https://arxiv.org/html/2504.17789/x13.png", "caption": "(c) Positional Embedding", "description": "This ablation study investigates the effect of adding positional embeddings to the Token-Shuffle mechanism.  The graph plots the training loss (log-scaled perplexity) over training iterations.  Three conditions are compared: a baseline with no additional positional embeddings, one with local positional embeddings added, and one with global positional embeddings. The results show that adding positional embeddings, either local or global, does not significantly improve performance compared to the baseline. This suggests that the inherent position-awareness of the MLP layers within Token-Shuffle is sufficient for encoding positional information.", "section": "4.5 Ablation study"}, {"figure_path": "https://arxiv.org/html/2504.17789/x14.png", "caption": "(d) Re-sampler & Simple impl.", "description": "This figure compares the training loss curves for different implementations of Token-Shuffle.  Specifically, it contrasts the standard Token-Shuffle method with a variant where only the last token in a local window is kept (Drop), and with simpler approaches that leverage re-sampling or purely linear operations. It visually demonstrates the effectiveness of the original Token-Shuffle approach compared to alternatives for high-resolution image synthesis within the context of autoregressive large language models.", "section": "4.5.1 Design choice of Token-Shuffle"}, {"figure_path": "https://arxiv.org/html/2504.17789/x17.png", "caption": "Figure 9: \nEffectiveness comparison of various Token-Shuffle implementations and alternatives. Our implementation shows reasonable alignment with the Token-Shuffle concept, as indicated by the training loss in a fair comparison.", "description": "Figure 9 presents a detailed ablation study comparing different implementations and variations of the Token-Shuffle method.  It shows training loss curves for several configurations, including different numbers of MLP blocks, strategies for handling tokens within shuffle windows (shuffling vs. dropping), and the inclusion or exclusion of positional embeddings.  By comparing the training loss across these various approaches, the authors demonstrate that their default Token-Shuffle implementation provides a reasonable balance between computational efficiency and image generation quality.", "section": "4.5 Ablation study"}, {"figure_path": "https://arxiv.org/html/2504.17789/x18.png", "caption": "Figure 10: Training losses for different shuffle window sizes.", "description": "This figure shows the training loss (measured as log-scaled perplexity) plotted against the number of training iterations for different shuffle window sizes used in the Token-Shuffle method.  The shuffle window size determines how many spatially local visual tokens are merged into a single token before being fed into the Transformer.  A smaller window size (e.g., 1) means no merging occurs, while larger sizes (e.g., 2, 4) lead to increased compression. The graph illustrates the trade-off between compression and model performance. As the shuffle window size increases, the training loss also increases, indicating a decrease in the model's ability to learn effectively from the compressed representations. This suggests a balance needs to be found between efficiency and image generation quality.", "section": "3 Token-Shuffle"}, {"figure_path": "https://arxiv.org/html/2504.17789/x19.png", "caption": "Figure 11: We plot the average loss (left) and gradient norm (right) when training with a resolution of 2048\u00d72048204820482048\\times 20482048 \u00d7 2048. Training shows instability after approximately 20K iterations.", "description": "This figure displays training instability issues encountered when training a model with an image resolution of 2048 x 2048. The left panel shows the average loss during training, while the right panel shows the gradient norm.  Both graphs illustrate that after approximately 20,000 iterations, the training process becomes unstable, with loss and gradient norm increasing significantly.  This instability highlights a challenge in training high-resolution image generation models.", "section": "3 Token-Shuffle"}, {"figure_path": "https://arxiv.org/html/2504.17789/x20.png", "caption": "Figure 12: Without explicitly appending <|start_of_image|> token, our model naturally generates text based on input and seamlessly transitions to an image, consistently and automatically concluding in line with training data format.", "description": "This figure demonstrates the model's ability to seamlessly transition from text generation to image generation without explicitly using a special token (<|start_of_image|>) to mark the beginning of the image sequence.  The model naturally switches to generating image tokens based solely on the input text prompt, maintaining consistency with the training data format.  The examples shown illustrate how the model smoothly integrates text and image, indicating a natural and efficient multimodal generation process.", "section": "3.2 Limitations for Image synthesis"}, {"figure_path": "https://arxiv.org/html/2504.17789/x21.png", "caption": "Figure 13: CFG scale vs. VQAScore.", "description": "This figure illustrates the relationship between the Classifier-Free Guidance (CFG) scale and the VQAScore. The x-axis represents different CFG scales used during image generation, while the y-axis displays the corresponding VQAScore, which measures the quality of generated images.  The graph shows that as the CFG scale increases, the VQAScore generally improves, indicating better image quality. However, the improvement plateaus at a certain point; thus, finding the optimal CFG scale to balance image quality and computation efficiency is vital.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.17789/x22.png", "caption": "Figure 14: Human evaluation of text alignment, comparing Token-Shuffle with various AR-based and diffusion-based models. Results may vary slightly from Fig.\u00a06 due to the generated images are assessed by different vendors.", "description": "This figure presents a human evaluation comparing the text alignment capabilities of the Token-Shuffle model against other Autoregressive (AR) and diffusion-based models. The evaluation was conducted using a large set of prompts and the results are presented in the form of a bar chart showing the percentage of times each model performed better than others in terms of text alignment.  The slight variation in results compared to Figure 6 is attributed to the fact that different vendors assessed the generated images in each experiment.", "section": "4.3 Human Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.17789/x23.png", "caption": "Figure 15: Examples of generated images under different CFG scales.", "description": "This figure shows a series of images generated using different Classifier-Free Guidance (CFG) scales.  The CFG scale is a hyperparameter that controls the balance between text fidelity (how well the image matches the text prompt) and visual coherence (how visually appealing and realistic the image is). Lower CFG values prioritize visual coherence, resulting in images that may not perfectly capture all aspects of the text prompt. Higher CFG values prioritize text fidelity, resulting in images that may be more faithful to the prompt but potentially less aesthetically pleasing or more likely to contain visual artifacts. This figure visually demonstrates the effect of varying the CFG scale on the generated images.", "section": "More Studies"}, {"figure_path": "https://arxiv.org/html/2504.17789/x24.png", "caption": "Figure 16: Attention maps of three implementations: bi-directional, causal, and Token-Shuffle. Illustrated with a feature map size of 4\u00d74444\\times 44 \u00d7 4 (16 tokens) and a shuffle window size of 2 for Token-Shuffle.", "description": "This figure compares the attention mechanisms of three different transformer implementations: bidirectional, causal, and Token-Shuffle.  The visual representation uses a feature map of size 4x4 (resulting in 16 tokens).  For the Token-Shuffle method, a shuffle window size of 2 is used, demonstrating how this technique merges spatially local tokens. The comparison highlights the differences in how these approaches handle the relationships between tokens in the sequence, showcasing the unique attention patterns of Token-Shuffle.", "section": "3.5 Token-Shuffle Implementation Details"}, {"figure_path": "https://arxiv.org/html/2504.17789/x25.png", "caption": "Figure 17: Visual examples comparing Token-Shuffle (compress ratio 8\u00d78\\times8 \u00d7 with Token-Shuffle window size of 2) and high compress VQGAN (compress ratio 16\u00d716\\times16 \u00d7).", "description": "This figure compares image generation results between two methods: Token-Shuffle and a high-compression VQGAN.  Both methods aim to generate images efficiently, but use different approaches.  Token-Shuffle merges spatially local image tokens to reduce computational costs, while maintaining high resolution. The high-compression VQGAN uses a highly compressed visual vocabulary, resulting in a significantly smaller number of visual tokens, but potentially at the cost of reduced image quality. The figure presents visual examples of images generated by both methods to highlight the difference in their visual outputs and efficiency tradeoffs.", "section": "High-Compress VQGAN or Token-Shuffle"}, {"figure_path": "https://arxiv.org/html/2504.17789/x26.png", "caption": "Figure 18: Human evaluation of Token-Shuffle (compress ratio 8\u00d78\\times8 \u00d7 with Token-Shuffle window size of 2) and high compress VQGAN (compress ratio 16\u00d716\\times16 \u00d7).", "description": "This figure presents a human evaluation comparing the image generation quality of two methods: Token-Shuffle and high-compression VQGAN.  Token-Shuffle uses a compression ratio of 8x (achieved by using a shuffle window size of 2), while the high-compression VQGAN uses a compression ratio of 16x. The evaluation metrics are text alignment, visual flaws, and visual appearance.  The results show how each method performs in terms of image quality and fidelity to the text prompt.", "section": "4.5 Ablation study"}]