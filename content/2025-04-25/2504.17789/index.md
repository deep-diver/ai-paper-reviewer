---
title: "Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models"
summary: "Token-Shuffle: Efficient AR-based high-resolution image generation via token reduction."
categories: ["AI Generated", "ü§ó Daily Papers"]
tags: ["Computer Vision", "Image Generation", "üè¢ Northeastern University",]
showSummary: true
date: 2025-04-24
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.17789 {{< /keyword >}}
{{< keyword icon="writer" >}} Xu Ma et el. {{< /keyword >}}
 
{{< keyword >}} ü§ó 2025-04-25 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.17789" target="_self" >}}
‚Üó arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.17789" target="_self" >}}
‚Üó Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.17789/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Autoregressive models, dominant in language, face challenges in image synthesis due to the high number of image tokens required, limiting efficiency and resolution. This work observes dimensional redundancy in visual vocabularies of Multimodal Large Language Models (MLLMs), where low-dimensional visual codes map to high-dimensional language vocabularies. The substantial number of tokens constrains both training and inference efficiency, as well as image resolution. 



To address this, the paper introduces Token-Shuffle, reducing image tokens in Transformers. **Token-Shuffle merges spatially local tokens to decrease input token number** and untangles inferred tokens to restore spatial arrangement. Training with textual prompts, the strategy enables MLLMs to support extremely high-resolution image synthesis while maintaining efficiency. The 2.7B model achieves state-of-the-art generation performance at 2048 √ó 2048.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Token-Shuffle, reduces the number of image tokens in Transformers, addressing a key limitation in autoregressive models for image synthesis. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The method achieves state-of-the-art results in text-to-image generation at high resolutions (2048x2048), outperforming existing AR and diffusion models. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Human evaluations demonstrate superior image generation capabilities in terms of text alignment, visual flaw, and visual appearance. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper is important for researchers because it introduces **Token-Shuffle, a novel approach to dramatically improve the efficiency of high-resolution image generation** with autoregressive models. This is especially relevant to researchers working on multimodal large language models (MLLMs) since it enhances capabilities in terms of text-alignment, visual flaw and visual appearance. The framework could **serve as a new baseline for future works**.

------
#### Visual Insights



![](https://arxiv.org/html/2504.17789/x1.png)

> üîº This figure showcases high-resolution images generated using a 2.7B parameter autoregressive (AR) model.  The key innovation is the application of the 'Token-Shuffle' technique, with a shuffle window size of 2. This method efficiently reduces the number of image tokens processed by the model, allowing for high-resolution generation while maintaining computational efficiency. The images represent a variety of scenes and subjects, demonstrating the model's capability to generate diverse and detailed imagery.
> <details>
> <summary>read the caption</summary>
> Figure 1:  High-resolution images generated by our 2.7B AR model with Token-Shuffle (shuffle window size = 2).
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S4.T1.1.1">
<tr class="ltx_tr" id="S4.T1.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.1.1.2.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.2.2" rowspan="3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.2.1">Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T1.1.1.2.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.3.1">"Basic" prompts</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.2.4"><span class="ltx_text" id="S4.T1.1.1.2.4.1" style="color:#FFFFFF;">.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T1.1.1.2.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.2.5.1">"Hard" prompts</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.1.1">Attribute</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.2.1">Scene</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T1.1.1.3.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.3.1">Relation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.4.1">Overall</span></td>
<td class="ltx_td" id="S4.T1.1.1.3.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.6" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.6.1">Count</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.7" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.7.1">Differ</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.8" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.8.1">Compare</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S4.T1.1.1.3.9"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.9.1">Logical</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.10" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.3.10.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.1">Spatial</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.2">Action</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.3">Part</td>
<td class="ltx_td" id="S4.T1.1.1.4.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.5">Negate</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.6">Universal</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.5.1">SDXL-v2.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.2">Diff.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.3">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.4">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.6">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.7">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.8">0.78</td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.1.5.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.10">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.11">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.12">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.13">0.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.14">0.64</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.15">0.62</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.6.1">SD-XL Turbo</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.3">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.4">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.5">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.6">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.7">0.89</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.8">0.84</td>
<td class="ltx_td" id="S4.T1.1.1.6.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.10">0.72</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.11">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.12">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.13">0.52</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.14">0.65</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.15">0.65</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.7.1">DeepFloyd-IF¬†<cite class="ltx_cite ltx_citemacro_cite">Saharia et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib44" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.3">0.83</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.4">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5">0.81</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.6">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.7">0.89</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.8">0.84</td>
<td class="ltx_td" id="S4.T1.1.1.7.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.10">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.11">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.12">0.71</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.13">0.53</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.14">0.68</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.15">0.66</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.8.1">Midjourney v6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.3">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.4">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.5">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.7">0.91</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.8">0.87</td>
<td class="ltx_td" id="S4.T1.1.1.8.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.10">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.11">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.12">0.79</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.13">0.50</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.14">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.15">0.69</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.9.1">DALL-E 3¬†<cite class="ltx_cite ltx_citemacro_cite">Betker et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib3" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.3">0.91</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.4">0.90</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.5">0.92</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.6">0.89</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7">0.91</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.8"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.9.8.1">0.90</span></td>
<td class="ltx_td" id="S4.T1.1.1.9.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.10">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.11">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.12">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.13">0.48</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.14">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.15">0.70</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.1.10.1">LlamaGen¬†<cite class="ltx_cite ltx_citemacro_cite">Sun et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib49" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.2">AR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.3">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.4">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.5">0.74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.6">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.7">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.8">0.74</td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.1.10.9"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.11">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.12">0.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.13">0.48</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.14">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.15">0.59</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.11">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.11.1">Lumina-mGPT-7B¬†<cite class="ltx_cite ltx_citemacro_cite">Liu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib30" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.2">AR</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.3">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.4">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.5">0.82</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.6">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.7">0.93</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.8">0.83</td>
<td class="ltx_td" id="S4.T1.1.1.11.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.10">0.75</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.11">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.12">0.73</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.13">0.47</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.14">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.15">0.63</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.12">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.12.1">EMU3¬†<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib58" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.2">AR</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.3">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.4">0.81</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.5">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.6">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.7">0.87</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.8">0.78</td>
<td class="ltx_td" id="S4.T1.1.1.12.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.11">0.62</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.12">0.70</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.13">0.45</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.14">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.15">0.60</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.13.1">SEED-X¬†<cite class="ltx_cite ltx_citemacro_cite">Ge et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib11" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.2">AR+Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.3">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.4">0.88</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.5">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.6">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.7">0.90</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.8">0.86</td>
<td class="ltx_td" id="S4.T1.1.1.13.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.10">0.79</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.11">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.12">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.13">0.56</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.14">0.73</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.15">0.70</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14">
<td class="ltx_td ltx_align_left" id="S4.T1.1.1.14.1">Token-Shuffle</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.2">AR</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.3">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.4">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.5">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.6">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.7">0.83</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.8">0.78</td>
<td class="ltx_td" id="S4.T1.1.1.14.9"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.10">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.11">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.13">0.58</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.14">0.64</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.15">0.67</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.1.1.1.1">Token-Shuffle<math alttext="{\dagger}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" mathcolor="#0000FF" xref="S4.T1.1.1.1.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">‚Ä†</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.2">AR</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.3">0.88</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.4">0.88</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.5">0.88</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.6">0.87</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.7">0.91</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.8">0.88</td>
<td class="ltx_td ltx_border_bb" id="S4.T1.1.1.1.9"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.10">0.81</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.11">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.12">0.81</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.13">0.68</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.14">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.1.1.15"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.15.1">0.77</span></td>
</tr>
</table>{{< /table-caption >}}

> üîº This table presents the results of evaluating image generation models on the GenAI-Bench benchmark using the VQAScore metric.  The benchmark assesses various aspects of image quality, including relation, logical reasoning, attributes, scenes, and overall quality for both 'basic' and 'hard' prompts.  The models are compared based on the VQAScore, which measures the alignment between text prompts and the generated images.  The '‚Ä†‚Ä†' symbol indicates that the model was evaluated using Llama3-rewritten prompts to ensure consistency in caption length between training and inference, improving the reliability of the comparison.
> <details>
> <summary>read the caption</summary>
> Table 1: VQAScore evaluation of image generation on GenAI-Bench. '‚Ä†‚Ä†\dagger‚Ä†' indicates that images are generated by Llama3-rewritten prompts to match the caption length in the training data, for training-inference consistency.
> </details>





### In-depth insights


#### Token-Shuffle
The Token-Shuffle heading introduces a method designed to **reduce the number of image tokens** in Transformers, specifically within Multimodal Large Language Models (MLLMs). This is crucial because the high token count in images hinders the efficiency of training and inference, especially at high resolutions. The key idea revolves around **exploiting dimensional redundancy** in visual vocabularies, where low-dimensional visual codes are mapped to high-dimensional language vocabularies. The method involves two operations: token-shuffle, which merges spatially local tokens to decrease the input token number, and token-unshuffle, which untangles the inferred tokens to restore the spatial arrangement. By jointly training with textual prompts, Token-Shuffle **enables MLLMs to support high-resolution image synthesis** without requiring additional pretrained text encoders. This approach aims to maintain efficient training and inference while pushing the boundaries of autoregressive text-to-image generation.

#### AR vs diffusion
The paper explores the landscape of image generation, contrasting Autoregressive (AR) models with diffusion models. AR models, dominant in language, are now applied to image synthesis, yet face challenges like the **substantial number of image tokens** required, hindering efficiency and resolution. Diffusion models excel in high-resolution generation, but AR models offer a **unified, general multimodal system** potential. A key distinction lies in handling visual tokens: AR models favor discrete tokens due to LLM compatibility, requiring vocabulary expansion. This contrasts with continuous tokens in diffusion models, demanding pipeline modifications. The paper introduces Token-Shuffle to tackle token count limitations in AR models, aiming for efficient high-resolution synthesis within MLLMs. This aims to surpass diffusion-based quality by **balancing quality and computational cost**.

#### Visual token MLLM
**Visual token MLLMs** represent a specific approach to building multimodal large language models, which leverage *discrete* visual tokens instead of continuous representations. This choice has significant implications. While continuous tokens may offer superior image quality and require fewer tokens, discrete tokens align more naturally with the LLM architecture by simply expanding the vocabulary size. The primary challenge with visual tokens is the sheer number needed for high-resolution images, leading to quadratic increases in computational cost. Despite this, many real-world MLLM applications, such as EMU3 and Chameleon, use discrete visual tokens, likely due to their easier integration into existing LLM pipelines. Research in this area focuses on optimizing the use of visual tokens to achieve high-resolution generation without prohibitive costs, often by exploring methods to reduce the number of tokens or improve their efficiency.

#### Dimension reduce
While the paper doesn't explicitly discuss a section titled "Dimension Reduce," the concept is intrinsically woven into its core innovation: **Token-Shuffle**. The method tackles the high computational cost of autoregressive image generation by reducing the number of visual tokens processed by the Transformer. Token-Shuffle achieves this **reduction** by merging spatially local tokens along the channel dimension, essentially condensing information. This **compression** step can be viewed as a form of dimension reduction, albeit applied strategically in the spatial domain rather than directly manipulating the embedding space of individual tokens. The operation is followed by a token-unshuffle step after Transformer blocks, restoring spatial arrangement. This whole process intelligently harnesses the **dimensional redundancy inherent in visual vocabularies within MLLMs**, where low-dimensional visual codes map to high-dimensional language vocabularies, thereby reducing the computational burden of high-resolution image generation in AR models.

#### High Resolution
The paper addresses the challenge of generating **high-resolution images** with autoregressive models, which typically require a large number of tokens, leading to computational inefficiencies. To overcome this limitation, the authors propose Token-Shuffle, a method that reduces the number of tokens by exploiting the dimensional redundancy of visual vocabularies in multimodal large language models (MLLMs). The key idea is to merge spatially local tokens along the channel dimension (**token-shuffle**) and then untangle them after Transformer blocks (**token-unshuffle**). This approach enables the generation of **2048 x 2048 resolution** images. The authors hope that Token-Shuffle can serve as a foundation for efficient high-resolution image generation within MLLMs.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.17789/x2.png)

> üîº The Token-Shuffle pipeline is a two-stage process designed to improve efficiency in Multimodal Large Language Models (MLLMs) by reducing the number of image tokens.  First, the token-shuffle operation merges spatially adjacent tokens in the input image, reducing redundancy and the number of tokens sent to the Transformer. Then, after processing by the Transformer, the token-unshuffle operation reverses this process, separating the processed tokens to reconstruct the original image spatial arrangement.  This method enables more efficient training and inference for high-resolution image generation.
> <details>
> <summary>read the caption</summary>
> Figure 2:  Token-Shuffle Pipeline: a plug-and-play operation pair for reducing visual token number in MLLMs, comprising a token-shuffle operation to merge spatially local visual tokens for Transformer input and a token-unshuffle operation to disentangle inferred visual tokens.
> </details>



![](https://arxiv.org/html/2504.17789/x5.png)

> üîº This figure demonstrates the redundancy in the dimensionality of visual tokens used in multimodal large language models (MLLMs).  The left panel shows the architecture where two Multilayer Perceptrons (MLPs) are used to reduce the dimensionality of visual tokens by a factor of 'r'. The right panel presents a graph showing the pre-training loss (measured as log-scaled perplexity) for different values of 'r'.  The graph shows that even with substantial dimensionality reduction, the pre-training loss does not increase significantly, indicating the presence of redundant information within the high-dimensional visual vocabulary.
> <details>
> <summary>read the caption</summary>
> Figure 3:  Illustration of visual vocabulary dimensional redundancy. Left: Two MLPs reduce visual token rank by a factor of rùëüritalic_r. Right: Pre-training loss (log-scaled perplexity) for different rùëüritalic_r values, showing substantial dimension reduction with minimal performance impact.
> </details>



![](https://arxiv.org/html/2504.17789/x6.png)

> üîº Figure 4 illustrates the impact of the Token-Shuffle technique on computational efficiency.  The left and right panels show the results for 1024x1024 and 2048x2048 resolution image generation, respectively.  Each panel presents a bar graph comparing the baseline (no Token-Shuffle) and three variations with shuffle window sizes of 2, 4, and 8. The y-axis shows the token number (in thousands) and the FLOPs (in trillions). The figure demonstrates that using Token-Shuffle reduces both the number of tokens and FLOPs, with a quadratic reduction in computation cost. For instance, a shuffle window size of 2 reduces the number of tokens and FLOPs by approximately 4 times compared to the baseline.  In the context of inference, where KV-cache is typically used, inference time is largely dependent on the number of tokens; therefore, Token-Shuffle leads to roughly linear improvements in inference speed.
> <details>
> <summary>read the caption</summary>
> Figure 4: Token-Shuffle can enhance efficiency quadratically. For instance, with a shuffle window size s=2ùë†2s=2italic_s = 2, we achieve approximately a 4√ó4\times4 √ó reduction in both training FLOPs and token number. Considering the use of KV-cache during inference, inference time scales roughly linearly with the token number.
> </details>



![](https://arxiv.org/html/2504.17789/x7.png)

> üîº This figure compares different Classifier-Free Guidance (CFG) schedulers for image generation.  The x-axis represents the inference step (or index of the next token generated), and the y-axis represents the CFG scale.  Five different CFG schedulers are shown: Baseline (constant CFG scale), Linear (linearly increasing CFG scale), Half-Linear (linearly increasing to a point, then constant), Drop-First (initial CFG scale of 1, then constant CFG scale), Sin (sinusoidal increase), and Sigmoid (sigmoid increase). The right panel showcases sample images demonstrating the impact of different schedulers on both visual aesthetics (image quality) and text alignment (how well the image matches the text prompt).  The figure shows that using a dynamic CFG schedule, as opposed to a constant CFG scale, generally improves both the quality and text-alignment of the generated images.
> <details>
> <summary>read the caption</summary>
> Figure 5: Comparison of different CFG schedulers with a monotonic increase in CFG scale from 1 to 7.5. Right: CFG-scheduler improves both visual aesthetics and text alignment, compared to the baseline of a consistent CFG value of 7.5 across all visual tokens.
> </details>



![](https://arxiv.org/html/2504.17789/x8.png)

> üîº This figure presents the results of a human evaluation comparing the performance of Token-Shuffle against three other image generation models: LlamaGen (an autoregressive model without text input), Lumina-mGPT (an autoregressive model with text input), and LDM (a diffusion-based model).  The evaluation focuses on three key aspects of image quality:  text alignment (how well the generated image matches the textual description), visual flaws (presence of errors or inconsistencies in the image), and visual appearance (overall aesthetic quality).  The bar charts show the percentage of times each model won, tied, or lost for each of these three metrics in a pairwise comparison.
> <details>
> <summary>read the caption</summary>
> Figure 6: Human evaluation comparing Token-Shuffle with LlamaGen¬†Sun et¬†al. (2024a) (AR-based model without text), Lumina-mGPT¬†Liu et¬†al. (2024) (AR-based model with text) and LDM¬†Rombach et¬†al. (2022) (diffusion-based model) on text alignment, visual flaws, and visual appearance.
> </details>



![](https://arxiv.org/html/2504.17789/x9.png)

> üîº This figure presents a visual comparison of images generated by the Token-Shuffle model against those from other open-source models, including both diffusion-based and autoregressive (AR) models.  The comparison highlights the visual differences in image quality, detail, and adherence to text prompts.  By zooming in, finer details and subtle distinctions between the different models become apparent, allowing for a more thorough qualitative assessment of the Token-Shuffle method's performance.
> <details>
> <summary>read the caption</summary>
> Figure 7: Visual comparison with other open-source diffusion-based and AR-based models (zoom in for details).
> </details>



![](https://arxiv.org/html/2504.17789/x10.png)

> üîº This figure presents a comparison of image generation results using different window sizes in the Token-Shuffle method.  The Token-Shuffle method reduces the number of visual tokens in a transformer model by merging spatially local tokens.  Different window sizes correspond to different levels of token merging and therefore affect computational efficiency and image quality.  Three different window sizes (1, 2, and 4) are shown, each generating an image for the same prompt.  The VQAScore (Lin et al., 2024) is provided for each generated image, offering a quantitative measure of the image quality and text-image alignment.  Fixed random seeds ensure consistency between images of the same size and prompt, making the comparison reliable.
> <details>
> <summary>read the caption</summary>
> Figure 8: Visual comparison of different Token-Shuffle window sizes. We tested each prompt with fixed random seeds and reported the VQAScore¬†Lin et¬†al. (2024) in the bottom-right corner.
> </details>



![](https://arxiv.org/html/2504.17789/x11.png)

> üîº This ablation study investigates the impact of increasing the number of Multi-Layer Perceptrons (MLPs) within the Token-Shuffle operation on model performance.  The experiment compares three configurations: the default setting with 2 MLP blocks, and variations with 4 and 6 MLP blocks.  The x-axis represents training iterations, and the y-axis represents the training loss (log-scaled perplexity). The graph visually demonstrates how changing the number of MLPs affects the training process and model convergence.
> <details>
> <summary>read the caption</summary>
> (a) More MLP blocks
> </details>



![](https://arxiv.org/html/2504.17789/x12.png)

> üîº This ablation study compares the performance of the standard Token-Shuffle method against a modified version where, within each local window of tokens, all but the last token are dropped.  The graph likely shows the training loss (or a related metric) over training iterations for both methods. This comparison helps determine the impact of each token's contribution within the local window to the overall generation performance. A significant difference would suggest that certain tokens within the window are more crucial than others, implying that simply retaining a single token isn't enough to capture the complete information needed for high-quality image generation.
> <details>
> <summary>read the caption</summary>
> (b) Drop tokens
> </details>



![](https://arxiv.org/html/2504.17789/x13.png)

> üîº This ablation study investigates the effect of adding positional embeddings to the Token-Shuffle mechanism.  The graph plots the training loss (log-scaled perplexity) over training iterations.  Three conditions are compared: a baseline with no additional positional embeddings, one with local positional embeddings added, and one with global positional embeddings. The results show that adding positional embeddings, either local or global, does not significantly improve performance compared to the baseline. This suggests that the inherent position-awareness of the MLP layers within Token-Shuffle is sufficient for encoding positional information.
> <details>
> <summary>read the caption</summary>
> (c) Positional Embedding
> </details>



![](https://arxiv.org/html/2504.17789/x14.png)

> üîº This figure compares the training loss curves for different implementations of Token-Shuffle.  Specifically, it contrasts the standard Token-Shuffle method with a variant where only the last token in a local window is kept (Drop), and with simpler approaches that leverage re-sampling or purely linear operations. It visually demonstrates the effectiveness of the original Token-Shuffle approach compared to alternatives for high-resolution image synthesis within the context of autoregressive large language models.
> <details>
> <summary>read the caption</summary>
> (d) Re-sampler & Simple impl.
> </details>



![](https://arxiv.org/html/2504.17789/x17.png)

> üîº Figure 9 presents a detailed ablation study comparing different implementations and variations of the Token-Shuffle method.  It shows training loss curves for several configurations, including different numbers of MLP blocks, strategies for handling tokens within shuffle windows (shuffling vs. dropping), and the inclusion or exclusion of positional embeddings.  By comparing the training loss across these various approaches, the authors demonstrate that their default Token-Shuffle implementation provides a reasonable balance between computational efficiency and image generation quality.
> <details>
> <summary>read the caption</summary>
> Figure 9:  Effectiveness comparison of various Token-Shuffle implementations and alternatives. Our implementation shows reasonable alignment with the Token-Shuffle concept, as indicated by the training loss in a fair comparison.
> </details>



![](https://arxiv.org/html/2504.17789/x18.png)

> üîº This figure shows the training loss (measured as log-scaled perplexity) plotted against the number of training iterations for different shuffle window sizes used in the Token-Shuffle method.  The shuffle window size determines how many spatially local visual tokens are merged into a single token before being fed into the Transformer.  A smaller window size (e.g., 1) means no merging occurs, while larger sizes (e.g., 2, 4) lead to increased compression. The graph illustrates the trade-off between compression and model performance. As the shuffle window size increases, the training loss also increases, indicating a decrease in the model's ability to learn effectively from the compressed representations. This suggests a balance needs to be found between efficiency and image generation quality.
> <details>
> <summary>read the caption</summary>
> Figure 10: Training losses for different shuffle window sizes.
> </details>



![](https://arxiv.org/html/2504.17789/x19.png)

> üîº This figure displays training instability issues encountered when training a model with an image resolution of 2048 x 2048. The left panel shows the average loss during training, while the right panel shows the gradient norm.  Both graphs illustrate that after approximately 20,000 iterations, the training process becomes unstable, with loss and gradient norm increasing significantly.  This instability highlights a challenge in training high-resolution image generation models.
> <details>
> <summary>read the caption</summary>
> Figure 11: We plot the average loss (left) and gradient norm (right) when training with a resolution of 2048√ó2048204820482048\times 20482048 √ó 2048. Training shows instability after approximately 20K iterations.
> </details>



![](https://arxiv.org/html/2504.17789/x20.png)

> üîº This figure demonstrates the model's ability to seamlessly transition from text generation to image generation without explicitly using a special token (<|start_of_image|>) to mark the beginning of the image sequence.  The model naturally switches to generating image tokens based solely on the input text prompt, maintaining consistency with the training data format.  The examples shown illustrate how the model smoothly integrates text and image, indicating a natural and efficient multimodal generation process.
> <details>
> <summary>read the caption</summary>
> Figure 12: Without explicitly appending <|start_of_image|> token, our model naturally generates text based on input and seamlessly transitions to an image, consistently and automatically concluding in line with training data format.
> </details>



![](https://arxiv.org/html/2504.17789/x21.png)

> üîº This figure illustrates the relationship between the Classifier-Free Guidance (CFG) scale and the VQAScore. The x-axis represents different CFG scales used during image generation, while the y-axis displays the corresponding VQAScore, which measures the quality of generated images.  The graph shows that as the CFG scale increases, the VQAScore generally improves, indicating better image quality. However, the improvement plateaus at a certain point; thus, finding the optimal CFG scale to balance image quality and computation efficiency is vital.
> <details>
> <summary>read the caption</summary>
> Figure 13: CFG scale vs. VQAScore.
> </details>



![](https://arxiv.org/html/2504.17789/x22.png)

> üîº This figure presents a human evaluation comparing the text alignment capabilities of the Token-Shuffle model against other Autoregressive (AR) and diffusion-based models. The evaluation was conducted using a large set of prompts and the results are presented in the form of a bar chart showing the percentage of times each model performed better than others in terms of text alignment.  The slight variation in results compared to Figure 6 is attributed to the fact that different vendors assessed the generated images in each experiment.
> <details>
> <summary>read the caption</summary>
> Figure 14: Human evaluation of text alignment, comparing Token-Shuffle with various AR-based and diffusion-based models. Results may vary slightly from Fig.¬†6 due to the generated images are assessed by different vendors.
> </details>



![](https://arxiv.org/html/2504.17789/x23.png)

> üîº This figure shows a series of images generated using different Classifier-Free Guidance (CFG) scales.  The CFG scale is a hyperparameter that controls the balance between text fidelity (how well the image matches the text prompt) and visual coherence (how visually appealing and realistic the image is). Lower CFG values prioritize visual coherence, resulting in images that may not perfectly capture all aspects of the text prompt. Higher CFG values prioritize text fidelity, resulting in images that may be more faithful to the prompt but potentially less aesthetically pleasing or more likely to contain visual artifacts. This figure visually demonstrates the effect of varying the CFG scale on the generated images.
> <details>
> <summary>read the caption</summary>
> Figure 15: Examples of generated images under different CFG scales.
> </details>



![](https://arxiv.org/html/2504.17789/x24.png)

> üîº This figure compares the attention mechanisms of three different transformer implementations: bidirectional, causal, and Token-Shuffle.  The visual representation uses a feature map of size 4x4 (resulting in 16 tokens).  For the Token-Shuffle method, a shuffle window size of 2 is used, demonstrating how this technique merges spatially local tokens. The comparison highlights the differences in how these approaches handle the relationships between tokens in the sequence, showcasing the unique attention patterns of Token-Shuffle.
> <details>
> <summary>read the caption</summary>
> Figure 16: Attention maps of three implementations: bi-directional, causal, and Token-Shuffle. Illustrated with a feature map size of 4√ó4444\times 44 √ó 4 (16 tokens) and a shuffle window size of 2 for Token-Shuffle.
> </details>



![](https://arxiv.org/html/2504.17789/x25.png)

> üîº This figure compares image generation results between two methods: Token-Shuffle and a high-compression VQGAN.  Both methods aim to generate images efficiently, but use different approaches.  Token-Shuffle merges spatially local image tokens to reduce computational costs, while maintaining high resolution. The high-compression VQGAN uses a highly compressed visual vocabulary, resulting in a significantly smaller number of visual tokens, but potentially at the cost of reduced image quality. The figure presents visual examples of images generated by both methods to highlight the difference in their visual outputs and efficiency tradeoffs.
> <details>
> <summary>read the caption</summary>
> Figure 17: Visual examples comparing Token-Shuffle (compress ratio 8√ó8\times8 √ó with Token-Shuffle window size of 2) and high compress VQGAN (compress ratio 16√ó16\times16 √ó).
> </details>



![](https://arxiv.org/html/2504.17789/x26.png)

> üîº This figure presents a human evaluation comparing the image generation quality of two methods: Token-Shuffle and high-compression VQGAN.  Token-Shuffle uses a compression ratio of 8x (achieved by using a shuffle window size of 2), while the high-compression VQGAN uses a compression ratio of 16x. The evaluation metrics are text alignment, visual flaws, and visual appearance.  The results show how each method performs in terms of image quality and fidelity to the text prompt.
> <details>
> <summary>read the caption</summary>
> Figure 18: Human evaluation of Token-Shuffle (compress ratio 8√ó8\times8 √ó with Token-Shuffle window size of 2) and high compress VQGAN (compress ratio 16√ó16\times16 √ó).
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S4.T2.2.2">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1"># Params</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.1">Single Obj.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.6.1">Two Obj.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.7.1">Counting</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.8.1">Colors</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.9.1">Position</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.10.1">Color Attri.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Overall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.1.1.m1.1.1.cmml">‚Üë</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.m1.1.1">‚Üë</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.m1.1d">‚Üë</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.2.3.1">LDM¬†<cite class="ltx_cite ltx_citemacro_cite">Rombach et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib43" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.2">Diff.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.3">1.4B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.4">0.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.5">0.29</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.6">0.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.7">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.8">0.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.9">0.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.3.10">0.37</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.4">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.4.1">SDv1.5¬†<cite class="ltx_cite ltx_citemacro_cite">Rombach et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib43" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.3">0.9B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.4">0.97</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.5">0.38</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.6">0.35</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.7">0.76</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.8">0.04</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.9">0.06</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4.10">0.43</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.5">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.5.1">PixArt-alpha¬†<cite class="ltx_cite ltx_citemacro_cite">Chen et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib4" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.3">0.6B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.4">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.5">0.50</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.6">0.44</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.7">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.8">0.08</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.9">0.07</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.5.10">0.48</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.6">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.6.1">SDv2.1¬†<cite class="ltx_cite ltx_citemacro_cite">Rombach et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib43" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.3">0.9B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.4">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.5">0.51</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.6">0.44</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.7">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.8">0.07</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.9">0.17</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.6.10">0.50</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.7">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.7.1">DALL-E 2¬†<cite class="ltx_cite ltx_citemacro_cite">Ramesh et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib41" title="">2022</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.3">6.5B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.4">0.94</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.5">0.66</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.6">0.49</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.7">0.77</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.8">0.10</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.9">0.19</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.7.10">0.52</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.8">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.8.1">SDXL¬†<cite class="ltx_cite ltx_citemacro_cite">Podell et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib37" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.3">2.6B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.4">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.5">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.6">0.39</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.7">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.8">0.15</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.9">0.23</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.8.10">0.55</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.9">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.9.1">SD3¬†<cite class="ltx_cite ltx_citemacro_cite">Esser et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib9" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.2">Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.3">2B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.4">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.5">0.74</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.6">0.63</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.7">0.67</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.8">0.34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.9">0.36</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.9.10">0.62</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.2.2.10.1">Show-o¬†<cite class="ltx_cite ltx_citemacro_cite">Xie et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib61" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.2">AR.+Diff.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.3">1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.4">0.95</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.5">0.52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.6">0.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.7">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.8">0.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.9">0.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.10.10">0.53</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.11">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.11.1">SEED-X¬†<cite class="ltx_cite ltx_citemacro_cite">Ge et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib11" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.2">AR.+Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.3">17B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.4">0.97</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.5">0.58</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.6">0.26</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.7">0.80</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.8">0.19</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.9">0.14</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.11.10">0.49</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.12">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.12.1">Transfusion¬†<cite class="ltx_cite ltx_citemacro_cite">Zhou et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib66" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.2">AR.+Diff.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.3">7.3B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.9">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.12.10">0.63</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.13">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.13.1">LlamaGen¬†<cite class="ltx_cite ltx_citemacro_cite">Sun et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib49" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.2">AR.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.3">0.8B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.4">0.71</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.5">0.34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.6">0.21</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.7">0.58</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.8">0.07</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.9">0.04</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.13.10">0.32</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.14">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.14.1">Chameleon¬†<cite class="ltx_cite ltx_citemacro_cite">Team (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib52" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.2">AR.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.3">7B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.9">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.14.10">0.39</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.15">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.15.1">EMU3¬†<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib58" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.2">AR.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.3">8B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.9">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.15.10">0.66</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.16">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.16.1">EMU3-DPO¬†<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib58" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.2">AR.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.3">8B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.5">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.9">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.16.10">0.64</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.17">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.17.1">Emu3-Gen¬†<cite class="ltx_cite ltx_citemacro_cite">Wang et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib58" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.2">AR.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.3">8B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.4">0.98</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.5">0.71</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.6">0.34</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.7">0.81</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.8">0.17</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.9">0.21</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.17.10">0.54</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.18">
<td class="ltx_td ltx_align_left" id="S4.T2.2.2.18.1">Janus¬†<cite class="ltx_cite ltx_citemacro_cite">Wu et¬†al. (<a class="ltx_ref" href="https://arxiv.org/html/2504.17789v1#bib.bib59" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.2">AR.</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.3">1.3B</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.4">0.97</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.5">0.68</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.6">0.30</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.7">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.8">0.46</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.9">0.42</td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.18.10">0.61</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.2.2.2.1">Token-Shuffle<math alttext="{\dagger}" class="ltx_Math" display="inline" id="S4.T2.2.2.2.1.m1.1"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo id="S4.T2.2.2.2.1.m1.1.1" mathcolor="#0000FF" xref="S4.T2.2.2.2.1.m1.1.1.cmml">‚Ä†</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">‚Ä†</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.1.m1.1d">‚Ä†</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.2">AR.</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.3">2.7B</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.4">0.96</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.5">0.81</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.6">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.7">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.8">0.40</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.9">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.2.2.2.10">0.62</td>
</tr>
</table>{{< /table-caption >}}
> üîº This table presents a quantitative evaluation of the Token-Shuffle model on the GenEval benchmark, a dataset designed to assess various aspects of image generation quality.  It compares the performance of Token-Shuffle against several other state-of-the-art models, including both autoregressive (AR) and diffusion-based methods. The metrics used in the evaluation cover a range of image generation capabilities, such as the model's ability to generate images that accurately reflect the given text prompts (text alignment), the absence of visual artifacts or inconsistencies (visual flaws), and the overall aesthetic quality of the generated images (visual appearance). Notably, the table highlights that even though the focus of the Token-Shuffle model is high-resolution image generation, it also demonstrates promising performance across all evaluation metrics when compared to existing models.
> <details>
> <summary>read the caption</summary>
> Table 2: Evaluation on the GenEval benchmark. Similar to ours results, EMU3 and EMU3-DPO also consider prompt rewriting, and results of EMU3-Gen are from Janus¬†Wu et¬†al. (2024). These results indicate our Token-Shuffle can also present promising generation quality besides high-resolution.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S7.T3.2.2">
<tr class="ltx_tr" id="S7.T3.2.2.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S7.T3.2.2.3.1">Ratio</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.2.2.3.2">Tokens</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.2.2.3.3">Codebook</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.2.2.3.4">PSNR</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.2.2.3.5">SSIM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T3.2.2.3.6">CLIP</td>
</tr>
<tr class="ltx_tr" id="S7.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S7.T3.1.1.1.1">Low (<math alttext="8\times" class="ltx_math_unparsed" display="inline" id="S7.T3.1.1.1.1.m1.1"><semantics id="S7.T3.1.1.1.1.m1.1a"><mrow id="S7.T3.1.1.1.1.m1.1b"><mn id="S7.T3.1.1.1.1.m1.1.1">8</mn><mo id="S7.T3.1.1.1.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S7.T3.1.1.1.1.m1.1c">8\times</annotation><annotation encoding="application/x-llamapun" id="S7.T3.1.1.1.1.m1.1d">8 √ó</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.2">4096</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.3">8192</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.4">27.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.5">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T3.1.1.1.6">0.98</td>
</tr>
<tr class="ltx_tr" id="S7.T3.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S7.T3.2.2.2.1">High (<math alttext="16\times" class="ltx_math_unparsed" display="inline" id="S7.T3.2.2.2.1.m1.1"><semantics id="S7.T3.2.2.2.1.m1.1a"><mrow id="S7.T3.2.2.2.1.m1.1b"><mn id="S7.T3.2.2.2.1.m1.1.1">16</mn><mo id="S7.T3.2.2.2.1.m1.1.2" lspace="0.222em">√ó</mo></mrow><annotation encoding="application/x-tex" id="S7.T3.2.2.2.1.m1.1c">16\times</annotation><annotation encoding="application/x-llamapun" id="S7.T3.2.2.2.1.m1.1d">16 √ó</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T3.2.2.2.2">1024</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T3.2.2.2.3">16384</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T3.2.2.2.4">22.89</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T3.2.2.2.5">0.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T3.2.2.2.6">0.96</td>
</tr>
</table>{{< /table-caption >}}
> üîº This table presents a comparison of the performance of two VQGAN (Vector Quantized Generative Adversarial Network) models with different compression ratios when reconstructing images from the MSCOCO-val dataset.  The metrics used to evaluate performance are PSNR (Peak Signal-to-Noise Ratio), SSIM (Structural Similarity Index), and CLIP (Contrastive Language-Image Pre-training) score. A lower compression ratio implies that more information is preserved, leading to potentially better reconstruction quality. The table provides insights into the trade-off between compression level and reconstruction accuracy, which is crucial for efficient image generation tasks.
> <details>
> <summary>read the caption</summary>
> Table 3: Reconstruction results of VQGAN models with different compress ratios. The results are achieved on MSCOCO-val set with a resolution of 512.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S7.T4.6.1">
<tr class="ltx_tr" id="S7.T4.6.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S7.T4.6.1.1.1" rowspan="3"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S7.T4.6.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.1.2.1">"Basic" prompts</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S7.T4.6.1.1.3"><span class="ltx_text" id="S7.T4.6.1.1.3.1" style="color:#FFFFFF;">.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S7.T4.6.1.1.4"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.1.4.1">"Hard" prompts</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.6.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.2.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.1.1">Attribute</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.2.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.2.1">Scene</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S7.T4.6.1.2.3"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.3.1">Relation</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.2.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.4.1">Overall</span></td>
<td class="ltx_td" id="S7.T4.6.1.2.5"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.2.6" rowspan="2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.6.1">Count</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.2.7" rowspan="2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.7.1">Differ</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.2.8" rowspan="2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.8.1">Compare</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2" id="S7.T4.6.1.2.9"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.9.1">Logical</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.2.10" rowspan="2"><span class="ltx_text ltx_font_bold" id="S7.T4.6.1.2.10.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S7.T4.6.1.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.3.1">Spatial</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.3.2">Action</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.3.3">Part</td>
<td class="ltx_td" id="S7.T4.6.1.3.4"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.3.5">Negate</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.3.6">Universal</td>
</tr>
<tr class="ltx_tr" id="S7.T4.6.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T4.6.1.4.1">D16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.2">0.80</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.3">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.4">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.5">0.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.6">0.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.7">0.80</td>
<td class="ltx_td ltx_border_t" id="S7.T4.6.1.4.8"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.9">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.10">0.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.11">0.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.12">0.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.13">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S7.T4.6.1.4.14">0.71</td>
</tr>
<tr class="ltx_tr" id="S7.T4.6.1.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S7.T4.6.1.5.1">D8+TS</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.2">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.3">0.85</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.4">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.5">0.82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.6">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.7">0.82</td>
<td class="ltx_td ltx_border_bb" id="S7.T4.6.1.5.8"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.9">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.10">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.11">0.77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.12">0.66</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.13">0.74</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S7.T4.6.1.5.14">0.72</td>
</tr>
</table>{{< /table-caption >}}
> üîº Table 4 presents a quantitative comparison of image generation quality using different approaches on the GenAI-Bench benchmark.  It contrasts two methods: 1) Directly using a high-compression VQGAN (Vector Quantized Generative Adversarial Network) with a downsampling ratio of 16x (D16), and 2) Using a low-compression VQGAN with a downsampling ratio of 8x in conjunction with the Token-Shuffle technique (D8+TS). The Token-Shuffle method is a novel approach introduced in the paper to improve efficiency and resolution in autoregressive image generation. The table provides VQAScore (Visual Question Answering Score) which evaluates the alignment between generated images and their text prompts.  The results show a comparison of scores across different aspects of image quality such as attributes, scenes, and overall quality for both 'basic' and 'hard' prompts.
> <details>
> <summary>read the caption</summary>
> Table 4: VQAScore evaluation of image generation on GenAI-Bench. 'D16' indicates directly using a high-compress VQGAN with a down-sampling ratio of 16√ó16\times16 √ó. 'D8+TS' indicates using a low-compress VQGAN with a down-sampling ratio of 8√ó8\times8 √ó and Token-Shuffle window size of 2.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.17789/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.17789/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}