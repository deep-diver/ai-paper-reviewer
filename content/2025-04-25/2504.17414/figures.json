[{"figure_path": "https://arxiv.org/html/2504.17414/x1.png", "caption": "Figure 1: Try-on videos generated by 3DV-TON. Our method can handle various types of clothing and body poses, while accurately restoring clothing details and maintaining consistent texture motion.", "description": "This figure showcases several examples of video try-on results generated using the 3DV-TON method.  The results demonstrate the model's ability to convincingly replace the clothing worn by individuals in the input videos with various target garments.  Notice the diversity of clothing types (e.g., dresses, shirts, skirts) and body poses across the examples.  Importantly, 3DV-TON successfully maintains realistic clothing details and consistent texture motion, even in complex scenarios with significant pose changes and dynamic movement, overcoming limitations of prior methods.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.17414/extracted/6386064/fig/3d.png", "caption": "Figure 2: Textured 3D guidance. We construct the textured 3D guidance based on image try-on results, then animate the mesh after pasting the texture, providing a consistent texture motion reference on the appearance level.", "description": "Figure 2 shows the creation of textured 3D guidance for the video try-on process.  First, image try-on is performed to generate an initial image with the target clothing on the person.  This image is then used to reconstruct a 3D mesh of the person wearing the clothes. The clothing texture from the try-on image is applied to this 3D mesh. Finally, the mesh is animated to match the movements of the person in the video, creating a dynamic 3D representation of the clothing that can be used to guide the video generation process and ensure consistent texture motion throughout the video.", "section": "3.1. Animatable Textured 3D Guidance"}, {"figure_path": "https://arxiv.org/html/2504.17414/x2.png", "caption": "Figure 3: The overview of 3DV-TON. Given a video, we first use our 3D guidance pipeline to select a frame I\ud835\udc3cIitalic_I adaptively, then reconstruct a textured 3D guidance and animate it align with the original video, i.e. V\ud835\udc49Vitalic_V. We employ a guidance feature extractor for the clothing image C\ud835\udc36Citalic_C and the try-on images Ctsubscript\ud835\udc36\ud835\udc61C_{t}italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, and perform feature fusion using the self-attentions in the denoising UNet.", "description": "Figure 3 provides a detailed overview of the 3DV-TON framework.  The process begins with an input video. A 3D guidance pipeline selects a keyframe (I) from the video. This keyframe undergoes image try-on processing to generate a textured 3D model which is then animated to match the video's motion (V).  A feature extractor processes the original clothing image (C) and the generated try-on images from each frame (Ct),  fusing these features within the denoising UNet via self-attention mechanisms to generate the final video try-on result. The figure visually represents each stage of this pipeline, highlighting the flow of data and the key components involved.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.17414/x3.png", "caption": "Figure 4: Qualitative comparison for dress try-on on the ViViD dataset.", "description": "This figure displays a qualitative comparison of dress try-on results on the ViViD dataset.  It showcases the input image, the target clothing item, and the try-on results generated by three different methods: ViViD, CatV2TON, and 3DV-TON (the authors' method). This allows for a visual assessment of each method's ability to accurately and realistically place the clothing onto the person in the image, considering factors like texture, shape, and overall consistency.", "section": "4. Experiments"}]