{"importance": "This paper is important because it presents QuEST, a novel method for training large language models (LLMs) with significantly reduced computational costs.  **QuEST achieves this by employing 1-bit weights and activations during training, a significant advancement over existing methods that often plateau at 8-bits.** The research opens new avenues for efficient LLM training and deployment, addressing a critical challenge in the field and influencing future research directions.", "summary": "QuEST enables stable, accurate LLM training using only 1-bit weights and activations, achieving Pareto-optimal performance compared to higher-precision models.", "takeaways": ["QuEST achieves stable and accurate training of LLMs with 1-bit weights and activations.", "QuEST outperforms existing QAT methods, pushing the Pareto-optimal frontier to 4-bits or less.", "QuEST demonstrates new scaling laws for LLMs across a wide range of precisions."], "tldr": "Large language models (LLMs) are computationally expensive.  One approach to reduce costs is using quantized representations, but existing Quantization-Aware Training (QAT) methods struggle to achieve high accuracy at low bit-widths, often plateauing around 8-bits.  The open problem is achieving accuracy comparable to higher-precision models while significantly reducing computational resources. \n\nThe paper introduces QuEST, a novel QAT method that solves this problem.  **QuEST uses Hadamard normalization and a new trust gradient estimator for accurate and fast quantization**.  It demonstrates that LLMs can be trained successfully with only 1-bit weights and activations, surpassing existing accuracy at much lower model sizes and significantly reducing inference cost. **Experiments show QuEST induces stable scaling laws, making it highly efficient across different hardware precisions.**", "affiliation": "ISTA", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2502.05003/podcast.wav"}