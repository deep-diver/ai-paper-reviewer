{"references": [{"fullname_first_author": "Hoffmann, J.", "paper_title": "Training compute-optimal large language models", "publication_date": "2022-MM-DD", "reason": "This paper introduces scaling laws for LLMs, providing a theoretical foundation for understanding the relationship between model size, training data, and compute, which is crucial for designing efficient models and interpreting the results of the current paper."}, {"fullname_first_author": "Frantar, E.", "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers", "publication_date": "2022-MM-DD", "reason": "This paper presents a post-training quantization method that improves the accuracy of quantized LLMs, serving as a key baseline and comparison point for the QuEST method, which focuses on quantization-aware training."}, {"fullname_first_author": "Ashkboos, S.", "paper_title": "Towards end-to-end 4-bit inference on generative large language models", "publication_date": "2023-MM-DD", "reason": "This paper explores the state-of-the-art in low-bit inference for LLMs, providing context for the challenges of working with extremely low-precision representations and motivating the need for improved training methods like QuEST."}, {"fullname_first_author": "Kumar, T.", "paper_title": "Scaling laws for precision", "publication_date": "2024-MM-DD", "reason": "This paper investigates the optimal bit-width for quantization-aware training of LLMs, providing a direct comparison to the QuEST method's results and highlighting the advancements in precision."}, {"fullname_first_author": "Dubey, A.", "paper_title": "The llama 3 herd of models", "publication_date": "2024-MM-DD", "reason": "This paper provides the dataset used for training the LLMs in the study and is critical for reproducing the results and understanding the experimental setup of the current paper."}]}