[{"figure_path": "https://arxiv.org/html/2502.05163/x1.png", "caption": "Figure 1: Illustration of the use-case of a guardrail model for LLMs, which functions as moderation between the user-LLM conversation.", "description": "The figure illustrates how a guardrail model acts as an intermediary between a user and a large language model (LLM).  The user provides input (which can be safe or unsafe), and the LLM generates an output. The guardrail model then assesses both the user's input and the LLM's output to determine if they are safe or unsafe.  If either is deemed unsafe, the guardrail model prevents it from being communicated, acting as a safety filter to maintain responsible and harmless interaction.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05163/x2.png", "caption": "Figure 2: Overview of our main results. In the left figure, we demonstrate a consistently superior performance of average f1 score across 6 benchmarks in the four languages. In the right figure, we show that our model maintains the lowest inference cost while achieving superior average performance across languages. We note that, although we focus on the four languages to demonstrate the two-player data synthesis framework, DuoGuard retains its base model Qwen-2.5\u2019s capacity to support all 29 languages.", "description": "DuoGuard, a novel multilingual guardrail model, outperforms existing models across six benchmarks and four languages (English, French, German, and Spanish).  The left panel shows DuoGuard's consistently higher average F1 score compared to baselines.  The right panel demonstrates DuoGuard's superior performance while maintaining the lowest inference cost.  While the figure focuses on four languages for clarity, DuoGuard can support all 29 languages included in its base model, Qwen-2.5.", "section": "5.1 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.05163/x3.png", "caption": "Figure 3: Overview of the two-player training pipeline. The generator produces synthetic data from seed data. The classifier makes predictions and we measure these examples as being predicted correctly or incorrectly based on their seed data label. We train the generator with DPO to create increasingly challenging examples, which in turn improve the classifier through iterative training.", "description": "This figure illustrates the iterative two-player reinforcement learning process used to train a multilingual LLM guardrail model.  It begins with a seed dataset containing examples of safe and unsafe content. The generator, guided by Direct Preference Optimization (DPO), creates new synthetic data points designed to challenge the classifier.  The classifier then makes predictions on the synthetic data, identifying correctly and incorrectly classified examples. This feedback loop is crucial;  misclassified data points are used to further train the generator, making it more adept at generating increasingly challenging examples. The correctly classified examples contribute to the classifier's training, thus improving its accuracy. This iterative process continues, leading to a continuous improvement in the quality of synthetic data and the performance of the classifier.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2502.05163/x4.png", "caption": "Figure 4: Relative performance decline (average F1 across six benchmarks and three languages) of various models compared to the English performance of DuoGuard.", "description": "This figure compares the average F1 scores of different multilingual guardrail models across six benchmarks, in three languages (French, Spanish, and German). The performance of each model in each of these three languages is shown relative to the model's performance in English. Lower values indicate better performance (less performance decline when moving from English to other languages).", "section": "5.1 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.05163/x5.png", "caption": "Figure 5: The F1 score on OpenAI benchmark of models trained with data containing different languages in our seed data. The inclusion of French in addition to English improves model performance on Spanish (36.9% to 62.8%) and German (31.9 to 59.6).", "description": "This figure displays the F1 scores achieved by models trained on seed datasets that include varying combinations of languages.  The baseline uses only English data.  Adding French data to the English seed data significantly boosts the performance of models evaluated on Spanish and German datasets. This demonstrates the effectiveness of incorporating multiple languages into the seed data for improving multilingual model generalization performance.", "section": "6.1 Seed Data"}, {"figure_path": "https://arxiv.org/html/2502.05163/x6.png", "caption": "Figure 6: Performance by languages of the model trained on seed data. With larger data proportion in seed data, the model\u2019s average performance on English is markedly higher than on other languages.", "description": "This figure displays the average performance of a multilingual model trained exclusively on seed data, without any synthetic data augmentation. The x-axis represents the four languages (English, French, Spanish, and German), while the y-axis shows the average F1 score across six benchmark datasets. The bars show that the model's performance on English is substantially higher than its performance on the other three languages, which all have similar performance levels. This disparity in performance highlights the impact of data imbalance in the seed dataset.  The relatively low performance on the non-English languages indicates the need for more balanced multilingual training data, especially for lower-resource languages.", "section": "6.1 Seed Data"}, {"figure_path": "https://arxiv.org/html/2502.05163/x7.png", "caption": "Figure 7: (a) Iterative performance improvements of DuoGuard. (b) Shift in data distribution across languages over iterations.", "description": "This figure shows the iterative performance improvements of DuoGuard and the changes in data distribution across languages.  Figure 7(a) demonstrates that the model's average F1-score across various languages consistently increases across iterations of training. Initially, there is a performance gap between English and other languages; however, after multiple iterations, these differences decrease. Figure 7(b) illustrates the change in data proportion of each language during training. Initially, English dominates the dataset.  However, as the two-player RL model iterates, the proportion of non-English data increases, leading to a more balanced dataset.", "section": "6.2 Synthetic Data"}, {"figure_path": "https://arxiv.org/html/2502.05163/x8.png", "caption": "Figure 8: Data proportion by language in our collected seed data from open sources.", "description": "This figure shows the distribution of the seed data across four languages: English, French, Spanish, and German.  English makes up the vast majority of the data (81.4%), significantly outweighing the other languages. French constitutes 8.9% of the data, while Spanish and German each account for around 5% and 4.5%, respectively.  This imbalance highlights the challenge of multilingual model training due to the scarcity of data in non-English languages and motivates the need for synthetic data generation to address this issue.", "section": "6.1 Seed Data"}, {"figure_path": "https://arxiv.org/html/2502.05163/x9.png", "caption": "Figure 9: Output Probability Distribution of False Positives and False Negatives in the Classifier Trained on Seed Data. A skewed distribution toward 0 for false negatives and toward 1 for false positives indicates higher classifier confidence in its incorrect predictions. Analysis across the four French datasets reveals that the classifier exhibits significant confidence in its false predictions.", "description": "This figure presents the probability distributions of the classifier's output for both false positives and false negatives when trained solely on the seed dataset.  The distributions are notably skewed: false negatives tend to cluster around a probability of 0, while false positives are concentrated near 1. This indicates that when making incorrect predictions, the model tends to have high confidence in those incorrect predictions. A further analysis of the four French datasets shows that this overconfidence in wrong predictions is particularly pronounced.", "section": "Additional Results"}]