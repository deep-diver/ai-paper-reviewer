[{"figure_path": "https://arxiv.org/html/2502.04376/extracted/6181287/figure/shadow_clone_architecture-v4.png", "caption": "Figure 1: Architecture of the meeting delegate system.", "description": "The figure illustrates the architecture of a prototype LLM-based meeting delegate system.  The system comprises three main components: 1) Information Gathering, which collects meeting-related information (manually provided topics of interest, background knowledge, or shareable materials) to assist LLMs in participating in meetings; 2) Meeting Engagement, which actively monitors the meeting status and determines appropriate timing and content for LLM engagement; and 3) Voice Generation, which produces a voice response mimicking the user's voice using text-to-speech technology. The figure shows the data flow and interactions between these components and the meeting environment, illustrating how the system gathers information, engages in the conversation, and generates responses to participate in a meeting on behalf of a user.", "section": "3 LLM-based Meeting Delegate System"}, {"figure_path": "https://arxiv.org/html/2502.04376/extracted/6181287/figure/demo_v4.png", "caption": "Figure 2: Workflow of an LLM-powered meeting delegate system. The process involves user input of meeting intent and shareable information prior to the meeting, real-time participation based on meeting transcripts, and response generation aligned with prompted instructions and meeting objectives.", "description": "This figure illustrates the architecture and workflow of an LLM-powered meeting delegate system.  Before a meeting, the user provides the system with their meeting objectives (intent) and any relevant documents or information.  During the meeting, the system uses a real-time transcript to monitor the conversation.  Based on the context of the conversation and the user's predefined intent, the LLM decides when and how to contribute to the meeting by generating responses. These responses are then delivered via a text-to-speech system, allowing the LLM to participate in the meeting as a virtual delegate for the user.  The entire process is designed to align with the user's instructions and stated meeting goals.", "section": "3 LLM-based Meeting Delegate System"}, {"figure_path": "https://arxiv.org/html/2502.04376/x1.png", "caption": "Figure 3: Data statistics of the Matched Dataset.", "description": "This figure presents a statistical overview of the Matched Dataset used in the paper.  It displays the distribution of several key characteristics within the dataset, including the number of involved persons in each meeting, the number of utterances, the number of points in the ground truth response, and the number of points in the input intents and shareable contextual information.  These statistics provide insights into the complexity and characteristics of the real-world meeting data used to benchmark the LLM-powered meeting delegate system.", "section": "4 Benchmark Dataset"}, {"figure_path": "https://arxiv.org/html/2502.04376/x2.png", "caption": "Figure 4: Response Rate on Matched Dataset vs. Silence Rate on Mismatched Dataset.", "description": "This figure shows the performance comparison of various LLMs in two scenarios. The Response Rate represents the percentage of times the LLMs generated responses in the Matched Dataset, where the context is aligned with the LLMs' intended role as a meeting delegate.  The Silence Rate represents the percentage of times the LLMs appropriately refrained from responding in the Mismatched Dataset, a scenario designed to test the LLMs' ability to avoid unnecessary interruptions.  It illustrates the tradeoff between actively participating and maintaining appropriate silence in meeting contexts for different LLMs.", "section": "Experiment"}, {"figure_path": "https://arxiv.org/html/2502.04376/extracted/6181287/figure/error_solution.png", "caption": "Figure 5: Solution directions from error analysis of bad cases in Response (Silence) Rate for Matched and Mismatched Datasets.", "description": "This figure presents an analysis of errors in the Response (Silence) Rate for the Matched and Mismatched datasets. The analysis focuses on identifying the underlying causes of failures and proposes solutions to improve the system's performance. It categorizes the errors observed in various models and suggests improvements such as enhancing reasoning capabilities in meeting contexts, improving instruction following, and improving general reasoning abilities.", "section": "5 Experiment"}, {"figure_path": "https://arxiv.org/html/2502.04376/x3.png", "caption": "Figure 6: Loose recall rate on Matched Dataset.", "description": "This figure presents the \"loose recall rate\" achieved by several Large Language Models (LLMs) on a matched dataset.  The loose recall rate is a metric that measures whether a model's generated response includes at least one main point from the ground truth response.  The models tested include various versions of GPT, Gemini, and Llama LLMs. The graph visually compares their performance, showing the percentage of responses that successfully addressed at least one key aspect of the ground truth.", "section": "4.2 Evaluation Metric"}, {"figure_path": "https://arxiv.org/html/2502.04376/x4.png", "caption": "Figure 7: The attribution rate on matched dataset.", "description": "This figure presents the attribution rates for different LLMs across various meeting scenarios within a matched dataset.  It displays the proportion of main points in the generated responses that are attributed to different sources: the expected ground-truth response, contextual information (not in the ground truth), prior transcript content, and hallucinated text. This analysis helps determine the extent to which LLMs rely on actual meeting data versus other sources in generating responses.", "section": "4.2 Evaluation Metric"}, {"figure_path": "https://arxiv.org/html/2502.04376/extracted/6181287/figure/dataset_construction_v2.png", "caption": "Figure 8: Example of evaluation dataset construction. Participants are represented by different ID numbers and icons. Colored boxes indicate utterances from different participants. The process includes extracting Input Context Information, creating a Transcript Snapshot, and generating a response with the LLM-powered meeting delegate. The Generated Response is evaluated by comparison with the Ground-Truth Response.", "description": "This figure illustrates the process of constructing a test case for evaluating a large language model (LLM)-powered meeting delegate system.  It starts with an original meeting transcript, visually represented with different participant IDs and colored boxes to distinguish utterances.  A \"snapshot\" of this transcript is taken, focusing on a specific participant's turn and the preceding conversation.  This snapshot, along with relevant background information (Input Context Information), serves as input to the LLM to generate a response.  The generated response is then compared against the actual response in the original transcript (Ground-Truth Response) to assess the system's performance.", "section": "4 Benchmark Dataset"}, {"figure_path": "https://arxiv.org/html/2502.04376/extracted/6181287/figure/error_chimein.png", "caption": "(a) Chine In (Matched Dataset)", "description": "The bar chart visualizes the distribution of error types in the \"Chime In\" scenarios within the matched dataset.  It shows the frequency of different reasons why the Large Language Model (LLM) failed to generate an appropriate response in situations where it should have chimed into the conversation. Each bar represents a specific error category, allowing for an analysis of common issues and potential areas for improvement in the model's ability to identify and participate in relevant conversation turns.", "section": "4.3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2502.04376/extracted/6181287/figure/error_explicit.png", "caption": "(b) Explicit Cue (Matched Dataset)", "description": "This figure shows the distribution of error types for response rate failure cases in the Explicit Cue scenarios within the Matched Dataset.  It presents a bar chart visualizing the frequency of different error types (e.g., incorrectly identifying a cue, failing to respond when appropriate, misinterpreting context) for each of several LLMs evaluated in the study (GPT-40, Gemini 1.5 Pro, Gemini 1.5 Flash, Llama3-8B). The relative heights of the bars indicate the frequency of each error type for a particular LLM.", "section": "4 Benchmark Dataset"}, {"figure_path": "https://arxiv.org/html/2502.04376/extracted/6181287/figure/error_mismatch.png", "caption": "(c) Mismatched Dataset", "description": "This figure shows the distribution of error types for response rate failure cases in the Mismatched Dataset.  The Mismatched Dataset is a subset of the data created for evaluating the performance of the LLM-based meeting delegate system. Specifically, it involves test cases where the principal participant (the one expected to respond) is replaced by someone not involved in the conversation. This is designed to evaluate the system's ability to remain silent when it is not appropriate to respond.", "section": "4 Benchmark Dataset"}, {"figure_path": "https://arxiv.org/html/2502.04376/x5.png", "caption": "Figure 9: \n(a) Error Types Distribution for Response Rate Failure Cases Study in Chine In Matched Dataset.\n(b) Error Types Distribution for Response Rate Failure Cases Study in Explicit Cue Matched Dataset.\n(c) Error Types Distribution for Response Rate Failure Cases Study in Mismatched Dataset.", "description": "This figure presents a breakdown of error types contributing to response rate failures in three scenarios: \"Chime In\" (a), \"Explicit Cue\" (b), and \"Mismatched\" (c) from a meeting delegate system evaluation. Each subfigure shows the frequency of different error categories, such as incorrect decision-making based on the conversation's latest utterance or failure to recognize cues for participation.  The goal is to illustrate the types of errors that commonly affect the LLM-based meeting delegate's performance in different contexts.", "section": "4 Benchmark Dataset"}]