[{"figure_path": "https://arxiv.org/html/2502.04363/x3.png", "caption": "Figure 1. Open-Sora\u00a0(Zheng et\u00a0al., 2024) generates realistic videos from the user prompt (text) through three stages: 1) prompt embedding, 2) latent video generation, and 3) video decoding.", "description": "Open-Sora is a text-to-video generation model.  The figure illustrates the three main stages of its video generation process. First, text input (a prompt describing the desired video) is converted into a numerical representation (embedding).  Second, this embedding is used as input for a latent video generation step which produces a compressed, encoded representation of the video. Finally, this latent video representation is decoded and upscaled to create a full-resolution, viewable video output.", "section": "2.1 Background: Open-Sora"}, {"figure_path": "https://arxiv.org/html/2502.04363/x4.png", "caption": "Figure 2. On-device Sora enables efficient text-to-video generation directly on the device by employing three key approaches: 1) Linear Proportional Leap, 2) Temporal Dimension Token Merging, and 3) Concurrent Inference with Dynamic Loading.", "description": "On-device Sora is a system designed for efficient text-to-video generation on mobile devices.  This figure illustrates the three main techniques used to achieve this efficiency: 1) Linear Proportional Leap reduces the number of steps in the video diffusion process. 2) Temporal Dimension Token Merging decreases the computational cost of processing tokens within the model's attention layers. 3) Concurrent Inference with Dynamic Loading handles the large model size by partitioning it into smaller blocks and loading those blocks into memory concurrently for processing, addressing memory limitations of mobile devices.", "section": "3 Overview: On-device Sora"}, {"figure_path": "https://arxiv.org/html/2502.04363/x6.png", "caption": "Figure 3. The size of Open-Sora models: T5\u00a0(Raffel et\u00a0al., 2020) (18.00 GB), STDiT\u00a0(Zheng et\u00a0al., 2024) (4.50 GB), and VAE\u00a0(Doersch, 2016) (0.82 GB), which exceeds the available memory capacity of iPhone 15 Pro\u00a0(Apple, 2023) (3.3 GB).", "description": "This figure illustrates the substantial memory requirements of the Open-Sora model components (T5, STDiT, and VAE).  The T5 model alone is 18GB, STDiT is 4.5GB, and VAE is 0.82GB, resulting in a cumulative memory demand of 23.32GB. This significantly exceeds the 3.3GB memory limit of an iPhone 15 Pro, highlighting a key challenge in deploying Open-Sora on mobile devices.", "section": "2.2 Challenges in On-device Video Generation"}, {"figure_path": "https://arxiv.org/html/2502.04363/x7.png", "caption": "Figure 4. An abstracted illustration of trajectories and latent visualizations for K=30\ud835\udc3e30K=30italic_K = 30 and n=15\ud835\udc5b15n=15italic_n = 15: (a) Rectified Flow\u00a0(Liu et\u00a0al., 2022) with full k=30\ud835\udc5830k=30italic_k = 30 denoising steps, generating intact and complete data, (b) Rectified Flow\u00a0(Liu et\u00a0al., 2022) with n+1=16\ud835\udc5b116n+1=16italic_n + 1 = 16 denoising steps without applying Linear Proportional Leap, resulting in low-quality data generation from variance with high step sizes (d\u2062tk\ud835\udc51subscript\ud835\udc61\ud835\udc58dt_{k}italic_d italic_t start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT), and (c) Linear Proportional Leap with n+1=15+1\ud835\udc5b1151n+1=15+1italic_n + 1 = 15 + 1 denoising steps, producing data nearly equivalent to (a).", "description": "Figure 4 illustrates the concept of Linear Proportional Leap (LPL) by comparing three scenarios of the denoising process in diffusion models using Rectified Flow.  Panel (a) shows the standard Rectified Flow process with 30 denoising steps, resulting in high-quality data. Panel (b) demonstrates what happens with only 16 steps using Rectified Flow without LPL; the resulting data is of low quality due to the large steps taken. Finally, panel (c) shows how LPL achieves similar quality to panel (a) using only 16 steps by strategically leaping towards the target distribution in the later stages of the process, instead of taking numerous small steps.", "section": "4 Linear Proportional Leap"}, {"figure_path": "https://arxiv.org/html/2502.04363/x8.png", "caption": "Figure 5. An example of cosine similarities between two consecutive drifts estimated from STDiT\u00a0(Zheng et\u00a0al., 2024), i.e., \ud835\udc97\u2062(Pn,tn)\ud835\udc97subscript\ud835\udc43\ud835\udc5bsubscript\ud835\udc61\ud835\udc5b\\boldsymbol{v}(P_{n},t_{n})bold_italic_v ( italic_P start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) and \ud835\udc97\u2062(Pn\u22121,tn\u22121)\ud835\udc97subscript\ud835\udc43\ud835\udc5b1subscript\ud835\udc61\ud835\udc5b1\\boldsymbol{v}(P_{n-1},t_{n-1})bold_italic_v ( italic_P start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT ) for 30 (red) and 50 steps (blue).", "description": "This figure shows a graph plotting cosine similarity values against the number of denoising steps in a video generation model.  Cosine similarity measures the similarity between consecutive drift vectors, which represent the change in the model's latent representation during the denoising process.  The graph shows two lines representing results for 30 and 50 denoising steps, respectively.  The higher cosine similarity values indicate that the trajectory of changes in the model's latent representation becomes more linear as the denoising process progresses. This supports the Linear Proportional Leap (LPL) method, as a more linear trajectory allows for a shorter denoising process.", "section": "4 Linear Proportional Leap"}, {"figure_path": "https://arxiv.org/html/2502.04363/x9.png", "caption": "Figure 6. In attention layers of STDiT\u00a0(Zheng et\u00a0al., 2024), two consecutive tokens are merged along the temporal dimension and subsequently unmerged after processing, reducing the token size by half and the computational complexity up to a quarter.", "description": "This figure illustrates the Temporal Dimension Token Merging (TDTM) technique used in the STDiT model.  Consecutive tokens representing adjacent frames in a video sequence are merged along the temporal dimension.  This merging operation reduces the number of tokens processed in the attention layers by half. Consequently, because the computational complexity of the attention mechanism is quadratic with respect to the number of tokens, TDTM reduces the overall computational complexity of those layers by up to a quarter. After passing through the attention mechanism, the merged tokens are unmerged to restore the original size and structure.", "section": "5 Temporal Dimension Token Merging"}, {"figure_path": "https://arxiv.org/html/2502.04363/x10.png", "caption": "Figure 7. An illustration of the token merging and unmerging process over the temporal dimension.", "description": "This figure illustrates the process of Temporal Dimension Token Merging (TDTM) in the paper.  It shows how two consecutive tokens representing video frames are merged along the temporal dimension, effectively reducing computational complexity.  The merging is done by averaging the features of the two frames, creating a single token. After processing, the single token is then unmerged back into two tokens to preserve the information needed for the video generation. This process reduces the number of tokens processed and hence lowers the computational cost of the attention layers in the network.", "section": "5 Temporal Dimension Token Merging"}, {"figure_path": "https://arxiv.org/html/2502.04363/x11.png", "caption": "Figure 8. The block loading and inference cycles for (a) sequential loading and inference, and (b) concurrent inference.", "description": "Figure 8 illustrates the performance difference between sequential and concurrent model block loading and inference.  In sequential loading (a), each block is loaded into memory before processing, causing idle time on the GPU.  The concurrent inference method (b), however, loads and processes blocks concurrently, keeping the GPU busy and improving efficiency. This is visually represented by the overlap of red (loading) and black (inference) bars in (b), highlighting the simultaneous execution, which is absent in (a).", "section": "6 Concurrent Inference with Dynamic Loading"}, {"figure_path": "https://arxiv.org/html/2502.04363/x12.png", "caption": "Figure 9. Example videos generated by On-device Sora and Open-Sora\u00a0(Zheng et\u00a0al., 2024) (68 frames, 256\u00d7256 resolution).", "description": "This figure showcases example video outputs generated by both On-device Sora and Open-Sora, highlighting the visual quality and similarity between the two models.  The videos are 68 frames long and have a resolution of 256x256 pixels.  Two examples are shown: a burning stack of leaves and a close-up of a lemur.  This visual comparison demonstrates the comparable quality of video generated by On-device Sora, which is optimized for mobile devices, when compared to the original Open-Sora model running on more powerful hardware.", "section": "8.1 Video Generation Performance"}, {"figure_path": "https://arxiv.org/html/2502.04363/x13.png", "caption": "Figure 10. The block loading and inference cycle for Dynamic Loading applied with Concurrent Inference.", "description": "This figure illustrates how Dynamic Loading enhances Concurrent Inference in the On-device Sora model.  Concurrent Inference allows parallel loading and execution of model blocks, improving efficiency. Dynamic Loading further optimizes this by keeping frequently used blocks in memory, reducing loading times. The diagram shows a comparison of sequential loading and concurrent loading with dynamic loading, demonstrating how the system loads and processes blocks to improve the speed of video generation.", "section": "6 Concurrent Inference with Dynamic Loading"}, {"figure_path": "https://arxiv.org/html/2502.04363/x14.png", "caption": "Figure 11. A visual comparison of videos generated by On-device Sora and Open-Sora\u00a0(Zheng et\u00a0al., 2024), evaluated using VBench\u00a0(Huang et\u00a0al., 2024).", "description": "Figure 11 presents a visual comparison of video samples generated using On-device Sora and Open-Sora, focusing on the quality differences between the two approaches.  The videos were created using the same prompts, allowing for a direct comparison based on visual quality metrics. The quality assessment was performed using VBench, a benchmark tool providing an overall evaluation score along with specific metrics such as temporal consistency, background consistency, flickering, motion smoothness, dynamic degree, aesthetic quality and imaging quality. This comparison visualizes the performance of On-device Sora in producing high-quality videos on a mobile device, despite the computational limitations compared to the Open-Sora model running on a high-end GPU.", "section": "8.1 Video Generation Performance"}, {"figure_path": "https://arxiv.org/html/2502.04363/x15.png", "caption": "Figure 12. The snapshots of videos (68 frames, 256\u00d7256 resolution) applied with various LPL settings (Table\u00a03).", "description": "This figure shows example video frames generated by On-device Sora using different Linear Proportional Leap (LPL) settings.  Each row represents a different LPL setting from Table 3 in the paper, indicating the number of denoising steps used before the linear leap is applied. The videos are 68 frames long and have a resolution of 256x256 pixels. The purpose of the figure is to visually demonstrate that despite using fewer denoising steps with different LPL settings, the generated videos maintain similar semantic content and quality.", "section": "8.2 Linear Proportional Leap"}]