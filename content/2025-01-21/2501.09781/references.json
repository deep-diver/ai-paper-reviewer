{"references": [{"fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report on GPT-4, a large language model, and its relevance stems from the paper's focus on text-based models in contrast to the video-based model presented in the current work."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-12-01", "reason": "This paper is foundational in demonstrating the capabilities of large language models (LLMs), which the authors contrast with their exploration of video-only knowledge learning."}, {"fullname_first_author": "Aaron van den Oord", "paper_title": "Neural discrete representation learning", "publication_date": "2017-12-01", "reason": "This paper introduces Vector Quantized Variational Autoencoders (VQ-VAEs), a crucial component of the VideoWorld architecture, making it a key foundational reference."}, {"fullname_first_author": "Diederik P Kingma", "paper_title": "Auto-encoding variational bayes", "publication_date": "2013-12-12", "reason": "This paper introduces variational autoencoders (VAEs), which are fundamental to the VQ-VAE used in VideoWorld and serve as a critical basis for the approach."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-01", "reason": "This paper introduces the Transformer architecture, a key component of VideoWorld's autoregressive model, thus highlighting its importance as a foundational element."}]}