{"importance": "This work matters because it studies **how well monolingual models generalize to multilingual tasks**. It addresses the crucial issue of **accessibility in vision-language models**, providing insights on creating models that perform well in multiple languages without needing extensive data for each. It also introduces a **method for dataset creation** that opens new avenues for future research.", "summary": "Florenz: Scaling laws for systematic generalization via monolingual vision-language models", "takeaways": ["Monolingual VLMs can achieve systematic generalization in multilingual tasks.", "Model scale is more critical than the number of seen training samples for generalization.", "A simple decoder prefix can enable zero-shot captioning in new languages."], "tldr": "Current vision-language models **struggle with multilinguality**, sacrificing downstream performance and facing lexical ambiguities. This paper addresses these issues by studying systematic generalization with monolingual VLMs for multilingual tasks. It focuses on the impact of model size and training data, aiming to improve cross-lingual transfer without extensive multilingual pre-training.\n\nThe paper introduces a new model family, **Florenz**, based on Florence-2 and Gemma-2, and trained it on a synthetic dataset with incomplete language coverage. It shows that **indirectly learning task-language pairs adheres to a scaling law**. The paper demonstrates that image captioning abilities can emerge in a specific language even with translation data only and achieves competitive results on downstream tasks.", "affiliation": "Fraunhofer IAIS", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.09443/podcast.wav"}