{"references": [{"fullname_first_author": "T. Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper introduces large language models, which can handle various general tasks with simple prompts without task-specific training, serving as a fundamental concept for the survey."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, which leverages visual information and is later used to assess text-image matching in the alignment methods, making it important for multimodal learning."}, {"fullname_first_author": "R. Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2023-01-01", "reason": "This paper introduces DPO which is an important optimization and reward method, used extensively in MLLM alignment for preference learning, making it a crucial reference."}, {"fullname_first_author": "W. Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "publication_date": "2023-01-01", "reason": "This paper provides the architecture for InstructBLIP, a general-purpose vision-language model that is later used by FDPO replacing final embedding layer with a classification head."}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper highlights the use of RL algorithms and preference data for improving LLMs in complex problem-solving, which has led to the popularity of DeepSeek-R1 and has brought new inspiration to the MLLM community."}]}