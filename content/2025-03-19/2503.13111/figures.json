[{"figure_path": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Dataset_Sample.png", "caption": "Figure 1: \n(Left) We generate the Cubify Anything VQA (CA-VQA) dataset and benchmark, covering various 1) input signals: single image, metric depth (sensor-based and estimated), multi-frame/-view, and 2) spatial understanding tasks: e.g., relationship prediction, metric estimation, 3D grounding.\n(Right) We train MM-Spatial, a generalist multimodal LLM that excels at 3D spatial understanding. It supports Chain-of-Thought spatial reasoning involving 2D grounding and depth estimation, and can also leverage depth input via tool-use.", "description": "Figure 1 illustrates the two main contributions of the paper. The left panel showcases the Cubify Anything VQA (CA-VQA) dataset and benchmark.  CA-VQA is designed to evaluate 3D spatial reasoning abilities in multimodal large language models (MLLMs). It offers diverse input modalities, including single images, sensor-based and estimated depth maps, and multiple views or frames.  The benchmark covers a wide array of spatial understanding tasks such as predicting relative spatial relationships between objects, estimating distances and sizes, and performing 3D object grounding. The right panel describes MM-Spatial, a novel multimodal LLM developed by the authors, which excels at 3D spatial understanding. MM-Spatial uses a chain-of-thought (CoT) reasoning process. It leverages 2D object grounding and depth estimation capabilities to answer complex spatial queries, and can even incorporate depth input through tool-use. ", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Depth_Tool-Use.png", "caption": "Figure 2: CA-VQA Data Example. Example of a single sample from our dataset. Each reference frame has between 0-4 multi-view support frames. All frames (reference and support) come with three metric depth maps: Ground truth (FARO laser), ARKit Depth (LiDAR-fused) and Monocular (DepthPro). Each support frame contains the relative pose from the reference image, along with camera intrinsics.", "description": "This figure shows a sample from the Cubify Anything VQA (CA-VQA) dataset.  It illustrates the multiple data modalities included in the dataset for each image.  A main reference image is shown, along with up to four additional support frames from slightly different viewpoints. Each frame (both reference and support) provides three depth maps: one from a high-accuracy FARO laser scanner (ground truth), one from Apple's ARKit (LiDAR-fused), and one from a monocular depth estimation model (DepthPro).  For each support frame, relative pose information (showing its position and orientation relative to the reference frame) and camera intrinsic parameters are also included.", "section": "3. Data"}, {"figure_path": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Qualitative_Example.png", "caption": "Figure 3: Example of leveraging depth maps via tool-use. The model predicts the objects\u2019 2D bounding boxes and function calls, receives the tool outputs (which is the median depth value within the box, marked with an \u00d7\\mathbf{\\times}\u00d7), and finally reasons about the answer.", "description": "This figure illustrates the process of using depth information to answer spatial questions.  The model first identifies objects in an image and determines their 2D bounding boxes.  Then, it queries a 'tool' (a function that extracts depth information) for the median depth value within each bounding box. This depth information is then used by the model in a chain-of-thought reasoning process to answer a question involving spatial relationships, such as 'Is the pillow behind the television?'", "section": "3.3 Multi-view and Metric Depth Data"}, {"figure_path": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/QA_Examples.png", "caption": "Figure 4: Qualitative Example. We show the predictions of various models on a challenging example from our CA-VQA benchmark. Strong commercial (2a&b) and research models (2c&d) fail. MM-Spatial (1a) is much better, and even more so with CoT enabled (1b), demonstrating our model\u2019s strong object grounding (see predicted 2D boxes in the image), depth estimation, and spatial reasoning ability. Accuracy improves further when leveraging ground-truth depth via tool-use (1c), although our CoT model\u2019s (1b) predictions are very close to that, for both the intermediate depth values and final answer; monocular estimated depth (1d) is less accurate and yields a worse result.", "description": "Figure 4 presents a qualitative comparison of different models' performance on a complex spatial reasoning task from the CA-VQA benchmark.  It highlights the superior performance of the MM-Spatial model. The figure shows that while strong commercial and research models fail to accurately answer the question, MM-Spatial provides a much better response.  Further improvements are observed when using Chain-of-Thought (CoT) reasoning and ground truth depth, demonstrating the model's ability to ground objects in 2D space, estimate depth accurately, and reason spatially. The use of monocular depth estimation, while also helpful, is shown to be less accurate than ground truth depth.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Spatial_Category_Examples_1.png", "caption": "Figure 5: \nCA-VQA Overview. Example QA pairs from our Cubify Anything VQA (CA-VQA) dataset, aiming to unlock object-centric 3D spatial understanding in MLLMs. Using high-quality 3D ground truth annotations from CA-1M [61], we generate spatial perception questions across a variety of different tasks, e.g., involving relative relationships, metric measurements, and 3D object bounding boxes.", "description": "Figure 5 showcases example question-answer pairs from the Cubify Anything VQA (CA-VQA) dataset.  CA-VQA is designed to improve 3D spatial reasoning capabilities in multimodal large language models (MLLMs). It leverages high-quality 3D ground truth annotations from the CA-1M dataset to create diverse spatial perception questions. These questions cover a wide range of tasks including relative spatial relationships between objects, metric measurements (distances and sizes), and 3D object bounding box identification.  The figure visually demonstrates the variety of question types and the dataset's focus on detailed 3D spatial understanding.", "section": "3. Data"}, {"figure_path": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/Spatial_Category_Examples_2.png", "caption": "Figure 6: Examples of CA-VQA data samples from the Binary, Counting and Multi-choice categories.", "description": "Figure 6 presents example questions and answers from the CA-VQA dataset, categorized into Binary, Counting, and Multi-choice question types.  The Binary examples showcase questions about spatial relationships (e.g., Is object A in front of object B?), relative object sizes, and object presence. Counting questions involve counting the number of objects of a specific type in the image.  Multi-choice examples combine different question types into a multiple-choice format. The figure visually demonstrates the variety of question formats and types of spatial reasoning tasks included in the CA-VQA dataset.", "section": "3. Data"}, {"figure_path": "https://arxiv.org/html/2503.13111/extracted/6286766/figures/AABB_vs_OBB.png", "caption": "Figure 7: Examples of CA-VQA data samples from the Regression (Metric Estimation) and 2D Grounding categories.", "description": "Figure 7 shows example questions and answers from the CA-VQA dataset that belong to two categories: Regression (Metric Estimation) and 2D Grounding.  The Regression examples demonstrate questions that require estimating distances (e.g., how far away is an object, distance between two objects) and sizes (e.g., height, length) of objects.  The 2D Grounding examples showcase questions that ask for the 2D image coordinates of objects given either their names or material properties.  The figure illustrates the diversity of questions and the types of answers expected in CA-VQA.", "section": "3. Data"}]