[{"figure_path": "https://arxiv.org/html/2503.14151/x1.png", "caption": "Figure 1: The architecture of Concat-ID. We utilize a VAE to extract image latents from reference images and concatenate them at the end of the video latents along the sequence dimension. Concat-ID relies solely on 3D self-attention, which are commonly present in state-of-the-art video generation models, without introducing additional modules and parameters.", "description": "Concat-ID uses variational autoencoders (VAEs) to extract image features from reference images.  These image features are then concatenated with video latents along the sequence dimension. The model uses only 3D self-attention mechanisms, which are commonly found in advanced video generation models. This approach avoids the need for extra modules or parameters, resulting in a simpler and more efficient architecture. The figure visually depicts this architecture, showing the flow of information from input images and text prompts to the final video generation.", "section": "4. Concat-ID"}, {"figure_path": "https://arxiv.org/html/2503.14151/x2.png", "caption": "(a) The procedure of data processing.", "description": "This figure illustrates the process of creating three types of paired image-video data for training the identity-preserving video generation model.  It starts with prompt-video pairs, which undergo face detection and video filtering to ensure data quality.  These filtered pairs are then divided into three categories: pre-training pairs (high cosine similarity between image and video frames), cross-video pairs (moderate cosine similarity, enhancing facial editability), and trade-off pairs (very high similarity, focusing on consistency). This multi-stage approach helps to balance identity consistency and facial editability in the generated videos.", "section": "4.2. Data construction"}, {"figure_path": "https://arxiv.org/html/2503.14151/x3.png", "caption": "(b) Some samples of paired cross-video reference images.", "description": "This figure shows examples of image-video pairs used in the Concat-ID model training.  These are called \"cross-video pairs.\" Unlike pairs from the same video, these pairs are from different videos but feature the same person, demonstrating a wider range of facial expressions and head poses. This diversity in the training data helps to balance identity consistency and facial editability in the generated videos.", "section": "4.2. Data construction"}, {"figure_path": "https://arxiv.org/html/2503.14151/x4.png", "caption": "(c) Some samples of trade-off pairs.", "description": "This figure shows example pairs of images and videos used in the training process of the Concat-ID model. Specifically, it focuses on 'trade-off pairs'.  These pairs are carefully selected to balance identity consistency and facial expressiveness. The images are relatively similar to the corresponding videos (high cosine similarity), helping to maintain a consistent identity, but not so close as to limit facial variability. This careful balance prevents the model from overly replicating expressions from the reference image while ensuring strong identity preservation.", "section": "4.2. Data construction"}, {"figure_path": "https://arxiv.org/html/2503.14151/x5.png", "caption": "Figure 2: Constructing three types of image-video pairs for a single identity: pre-training, cross-video and trade-off pairs.", "description": "Figure 3 illustrates the three types of image-video pairings used in the Concat-ID model training for a single identity.  These pairings are designed to balance identity consistency and facial expressiveness in the generated videos.  Pre-training pairs utilize images and videos from the same video, ensuring high identity consistency.  Cross-video pairs use images and videos from different videos, but with similar facial features, promoting facial variability. Finally, trade-off pairs, with even greater similarity between images and videos, are used to further fine-tune the model and maintain a balance between identity preservation and editability.", "section": "4.2. Data construction"}, {"figure_path": "https://arxiv.org/html/2503.14151/x6.png", "caption": "Figure 3: Qualitative comparisons for single-identity generation. ID-Animator fails to preserve facial details, while ConsisID replicates the expressions of the reference images, particularly in the third case, where the semantic gap between texts and reference is significant. Concat-ID effectively preserves identity, while simultaneously preventing the direct replication of facial expressions from reference images.", "description": "Figure 3 presents a comparison of three different video generation models (ID-Animator, ConsisID, and Concat-ID) in terms of their ability to generate videos that accurately reflect the identity of a reference image, while also offering some degree of facial expression variability.  Each model is tested with three different scenarios: a man in a Superman outfit, a street artist, and a person crying on a bench.  The results illustrate that ID-Animator struggles to maintain facial detail, while ConsisID generates videos that are too heavily influenced by the facial expressions of the input image; this is especially pronounced in the third scenario, where the prompt's emotional content contrasts strongly with the reference image. In contrast, Concat-ID offers an excellent balance between preserving identity and allowing for more natural facial expressions, showing less direct imitation of the reference image's expressions.", "section": "5.2 Main results"}, {"figure_path": "https://arxiv.org/html/2503.14151/extracted/6289967/images/user_study.png", "caption": "Figure 4: Qualitative comparisons for multi-identity generation. Concat-ID better maintains different identities.", "description": "Figure 4 presents a qualitative comparison of multi-identity video generation results between Concat-ID and its main competitor, Ingredients.  The figure showcases several examples where different identities are present in a single video.  It visually demonstrates Concat-ID's superior ability to maintain distinct identities across various individuals in the video compared to Ingredients, which struggles to preserve individual identities clearly. The results highlight Concat-ID's effectiveness in managing complex scenarios with multiple individuals and preserving the distinct features of each identity. ", "section": "5.2 Main results"}, {"figure_path": "https://arxiv.org/html/2503.14151/x7.png", "caption": "Figure 5: Human evaluation. Concat-ID produces more precise and natural videos while effectively preserving identity.", "description": "This figure displays the results of a user study comparing the video generation quality of Concat-ID against ConsisID, focusing on identity consistency, facial motion alignment, and naturalness.  Three groups of users were tasked to rate videos from both models.  The bar chart shows that Concat-ID significantly outperforms ConsisID in all three aspects, demonstrating the effectiveness of its architecture in producing identity-preserving videos with natural facial movements.", "section": "5.2. Main results"}]