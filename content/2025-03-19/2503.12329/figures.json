[{"figure_path": "https://arxiv.org/html/2503.12329/x1.png", "caption": "Figure 1: Model rankings from CapArena in detailed captioning. Top models are comparable to humans, while most open-source models lag behind.", "description": "This figure presents the results of a pairwise comparison-based evaluation of various vision-language models (VLMs) on the task of detailed image captioning. The models are ranked according to their performance in head-to-head comparisons against human-generated captions. The ranking reveals that top-performing, commercial VLMs achieve performance on par with human annotators, demonstrating the advancement of this technology.  Conversely, many open-source models lag significantly behind, indicating a considerable performance gap between commercial and open-source VLMs in this area.", "section": "3 CapArena: Benchmarking VLMs in Detailed Image Captioning"}, {"figure_path": "https://arxiv.org/html/2503.12329/x2.png", "caption": "Figure 2: Caption length distribution of different VLMs.", "description": "This figure shows the distribution of caption lengths generated by various Vision-Language Models (VLMs) and human annotators.  The x-axis represents the caption length in words, and the y-axis represents the frequency of captions with that length.  The distributions are shown as histograms, allowing for a visual comparison of how concise or verbose the different models tend to be in their image captioning.  It helps to visualize whether a model produces relatively short or long captions and how that compares to human-generated captions.", "section": "3 CapArena: Benchmarking VLMs in Detailed Image Captioning"}, {"figure_path": "https://arxiv.org/html/2503.12329/x3.png", "caption": "Figure 3: Battle counts and win rate matrix between models in CapArena.", "description": "This figure visualizes the results of pairwise comparisons between different vision-language models (VLMs) in the CapArena benchmark.  The heatmap displays the number of pairwise comparisons conducted between each model pair (battle counts), showing how often each model was pitted against every other model.  The win rate matrix uses color intensity to represent each model's win rate in its head-to-head battles against other models, providing a visual representation of the relative performance of the different VLMs.", "section": "3 CapArena: Benchmarking VLMs in Detailed Image Captioning"}, {"figure_path": "https://arxiv.org/html/2503.12329/x4.png", "caption": "Figure 4: Correlation between vision-language benchmark scores and CapArena ranking. Models that perform well on general benchmarks do not necessarily excel in captioning. The size of each point represents the model size.", "description": "This figure displays the correlation between a model's performance on standard vision-language benchmarks and its ranking in the CapArena benchmark specifically designed for detailed image captioning.  The x-axis represents the scores from various vision-language benchmarks (MathVista, POPE, and VCR), while the y-axis shows the ranking in CapArena.  Each point corresponds to a specific Vision-Language Model (VLM), and its size reflects the model's parameter count.  The plot reveals that strong performance on general vision-language benchmarks does not guarantee strong performance on detailed image captioning tasks.", "section": "3.3 VLMs Performance Analysis"}, {"figure_path": "https://arxiv.org/html/2503.12329/x5.png", "caption": "Figure 5: Metrics exhibit systematic biases\u2014overestimating (positive values) or underestimating (negative values) certain models. Color saturation represents bias magnitude. Different metrics favor different models. GPT-4o exhibits lower biases (lighter overall colors), contributing to higher model-level agreement observed in Table\u00a02.", "description": "Figure 5 is a heatmap visualizing the systematic biases of various captioning metrics. Each row represents a model, and each column corresponds to a specific metric.  The color intensity indicates the bias magnitude, with warmer colors indicating overestimation and cooler colors indicating underestimation of the model's performance by that metric. The figure reveals that different metrics exhibit varying degrees of bias towards different models, highlighting the lack of consistency in metric evaluation. Notably, GPT-40 shows relatively low biases across most metrics, which contributes to its higher model-level agreement with human judgments, as shown in Table 2.", "section": "4 Analysis of Captioning Metrics"}, {"figure_path": "https://arxiv.org/html/2503.12329/x6.png", "caption": "Figure 6: Detailed caption generated with different prompt.", "description": "This figure shows two detailed captions generated by a Vision-Language Model (VLM) for the same image.  The difference highlights how variations in prompt wording can significantly affect the generated caption's level of detail, descriptive style, and the specific aspects of the image that are emphasized.  The captions demonstrate the VLM's ability to generate detailed and descriptive text based on different prompts.  The image itself is not shown.", "section": "3.2 CapArena: Pairwise Battle Platform"}]