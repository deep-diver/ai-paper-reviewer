{"importance": "This work provides a crucial benchmark for evaluating detailed image captioning, a task becoming increasingly important. It reveals the limitations of existing metrics and open-source models, while also introducing a new automated benchmark. **This paves the way for more accurate and efficient development of VLMs** and deeper understanding in image content.", "summary": "CapArena: Detailed image caption benchmark in the LLM era, revealing metric biases and advancing automated evaluation.", "takeaways": ["Leading VLMs now rival or surpass human performance in detailed image captioning.", "Traditional and recent captioning metrics are often unreliable for detailed captions due to inherent biases.", "VLM-as-a-Judge shows promise for automated evaluation, especially with reference captions."], "tldr": "Image captioning faces new challenges with the rise of VLMs, where models can generate longer descriptions. However, reliably evaluating such detailed captions is difficult because of the fine-grained details, diversity and lack of reliable benchmarks. This paper addresses the question of how well VLMs perform when compared to human captioning & how we can reliably assess the quality of the captions they generate. It also shows that current evaluation methods are unsuitable due to their inherent biases.\n\nTo tackle these issues, the authors introduce **CapArena**, an arena-style platform for evaluating VLMs via pairwise caption battles. **It provides high-quality human preference votes to benchmark models**. They show that leading models like GPT-4o can match or exceed human-level performance. The authors also introduce **CapArena-Auto**, a new automated benchmark for detailed captioning, achieving high correlation with human rankings at a low cost.", "affiliation": "National Key Laboratory for Novel Software Technology, Nanjing University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.12329/podcast.wav"}