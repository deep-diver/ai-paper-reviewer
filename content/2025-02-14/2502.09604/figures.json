[{"figure_path": "https://arxiv.org/html/2502.09604/x1.png", "caption": "Figure 1: The SelfCite framework calculates rewards based on two metrics: necessity score (probability drop) and sufficiency score (probability hold). First, the full context is used to generate a response. Then, the framework evaluates the probability of generating the same response after (1) removing the cited sentences from the context and (2) using only the cited sentences in the context. The probability drop and hold are computed from these probability differences, and their sum is used as the final reward.", "description": "The SelfCite framework uses context ablation to determine if a citation is necessary and sufficient.  First, a model generates a response given the full context.  Next, the model generates responses with the cited sentences (1) removed and (2) retained alone. If removing the cited text significantly reduces the probability of generating the same response, the citation is deemed necessary.  If keeping only the cited sentences maintains a high probability of the same response, the citation is deemed sufficient. The necessity score (probability drop) and sufficiency score (probability hold) are combined to compute a final reward used in training.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2502.09604/extracted/6200181/figures/merged_figure.png", "caption": "Figure 2: Iteratively applying SimPO for three iterations.", "description": "This figure shows the results of iteratively applying the SimPO (Simple Preference Optimization) algorithm for three iterations.  Each iteration involves training the language model with a new dataset generated by the best-of-N sampling method. The graphs display how the F1 score (a measure of citation quality) and citation length change across the iterations for two different models: Llama-3.1-8B-Instruct and LongCite-8B.  The figure demonstrates that iterative SimPO continues improving the citation quality over multiple training rounds, but also suggests potential performance degradation after repeated iterations, possibly due to off-policy training limitations.", "section": "2.4 Preference Optimization"}]