[{"figure_path": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/HF_documentation.png", "caption": "Figure 1: Hugging Face Documentation. We analyze the model cards of 1.2\u2062M1.2\ud835\udc401.2M1.2 italic_M Hugging Face models. We discover that the majority of models are either undocumented or poorly documented.", "description": "The figure is a pie chart visualizing the state of documentation for 1.2 million models on the Hugging Face model repository.  It reveals that a significant portion (39.5%) have either no documentation or only minimal documentation (rest).  A smaller portion have template READMEs (16.6%) or completely empty READMEs (12.3%). Only a minority (31.6%) have adequate README files.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09619/x1.png", "caption": "Figure 2: Classification Model Search. We present a new task of Classification Model Search, where the goal is to find classifiers that can recognize a target concept. Concretely, given an input prompt, such as \u201cDog\u201d, we wish to retrieve all classifiers that one of their classes is \u201cDog\u201d. The search space is a large model repository, that contains many models and concepts to search from. The retrieved models can replace model training, increasing accuracy, reducing cost and environmental impact.", "description": "This figure illustrates the task of Classification Model Search.  The goal is to identify classifiers from a large model repository that can recognize a specific target concept (e.g., \"Dog\").  Given a text prompt like \"Dog\", the search aims to retrieve all classifiers that include \"Dog\" as one of their classification classes.  The challenge lies in the massive size of the model repository and the diversity of concepts within it. Successful retrieval replaces the need for model training, offering potential improvements in accuracy, cost reduction, and environmental benefits.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/logit_level_descriptors_v3.png", "caption": "Figure 3: ProbeLog Descriptors. Our method generates a descriptor for individual output dimensions (logits) of models. First, we sample and a set of inputs (e.g., from the COCO dataset), and fix them as our set of probes. Then, to create a new ProbeLog descriptor for a model logit, we feed the set of ordered probes nto the model and observe their outputs. Finally, we take all values of the logit we wish to represent, and normalize them. We use this representation to accurately retrieve model logits associated with similar concepts. In Fig.\u00a05, we extend this idea to zero-shot concept descriptors.", "description": "ProbeLog generates a descriptor for each model's output dimension (logit) by feeding a fixed set of input samples (probes) into the model and observing its responses.  The responses are normalized to create the ProbeLog descriptor, which is used to retrieve similar model logits. This figure illustrates how ProbeLog uses probes to create a vector representation for a specific logit, enabling comparison with other logits for retrieval. Figure 5 extends this concept to zero-shot retrieval from text.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/zero_shot_probelog_v3.png", "caption": "Figure 4: CIFAR10 Logit Similarities.(a) Ground truth label. (b) ProbeLog representations using 1,00010001,0001 , 000 out-of-distribution COCO image probes. (c) ProbeLog representations using 1,00010001,0001 , 000 in-distribution CIFAR10 image probes. Both find meaningful similarities, although in-distribution probes work better.", "description": "This figure compares the ProbeLog representations of CIFAR-10 logits using two different sets of probes: 1,000 out-of-distribution COCO images and 1,000 in-distribution CIFAR-10 images.  Subfigure (a) shows the ground truth label for comparison. Subfigures (b) and (c) visualize the ProbeLog representations generated using the respective probe sets.  The results demonstrate that while both sets of probes reveal meaningful similarities between logits, using in-distribution probes yields better results, highlighting the importance of probe selection in generating effective representations.", "section": "3.2 The Challenge"}, {"figure_path": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/Collaborative_Probing_diagram_v2.png", "caption": "Figure 5: Text-Aligned ProbeLog Representation. We present a method to create ProbeLog-like representations for text prompts. We encode and store each of our ordered probes using the CLIP image encoder. At inference time, we embed the target text prompt, and compute its similarity with respect to the stored probe representations. We demonstrate that by normalizing this zero-shot ProbeLog descriptor, we can effectively search descriptors of real model logits, accurately retrieving similar concepts.", "description": "This figure illustrates the process of generating Text-Aligned ProbeLog representations.  First, a set of pre-selected image probes are encoded using the CLIP image encoder, creating a set of probe embeddings.  Then, at the time of inference, a target text prompt is provided and embedded using the same CLIP text encoder. The similarity between the text embedding and each of the stored probe embeddings is computed using cosine similarity.  These similarity scores form a new descriptor, representing the target concept. Finally, this zero-shot ProbeLog descriptor is normalized to be comparable to ProbeLog descriptors of actual model logits.  The normalization enables the effective retrieval of relevant model logits that recognize similar concepts to the input text prompt.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/collaborative_probing_v2.png", "caption": "Figure 6: Collaborative Probing. We pass a random subset of probes through each model in the repository to obtain partial logit representations. By performing factorization based matrix imputation we can complete the missing information. This saves a substantial part of the computational resources needed to build our repository\u2019s logit descriptors gallery.", "description": "This figure illustrates the concept of collaborative probing, a technique to efficiently generate logit descriptors for a large model repository.  Instead of running all probes through every model (which is computationally expensive), a random subset of probes is used for each model. This results in a sparse matrix of logit representations.  Matrix factorization (specifically, truncated SVD) is then employed to impute the missing values, effectively reconstructing the complete matrix of logit descriptors. This method significantly reduces the computational cost compared to probing all models with all probes, making the process more scalable for large repositories.", "section": "4.4. Collaborative Probing"}, {"figure_path": "https://arxiv.org/html/2502.09619/extracted/6201838/figs/coco_inet_n_probes.png", "caption": "Figure 7: Collaborative Probing. We test our method using collaborative probing on the text \u2192\u2192\\rightarrow\u2192 INet-Hub retrieval task. While the full size of the dataset is 8,00080008,0008 , 000 COCO probes, we show cases where each model is probed by less than 15%percent1515\\%15 % of these probes. We can see that for the limited probe regime, collaborative probing can improve accuracy by as much as 2\u00d72\\times2 \u00d7.", "description": "This figure demonstrates the effectiveness of Collaborative Probing, a technique to reduce the computational cost of creating ProbeLog representations.  The experiment focuses on the text-based search task using the INet-Hub dataset.  The standard approach uses 8000 COCO probes for each model. Collaborative Probing, however, uses significantly fewer probes (less than 15% of the 8000 probes) for each model. Despite this reduction, the figure shows that Collaborative Probing achieves comparable accuracy to the standard approach, often even doubling its accuracy for this limited probe regime.", "section": "5.3. Collaborative Probing"}]