{"importance": "This paper introduces **DehazeXL, a novel method for effective haze removal** in large images. It contributes a large-scale dataset, opening new avenues for dehazing research on high-resolution images and fostering advancements in the field.", "summary": "DehazeXL: Global context fusion for haze removal in large images.", "takeaways": ["DehazeXL effectively balances global context and local feature extraction for haze removal in large images.", "The Dehazing Attribution Map (DAM) helps identify key features contributing to model performance.", "The 8KDehaze dataset provides a new benchmark for training and evaluating haze removal algorithms on ultra-high-resolution images."], "tldr": "Haze removal in large, high-resolution images is challenging due to GPU memory limitations. Existing methods often resort to image slicing or downsampling, which diminishes global information or discards high-frequency details. Therefore, innovative solutions are needed to efficiently balance global context and local details for effective haze removal in high-resolution imagery.\n\nThis paper proposes **DehazeXL**, a novel method that effectively integrates global information interaction with local details extraction, enabling the processing of large images. DehazeXL partitions the input image into patches, encodes them into feature vectors, and utilizes a global attention module for context integration. This approach, combined with a custom visual attribution method and the **8KDehaze dataset**, achieves state-of-the-art haze removal performance with reduced memory usage.", "affiliation": "Beijing Institute of Technology", "categories": {"main_category": "Computer Vision", "sub_category": "Image Segmentation"}, "podcast_path": "2504.09621/podcast.wav"}