{"references": [{"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-01", "reason": "This work introduces LLaMA, which serves as a base model for many instruction-tuning experiments in the current paper."}, {"fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This work is fundamental as it demonstrates the ability of large language models to perform few-shot learning, which motivates the instruction tuning paradigm."}, {"fullname_first_author": "Peter Clark", "paper_title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "publication_date": "2018-03-01", "reason": "This paper introduces the ARC dataset which is used as a benchmark to evaluate reasoning capabilities in the current work."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2021-01-01", "reason": "This work introduces the MMLU benchmark, which is used to evaluate the factual knowledge and reasoning abilities of language models in the current work."}, {"fullname_first_author": "Lambert", "paper_title": "T\u00fclu 3: Pushing frontiers in open language model post-training.", "publication_date": "2024-11-15", "reason": "This paper introduces the Tulu3 dataset used in the current paper, which serves as the experimental pool for most experiments."}]}