[{"figure_path": "https://arxiv.org/html/2504.11833/x1.png", "caption": "Figure 1: English is not always better than other languages. Evaluation results on the human-translated GPQA\u00a0Rein et\u00a0al. (2023) and MGSM\u00a0Shi et\u00a0al. (2023) datasets\u00a0(obtained from\u00a0Huang et\u00a0al. (2025)). The red cells indicate greater-than-English scores.", "description": "This figure compares the performance of several large language models (LLMs) on two reasoning tasks, GPQA and MGSM, when the tasks are presented in different languages.  It shows the accuracy of the models (Acc@k) across multiple languages including English, and highlights in red when a particular non-English language outperforms English for a given model and task. This demonstrates that the performance of LLMs is not consistently higher when tasks are in English, challenging the commonly held belief of an 'English bias' in LLMs.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.11833/x2.png", "caption": "Figure 2: An introduction to input samples across various comparison methods, including Multilingual, Repeat, Paraphrase, Repeat-Mix, and Paraphrase-Mix.", "description": "Figure 2 illustrates different input methods used to evaluate multilingual reasoning in LLMs.  It compares five approaches: 1) **Multilingual:** Questions are translated into multiple languages before being fed to the model. 2) **Repeat:** The same English question is input multiple times to the model, using different random seeds for each run.  3) **Paraphrase:** The original English question is paraphrased using an LLM before being fed into the model. 4) **Repeat-Mix:**  Combines the Repeat and Multilingual methods; half the inputs are repeated questions and half are translated versions. 5) **Paraphrase-Mix:** Combines Paraphrase and Multilingual in a similar 50/50 ratio. The diagram visually represents these techniques, helping to understand the different ways input variations impact the model's reasoning performance. The figure is essential in evaluating whether the performance improvements observed in multilingual settings are simply due to random variation in input or decoding.", "section": "3 High Upper Bound of Multilingual Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.11833/x3.png", "caption": "Figure 3: Compared to Repeat and Paraphrase, Multilingual demonstrates a higher performance upper bound. Acc@17 scores of Multilingual, Paraphrase and Repeat settings of the three models on the human-translated GPQA dataset.", "description": "This figure compares the performance upper bounds achieved by three different approaches on the human-translated GPQA dataset: Multilingual, Repeat, and Paraphrase.  The Multilingual approach involves translating the questions into multiple languages and aggregating model responses, while Repeat and Paraphrase represent baselines using multiple runs with the same input and paraphrased inputs respectively.  The Acc@17 metric indicates the probability of obtaining at least one correct answer from 17 model responses.  The results show that the Multilingual approach consistently outperforms the Repeat and Paraphrase baselines across three different LLMs (Qwen2.5-72B, LLaMA3.1-70B, and R1-Distill-LLaMA-70B), highlighting the potential benefits of leveraging multilingualism in LLM reasoning tasks. The y-axis represents the accuracy (Acc@17), and the x-axis shows three different LLMs.", "section": "3 High Upper Bound of Multilingual Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.11833/x4.png", "caption": "Figure 4: Multilingual surpasses Paraphrase and Repeat in Acc@k\ud835\udc58kitalic_k after k=3\ud835\udc583k=3italic_k = 3 in a growing margin. Best Acc@k\ud835\udc58kitalic_k (out of 17) of Multilingual, Paraphrase and Repeat settings for Qwen2.5-72B with increasing numbers of languages or candidates on the human-translated GPQA dataset.", "description": "Figure 4 presents a graph comparing the performance of three different methods\u2014Multilingual, Paraphrase, and Repeat\u2014on the GPQA dataset using the Qwen2.5-72B model.  The x-axis represents the increasing number of languages or answer candidates (k), while the y-axis shows the corresponding Acc@k score. Acc@k indicates the probability that at least one of the top k answers is correct. The graph demonstrates that the Multilingual approach significantly outperforms both Paraphrase and Repeat methods, especially when the number of languages or candidates exceeds three (k>3). The Multilingual approach shows a consistently increasing advantage as more languages or candidates are considered, highlighting the benefit of utilizing multiple languages for improved reasoning performance.", "section": "3 High Upper Bound of Multilingual Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.11833/x5.png", "caption": "Figure 5: Fully utilizing non-English languages can improve the upper bound. Distribution of Acc@4 scores of all possible 4-candidate-combinations with Qwen2.5-72B on the human-translated GPQA dataset, under different settings.", "description": "This figure displays the distribution of Acc@4 scores for all possible combinations of four languages when using the Qwen2.5-72B model on the human-translated GPQA dataset.  It compares the performance of the Multilingual approach against baselines using only paraphrased English inputs or repeated runs with the same English input. The figure shows that the Multilingual method consistently achieves higher Acc@4 scores, demonstrating the potential of multilingual reasoning to improve the upper bound of model performance. The different boxes represent the various methods used to prepare the inputs, highlighting the consistent superior performance of the multilingual approach.", "section": "3 High Upper Bound of Multilingual Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.11833/x6.png", "caption": "Figure 6: The Multilingual upper bound is stable regardless of the question translation quality. Comparison of Acc@4 on human- and machine-translated GPQA dataset among all possible 4-language combinations in Multilingual setting. The values and error bars denote mean, max and min scores.", "description": "This figure displays the robustness of the multilingual upper bound in reasoning tasks. It compares the performance (Acc@4) across various 4-language combinations within a multilingual setting, differentiating between datasets with human-translated and machine-translated questions. Error bars represent the variation in performance (minimum and maximum Acc@4 scores). The results demonstrate consistent high performance, suggesting that multilingual reasoning benefits are not significantly affected by translation quality.", "section": "3 High Upper Bound of Multilingual Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.11833/x7.png", "caption": "Figure 7: Voting performance does not increase with candidate number. Best Vote@k\ud835\udc58kitalic_k (out of 17) of Paraphrase, Repeat and Multilingual with human (Multilingual-h) and machine translation (Multilingual-m) on the GPQA dataset for Qwen2.5-72B with increasing numbers of languages or candidates.", "description": "This figure illustrates the performance of three different methods (Multilingual, Paraphrase, and Repeat) on the GPQA dataset using the Qwen2.5-72B model.  The x-axis represents the increasing number of languages or candidates (k), and the y-axis shows the Vote@k score, indicating the accuracy of the majority vote among k answers.  The Multilingual method uses translations of the questions into different languages before feeding them to the model.  The Paraphrase method uses paraphrased versions of the original questions. The Repeat method runs the model multiple times on the same questions with different random seeds.  Both human-translated and machine-translated versions of the Multilingual approach are included for comparison. The graph demonstrates that while accuracy (Acc@k) generally increases with more languages or candidates for the Multilingual method (as shown in a previous figure), majority voting (Vote@k) does not necessarily follow the same trend; it does not consistently improve with an increasing number of languages, suggesting limitations of majority voting as an answer selection strategy. ", "section": "3.2 Intriguing Phenomena"}, {"figure_path": "https://arxiv.org/html/2504.11833/x8.png", "caption": "Figure 8: Multilingual performance is sensitive to the optimality of the language combination. Comparison of Vote@4 of Repeat, Paraphrase and Multilingual with human (Multilingual-h) and machine translation (Multilingual-m) on the GPQA dataset. The values and error bars denote mean, max and min scores.", "description": "Figure 8 presents a box plot visualizing the sensitivity of multilingual reasoning performance to the quality of language combinations used.  It compares the Vote@4 scores (accuracy after majority voting from 4 candidates) obtained from three different methods: Repeat (repeatedly feeding the same English prompt to the model), Paraphrase (feeding paraphrased English prompts), and Multilingual (feeding translated versions of the prompt in multiple languages). The comparison includes results using both human-translated (Multilingual-h) and machine-translated (Multilingual-m) multilingual prompts.  The box plots show the mean, maximum, and minimum scores for each method, providing a comprehensive view of the performance variation and the impact of translation quality on the success of the multilingual approach.", "section": "3.2 Intriguing Phenomena"}]