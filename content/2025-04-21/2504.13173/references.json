{"references": [{"fullname_first_author": "Ali Behrouz", "paper_title": "Titans: Learning to memorize at test time", "publication_date": "2024-01-01", "reason": "This paper focuses on learning to memorize at test time, which is a central theme of the current paper and provides a baseline for comparison."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is All You Need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, which serves as a foundation for many of the models discussed and compared in the current work."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces", "publication_date": "2024-01-01", "reason": "This paper introduces the Mamba architecture, a linear-time sequence modeling approach that serves as a key comparison point in the current work."}, {"fullname_first_author": "Yutao Sun", "paper_title": "Retentive network: A successor to transformer for large language models", "publication_date": "2023-01-01", "reason": "This paper introduces the Retentive Network, a modern linear recurrent model that offers an alternative to Transformers and is compared extensively in the current work."}, {"fullname_first_author": "J\u00fcrgen Schmidhuber", "paper_title": "Long Short-term Memory", "publication_date": "1997-01-01", "reason": "This paper presents the LSTM architecture and is included since LSTM is a classic recurrent neural network that many sequence models derive from."}]}