[{"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.4.1.1.2.1\">Accuracy (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.4.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.4.2.1.1\">CLIP ViT-B/32</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.4.2.1.2\">42.80</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.3.2.1\">CLIP ViT-B/16</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.3.2.2\">40.87</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.4.3.1\">CLIP ViT-L/14</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.4.3.2\">43.10</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.4.5.4.1\">SigLIP Base 224</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.4.5.4.2\">45.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.4.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T1.4.6.5.1\">SigLIP Base 256 Multilingual</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.4.6.5.2\">47.21</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: Experiment 1 Overall Accuracy Comparison Across Models", "description": "This table presents the overall accuracy achieved by different Vision Language Models (VLMs) in Experiment 1, which assesses their ability to understand augmented descriptions of images.  The models evaluated include various versions of CLIP (CLIP ViT-B/32, CLIP ViT-B/16, CLIP ViT-L/14) and SigLIP (SigLIP Base 224, SigLIP Base 256 Multilingual).  The accuracy reflects the models' success in correctly associating augmented image descriptions with corresponding visual changes.", "section": "4.1.2. Results"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T2.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T2.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.4.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T2.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.4.1.1.2.1\">Mean Accuracy</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T2.4.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T2.4.2.1.1\">CLIP ViT-B/16</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.4.2.1.2\">99.57%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.4.3.2.1\">CLIP ViT-B/32</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.3.2.2\">98.67%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.4.4.3.1\">CLIP ViT-L/14</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.4.3.2\">98.15%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T2.4.5.4.1\">SigLIP Base 224</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.4.5.4.2\">64.40%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.4.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T2.4.6.5.1\">SigLIP Base 256 Multilingual</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.4.6.5.2\">47.41%</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 2: Experiment 2 Mean Accuracy Comparison", "description": "This table presents the mean accuracy achieved by different Vision Language Models (VLMs) in Experiment 2, which focuses on matching augmented images with their corresponding descriptions.  The models tested include various versions of CLIP (ViT-B/16, ViT-B/32, ViT-L/14) and SigLIP (Base 224, Base 256 Multilingual).  The mean accuracy reflects the models' overall performance in correctly associating augmented images with descriptions that accurately reflect the applied image transformations.", "section": "4.2 Matching Augmented Images with Descriptions"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.4\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.4.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T3.4.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.4.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.1.1.2.1\">Top-1 Accuracy (%)</span></th>\n<th class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T3.4.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.4.1.1.3.1\">Top-5 Accuracy (%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.4.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.4.2.1.1\">ViT-B/32</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.1.2\">3.61</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T3.4.2.1.3\">18.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.4.3.2.1\">ViT-B/16</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.3.2.2\">3.50</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.4.3.2.3\">17.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.4.4.3.1\">ViT-L/14</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.4.3.2\">3.57</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.4.4.3.3\">15.28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.4.5.4.1\">SigLIP Base 224</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.4.5.4.2\">2.81</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S4.T3.4.5.4.3\">16.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.4.6.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S4.T3.4.6.5.1\">SigLIP Base 256 Multilingual</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.4.6.5.2\">3.19</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\" id=\"S4.T3.4.6.5.3\">18.06</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 3: Comparison of Top-1 and Top-5 Accuracies for Each Model", "description": "This table presents a comparison of the Top-1 and Top-5 accuracies achieved by various Vision Language Models (VLMs) in an image transformation classification task.  The Top-1 accuracy represents the percentage of times the model correctly identifies the specific image transformation applied to an image as the top prediction. The Top-5 accuracy indicates the percentage of times the correct transformation is ranked within the top 5 predictions made by the model.  The models compared include different variants of CLIP (ViT-B/32, ViT-B/16, ViT-L/14) and SigLIP (Base 224, Base 256 Multilingual).  The results illustrate the relative performance of each model in accurately classifying image transformations.", "section": "4.3 Classifying Image Transformations"}, {"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.6\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.6.7.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T4.6.7.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.6.7.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T4.6.7.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.6.7.1.2.1\">Input Image</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T4.6.7.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.6.7.1.3.1\">Output Image</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T4.2.2.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">DALL\u00b7E</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"137\" id=\"S5.T4.1.1.1.g1\" src=\"extracted/6274377/fig/dalle_orig.jpg\" width=\"137\"/></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.2.2.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_square\" height=\"137\" id=\"S5.T4.2.2.2.g1\" src=\"extracted/6274377/fig/Dalle_rotated_90.jpg\" width=\"137\"/></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T4.4.4.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">Instruct Pix2Pix</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.3.3.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"103\" id=\"S5.T4.3.3.1.g1\" src=\"extracted/6274377/fig/14866578404_d4ba6f82be_c.jpg\" width=\"137\"/></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T4.4.4.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"103\" id=\"S5.T4.4.4.2.g1\" src=\"extracted/6274377/fig/instruct_pix2pixrotate70.png\" width=\"137\"/></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.6.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T4.6.6.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">IP Adapter</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T4.5.5.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"103\" id=\"S5.T4.5.5.1.g1\" src=\"extracted/6274377/fig/14866578404_d4ba6f82be_c.jpg\" width=\"137\"/></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" id=\"S5.T4.6.6.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"103\" id=\"S5.T4.6.6.2.g1\" src=\"extracted/6274377/fig/ip_adapter_90.jpg\" width=\"137\"/></td>\n</tr>\n</tbody>\n</table>", "caption": "Table 4: Qualitative analysis table comparing input images and output transformations (rotation 90 degrees) for different models.", "description": "This table presents a qualitative comparison of how different image editing models (DALL-E 2, Instruct Pix2Pix, and IP Adapter) respond to the instruction to rotate an input image by 90 degrees.  It showcases the input image and the respective output images generated by each model, highlighting the varying levels of success in correctly executing the rotation instruction. The table demonstrates the limitations of current AI models in performing even simple geometric image transformations.", "section": "5. Impact on Downstream task"}]