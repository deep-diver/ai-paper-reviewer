[{"Alex": "Welcome to the podcast, future of AI navigators! Today, we're diving deep into uncharted territories of the AI world. We're tackling a new 'Model Atlas' for Hugging Face. Think of it as Google Maps, but for millions of AI models. Get ready to explore!", "Jamie": "Wow, an AI model atlas? That sounds incredibly ambitious and kinda needed. So, what exactly is Hugging Face in this context, and why do we need an atlas for it?"}, {"Alex": "Hugging Face is like the GitHub of pre-trained AI models. It's a massive repository where anyone can share or download models. But with millions of models, finding the right one is like finding a needle in a haystack. That's where our 'Model Atlas' comes in \u2013 to provide structure and guidance.", "Jamie": "Okay, that makes sense. So, you're essentially mapping out this crazy AI landscape. How does this 'Atlas' actually work? What's the core idea behind it?"}, {"Alex": "At its core, the Atlas visualizes models as nodes in a graph, and the relationships between them (like fine-tuning or quantization) as edges. It's like a family tree, but for AI. The position of each node is carefully optimized. Then, zooming into each path, the size of each node is determined by number of downloads and the color is the type of transformation it does, like an adapter or a complete fine tune.", "Jamie": "Hmm, interesting. So, you're tracking how models evolve from one another. How do you determine the relationships between these models? I mean, how do you know model A was fine-tuned from model B?"}, {"Alex": "That's the million-dollar question! Ideally, this information would be clearly documented, but often it isn't. The relationships could be described using weight distance but the Atlas aims to do more than that.", "Jamie": "Okay, so documentation is spotty. What other techniques are there?"}, {"Alex": "We also tap into model weights, which are those parameter values that define a neural net. Additionally, real-world data training practices can reveal hidden relationships which have boosted our method greatly", "Jamie": "Weights, huh? That's getting pretty technical. How exactly do you leverage weight similarity to map the atlas?"}, {"Alex": "Well, conceptually, models with similar weights are likely related. But it\u2019s not as simple as comparing numbers. Imagine two siblings - very similar but with important distinguishing features. By identifying key features in weight space, we can track model lineage. Real-data training practices is another important insight we put into the Atlas.", "Jamie": "Real data training practices? I see! How does real-data training practices help the Atlas structure to be built, and which data do you leverage?"}, {"Alex": "Yes, by identifying some high-confidence structural priors. For example, quantizations, those models with less weights in order to be run faster, are almost always leaf nodes on the Atlas structure, and you will rarely see anything being built based on them.", "Jamie": "Woah, so that tells you that if a model has a quantization, it should be a leaf node. Do you have other examples?"}, {"Alex": "Absolutely! Another one is the timestamps of different models. We realized that parents are always created earlier than the children, with only 0.27% violating this, which in turns allows us to infer the origin of the graph.", "Jamie": "Okay, so you have a time-based directionality. That is extremely clever! Does this address a non-tree problem?"}, {"Alex": "Exactly! As models can be created through a merging process, we can see the merging process, or if they're a tree or a DAG instead, which stands for directed acyclic graph, indicating that there are multiple incoming sources rather than a clear root and branch scenario. This allows us to represent those models more accurately.", "Jamie": "That's a great improvement! Can you tell me a bit more about these use cases? For example, how does it help with model attributes?"}, {"Alex": "One use case is predicting model accuracy. Only a fraction of models report accuracy metrics, but the Atlas structure allows us to infer it based on neighboring models. If a model is surrounded by highly accurate models of a similar type, it's more likely to also be accurate. We see this improve on the Mistral model family.", "Jamie": "Okay, so it's like 'wisdom of the crowds' but for AI models. I see how this could be really valuable. Does this method improve on a single model?"}, {"Alex": "Exactly! We saw significant improvements in predicting TruthfulnessQA scores just by observing the model's neighbors in the Atlas. We also do other missing attributes as well, where the Atlas greatly improves those predictions.", "Jamie": "That's really impressive. How else can the Atlas help analyze those models?"}, {"Alex": "Another cool application is analyzing trends. For example, we discovered that Llama-based models use more diverse training practices compared to Stable Diffusion. Llama uses quantization way more often!", "Jamie": "Interesting, so different communities have different habits. What about measuring model impact, how are you measuring the real impact of those models?"}, {"Alex": "Traditional metrics like download counts only tell part of the story. We introduced a new metric: sub_tree_downloads. This sums the downloads of a model and all its descendants, capturing its broader influence. The total number of downloads that this model generated.", "Jamie": "Woah, that is a totally new, and a must-have metric. Finally! Now it can be measured how biases and data affected not just that model but the entire family after it."}, {"Alex": "Exactly! It's a much more holistic view of a model's impact, factoring in its ripple effects across the AI ecosystem.", "Jamie": "This atlas sounds incredible, but what about model deletions? When models are removed from the repository, how does this affect the atlas?"}, {"Alex": "That's a great point. Model deletions can break the Atlas structure. To address this, we try to reconstruct missing nodes and restore their connections. It\u2019s like being an archeologist, putting the pieces back together.", "Jamie": "Hmm, that sounds challenging. What are some of the limitations of this atlas?"}, {"Alex": "Our Atlas charting isn't perfect. The method is fast, but is far from perfect. One limitation is that we currently rely on weight similarity, which can be misleading in some cases. We do leverage the timestamp of each model, however, there are still lots of areas for improvement.", "Jamie": "I see. What's the computational cost of building such a huge atlas?"}, {"Alex": "The most expensive part is calculating the distances between models which scales with the number of weights. As the weights are too big to be loaded to memory, we subsample those weights. By keeping only 100 random neurons, the computational cost drops dramatically and also becomes scalable", "Jamie": "Okay, subsampling sounds like a smart solution. So, what are the next steps for this research? What are you hoping to achieve in the future?"}, {"Alex": "We plan to incorporate more types of relationships, like distillation. We're also exploring how to represent the relationships between individual neurons in more details. This approach can greatly help in the future, allowing the user to download datasets from the atlas, or check model's attributes or Intellectual Property. Atlas also benefits the user as it allows to check the model's safety before installing it, checking its history and reliability.", "Jamie": "That sounds like a huge leap in terms of model safety! Any other repositories you are planning to analyze in the future?"}, {"Alex": "We believe that our work will greatly expand as it can be implemented in any model repository, as we speak of a model that is in the hands of a researcher to the model in the entire organization. We want it to be public for anyone to use, check, improve and contribute to.", "Jamie": "Wow, that's amazing! So, is the Atlas publicly available right now?"}, {"Alex": "Yes! You can find the interactive demo website. This Model Atlas isn't just a research project, it\u2019s a tool for anyone working with AI. By visualizing and navigating the AI landscape, we can all make better decisions, reuse existing resources, and ultimately accelerate AI innovation. Thanks for joining me on this journey into the Model Atlas!", "Jamie": "Thank you Alex, that was a really exciting journey!"}]