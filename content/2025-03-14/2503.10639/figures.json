[{"figure_path": "https://arxiv.org/html/2503.10639/x2.png", "caption": "Figure 1: Generation Chain-of-Thought (GoT) with Semantic-Spatial Reasoning. Our approach transforms input prompts into explicit reasoning chains with coordinates (middle), which guides vivid image generation and precise editing (right). This reasoning-based generation paradigm unifies spatial understanding across visual tasks: semantically-grounded visual generation (top), controllable interactive generation (middle), and localized image editing (bottom).", "description": "Figure 1 illustrates the Generation Chain-of-Thought (GoT) process, a novel approach for image generation and editing.  The figure showcases how input prompts are transformed into detailed reasoning chains, incorporating spatial coordinates. This structured reasoning guides both the generation of vivid images and precise, localized editing. The top panel demonstrates semantically-grounded visual generation; the middle shows controllable interactive generation; and the bottom demonstrates localized image editing.  The central role of the reasoning chain in unifying spatial understanding across different visual tasks is highlighted.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10639/x3.png", "caption": "Figure 2: GoT Dataset Construction Process. Left: Text-to-image GoT annotation pipeline that labels detailed GoT with semantic content and spatial coordinates. Right: Editing GoT annotation pipeline that processes source image, target image, and instruction to generate entity-aware reasoning GoT with precise spatial grounding. Both pipelines leverage Qwen2-VL\u00a0[46] and Qwen2.5\u00a0[51] models for various stages of the annotation process.", "description": "This figure details the process of constructing the GoT (Generation Chain-of-Thought) dataset, which is crucial for training the model.  The left side illustrates the pipeline for text-to-image generation.  Starting from a prompt, the pipeline uses Qwen2-VL [46] and Qwen2.5 [51] to generate detailed GoT annotations. These annotations include semantic content describing the scene and spatial coordinates precisely locating objects within the image. The right side shows the pipeline for image editing.  This pipeline takes a source image, a target image, and an editing instruction as input.  Again, using Qwen2-VL [46] and Qwen2.5 [51], entity-aware reasoning GoTs are generated with precise spatial grounding to guide the editing process.  Both pipelines ensure the generated GoTs capture comprehensive semantic-spatial relationships.", "section": "4. GoT Dataset: Semantic-Spatial Reasoning Chains for Visual Generation and Editing"}, {"figure_path": "https://arxiv.org/html/2503.10639/x4.png", "caption": "Figure 3: GoT Framework with Semantic-Spatial Guidance. Left: Our dual-task framework handling both text-to-image generation (T2I) and image editing. Right: The SSGM Diffusion Module, which combines spatial layouts guidance Gssubscript\ud835\udc3a\ud835\udc60G_{s}italic_G start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, reference image guidance Grsubscript\ud835\udc3a\ud835\udc5fG_{r}italic_G start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and semantic guidance Gtsubscript\ud835\udc3a\ud835\udc61G_{t}italic_G start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to generate the final image with precise content and spatial control.", "description": "Figure 3 illustrates the GoT framework, a unified model for both text-to-image generation and image editing. The left panel shows the overall framework architecture, highlighting the integration of a multimodal large language model (MLLM) for reasoning and a diffusion model for image generation, guided by semantic and spatial information. The MLLM generates reasoning chains, which are then used by the SSGM (Semantic-Spatial Guidance Module) to condition the diffusion model. The right panel details the SSGM, which incorporates three types of guidance: spatial layout guidance (Gs), reference image guidance (Gr), and semantic guidance (Gt), to produce an image with precise content and spatial arrangement.", "section": "5. GoT Framework: Reasoning-guided Visual Generation and Editing"}, {"figure_path": "https://arxiv.org/html/2503.10639/x5.png", "caption": "Figure 4: Text-to-Image samples generated by our model. The GoT framework can plan object placement based on the input caption and generate highly aligned and aesthetic images accordingly.", "description": "This figure showcases example images generated by the GoT model.  The images demonstrate the model's ability to accurately interpret and translate textual descriptions into realistic and aesthetically pleasing visuals.  The GoT framework's capacity to reason about spatial relationships and object placement is evident in the precise positioning and arrangement of elements within each scene.  The high degree of alignment between the generated images and the input captions highlights the model's effectiveness in generating visually coherent and faithful representations of the intended concepts.", "section": "6.1 Text-to-Image Generation"}, {"figure_path": "https://arxiv.org/html/2503.10639/x6.png", "caption": "Figure 5: Samples on interactive generation with GoT framework. By modifying GoT content (description and bounding box position), user can customize their text-to-image process with: 1. Object replacement 2. Object position adjustment 3. Object attribute modification.", "description": "This figure showcases the interactive capabilities of the GoT framework.  Users can modify the generated image by adjusting the GoT's description and bounding box coordinates.  Three specific interactive editing operations are demonstrated: 1) replacing one object with another; 2) repositioning objects within the scene; and 3) altering an object's attributes (e.g., color).  The examples highlight the framework's ability to maintain scene coherence while precisely implementing user-specified changes.", "section": "6.2. Interactive Generation"}, {"figure_path": "https://arxiv.org/html/2503.10639/x7.png", "caption": "Figure 6: Qualitative results of image editing. Our GoT framework demonstrates superior performance in settings that require semantic-spatial reasoning. Red bounding boxes indicate the coordinates predicted by MLLM within the GoT framework.", "description": "Figure 6 showcases example results from image editing tasks, highlighting the GoT framework's ability to handle complex edits requiring both semantic understanding and precise spatial reasoning.  The examples demonstrate successful modifications of image content based on user instructions, such as removing objects, adding objects, or changing object attributes. Red bounding boxes overlaid on the images indicate the spatial regions identified by the multimodal large language model (MLLM) as relevant to the editing instructions. This visualization serves to illustrate the framework's capacity for precise control over visual elements and its superior performance compared to methods lacking explicit reasoning and spatial awareness.", "section": "6.3 Image Editing"}, {"figure_path": "https://arxiv.org/html/2503.10639/x8.png", "caption": "Figure 7: More samples on image editing with the GoT content generated by our model.", "description": "Figure 7 presents additional examples showcasing the capabilities of the GoT framework in image editing.  Each example shows an original image, the GoT reasoning chain generated by the model, the editing instructions and the final edited image. The GoT reasoning chain details the specific changes to be made, including coordinates for precise control of the editing process.", "section": "6.3 Image Editing"}, {"figure_path": "https://arxiv.org/html/2503.10639/x9.png", "caption": "Figure 8: More examples on interactive generation.", "description": "Figure 8 presents additional examples showcasing the interactive image generation capabilities of the GoT framework.  It demonstrates how users can modify various aspects of the generated image, such as object attributes, positions, and even the addition or removal of objects, by interactively adjusting the GoT reasoning chain.  The examples highlight the framework's flexibility and precision in allowing users to precisely control the final image output through explicit reasoning modifications.", "section": "6.2. Interactive Generation"}, {"figure_path": "https://arxiv.org/html/2503.10639/x10.png", "caption": "Figure 9: Visualization on Multi-Guidance Strategy Hyper-parameter Selection. The above are text-to-image samples generated by GoT framework under different hyper-parameters.", "description": "This figure visualizes the impact of different hyperparameter settings within the GoT framework's multi-guidance strategy on the quality of text-to-image generation.  It shows examples of images generated using various combinations of \u03b1 and \u03b1s parameters, demonstrating how these settings affect the balance between semantic, spatial, and reference image guidance in the final output. The results illustrate the framework's ability to produce visually diverse outputs depending on the chosen hyperparameters.", "section": "5. GoT Framework: Reasoning-guided Visual Generation and Editing"}, {"figure_path": "https://arxiv.org/html/2503.10639/x11.png", "caption": "Figure 10: Examples of GoT dataset for text-to-image generation, including FLUX-GoT, JourneyDB-GoT, and Laion-Aesthetics-High-Resolution-GoT.", "description": "Figure 10 shows examples from three datasets used in the paper: FLUX-GoT, JourneyDB-GoT, and Laion-Aesthetics-High-Resolution-GoT. Each example displays the image prompt, the generated image, and the corresponding GoT (Generation Chain-of-Thought). The GoT is a detailed, structured description of the image's content and layout that is automatically created by the model.  It includes semantic information (what objects are present) as well as spatial information (where they are located). These examples illustrate the structure of the dataset and how the GoT reasoning chains guide the image generation process.", "section": "4. GoT Dataset: Semantic-Spatial Reasoning Chains for Visual Generation and Editing"}]