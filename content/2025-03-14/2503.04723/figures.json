[{"figure_path": "https://arxiv.org/html/2503.04723/extracted/6259524/FIG/long-comparison.png", "caption": "Figure 1: Difference between long-input and long-output LLMs.", "description": "This figure illustrates the key distinction between traditional long-input LLMs and the emerging long-output LLMs.  Long-input LLMs excel at processing extensive input contexts, as shown in the left panel, enabling tasks like question answering and document summarization that yield relatively short responses. Conversely, the right panel showcases long-output LLMs designed to generate extended, coherent, and contextually rich outputs.  These models are crucial for applications requiring longer-form content generation such as creative writing, lesson planning, and dialogue systems.", "section": "Long-Output LLMs"}, {"figure_path": "https://arxiv.org/html/2503.04723/x1.png", "caption": "Figure 2: Proportion of real-user demand: The aforementioned 2K (words) range refers to the interval [2K, 4K), and similarly for the other ranges. Solid color fill for input demand, slash fill for output.", "description": "This figure shows the proportion of real user demand for both input and output text lengths in four different ranges: [2K, 4K), [4K, 8K), [8K, 16K), and [16K, \u221e). The x-axis represents the word count ranges, and the y-axis represents the percentage of user requests falling within each range.  The bars are segmented into input (solid color) and output (slash fill) showing that demand for longer outputs significantly exceeds that for longer inputs.  The data highlights a significant discrepancy between the available resources focused on processing long inputs and the actual demand for generating long-form outputs. ", "section": "2. Long-Output LLMs"}, {"figure_path": "https://arxiv.org/html/2503.04723/x2.png", "caption": "Figure 3: ML and NLP Conf Long-context Research Trends Statistics (sorted by conference date). Solid color fill for Input-paper, slash fill for Output-paper.", "description": "This figure shows the number of research papers on long-context LLMs published in major ML and NLP conferences in 2024, categorized by whether the papers focused on long input or long output.  The x-axis represents the conference, sorted chronologically. The y-axis shows the count of papers.  Solid bars represent papers focusing on long input processing while slashed bars represent papers focusing on long output generation. The figure visually demonstrates the significant research emphasis on long input processing compared to the relatively limited research on long output generation.", "section": "3. Current State of Long-Output LLMs"}, {"figure_path": "https://arxiv.org/html/2503.04723/extracted/6259524/FIG/Umap_Test.png", "caption": "Figure 4: UMAP visualization results for different SFT datasets. WildChat is derived from the long output demands of real users, filtered and referenced in Section 2.1.", "description": "This UMAP visualization displays the relationships between different supervised fine-tuning (SFT) datasets used for training long-output LLMs. Each point represents a dataset, and the proximity of points indicates their similarity in terms of input-output characteristics. The dataset \"WildChat\", which is distinct and represented separately, is derived from actual user requests for long-form outputs.  The clustering suggests varying degrees of overlap between the SFT datasets, highlighting potential differences in their ability to effectively capture and represent real-world long-output task requirements. The unique positioning of WildChat illustrates the potential gap between the datasets typically used for training and the actual demands of real-world applications.", "section": "Data"}, {"figure_path": "https://arxiv.org/html/2503.04723/extracted/6259524/FIG/Umap_Test_2.png", "caption": "Figure 5: UMAP visualization results for different benchmark. We use the instructions from the benchmark to evaluate whether the benchmark assesses a wide range of long-output demand.", "description": "This UMAP visualization compares various long-output benchmarks against real-world user demands for long-form text generation.  It shows how well each benchmark captures the diversity of length and complexity in actual user requests. Benchmarks closely clustered with the real-world data points indicate a good representation of real-world long-output needs, while benchmarks far from the real-world cluster suggest a limited scope and potentially poor generalizability. The visualization helps assess the extent to which different benchmarks effectively evaluate the generation of long-form outputs.", "section": "5.2 Benchmarks"}, {"figure_path": "https://arxiv.org/html/2503.04723/x3.png", "caption": "Figure 6: We set the total context length to 12,000 and gradually increased the proportion of output tokens.", "description": "This figure illustrates the impact of increasing the proportion of output tokens within a fixed total context length of 12,000 tokens on the decoding duration (time taken for model to generate text). It shows that the longer the output sequence the longer the decoding time.  The experiment uses several different LLMs (Large Language Models) to demonstrate this trend, showing that decoding time increases linearly with the number of output tokens.", "section": "5.3 Train & Inference"}, {"figure_path": "https://arxiv.org/html/2503.04723/x4.png", "caption": "Figure 7: Proportion of real-user demand: The aforementioned 2K range refers to the interval [2K, 4K), and similarly for the other ranges. Solid color fill for input demand, slash fill for output demand in the Wildchat dataset.", "description": "Figure 7 illustrates the disproportionate demand for long-form outputs compared to long inputs in real-world scenarios.  The data comes from analyzing 100,000 user requests in the WildChat dataset. The x-axis categorizes requests by input and output length (in word count), grouped into ranges: [2K, 4K), [4K, 8K), [8K, 16K), and [16K+). The y-axis shows the frequency (count) of requests in each range.  Solid bars represent the number of requests with long inputs, while hatched bars represent the number of requests with long outputs of corresponding lengths.  The figure visually emphasizes the significantly higher demand for long outputs across all length categories, especially pronounced in the [4k, 8k) word range, indicating a critical need for improved long-form text generation capabilities in LLMs.", "section": "2. Long-Output LLMs"}, {"figure_path": "https://arxiv.org/html/2503.04723/extracted/6259524/FIG/LongWrite_ruler.png", "caption": "Figure 8: LongWriter-Ruler test demonstrates a\nmaximum output length limitation of approximately 2k words for all models tested.", "description": "The LongWriter-Ruler test evaluates the maximum output length various large language models can generate.  The results show that across different models, including GLM-4, Llama 2, Mistral, GPT-4, and Claude, there's a consistent limitation: none of the models could reliably generate text exceeding approximately 2,000 words. This suggests a significant constraint on current LLMs' ability to produce truly long-form outputs.", "section": "Current State of Long-Output LLMs"}]