[{"figure_path": "https://arxiv.org/html/2503.10582/x1.png", "caption": "Figure 1: Overview of our automated data curation approach and major experimental results.", "description": "This figure illustrates the VisualWebInstruct dataset creation process and its impact.  The pipeline starts with 30,000 seed images, uses Google Lens to find similar images on the web, extracts QA pairs from the resulting HTML, and filters for quality. The final dataset contains approximately 900k QA pairs.  The bar charts show the performance gains achieved by fine-tuning language models (LLaVA-CoT, Qwen2-VL, and MAmmoTH-VL2) on this new dataset compared to their performance on several benchmarks (MMMU, MMMU-PRO, MathVista, and DynaMath).  The improvements demonstrate the effectiveness of VisualWebInstruct in enhancing visual reasoning capabilities.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10582/x2.png", "caption": "Figure 2: Comprehensive Pipeline for VisualWebInstruct Dataset Generation. The workflow illustrates our multi-stage approach for creating high-quality multimodal instruction data. Stage 1: starting with seed images, we leverage Google Image search to identify relevant webpages, which are processed into accessibility trees. The raw QA pairs are extracted from the trees and refined through a post-processing step to ensure the vadality the data. Stage 2: we first generat multiple synthesized answers for consistency filtering, then align these with original web-sourced content to enhance the accuracy of the answers.", "description": "This figure details the two-stage pipeline used to create the VisualWebInstruct dataset. Stage 1 begins with seed images, uses Google Image Search to find related webpages, converts those pages into accessibility trees, extracts initial question-answer pairs, and then refines them. Stage 2 generates multiple synthesized answers to filter for consistency, aligning them with original web content to improve accuracy.", "section": "2. Stage 1: Mining Data from the Internet"}, {"figure_path": "https://arxiv.org/html/2503.10582/x3.png", "caption": "Figure 3: Example of Google Lens search functionality for circle geometry problems.", "description": "This figure shows how Google Lens was used to search for webpages containing images related to circle geometry problems.  The query image (a circle diagram) is shown, illustrating how Google Lens's image search capability was used to find relevant webpages for data collection. The process starts with a seed image as input, and the search engine returns a list of URLs to webpages containing visually similar content. These webpages were then used to extract the question answer pairs for creating the final dataset.", "section": "2. Google Image Searching"}, {"figure_path": "https://arxiv.org/html/2503.10582/x4.png", "caption": "Figure 4: Example of an accessibility tree structure extracted from an educational website.", "description": "Figure 4 shows an example of how the authors extract structured data from an educational website.  The figure presents an accessibility tree, a hierarchical representation of a webpage's content.  The tree nodes represent either textual content (questions, options, answers) or image elements.  The tree structure is used to extract question-answer pairs, preserving the relationship between textual components and related images.", "section": "2. Stage 1: Mining Data from the Internet"}, {"figure_path": "https://arxiv.org/html/2503.10582/x5.png", "caption": "Figure 5: Illustration of our consistency checking methodology.", "description": "This figure illustrates the process of ensuring answer consistency in the VisualWebInstruct dataset.  Multiple answers are generated for each question using GPT-40. Then, an LLM judge (also GPT-40) evaluates the consistency of these answers. Only questions with a majority of consistent answers are retained in the final dataset, improving the quality and reliability of the data.", "section": "3. Stage 2: Dataset Refinement"}]