[{"figure_path": "https://arxiv.org/html/2503.09642/x9.png", "caption": "Figure 1: Human preference evaluation of Open-Sora 2.0 against other leading video generation models. Win rate represents the percentage of comparisons where our model was preferred over the competing model. The evaluation is conducted on 100 prompts carefully designed to cover three key aspects: 1) visual quality, 2) prompt adherence, and 3) motion quality. Results show that our model performs favorably against other top-performing models in all three aspects.", "description": "This figure displays the results of a human preference evaluation comparing Open-Sora 2.0's video generation capabilities against several leading competitors.  The evaluation involved 100 diverse prompts carefully selected to assess three key aspects of video generation: visual quality, prompt adherence (how well the generated video reflects the prompt), and motion quality (smoothness and realism of movement).  The 'win rate' for Open-Sora 2.0 is presented as the percentage of times it was preferred over each competing model for each aspect.  The results visually demonstrate that Open-Sora 2.0 performs favorably overall.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09642/x10.png", "caption": "Figure 2: The hierarchical data filtering pipeline. The raw videos are first transformed into trainable video clips. Then, we apply various complimentary score filters to obtain data subsets for each training stage.", "description": "The figure illustrates the hierarchical data filtering pipeline used for video data preprocessing.  Raw videos are initially transformed into shorter, trainable video clips. Then, a series of complementary filters are sequentially applied.  These filters assess different aspects of video quality (aesthetic score, motion score, blurriness, presence of text, and camera jitter) and progressively remove lower-quality data. This process creates a data pyramid with smaller, higher-quality datasets at each stage that are then used for different phases of the video generation model training.", "section": "2 Data"}, {"figure_path": "https://arxiv.org/html/2503.09642/x11.png", "caption": "Figure 3: Distribution of key attributes of the whole video dataset.", "description": "This figure presents the distribution of key attributes within the video dataset used to train the Open-Sora 2.0 video generation model.  It showcases the statistical distribution of aesthetic scores (a measure of visual appeal), video durations (in seconds), aspect ratios (the ratio of video height to width), and caption lengths (number of words in the video descriptions). The distributions are visualized using pie charts, giving a clear overview of the data characteristics.", "section": "2 Data"}, {"figure_path": "https://arxiv.org/html/2503.09642/x12.png", "caption": "Figure 4: Word cloud of the video captions.", "description": "This word cloud visualizes the most frequent words appearing in the video captions of the dataset used to train the Open-Sora 2.0 video generation model.  It highlights the key themes and subjects prevalent within the videos, such as common objects ('person', 'hand', 'clothing'), actions ('wearing', 'standing'), settings ('background', 'outdoor'), and atmospheric conditions ('lighting', 'atmosphere'). The prominence of words like 'person' and 'individual' indicates a significant portion of the videos contain human subjects.", "section": "2.3 Data Statistics"}, {"figure_path": "https://arxiv.org/html/2503.09642/x13.png", "caption": "Figure 5: Architecture of Video DC-AE. (a) Overview of Video DC-AE: Each block in encoder introduces spatial downsampling, while temporal downsampling occurs at blocks 4 and 5, with a corresponding symmetric structure in the decoder. (b) Residual Connection in Video DC-AE Blocks.", "description": "Figure 5 illustrates the architecture of the Video DC-AE (Deep Compression Autoencoder), a crucial component of the Open-Sora 2.0 video generation model.  Part (a) provides a high-level overview of the encoder and decoder, highlighting the spatial and temporal downsampling strategies.  The encoder progressively reduces the spatial and temporal resolution of the input video, while the decoder symmetrically reconstructs the video from the compressed representation. Noteworthy is that temporal downsampling is specifically applied in blocks 4 and 5 of the encoder, indicating a focus on efficient compression of temporal information. Part (b) zooms in on the residual connections within the Video DC-AE blocks. These connections are designed to facilitate efficient gradient propagation during training and to improve the autoencoder's performance, especially at high compression ratios.", "section": "3 Model Architecture"}]