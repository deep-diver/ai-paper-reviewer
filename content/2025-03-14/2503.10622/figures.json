[{"figure_path": "https://arxiv.org/html/2503.10622/x1.png", "caption": "Figure 1: Left: original Transformer block. Right: block with our proposed Dynamic Tanh (DyT) layer. DyT is a straightforward replacement for commonly used Layer Norm\u00a0(Ba et\u00a0al., 2016) (in some cases RMSNorm\u00a0(Zhang and Sennrich, 2019)) layers. Transformers with DyT match or exceed the performance of their normalized counterparts.", "description": "This figure illustrates the architecture of a standard Transformer block and a modified version incorporating the proposed Dynamic Tanh (DyT) layer. The left panel depicts a traditional Transformer block, highlighting its core components: Attention/Feed-Forward Network (Attention/FFN), Layer Normalization (LN), and scale & shift operations. The right panel shows the integration of the DyT layer as a direct replacement for LN.  The DyT layer functions as a learnable element-wise operation, approximating the behavior of LN and RMSNorm (another normalization technique) while omitting the computationally expensive statistical calculations. The caption emphasizes that using DyT instead of LN results in similar or superior performance in Transformers.", "section": "4 Dynamic Tanh (DyT)"}, {"figure_path": "https://arxiv.org/html/2503.10622/x2.png", "caption": "Figure 2: Output vs. input of selected layer normalization (LN) layers in Vision Transformer (ViT)\u00a0(Dosovitskiy et\u00a0al., 2020), wav2vec 2.0 (a Transformer model for speech)\u00a0(Baevski et\u00a0al., 2020), and Diffusion Transformer (DiT)\u00a0(Peebles and Xie, 2023).\nWe sample a mini-batch of samples and plot the input / output values of four LN layers in each model. The outputs are before the affine transformation in LN. The S\ud835\udc46Sitalic_S-shaped curves highly resemble that of a tanh function (see Figure\u00a03). The more linear shapes in earlier layers can also be captured by the center part of a tanh curve. This motivates us to propose Dynamic Tanh (DyT) as a replacement, with a learnable scaler \u03b1\ud835\udefc\\alphaitalic_\u03b1 to account for different scales on the x\ud835\udc65xitalic_x axis.", "description": "This figure displays scatter plots visualizing the relationship between input and output values for selected layer normalization (LN) layers within three different Transformer models: Vision Transformer (ViT), wav2vec 2.0, and Diffusion Transformer (DiT).  For each model, four different LN layers are represented. The x-axis represents the input values to the LN layer, and the y-axis shows the corresponding output values (before the affine transformation within the LN layer). The plots reveal that the input-output mappings of the LN layers often exhibit a characteristic S-shaped or tanh-like curve, particularly in deeper layers, which is a key observation in the paper that motivates the proposed Dynamic Tanh (DyT) method as a more efficient and effective replacement for the LN layer.", "section": "3 What Do Normalization Layers Do?"}, {"figure_path": "https://arxiv.org/html/2503.10622/extracted/6278329/figures/inout_color.png", "caption": "Figure 3: tanh\u2061(\u03b1\u2062x)\ud835\udefc\ud835\udc65\\tanh(\\alpha x)roman_tanh ( italic_\u03b1 italic_x ) with three different \u03b1\ud835\udefc\\alphaitalic_\u03b1 values.", "description": "This figure shows plots of the function y = tanh(\u03b1x) for three different values of \u03b1 (alpha).  The parameter \u03b1 controls the steepness of the curve. A larger \u03b1 value results in a steeper curve, while a smaller \u03b1 value results in a shallower curve. The tanh function itself is a sigmoid function that maps values from negative infinity to positive infinity to a range between -1 and 1. This function's shape is similar to the input-output relationship observed in layer normalization (LN) layers of Transformer models, which inspired the use of the Dynamic Tanh (DyT) function in the paper. The figure visually illustrates how the DyT function with its learnable parameter \u03b1 can mimic the behavior of LN layers in squashing extreme values while maintaining a relatively linear response in the central region.", "section": "What Do Normalization Layers Do?"}, {"figure_path": "https://arxiv.org/html/2503.10622/x9.png", "caption": "Figure 4: Output vs. input of two LN layers, with tensor elements colored to indicate different channel and token dimensions. The input tensor has a shape of (samples, tokens, and channels), with elements visualized by assigning consistent colors to the same tokens (left two panels) and channels (right two panels). Left two panels: points representing the same token (same color) form straight lines across different channels, as LN operates linearly across channels for each token. Interestingly, when plotted collectively, these lines form a non-linear tanh-shaped curve.\nRight two panels: each channel\u2019s input spans different ranges on the x\ud835\udc65xitalic_x-axis, contributing distinct segments to the overall tanh-shaped curve. Certain channels (e.g., red, green, and pink) exhibit more extreme x\ud835\udc65xitalic_x values, which are squashed by LN.", "description": "Figure 4 visualizes the input-output relationships of layer normalization (LN) layers in a vision transformer model.  The plots show how LN processes inputs by separating and handling data across token and channel dimensions. The left two panels highlight that LN acts linearly across channels for each token.  Individual tokens (same color) plot as straight lines. However, the collection of these lines creates a nonlinear, S-shaped (tanh-like) curve.  The right two panels illustrate the different input ranges across individual channels. Some channels have more extreme input values that are squashed by LN, resulting in the S-shaped curve. This visualization clarifies LN's dual nature of linear processing per token and nonlinear squashing of extremes across all channels.", "section": "3 What Do Normalization Layers Do?"}, {"figure_path": "https://arxiv.org/html/2503.10622/x12.png", "caption": "Figure 5: Training loss curves for ViT-B and ConvNeXt-B models. The loss curves for both model types exhibit similar patterns between LN and DyT, suggesting that LN and DyT may share similar learning dynamics.", "description": "This figure displays the training loss curves for two different models, Vision Transformer (ViT-B) and ConvNeXt-B, both trained using two different normalization methods: Layer Normalization (LN) and Dynamic Tanh (DyT).  The curves for both models and both normalization techniques show remarkably similar patterns and rates of convergence,  suggesting a strong similarity in how these different methods affect the training process. This implies that DyT, a much simpler normalization technique, might effectively replicate the behavior and benefits of the more complex LN during training.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10622/x13.png", "caption": "Figure 6: LLaMA pretraining loss. The loss curves of DyT and RMSNorm models are closely aligned across model sizes.", "description": "This figure displays the training loss curves for four different sizes of the LLaMA large language model (LLaMA 7B, 13B, 34B, and 70B) during pretraining.  Each model size is shown with two curves: one using RMSNorm (the original normalization method) and another using the proposed DyT (Dynamic Tanh) method. The closeness of the curves for each model size shows that DyT achieves comparable performance to RMSNorm during training, regardless of model size.", "section": "Large Language Models"}]