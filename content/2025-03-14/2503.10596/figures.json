[{"figure_path": "https://arxiv.org/html/2503.10596/x1.png", "caption": "Figure 1: GSScuplt Automatic Annotation Framework. Our pipeline consists of three sequential phases: (1) Entity Spatial Localization, where we first identify potential objects of interest and generate high-quality segmentation masks; (2) Grounding Text Generation, where we then create unambiguous natural language descriptions that uniquely reference the segmented objects; and (3) Noise Filtering, where we finally eliminate ambiguous or low-quality samples to ensure dataset reliability.", "description": "The figure illustrates the GSSculpt automatic annotation framework, a three-phase pipeline for generating high-quality pixel grounding data. Phase 1 (Entity Spatial Localization) identifies objects of interest and creates accurate segmentation masks using a combination of global caption generation, phrase grounding, and mask generation techniques.  Phase 2 (Grounding Text Generation) produces precise and unambiguous natural language descriptions for these objects, ensuring unique identification. Finally, Phase 3 (Noise Filtering) removes ambiguous or low-quality annotations, enhancing the overall reliability of the resulting dataset. Each phase is visually represented with example images and annotations.", "section": "3. GSSculpt: Large-scale Grounding Labeling"}, {"figure_path": "https://arxiv.org/html/2503.10596/x2.png", "caption": "Figure 2: Curation pipeline for GSEval Benchmark.\nFirst, we apply our annotation pipeline to unlabeled COCO images. Next, we use a VLM classifier to ensure the categories of referring prompts. Then, we translate the coarse masks to trimaps and apply matting methods for precise object boundaries. Finally, we organize human reviewers for manual checks.", "description": "This figure details the process of creating the GSEval benchmark dataset.  It begins with applying an automated annotation pipeline to unlabeled images from the COCO dataset. A Vision-Language Model (VLM) classifier then verifies the categories of the generated referring expressions.  The process continues by refining the initial segmentation masks (which may be coarse) into precise trimaps, and then applying matting techniques to achieve accurate object boundaries. Finally, human reviewers conduct a manual review of the results to ensure data quality and accuracy.", "section": "4. GSEval: A Comprehensive Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.10596/x3.png", "caption": "Figure 3: The visualization comparisons of different methods on GSEval. All methods are evaluated under the zero-shot setting with the public code and weights.", "description": "Figure 3 displays a comparison of different methods' performance on the GSEval benchmark.  Each method was evaluated using publicly available code and pre-trained weights, without any fine-tuning or adaptation for this specific dataset. The zero-shot evaluation ensures that the comparison reflects the inherent capabilities of each model in handling unseen data, with all methods operating under the same conditions.", "section": "5. Evaluation on GSEval"}, {"figure_path": "https://arxiv.org/html/2503.10596/x4.png", "caption": "Figure 4: The visualization comparisons of differnet methods on our GSEval-BBox. All open-source methods are evaluated under the zero-shot setting with the public code and weights.", "description": "Figure 4 presents a comparison of several methods' performance on the GSEval-BBox benchmark, focusing on bounding box accuracy.  Specifically, it visually illustrates the results of different models (InstructSeg, LISA, EVF-SAM) in localizing objects within images, highlighting the differences in their accuracy and ability to precisely identify bounding boxes according to the given textual descriptions. All models were evaluated using publicly available code and weights under zero-shot settings.", "section": "5. Evaluation on GSEval"}, {"figure_path": "https://arxiv.org/html/2503.10596/extracted/6278345/fig/wordcloud.png", "caption": "Figure 5: The word cloud of GSEval", "description": "This word cloud visualizes the frequency of words used in the textual descriptions within the GSEval benchmark dataset.  It offers a quick overview of the types of objects, attributes, and relationships described in the dataset, highlighting the diversity and complexity of language used to refer to visual elements within the images.  The size of each word corresponds to its frequency of occurrence.", "section": "4. GSEval: A Comprehensive Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.10596/x5.png", "caption": "Figure 6: More selected samples from our GSEval. Stuff class and part level", "description": "Figure 7 presents a diverse set of examples from the GSEval benchmark, showcasing the variety of \"stuff\" class and part-level annotations.  The images illustrate the complexity of real-world scenes and the challenges in accurately identifying and segmenting objects based on natural language descriptions. The \"stuff\" class examples highlight the difficulty of segmenting amorphous regions, like 'sky' or 'water', while the part-level examples demonstrate the need for fine-grained understanding to locate and segment specific parts of objects, such as 'a dog's mouth' or 'a woman's hair'.  These examples emphasize the nuanced nature of pixel grounding and the importance of a robust benchmark for evaluating model performance.", "section": "4. GSEval: A Comprehensive Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.10596/x6.png", "caption": "Figure 7: More selected samples from our GSEval. Multi object and single object", "description": "Figure 7 presents additional examples from the GSEval benchmark dataset, showcasing both multi-object and single-object scenarios.  The images illustrate the diversity of scenes and object types included in the dataset, demonstrating its ability to evaluate models across various complexities in visual grounding tasks.", "section": "4. GSEval: A Comprehensive Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.10596/x7.png", "caption": "Figure 8: Prompt for global caption and grounding text generation", "description": "This figure shows the prompt templates used in the GroundingSuite framework for generating global image captions and grounding texts for object localization.  The global caption prompt instructs the model to produce a concise, single-paragraph description of the image, avoiding speculation and focusing only on clearly visible elements. The grounding text generation prompt is more specific, asking for a short description of a particular object (identified by its category name and bounding box coordinates). It emphasizes clarity and distinction from other objects, providing detailed instructions on handling multiple objects or incorrect classifications.", "section": "3. GSSculpt: Large-scale Grounding Labeling"}, {"figure_path": "https://arxiv.org/html/2503.10596/x8.png", "caption": "Figure 9: Prompt for noise filtering", "description": "Figure 9 shows the prompts used in the noise filtering stage of the GSSculpt framework.  Three prompts are displayed, targeting different aspects of quality control. The first prompt is for a Vision-Language Model (VLM) to assess if a red-box mask accurately represents the referring expression.  It specifies criteria for accurate annotation, such as object consistency, completeness, and avoidance of redundancy. The second prompt is for classifying the referring expression into one of four categories: stuff, part, multi-object, or single object.  The third prompt provides instructions on providing a concise numerical classification (1-4) without detailed analysis. These prompts aim to efficiently filter out inaccurate or ambiguous annotations during dataset creation.", "section": "3. GSSculpt: Large-scale Grounding Labeling"}]