{"references": [{"fullname_first_author": "DeepSeek-AI", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper introduces the DeepSeek-R1 model, which this paper aims to replicate and surpass."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper is important because it introduces the chain-of-thought prompting technique that enables large language models to perform complex reasoning."}, {"fullname_first_author": "Qwen", "paper_title": "Qwq-32b: Embracing the power of reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper is a reference point for comparing reinforcement learning techniques in large language models."}, {"fullname_first_author": "Zhihong Shao", "paper_title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models", "publication_date": "2024-02-01", "reason": "This paper represents related work in improving mathematical reasoning capabilities of language models."}, {"fullname_first_author": "Alon Albalak", "paper_title": "Big-math: A large-scale, high-quality math dataset for reinforcement learning in language models", "publication_date": "2025-02-01", "reason": "This paper introduces the Big-Math dataset, which is used in the current paper for reinforcement learning."}]}