{"references": [{"fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model.", "publication_date": "2023-05-18", "reason": "This paper introduces Direct Preference Optimization (DPO), which is the foundation of the D2PO framework used in the main paper."}, {"fullname_first_author": "Shidhar et al.", "paper_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks.", "publication_date": "2019-01-01", "reason": "This paper introduces ALFRED, a benchmark for interpreting grounded instructions, and the VoTa-Bench dataset used in the main paper is based on it."}, {"fullname_first_author": "Yao et al.", "paper_title": "UNKNOWN", "publication_date": "2022-01-01", "reason": "This paper is important as an early LLM-based method for embodied task planning."}, {"fullname_first_author": "Sutton", "paper_title": "UNKNOWN", "publication_date": "1990-01-01", "reason": "This paper is important because it establishes the theoretical foundation of the world model, a key component in this paper's approach."}, {"fullname_first_author": "Chen et al.", "paper_title": "UNKNOWN", "publication_date": "2024-01-01", "reason": "This paper offers insights on how to conduct and evaluate multimodal assessments, and it also focuses on SFT from expert demonstrations."}]}