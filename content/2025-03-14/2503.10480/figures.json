[{"figure_path": "https://arxiv.org/html/2503.10480/x1.png", "caption": "Figure 1: Overview of D2PO: World modeling enables better embodied task planning through joint preference optimization of state prediction and action selection.", "description": "The figure illustrates the Dual Preference Optimization (D2PO) framework.  D2PO jointly optimizes two key components: a state prediction model (world model) that learns to forecast how the environment changes over time, and an action selection model (policy model) that learns to choose optimal actions. These models are trained using preference learning to predict the better next state and better next action. The combined result is a system that is better able to plan embodied tasks because it understands the dynamic nature of the environment, rather than relying on just static snapshots of the world. The framework receives perception from the environment, then uses a policy model and a world model to determine an action which then changes the environment state.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10480/x2.png", "caption": "Figure 2: Our method consists of two dimensions: (a) Data Exploration via Step-wise Tree Search (Sec 3.2), which collects preference data through sampling and selecting potential actions, iterative tree expansion, and trajectory backtracking; (b) Dual Preference Optimization (D2PO) framework (Sec 3.3) that leverages the collected preference pairs to jointly optimize action selection and state prediction.", "description": "This figure illustrates the two main components of the proposed method: Data Exploration and Dual Preference Optimization.  The Data Exploration component (a) uses a step-wise tree search to systematically explore possible action sequences in the environment. This involves sampling potential actions, iteratively expanding the search tree, and backtracking when necessary. This process automatically collects preference data, comparing chosen actions and their outcomes to alternatives. The Dual Preference Optimization (D2PO) component (b) then leverages the collected preference pairs to jointly optimize both state prediction (world modeling) and action selection. This allows the model to better understand the environment's dynamics and plan more effectively.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2503.10480/x3.png", "caption": "(a) Impact of data scale on performance (SR).", "description": "This figure shows the relationship between the amount of training data used and the success rate (SR) achieved by three different methods: Standard Fine-tuning (SFT), Direct Preference Optimization (DPO), and Dual Preference Optimization (D2PO).  The x-axis represents different data scales, and the y-axis represents the success rate.  The results demonstrate that D2PO consistently outperforms the other two methods across all data scales, showcasing its ability to leverage data effectively. There is also a slight non-monotonic trend in the D2PO performance at larger data sizes, which might be due to overfitting.", "section": "Further Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10480/x4.png", "caption": "(b) Impact of model scale on performance (SR).", "description": "This figure shows the relationship between the success rate (SR) and the model size in various embodied task planning models. The larger the model size, the higher the success rate is.  The results are presented using bar charts, with different models and approaches (SFT, DPO, D2PO) clearly differentiated.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2503.10480/x5.png", "caption": "Figure 3: Analysis of data scale and model scale.", "description": "This figure presents a dual analysis of the impact of data scale and model scale on the performance of the proposed D2PO method. Subfigure (a) shows how the success rate (SR) changes with varying amounts of training data, indicating the relationship between data size and model performance. Subfigure (b) demonstrates how the SR changes with varying model sizes.  It allows for a comparison of the effectiveness of D2PO across different data and model scales.", "section": "5 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10480/x6.png", "caption": "Figure 4: Success rates (SR) of action-conditioned and goal-directed world models across seen and unseen scenarios.", "description": "This figure compares the success rates (SR) of two different types of world models: action-conditioned and goal-directed.  The action-conditioned model predicts the next state based on the current state and the chosen action, while the goal-directed model predicts the future states based on the goal and history of past states and actions. The comparison is performed for both 'seen' (familiar) and 'unseen' (novel) scenarios to evaluate the generalization ability of each model.  The results show that while the action-conditioned model performs better on seen scenarios, the goal-directed model generalizes better to unseen scenarios.", "section": "5.3 Action-conditioned v.s. Goal-directed World Modeling"}, {"figure_path": "https://arxiv.org/html/2503.10480/x7.png", "caption": "(a) ALFRED (high-level planning) (Shridhar et\u00a0al., 2019)", "description": "The figure shows a comparison of high-level task planning in ALFRED. ALFRED uses step-by-step instructions, breaking the task into subgoals. The example shows the task of placing a cold tomato in the sink.  ALFRED decomposes this into finding the counter top, picking up the tomato, finding the fridge, cooling the tomato, finding the sink, and finally putting down the tomato. Each step is depicted with images from the simulation.", "section": "4.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2503.10480/x8.png", "caption": "(b) LoTa-Bench (Choi et\u00a0al., 2024)", "description": "This figure shows the task decomposition in LoTa-Bench.  It illustrates that the high-level goal is broken down into a sequence of simpler, executable actions for an embodied AI agent to follow within a simulated environment. The example shows that, for the task of placing a cold tomato in the sink, LoTa-Bench decomposes the task into more fine-grained steps than ALFRED (another dataset). For example, it involves finding the tomato, picking it up, finding the fridge, opening it, putting the tomato inside, closing the fridge, finding the sink, and finally placing the tomato in the sink. This decomposition makes the task easier to complete for agents but also makes it less realistic compared to a human's approach.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2503.10480/x9.png", "caption": "(c) VoTa-Bench (ours)", "description": "This figure shows a comparison of three different embodied task planning benchmarks: ALFRED, LoTa-Bench, and the proposed VoTa-Bench.  Each benchmark is illustrated with the example task of placing a cold tomato in a sink.  ALFRED uses detailed step-by-step instructions, LoTa-Bench uses only a high-level goal instruction, and VoTa-Bench incorporates both a high-level goal instruction and egocentric visual observations at each step, providing a more realistic and challenging evaluation of embodied AI systems.", "section": "4.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2503.10480/x10.png", "caption": "Figure 5: Comparison of ALFRED, LoTa-Bench, and VoTa-Bench in the task \u201cPlace a cold tomato in the sink\u201d. (a) ALFRED emphasizes high-level task planning with human-written step-by-step instructions, breaking the task into subgoals like \u201cCool Tomato\u201d (step 4). (b) LoTa-Bench provides only goal instructions and decomposes tasks into fine-grained low-level actions (e.g., \u201cOpen Fridge\u201d, \u201cPutDown Tomato\u201d, etc.; steps 4\u201310) but lacks guidance from visual input, relying on predefined executable actions, choosing actions based on maximum logits to ensure they are valid in the simulation. (c) VoTa-Bench extends LoTa-Bench by incorporating egocentric visual observations, requiring models to generate open-domain actions based on visual information to handle both seen and unseen environments.", "description": "Figure 5 compares three different approaches to embodied task planning using the example task \"Place a cold tomato in the sink\".  ALFRED (a) uses high-level instructions broken down into sub-goals (like 'Cool Tomato'). LoTa-Bench (b) uses only a goal instruction and breaks the task into very specific, low-level actions, but lacks visual input, relying on pre-defined actions. VoTa-Bench (c), the proposed method, extends LoTa-Bench by adding egocentric visual input, requiring the model to generate more open-ended actions based on both the goal and visual observations. This allows it to handle both seen and unseen environments.", "section": "4.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2503.10480/x11.png", "caption": "(a) Seen Scenes", "description": "This figure shows examples of scenes from the VoTa-Bench dataset used in the experiments.  Panel (a) specifically displays examples of *seen* scenes, meaning these scene layouts and object arrangements were present in the training data for the models.  The figure helps to illustrate the visual environment the embodied AI agents are interacting with. The visual information is crucial input to the models in this embodied task planning research.", "section": "4.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190909_112854_740612_1_fail.png", "caption": "(b) Unseen Scenes", "description": "This figure shows example images of unseen scenes from the VoTa-Bench dataset.  These scenes represent environments not included in the training data, and are used to evaluate the model's generalization ability to novel and unseen layouts and object configurations within the AI2-THOR simulator.", "section": "4.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190909_112854_740612_1_success.png", "caption": "Figure 6: Examples of seen and unseen scenes.", "description": "This figure visualizes example scenes from the VoTa-Bench dataset, showcasing both \"seen\" and \"unseen\" environments.  Seen scenes represent environments with layouts and object distributions similar to those in the training data, allowing the model to leverage prior experience. In contrast, unseen scenes present novel layouts and object arrangements that the model hasn't encountered during training. This distinction helps illustrate the generalization capabilities of embodied AI models. The figure demonstrates the dataset's diversity in scene arrangement and object placement, highlighting the challenges and opportunities for more robust AI models that can handle unseen situations effectively.", "section": "4.1 Experimental Settings"}, {"figure_path": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190908_070946_578973_2_success.png", "caption": "Figure 7: Distribution of the SFT and DPO dataset across different task types.", "description": "Figure 7 presents a comparative analysis of the dataset distributions for two distinct training methods: Supervised Fine-Tuning (SFT) and Dual Preference Optimization (DPO).  The figure uses a bar chart to visually represent the proportion of each task type within each dataset.  The task types include 'Examine & Light', 'Pick & Place', 'Stack & Place', 'Clean & Place', 'Heat & Place', and 'Cool & Place'. By comparing the distributions, we can gain insights into whether the two methods exhibit similar or distinct preferences in terms of task complexity or types of interaction.", "section": "5.4 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10480/extracted/6278086/imgs/trial_T20190908_070946_578973_2_success_1.png", "caption": "(a) SFT Trajectory (Fail)", "description": "This figure shows a sequence of images depicting the steps taken by a model trained using supervised fine-tuning (SFT) while attempting a specific task.  The trajectory ultimately fails to complete the task successfully, highlighting issues such as incorrect action sequencing and a lack of understanding of task dependencies. Each image represents a step in the process, and the caption indicates that the attempt is unsuccessful.  The figure is used to contrast the performance of the SFT model with models trained using other methods, thereby showcasing the effectiveness of the proposed approach.", "section": "4.3 Generalization: Unseen Scene"}]