[{"content": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">Qwen2.5-7B-Instruct</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">ARWKV</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">active MLP</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S4.T1.1.1.1.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">w/ gate &amp; active MLP</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">ARWKV-from32B</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T1.1.2.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">MMLU</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T1.1.2.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">71.72</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T1.1.2.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">62.41</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T1.1.2.1.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">58.22</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S4.T1.1.2.1.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">64.77</td>\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T1.1.2.1.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">61.78</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">Squad</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.2.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">47.89</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.2.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">40.05</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.2.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">40.35</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.3.2.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">38.74</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.3.2.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">39.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.3.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">GPQA(Diamond)</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.3.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">49.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.3.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">45.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.3.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">51.1</td>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S4.T1.1.4.3.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S4.T1.1.4.3.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.5.4.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">WinoGrande</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.5.4.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">71.35</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.5.4.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">68.67</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.5.4.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">69.67</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.5.4.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">68.98</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.5.4.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">68.35</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.6.5.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">GSM8K</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.6.5.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">82.34</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.6.5.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">39.95</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.6.5.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">51.93</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.6.5.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">47.99</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.6.5.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">43.44</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.7.6.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">IfEval</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.7.6.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">73.62</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.7.6.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">52.16</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.7.6.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">48.68</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S4.T1.1.7.6.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">52.16</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.7.6.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">44.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.8.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.8.7.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">Arc-c</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.8.7.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">54.86</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.8.7.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">52.22</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.8.7.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">53.52</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" id=\"S4.T1.1.8.7.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">52.22</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"S4.T1.1.8.7.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">50.77</td>\n</tr>\n</tbody>\n</table>", "caption": "Table 1: This is a ongoing work, benchmark based on stage-2 , currently we limit the context length to 2048 and use same datasets", "description": "This table presents the performance of different language models on various benchmark datasets.  The models compared include the ARWKV model (in several variations), and the Qwen2.5-7B-Instruct model. The evaluation is based on results from Stage 2 of the model training process.  Note that this is ongoing research, and the context length for the benchmark is currently limited to 2048 tokens. All models were tested on the same datasets.", "section": "4 Evaluation"}]