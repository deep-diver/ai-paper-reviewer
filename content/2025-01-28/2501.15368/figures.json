[{"figure_path": "https://arxiv.org/html/2501.15368/extracted/6156374/figures/performance.png", "caption": "Figure 1: Evaluation across image, video, and audio modalities. (Left) Baichuan-Omni-1.5\u00a0covers more modalities than Qwen2 VL\u00a0[142] and outperforms the current leading omni-modal model, VITA-1.5\u00a0[45] and MiniCPM-o 2.6[165].\n(Right) Average scores across benchmarks for all modalities. All the scores are normalized by xnorm=(x\u2212xmin+10)/(xmax\u2212xmin+10)subscript\ud835\udc65norm\ud835\udc65subscript\ud835\udc65min10subscript\ud835\udc65maxsubscript\ud835\udc65min10x_{\\text{norm}}=(x-x_{\\text{min}}+10)/(x_{\\text{max}}-x_{\\text{min}}+10)italic_x start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT = ( italic_x - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ) / ( italic_x start_POSTSUBSCRIPT max end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT min end_POSTSUBSCRIPT + 10 ).", "description": "Figure 1 presents a comparative analysis of Baichuan-Omni-1.5 against other leading multimodal models (Qwen2-VL, VITA-1.5, and MiniCPM-0 2.6) across various modalities (image, video, and audio).  The left panel is a radar chart visualizing the relative performance of these models on multiple benchmarks.  It highlights that Baichuan-Omni-1.5 surpasses the others in terms of both the number of modalities it supports and its overall performance. The right panel displays average benchmark scores, again demonstrating Baichuan-Omni-1.5's superiority. Note that all scores are normalized using a specified formula to ensure fair comparison.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.15368/x1.png", "caption": "Figure 2: Architecture of Baichuan-Omni-1.5\u00a0. Our model is designed to process both pure text/audio inputs and combinations of video/image with text/audio. When generating audio, the Baichuan-Omni-1.5 LLM Decoder alternately predicts text tokens and audio tokens. The audio tokens are then decoded by the Audio Decoder to produce the final audio.", "description": "Baichuan-Omni-1.5 is a multimodal model that can process various input modalities (text, audio, image, and video) and generate corresponding outputs (text and audio).  The figure details the model's architecture.  It shows how visual and audio information is encoded and fed into a shared LLM decoder. This decoder alternately generates text and audio tokens. The audio tokens are then passed to an audio decoder to generate the final audio output. This allows for end-to-end audio generation and seamless integration across modalities.", "section": "3.2 Model Architecture"}, {"figure_path": "https://arxiv.org/html/2501.15368/extracted/6156374/figures/data_example.png", "caption": "Figure 3: Pretrain Data illustration of Baichuan-Omni-1.5\u00a0. We construct an extensive omni-modal dataset, including text, image-text, video-text, audio-text, and their interactions. Our collection also contains interleaved image-audio-text and video-audio-text data.", "description": "Figure 3 illustrates the diverse multimodal data used to pre-train Baichuan-Omni-1.5.  The dataset includes various data types: pure text, image-text pairs, video-text pairs, and audio-text pairs.  Importantly, it also incorporates more complex interleaved data formats, such as image-audio-text triplets and video-audio-text triplets. This rich and varied dataset allows the model to learn comprehensive cross-modal relationships and achieve a high level of multimodal understanding.", "section": "3.1 High-Quality Multimodal Pretrain Data"}, {"figure_path": "https://arxiv.org/html/2501.15368/x2.png", "caption": "Figure 4: Audio tokenizer and audio decoder based on flow matching model.", "description": "This figure illustrates the architecture of the Baichuan-Omni-1.5 model's audio branch.  It uses a Residual Vector Quantization (RVQ) based audio tokenizer to convert raw audio into discrete tokens, which are then processed by a Large Language Model (LLM).  The audio tokens are further decoded into a speech waveform using a flow-matching model based on a U-Net architecture and a HiFi-GAN vocoder.  The model utilizes a multi-stage training strategy to improve speech quality, incorporating techniques such as multi-scale Mel loss. The figure highlights the different components of the audio processing pipeline and their interactions, showing how raw audio is transformed into meaningful representations understandable by the LLM, then ultimately converted back into speech.", "section": "3.2 Model Architecture"}, {"figure_path": "https://arxiv.org/html/2501.15368/x3.png", "caption": "Figure 5: Training Pipeline of Baichuan-Omni-1.5\u00a0. The pretraining phase is divided into three stages to incrementally incorporate vision and audio into the LLM while relieving modality conflicts. Stage 1 focuses on image-text training, which extends an LLM to process and understand visual input. Stage 2 extends an LLM pre-trained on visual data to understand audio input in end-to-end manner by incorporating our Baichuan-Audio-Tokenizer, a newly introduced audio embedding layers and an independent audio head. Stage 3 focuses on training Baichuan-Omni-1.5\u00a0using high-quality cross-modal interaction datasets encompassing image-audio-text and video-audio-text format, and extends the maximum sequence length to 64k to support long audio and video stream. Stage 4 enhances the model\u2019s instruction following and audio capabilities through supervised fine-tuning with omni-modal data. Stage 4.1: Freeze the Audio Head using omni-modal understanding data to boost modality interactivity and multitasking comprehension. Stage 4.2: Activate only the Audio Head and Audio Embed layer, with audio generation data to improve speech generation capabilities.", "description": "This figure illustrates the training pipeline of the Baichuan-Omni-1.5 model.  The training process is divided into four stages. Stage 1 pre-trains the model on image-text data to enable visual understanding. Stage 2 incorporates audio using a newly designed audio tokenizer and embedding layer, enabling end-to-end audio processing. Stage 3 integrates image, audio, video, and text data to achieve comprehensive omni-modal understanding, extending the model's maximum sequence length for handling longer inputs.  Finally, Stage 4 fine-tunes the model through supervised learning using omni-modal data, with specific sub-stages focusing on improving both instruction following and audio generation capabilities.", "section": "3 Baichuan-Omni-1.5"}]