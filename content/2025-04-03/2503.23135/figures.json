[{"figure_path": "https://arxiv.org/html/2503.23135/x1.png", "caption": "Figure 1: The mechanism of self attention (a) and convolution (b). (c) shows that the human vision system can \u201cSee Large\u201d through the peripheral vision, and \u201cFocus Small\u201d through the central vision. (d) shows the distribution of rods and cones depending on the eccentricity from the fovea of the human eye. They contribute to the formation of extensive peripheral vision and focal central vision.", "description": "Figure 1 illustrates the mechanisms of self-attention and convolution, highlighting their differences in perception and aggregation.  Panel (a) shows self-attention's global perception and aggregation, while (b) depicts convolution's localized approach.  Panel (c) contrasts this with the human visual system's ability to simultaneously 'see large' (peripheral vision) and 'focus small' (central vision).  This dual-scale processing is enabled by the different distribution and functions of rod and cone cells in the retina, as detailed in panel (d), which shows the density of these cells across the retina's eccentricity from the fovea.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.23135/x2.png", "caption": "Figure 2: Comparison of self-attention, convolution, and LS conv.", "description": "This figure compares the token mixing mechanisms of self-attention, convolution, and the proposed LS convolution.  It illustrates how each method handles perception (capturing contextual relationships) and aggregation (integrating token features) differently.  Self-attention uses global perception and aggregation, convolution uses local perception and aggregation with fixed kernel weights, and LS convolution combines large-kernel perception for a broad overview with small-kernel dynamic aggregation for precise feature fusion in local regions. This approach mimics the human visual system\u2019s ability to simultaneously \u2018see large\u2019 and \u2018focus small.\u2019", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.23135/x3.png", "caption": "Figure 3: (a) The illustration of our proposed LS convolution. (b) The illustration of our proposed LSNet. LSNet has four stages with H8\u00d7W8\ud835\udc3b8\ud835\udc4a8\\frac{H}{8}\\times\\frac{W}{8}divide start_ARG italic_H end_ARG start_ARG 8 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 8 end_ARG, H16\u00d7W16\ud835\udc3b16\ud835\udc4a16\\frac{H}{16}\\times\\frac{W}{16}divide start_ARG italic_H end_ARG start_ARG 16 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 16 end_ARG, H32\u00d7W32\ud835\udc3b32\ud835\udc4a32\\frac{H}{32}\\times\\frac{W}{32}divide start_ARG italic_H end_ARG start_ARG 32 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 32 end_ARG, and H64\u00d7W64\ud835\udc3b64\ud835\udc4a64\\frac{H}{64}\\times\\frac{W}{64}divide start_ARG italic_H end_ARG start_ARG 64 end_ARG \u00d7 divide start_ARG italic_W end_ARG start_ARG 64 end_ARG resolutions respectively, where H\ud835\udc3bHitalic_H and W\ud835\udc4aWitalic_W denote the width and height of the input image. C\ud835\udc36Citalic_C represents the channel dimension. The norm layer and nonlinearity are omitted for simplicity.", "description": "Figure 3(a) shows the LS convolution which combines large-kernel perception and small-kernel aggregation to efficiently capture a wide range of perceptual information and achieve precise feature aggregation. Figure 3(b) shows the LSNet architecture which uses LS convolution as the building block. The network consists of four stages with progressively decreasing resolutions (H/8*W/8, H/16*W/16, H/32*W/32, H/64*W/64), each stage comprising multiple LS blocks.  The number of channels increases with each stage.  Normalization and activation functions are omitted for simplicity.", "section": "3.3 LSNet: Large-Small Network"}, {"figure_path": "https://arxiv.org/html/2503.23135/x4.png", "caption": "(a)", "description": "This figure shows a comparison of three token mixing approaches in lightweight vision networks: self-attention, convolution, and the proposed LS convolution.  Self-attention (a) uses a global perception of all tokens, and a global aggregation, which can be inefficient with many tokens and insensitive to less important connections. Convolution (b) uses local perception and aggregation, using a fixed kernel, leading to limited perception range and contextual insensitivity. LS convolution (c) combines large-kernel perception to capture a wide range of context, followed by small-kernel aggregation which focuses on more precise features, mimicking the human vision system\u2019s ability to see large areas and then focus on details.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.23135/x5.png", "caption": "(b)", "description": "This figure shows the architecture of LSNet, a lightweight vision network based on the proposed LS convolution. LSNet consists of four stages with decreasing resolutions (H x W) from 64x64 to 8x8, reflecting the processing of information through the network from a larger field of view (See Large) to a smaller focus (Focus Small). Each stage consists of multiple LS blocks. The number of blocks and channels differ between LSNet variants (tiny, small, base) based on the computational resources available.  A stem module processes the input image into a feature map. Downsampling techniques reduce the spatial resolution between stages.  The MSA block at the end of the final stage incorporates multi-head self-attention, which accounts for long-range dependencies in the lower resolution feature maps.", "section": "3.3 LSNet: Large-Small Network"}, {"figure_path": "https://arxiv.org/html/2503.23135/x6.png", "caption": "Figure 4: Visualization of the effective receptive field. Best viewed when zoomed in. (a) and (b) show that RepMixer and CGA exhibit unnatural patterns in the effective receptive field. (c) illustrates that LS convolution enables broad peripheral perception and central view focusing simultaneously. (d) shows that without LKP, LS convolution presents a smaller receptive field compared with (c), indicating the effectiveness of LKP.", "description": "This figure visualizes the effective receptive fields of different convolution methods.  RepMixer (a) and CGA (b), representing typical convolution and self-attention methods, respectively, show unnatural, irregular patterns in their receptive fields.  In contrast, the LS convolution (c) demonstrates a more natural receptive field, combining both a wide peripheral view (similar to human peripheral vision) and a focused central area (similar to human foveal vision).  The final image (d) shows that removing the large kernel perception (LKP) component from the LS convolution results in a much smaller receptive field, highlighting the importance of LKP in achieving the wide perceptual range of the LS convolution.", "section": "3.2. LS (Large-Small) Convolution"}, {"figure_path": "https://arxiv.org/html/2503.23135/x7.png", "caption": "Figure 5: Visualization of the aggregation weights in LS convolution. The second row shows that the aggregation weights are well correlated with semantic relevant areas. The third row indicates that integrating LKP enables LS convolution to capture more precise visual patterns with improved contextual information.", "description": "This figure visualizes the aggregation weights learned by the LS convolution, a novel method for efficiently capturing both broad contextual information and fine-grained details. The top row displays input images. The second row shows heatmaps of aggregation weights.  These heatmaps demonstrate that the weights are strongly correlated with semantically meaningful regions in the input image. The bottom row shows the same visualization, but when the Large-Kernel Perception (LKP) component is removed from the LS convolution. Comparing the second and third rows reveals that integrating LKP significantly enhances the precision of the aggregation weights, enabling the LS convolution to capture more precise visual patterns and improve contextual information.", "section": "3.2. LS (Large-Small) Convolution"}, {"figure_path": "https://arxiv.org/html/2503.23135/x8.png", "caption": "Figure 6: Visualization of the feature maps of LKP and SKA. The second column in each part shows that LKP can encompass a broad view of the scene. The third column in each part indicates that based on LKP, SKA can further grasp more subtle features and detailed patterns.", "description": "Figure 6 visualizes the feature maps generated by the Large Kernel Perception (LKP) and Small Kernel Aggregation (SKA) modules of the LS convolution.  The left column shows the input image. The middle column displays the feature maps produced by LKP, demonstrating its ability to capture a wide, contextual view of the scene. The right column shows the feature maps produced by SKA, illustrating its ability to extract more subtle and detailed features from the scene using the information provided by LKP.  This figure highlights the complementary nature of LKP and SKA in capturing both global context and local details.", "section": "3.2. LS (Large-Small) Convolution"}]