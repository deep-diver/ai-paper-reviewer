[{"figure_path": "https://arxiv.org/html/2504.01934/x1.png", "caption": "Figure 1: ILLUME+\u00a0can understand and generate images at any resolution. Compared to our previous work, ILLUME\u00a0wang2024illume , it demonstrates improved texture preservation in image editing tasks.", "description": "ILLUME+ is an enhanced version of ILLUME that uses dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation.  Figure 1 showcases this by demonstrating ILLUME+'s ability to understand and generate images at various resolutions, highlighting the improved texture preservation during image editing tasks when compared to its predecessor, ILLUME.  Several example images and editing tasks are presented to illustrate these advancements.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.01934/x2.png", "caption": "Figure 2: Characteristics comparison among existing unified models. Existing methods explore distinct paradigms to balance visual understanding, generation, and editing capabilities. Early approaches using VQGAN discretization struggle in understanding and context-aware generation tasks due to limited semantic alignment. Later frameworks incorporate semantic encoders, achieving better alignment but compromising texture preservation essential for fine-grained editing. ILLUME+ deep-integrates image understanding, generation, and editing into a single, unified architecture, enabling more intelligent and flexible interactions and task execution.", "description": "Figure 2 compares various unified multimodal models based on their visual understanding, generation, and editing capabilities.  Early models using VQGAN for image discretization show poor performance in understanding and context-aware generation due to limited semantic alignment.  Later models improve alignment by using semantic encoders, but they sacrifice texture preservation necessary for detailed editing. In contrast, ILLUME+ integrates these three capabilities seamlessly within a unified architecture, allowing for more flexible and intelligent interactions.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.01934/x3.png", "caption": "Figure 3: Architecture of ILLUME+.\n(a) The dual vision tokenizer preserves both semantic and texture information.\n(b) The diffusion refiner decodes discrete tokens into high-quality images.\n(c) The unified MLLM enables deep semantic interactions and context-aware image generation.\n(d) We introduce an unambiguous image representation of discrete tokens in a chain-of-thought pattern (semantic tokens first, followed by pixel tokens), resulting in improved generation performance.", "description": "Figure 3 provides a detailed illustration of the ILLUME+ architecture, breaking down its four core components: (a) Dual Vision Tokenizer, which uniquely preserves both semantic and textural information from images; (b) Diffusion Refiner, responsible for decoding discrete tokens into high-quality images, especially enhancing image resolution; (c) Unified Multimodal Large Language Model (MLLM), which facilitates deep semantic interactions and context-aware image generation; and (d) a novel, unambiguous image representation method using a chain-of-thought approach. This method prioritizes semantic tokens and subsequently incorporates pixel tokens for more effective image generation.  The figure effectively visualizes the interplay between these components in generating and processing images, showcasing the model's ability to handle high-resolution and detail-rich output.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2504.01934/x4.png", "caption": "Figure 4: Illustration of our progressive training pipeline. We first pre-train the dual-tokenizer system by reconstruction of the semantic and pixel information. We then fine-tune the diffusion model as a high-quality image decoder. The MLLM training consists of three main stages that gradually increase task resolution and complexity.", "description": "Figure 4 illustrates the three-stage progressive training process for ILLUME+.  The first stage pre-trains the dual-tokenizer system by reconstructing semantic and pixel information from images, ensuring a robust representation for the subsequent stages. The second stage fine-tunes a diffusion model to serve as a high-quality image decoder, enhancing the fidelity of image generation.  The third stage focuses on training the Multimodal Large Language Model (MLLM) in three progressive steps, each incrementally increasing the complexity of the tasks and the resolution of the images. This gradual approach enhances the model's ability to handle diverse tasks and resolutions efficiently.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2504.01934/x6.png", "caption": "Figure 5: Summary of the data mixture in each stage. Our training data gradually covers a wide range of tasks and various image resoluton.", "description": "Figure 5 shows the composition of the training dataset used in each stage of the ILLUME+ model training. The training process consists of three stages, and each stage uses a different mix of datasets to train various parts of the model. Stage 1 focuses primarily on datasets that help initialize the visual embedding and includes image reconstruction and image captioning data. Stage 2 focuses on image-text alignment using multimodal datasets. This stage is further broken into two sub-stages, each with a different resolution of images to gradually increase the model's capability. Stage 3 incorporates various datasets representing diverse downstream tasks, including visual understanding, image generation and editing. The figure shows the percentage of each dataset in each stage, along with the image resolution for the specific datasets involved.  The purpose is to illustrate the progressive increase in data diversity and task complexity during the training process, allowing for a more robust and adaptable multimodal model.", "section": "3.4 Training Procedure and Data Composition"}, {"figure_path": "https://arxiv.org/html/2504.01934/x7.png", "caption": "Table 4: Comparisons with other visual tokenizers. The evaluations are on ImageNet 50k validation set under different image resolution.", "description": "This table compares the performance of ILLUME+'s DualViTok visual tokenizer against other state-of-the-art visual tokenizers on the ImageNet 50k validation set.  The comparison considers various image resolutions, evaluating each tokenizer's ability to reconstruct images based on semantic and texture information. Metrics such as rFID (Fr\u00e9chet Inception Distance), PSNR (Peak Signal-to-Noise Ratio), and SSIM (Structural Similarity Index) are used to assess the quality of the reconstructed images.  The table allows for a quantitative comparison of how effectively different tokenizers handle various image sizes and complexities.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2504.01934/x8.png", "caption": "Table 5: Quantitative results on image editing benchmarks.  The performance with top-1 and top-2 value are denoted in bold and underline.", "description": "Table 5 presents a quantitative comparison of different models' performance on image editing benchmarks. The table likely shows metrics such as FID (Fr\u00e9chet Inception Distance), PSNR (Peak Signal-to-Noise Ratio), and SSIM (Structural Similarity Index) to evaluate the quality and similarity of edited images compared to ground truth.  The top-1 and top-2 values for each metric are highlighted for each model, indicating the best-performing models and the consistency of their results.  This allows for a direct comparison of the effectiveness of various models in performing image editing tasks. The benchmarks themselves likely involve diverse image editing tasks to assess model robustness and generalizability.", "section": "4 Experiments"}]