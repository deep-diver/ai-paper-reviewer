[{"figure_path": "https://arxiv.org/html/2503.23573/x1.png", "caption": "Figure 1: DASH: Systematic Hallucinations of PaliGemma-3B.", "description": "The figure showcases a visual example of systematic hallucinations in the PaliGemma-3B vision-language model.  It presents a series of real-world images to the model, asking whether a specific object (in this case, a 'fireboat') is present.  The model incorrectly identifies the presence of the fireboat in multiple images where it is clearly absent, highlighting a systematic error in the model's object recognition capabilities. This illustrates the model's tendency to hallucinate objects even in the absence of visual evidence.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/extracted/6321901/images/pipeline.png", "caption": "Figure 2: DASH: Given an object class, e.g. dining table, we generate text-based queries with DASH-LLM or image-based queries with DASH-OPT. Optimization: we optimize the latent variables of a diffusion process to generate an image which yields \u201cyes\u201d for the VLM (\u201cCan you see a dining table in this image?\u201d) and at the same time the object detector states that no \u201cdining table\u201d is present in the image. Exploration: the text and image queries are used for kNN-retrieval using CLIP similarity on ReLaion-5B. Exploitation: for successful images (VLM \u201cyes\u201d, object detector \u201cno\u201d) of the exploration phase we retrieve novel images via kNN-retrieval to check if the hallucination transfers to semantically similar images. Clustering: Finally, we cluster successful images of the exploitation step into semantically similar clusters of hallucinations of the VLM.", "description": "This figure illustrates the DASH pipeline.  The pipeline begins by selecting an object class (e.g., \"dining table\").  Two methods are used to generate queries: DASH-LLM generates text-based queries, while DASH-OPT optimizes latent variables in a diffusion process to create images likely to cause the Vision-Language Model (VLM) to hallucinate.  These queries are then used to retrieve images from the ReLAION-5B dataset.  The retrieval process is done in two stages: exploration (initial retrieval) and exploitation (retrieving similar images to those already identified as causing hallucinations).  Finally, images resulting in a \"yes\" response from the VLM (indicating hallucination) but a \"no\" from the object detector (confirming the object's absence) are clustered to identify groups of semantically similar images that consistently trigger false-positive hallucinations.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/x2.png", "caption": "Figure 3: Examples of systematic FP-hallucination\u00a0clusters found by DASH for PaliGemma: We present six hallucination clusters, each for a different object\u2014three identified by DASH-LLM and three by DASH-OPT. For each cluster, we show a sample of images and the total number of images. For each of these images, PaliGemma answers \u201cyes\u201d to \u201cCan you see a OBJ in this image?\u201d while the object detector reports a confidence below 0.1. None of the images actually contain the object. We also provide the text (DASH-LLM) and image queries (DASH-OPT) used for retrieval during exploration for the majority of the cluster.", "description": "This figure showcases examples of systematic false-positive hallucinations identified by the DASH framework in the PaliGemma vision-language model (VLM). Six clusters of images are displayed, three generated using DASH-LLM (text-based queries) and three using DASH-OPT (image-based queries). Each cluster contains semantically similar images where the model incorrectly identifies the presence of a specific object. The figure highlights the model's tendency to hallucinate objects based on contextual cues or associations between other present image elements, even when the object is absent.  Each cluster shows a small selection of the images, along with the total number of images in that cluster, the query used, and confidence scores (object detection confidence < 0.1 and model's 'yes' response). This illustrates how DASH can identify systematic errors in VLMs, revealing both expected and unexpected patterns in model behaviour.", "section": "3 DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/x3.png", "caption": "Figure 4: Histogram illustrating the minimum embedding distance from success images to the nearest LLM prompt for DASH-LLM and DASH-OPT. While both methods use these LLM prompts in their exploration stage, the image-based method is able to find unexpected hallucinations far away from the initial LLM prompts.", "description": "This histogram displays the minimum CLIP embedding distance between images successfully identified as causing false positive hallucinations by the model (VLM says \"yes\", object detector says \"no\") and their nearest initial LLM-generated prompt.  The distribution for DASH-OPT (image-based query generation) shows a wider spread and higher minimum distances compared to DASH-LLM (text-based query generation). This indicates that DASH-OPT is more effective at uncovering unexpected hallucinations that are less semantically related to the initial prompts than DASH-LLM.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/x4.png", "caption": "Figure 5: All clusters for DASH-LLM and DASH-OPT for the object \u2019Dam\u2019 using LLaVA-NeXT Vicuna. DASH-OPT identifies a larger total number of clusters and images, capturing a broader diversity of visuals. This demonstrates that DASH-OPT can uncover unexpected systematic hallucination patterns, such as cartoon frogs and dinosaurs, orange leaves, bare feet, or a park bench, whereas DASH-LLM tends to highlight failure modes more directly linked to the object, such as water associated with \u2019Dam\u2019.", "description": "Figure 5 presents a comparison of results from two different methods, DASH-LLM and DASH-OPT, used to identify systematic hallucinations in the LLaVA-NeXT Vicuna vision-language model. Both methods aim to find clusters of images that cause the model to incorrectly identify the presence of a \"Dam\" even when it is absent.  DASH-LLM uses text-based queries generated by a large language model (LLM) to retrieve relevant images. DASH-OPT, on the other hand, uses an image-based retrieval method to generate images specifically designed to mislead the VLM. The figure shows that DASH-OPT discovers a significantly larger number of image clusters and a wider range of visual patterns that trigger hallucinations, including unexpected and unrelated objects like cartoon frogs and dinosaurs.  DASH-LLM, in contrast, primarily identifies clusters that are semantically related to the concept of a dam, such as images of water. This difference highlights the ability of DASH-OPT to uncover more model-specific and systematic errors compared to DASH-LLM.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/x5.png", "caption": "Figure 6: Object hallucination benchmark DASH-B: examples from the negative set of DASH-B (images and object label) where GPT-4o-mini, the best scoring model on DASH-B (see Tab.\u00a0J) hallucinates the object even though it is not present in the image.", "description": "Figure 6 showcases examples from the DASH-B benchmark dataset, specifically focusing on cases where the object is absent.  The dataset is comprised of images and their corresponding object labels.  The figure highlights instances where the GPT-40-mini model, identified as the top-performing model on the DASH-B benchmark (detailed in Table J of the paper), incorrectly identifies the presence of objects that are not actually present within the image. This demonstrates the model's susceptibility to generating false positives even in a controlled setting.", "section": "4.3 DASH-B: Object Hallucination Benchmark"}, {"figure_path": "https://arxiv.org/html/2503.23573/x6.png", "caption": "Figure 7: DASH-LLM prompt for generating the text queries (1/3)", "description": "This figure shows the prompt used to instruct the large language model (LLM) to generate text queries for the DASH-LLM component of the DASH pipeline. The goal is to produce prompts that might cause a vision-language model (VLM) to incorrectly identify an object in an image due to spurious features, even when that object is not actually there.  The instructions emphasize creating unique prompts, avoiding mentioning the target object or its parts, focusing on spurious features (visual elements that frequently co-occur with the object but aren't part of it), and maintaining a realistic, unbiased style.  Examples for different objects are provided to illustrate the desired style of the prompts.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/x7.png", "caption": "Figure 8: DASH-LLM prompt for generating the text queries (2/3)", "description": "This figure shows the second part of a three-part prompt used to instruct a large language model (LLM) to generate text queries for the DASH system. The goal is to create prompts that would lead an AI model to incorrectly identify objects in images due to spurious features (elements that frequently co-occur with a given object but are not part of the object itself).  The prompt provides examples of correct and incorrect prompts to help the LLM understand the task.  It emphasizes the importance of not mentioning the object or its parts, avoiding direct or indirect references, and focusing on the spurious features.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/x8.png", "caption": "Figure 9: DASH-LLM prompt for generating the text queries (3/3)", "description": "This figure shows the detailed instructions and guidelines for using the DASH-LLM prompt to generate text queries. It emphasizes the importance of avoiding any direct or indirect mention of the object's name or parts, focusing instead on creating prompts that emphasize spurious features frequently associated with the object. The instructions highlight the need for prompt diversity, avoiding repetitions, and ensuring adherence to proper language and grammar while being inclusive and free of bias.  It provides examples of correct and incorrect prompts for various objects, further clarifying the guidelines.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}, {"figure_path": "https://arxiv.org/html/2503.23573/x13.png", "caption": "Figure 10: DASH-LLM follow-up prompt for generating the text queries", "description": "This figure shows the follow-up prompt used in the DASH-LLM method of the paper.  The initial prompt instructs the LLM to generate image captions containing only spurious features, avoiding any mention of the target object or its parts.  This follow-up prompt emphasizes the importance of reviewing and correcting any previously generated prompts that deviate from the given guidelines, ensuring that the final prompts precisely adhere to the instructions and avoid direct or indirect mentions of the object.", "section": "3. DASH: Detection and Assessment of Systematic Hallucinations"}]