[{"Alex": "Hey everyone, welcome to the podcast! Today we're diving into the wild world of AI and visual perception\u2014think robots seeing things\u2026 but sometimes getting it hilariously wrong. We're tackling a fascinating new paper on something called \u2018Systematic Hallucinations\u2019 in Vision Language Models. Get ready to learn why your smart devices might be seeing things that aren't actually there!", "Jamie": "Systematic Hallucinations? That sounds both intriguing and a little scary. I mean, are we talking AI losing its mind, hallucinating like it\u2019s on something?"}, {"Alex": "Haha, not quite that dramatic! But in AI terms, a \u2018hallucination\u2019 is when a Vision Language Model, or VLM, confidently identifies something in an image that simply isn\u2019t there. This paper introduces DASH, which stands for Detection and Assessment of Systematic Hallucinations. It's a new way to find out when these models aren't just making random mistakes, but consistently 'seeing' the wrong things.", "Jamie": "Okay, so it\u2019s not random\u2026 that makes it even more interesting. So, Alex, what exactly are Vision Language Models, or VLMs, doing? I'm not super familiar with the techie side of things, umm."}, {"Alex": "No problem, Jamie. VLMs are AI systems designed to understand and connect both images and text. Think of them as trying to describe a picture \u2013 they should be able to accurately identify objects and relationships within an image, then put that into words. But\u2026 they often make mistakes.", "Jamie": "And those mistakes are the hallucinations we're talking about. Gotcha. So, how big of a problem are we talking? Is this just like, a rare glitch, or is it a serious issue in the AI world?"}, {"Alex": "That\u2019s exactly what the paper addresses. Existing methods for finding these hallucinations rely on small, carefully labeled datasets. But the real world is messy! DASH is designed to tackle this problem on a large scale, finding systematic errors in real-world images\u2014basically, in the kind of situations where these VLMs are actually being used.", "Jamie": "Okay, so they're saying the existing benchmarks aren\u2019t cutting it. Hmm, what makes DASH different? What new approach does it bring to the table?"}, {"Alex": "The key is scale and automation. DASH is a fully automated pipeline, meaning it doesn't need humans to manually label data. It uses a clever technique involving what they call DASH-OPT for image retrieval, optimizing the way images are generated to actively mislead the VLM. It\u2019s like creating visual traps for the AI.", "Jamie": "Visual traps, I love it. Can you break down DASH-OPT a little more? How do you optimize image generation to fool an AI? It sounds like an art and a science."}, {"Alex": "It really is. DASH-OPT plays around with a generative model, like those used to create AI art, but with a twist. The researchers tweaked the model to produce images where the VLM *thinks* it sees a specific object, even when an open-world object detector\u2014another AI\u2014says it\u2019s not there. It's a tug-of-war between two AIs, basically.", "Jamie": "That\u2019s a seriously smart approach. So, DASH identifies when the VLM is confidently wrong, but how does it determine if these errors are systematic or just one-off blips?"}, {"Alex": "That\u2019s where the clustering comes in. DASH groups together images that are semantically similar and consistently trigger the same hallucination in the VLM. If a VLM repeatedly 'sees' a fireboat in images of swimming pools, that's a systematic hallucination, not just a random error.", "Jamie": "Okay, clustering makes total sense \u2013 finding those patterns in the noise. So, what kind of objects are these VLMs commonly hallucinating?"}, {"Alex": "The paper found some really interesting examples. With one model, PaliGemma, DASH identified clusters of images where the AI consistently hallucinates things like ptarmigans in mountain valleys, or Baumkuchen\u2014that German cake\u2014in Christmas pyramids. It's like the AI is making unexpected associations between objects and their typical environments.", "Jamie": "Wow, the ptarmigan in the mountain and German cake in Christmas pyramid! That is so specific and definitely not things that would usually be wrong. I\u2019m starting to wonder if my phone is secretly seeing sausage rolls on my desk right now. So where were these models trained? Are they to blame?"}, {"Alex": "Yeah it's interesting to think about. So DASH experiments are based on the fact the open source world provides great places to train the models and the paper uses data sets like Objects365 and ReLAION to train the VLMs. Both of these provide large datasets of labeled images that are used to teach the VLM to understand the world. They then found, through analysis, that even the best models had certain short comings", "Jamie": "So what happened after DASH was used on those models? What did they do with this information?"}, {"Alex": "Well, the researchers went further than just identifying the hallucinations. They tested whether these errors would transfer to other VLMs and developed a new benchmark called DASH-B to better evaluate this issue. Then, they even showed that fine-tuning PaliGemma with the images DASH uncovered could help mitigate those object hallucinations.", "Jamie": "Oh, wow the found it was the images that were leading to the errors, not the models. What was their new solution called? Are they looking at creating more?"}, {"Alex": "Their solution was DASH-B and what it is is a completely new benchmark to test and measure the effectiveness of systems like this in identifying hallucinations. This helps to reliably evaluate if a VLM is really fixed. The found that using DASH they could help mitigate issues", "Jamie": "So, DASH isn't just about finding the problem; it\u2019s also part of the solution. But if these hallucinations are so widespread, what are the real-world implications? I mean, are we just talking about slightly embarrassing AI errors, or are there more serious concerns?"}, {"Alex": "That\u2019s the crucial question. In critical applications like medical diagnosis or autonomous driving, these hallucinations could have severe consequences. Imagine an AI misinterpreting a scan and missing a tumor or incorrectly identifying a stop sign. It\u2019s not just about accuracy; it\u2019s about safety and reliability.", "Jamie": "Okay, that definitely ups the ante. Medical diagnoses gone wrong\u2026 that's a scary thought. So, if this paper is highlighting a significant problem, what are the next steps for researchers in this field?"}, {"Alex": "The paper opens up a lot of avenues. First, we need more robust benchmarks like DASH-B to rigorously evaluate VLMs in open-world settings. Second, we need to develop more effective methods for mitigating these hallucinations, perhaps by incorporating insights from how humans learn to avoid misinterpreting visual information.", "Jamie": "Hmm, interesting that you say about Humans. Can you tell me about their method? Are these systems learning in a way to remove these hallucinations?"}, {"Alex": "Yeah of course. So I think its important to start by saying that VLMs are designed to work exactly how a system would in your everyday life. Take for example, an airport scanner. Say the Airport Scanner incorrectly says you have brought water in your bag but you can see you haven't. This has many potential reasons from the system not being maintained to there being interference from other technology. These systems should be as robust as possible.", "Jamie": "What kind of other technology could be interfering with the Airport Scanner? This is so cool! Like something out of a movie."}, {"Alex": "Of course, so for example, Radio Frequency systems could affect the VLM and the Airport Scanners! Things that you would never imagine would come and affect your everyday life are playing some kind of a role without you even knowing! Its crazy! I recommend you check out the code from the paper as well!", "Jamie": "It sounds exciting. And do they have any tests that can say that, with a degree of scientific certainty, that there is an interference"}, {"Alex": "Yes of course! All these tests are completely data-driven! In an ideal scientific world you would want to be able to analyse every potential reason that could affect the accuracy of a VLM but it is impossible with current technology and computing capabilities!", "Jamie": "That's so cool! Do you think AI will be able to figure out the tests for scientific accuracy that would help us test these VLMs?!"}, {"Alex": "Haha! You read my mind, Jamie! Yes, I think the next steps are for AI to create AI. What I mean is that we can ask AI to come up with what it believes are the tests for its own accuracy and performance. This could get into an interesting recursive loop, but that is what research is about", "Jamie": "Yeah that's super real. Is that it then? Time for the listeners to think about the crazy applications of our chat?!"}, {"Alex": "Not just yet!. It's also about shifting our perspective on AI. We often think of these models as objective, all-seeing eyes, but this paper reminds us that they have their own biases and blind spots. We need to be more critical and aware of these limitations as we integrate VLMs into more aspects of our lives.", "Jamie": "Well, Alex, this has been truly eye-opening. Thanks for shedding light on these hidden AI hallucinations and their potential implications. I\u2019ll definitely be looking at my phone a bit differently now!"}, {"Alex": "My pleasure, Jamie! It\u2019s crucial to remember that while AI is rapidly advancing, it's not infallible. Continued research, like this paper, is essential to ensure these systems are reliable and beneficial for everyone.", "Jamie": "Before we go, I think it's worth saying about open weights for VLM again to our listeners Alex"}, {"Alex": "Yes that's a great point. Most top VLMs aren't even open for researchers to modify. So the best model to base a project like the one we are discussing today isn't ever truly ready and capable as these models are closed source! Thanks again Jamie. I was Alex, and this was the end of today's podcast!", "Jamie": "Thanks for listening everyone!"}]