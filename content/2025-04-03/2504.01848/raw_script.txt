[{"Alex": "Hey everyone, welcome to the podcast! Today, we're diving into something mind-blowingly ambitious: Can AI actually *replicate* cutting-edge AI research? I'm Alex, and I've been buried in the details of a fascinating paper that tries to answer just that.", "Jamie": "Wow, Alex, that sounds\u2026 intense. Replicating *AI research*? Like, an AI trying to do what AI researchers do? I'm Jamie, and I'm super curious \u2013 and maybe a little intimidated \u2013 to hear all about it."}, {"Alex": "Exactly! The paper is titled 'PaperBench: Evaluating AI's Ability to Replicate AI Research'. The core idea is to benchmark how well AI agents can understand, code, and execute experiments from recent, top-tier AI papers.", "Jamie": "Okay, so it's a test, basically. But instead of, umm, testing if an AI can classify images or play chess, it's seeing if it can\u2026 redo someone else's AI project?"}, {"Alex": "Precisely. And not just redo it, but do it from scratch. The agents get the paper and\u2026 go. No access to the original code, just the published paper. This forces the AI to truly understand the concepts and implement them independently.", "Jamie": "That sounds incredibly difficult! So, what kind of AI agents are we talking about here? Like, are we pitting GPT-4 against some poor intern?"}, {"Alex": "Haha, not quite. The researchers evaluated several frontier models, including GPT-4, Claude 3.5 Sonnet, Gemini, and DeepSeek. These are powerful models with impressive coding and reasoning abilities.", "Jamie": "Okay, so serious contenders. And what exactly are they trying to replicate from these papers? Are we talking about rewriting an entire neural network architecture or...?"}, {"Alex": "It's about replicating the *empirical contributions* of the paper \u2013 the key experiments and results. This involves understanding the paper\u2019s innovations, developing a codebase from scratch to implement those ideas, running the experiments, and reproducing the reported metrics, figures, and tables.", "Jamie": "So, a full stack replication, almost. That's pretty comprehensive. How do they even begin to grade something so complex? It's not like a simple pass/fail test."}, {"Alex": "That's where the 'PaperBench' part comes in. The authors created detailed rubrics for each paper, breaking down the replication task into smaller, gradable sub-tasks. These rubrics are hierarchical, so each task can be further decomposed into finer-grained requirements.", "Jamie": "Ah, a rubric! So, it's kind of like a decision tree, walking through all the requirements from zero to one hundred? But who creates this rubric? Who is qualified?"}, {"Alex": "Exactly. They worked with the original authors of each ICML paper to co-develop the rubrics, ensuring accuracy and realism. These rubrics specify all the necessary outcomes for successfully replicating the paper, resulting in a total of over 8,000 individually gradable tasks across the 20 papers.", "Jamie": "Okay, so it\u2019s expert-approved. And these rubrics include how code and metrics can be validated? But I assume that grading requires human and would require hours? "}, {"Alex": "You're right that grading is very expensive. That's why they introduced an auxiliary evaluation, JudgeEval, which compares the outputs of automated judges against a dataset of gold labels from human expert judges. This helps them assess the performance and reliability of their LLM-based judges.", "Jamie": "OK, so they're using AI to grade the AI\u2026 that\u2019s very meta! So, how did the AI judges stack up against the human judges? What was the accuracy or metrics between AI judges and human one?"}, {"Alex": "Their best LLM-based judge, using o3-mini with custom scaffolding, achieved an F1 score of 0.83 on the JudgeEval auxiliary evaluation. That suggests that this judge is a reasonable stand-in for a human judge", "Jamie": "That\u2019s actually pretty impressive. So, with the rubrics and the AI judges in place, what did they actually *find*? Could the AI agents replicate the research or is it more clickbait?"}, {"Alex": "Well, the best-performing agent they tested, Claude 3.5 Sonnet (New) with open-source scaffolding, achieved an average replication score of 21.0%.", "Jamie": "Only 21%? Ouch! That's\u2026 pretty low. I guess replicating research is a lot harder than it looks, even for AI. "}, {"Alex": "Exactly! It highlights how much implicit knowledge and problem-solving skill is required to be a researcher, such as debugging an issue during the experiment. And models still fail to strategize how to best replicate paper given limited time. But, models succeeded in implementing and validating various methods.", "Jamie": "Hmm, so they\u2019re good at following instructions to write code, but not so great at figuring out *what* code to write to meet the research goal, right? And that leads to time management issues."}, {"Alex": "Precisely. In fact, when they recruited top ML PhDs to attempt a subset of PaperBench, the models still didn't outperform the human baseline.", "Jamie": "Wow, human still wins in that experiment. So, it seems that even with the best models and all these smart supports, the AI agents are not quite ready to replace AI researchers"}, {"Alex": "Not *yet*. The researchers did find some interesting failure modes. For example, many of the models would finish early, claiming they\u2019d either completed the replication or hit an unsolvable problem.", "Jamie": "Almost sounds like they gave up! Were there any steps or methods to fix or make the models try more?"}, {"Alex": "Well, the researchers did try a variant of their agent, called 'IterativeAgent', and force it to run for its available time. That significantly boosted scores for some of the models by telling the model to try again until time is fully depleted.", "Jamie": "Interesting! so they didn\u2019t just measure pure intelligence, but also tenacity, by getting AI model work harder or ask for more help for the problem. So how much work can we expect?"}, {"Alex": "A fully roll-out costs around $400 in API credits, which translates to $8000 USD per eval run. The team designed an version called 'PBCD', but at the cost of foregoing the tasks in execution and result matching. ", "Jamie": "Okay, but the results is just around 20%. What kind of future improvements we can expect for better result?"}, {"Alex": "The team believed that this area can better improved by improving the agentic scaffolds. Agentic scaffolds are essential to control the action, and strategy to solve task with better control.", "Jamie": "I see. so it is like giving the AI an executive function, helping it organize itself. Let's move on a bit with what are the limitations of this paper"}, {"Alex": "Well, the research admits its dataset is limited to only 20 papers, which in turn also limits amount of rubrics or grading point. For contamination, the original author's code might affect performance from pretrained model. The team also admits it is challenging to train others to create rubrics at high quality", "Jamie": "Okay, so the the paper admits to those factors. But after all of those limitations, is the work meaningful to future AI trend?"}, {"Alex": "Well. The most important thing is that this paper sets standard on how to check AI or agents for complex work with unstructured task, such as reading documents, perform experiments. Also, this result can be useful on areas like agentic scaffolds, or automated judges", "Jamie": "Okay, so there's room to develop to do this assessment more effectively with human in the loop, or use model assistant for creating automated rubrics. It sounds very exciting"}, {"Alex": "Absolutely. This is the first step. In conclusion, AI agents show some capacity to replicate certain facets of machine learning papers, they are still far from competently performing the full range of tasks. However, these early results underscore non-trivial progress: AI agents succeed in implementing and validating various methods, suggesting promise for future improvements.", "Jamie": "So, it is an open question, what can be the next stage? "}, {"Alex": "We can contribute to evaluating, monitoring, and forecasting the capabilities of AI systems to conduct AI R&D of their own, and it needs a more study. So, for future direction, it needs more complex metrics, as a benchmark to measure progress in this field. But the overall, we mark a substantive step towards rigorous evaluation of AI autonomy in ML research.", "Jamie": "Well, Alex, this has been incredibly insightful. Thanks for breaking down such a complex topic for us. It sounds like AI autonomy in research is still a long way off, but this 'PaperBench' is a valuable step in that direction. Thank you for pointing a better path in AI. "}]