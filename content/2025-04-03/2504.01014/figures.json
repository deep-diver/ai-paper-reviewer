[{"figure_path": "https://arxiv.org/html/2504.01014/x3.png", "caption": "Figure 1: Comparison of AnimeGamer with previous methods. Unbounded employs an LLM to translate multi-turn text-only dialogues into language descriptions for static image generation, with an additional condition based on reference images. In contrast, AnimeGamer utilizes an MLLM to predict multimodal representations \u211b\u211b\\mathcal{R}caligraphic_R by incorporating historical multimodal context as input. These generated representations can then be directly decoded into consistent dynamic clips using a video diffusion model.", "description": "The figure compares AnimeGamer with previous methods, specifically Unbounded. Unbounded uses an LLM to translate text-based dialogues into instructions for static image generation, relying on reference images. AnimeGamer, in contrast, uses an MLLM to predict multimodal representations (\u211b), incorporating past visual and textual data.  These representations are then decoded into consistent, dynamic video clips via a video diffusion model, showcasing AnimeGamer's ability to generate continuous and coherent animations.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.01014/x4.png", "caption": "Figure 2: Overview of our AnimeGamer. The training process consists of three phases: (a) We model animation shots using action-aware multimodal representations through an encoder and train a diffusion-based decoder to reconstruct videos, with the additional input of motion scope that indicates action intensity. (b) We train an MLLM to predict the next game state representations by taking the history instructions and game state representations as input. (c) We further enhance the quality of decoded animation shots from the MLLM via an adaptation phase, where the decoder is fine-tuned by taking MLLM\u2019s predictions as input.", "description": "AnimeGamer's training involves three stages. First, an encoder generates action-aware multimodal representations of animation shots, which are then decoded into videos by a diffusion-based decoder using motion scope to control action intensity.  Second, an MLLM (Multimodal Large Language Model) predicts the next game state's multimodal representations using past instructions and game states. Finally, the decoder is fine-tuned using the MLLM's predictions to improve the quality of generated animation shots.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2504.01014/x13.png", "caption": "Figure 3: Architecture of animation shot encoder and decoder. The action-aware multimodal representation integrates visual features of the first frame with textual features of action description, and serve as the input to the modulation module of the decoder. Additional motion scope indicating action intensity is injected using a condition module.", "description": "Figure 3 illustrates the architecture of the animation shot encoder and decoder within the AnimeGamer model.  The encoder takes in both visual features from the first frame of an animation clip (processed by CLIP) and textual features describing the action (processed by T5). These multimodal features are then combined and used to generate an action-aware multimodal representation. This representation serves as input to the modulation module of the decoder, a video diffusion model (CogVideoX).  A further input, 'motion scope', representing the intensity of the motion in the clip, is separately injected via a condition module into the decoder to control the generated video's dynamism.  This detailed process allows for the generation of high-quality, contextually consistent video clips reflecting the specified animation.", "section": "3. Methods"}]