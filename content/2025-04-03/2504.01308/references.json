{"references": [{"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2024-01-01", "reason": "This paper describes LLaVA, which is extensively utilized and adapted, highlighting its importance in visual instruction tuning."}, {"fullname_first_author": "Yongshuo Zong", "paper_title": "Safety fine-tuning at (almost) no cost: A baseline for vision large language models", "publication_date": "2024-01-01", "reason": "This paper introduces VLGuard, which serves as a direct baseline for the new Robust-VLGuard dataset and fine-tuning approach."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, which is used as a foundation for aligning images and texts, demonstrating a key method for zero-shot learning."}, {"fullname_first_author": "Deyao Zhu", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "publication_date": "2024-01-01", "reason": "This paper describes MiniGPT-4, one of the main models evaluated in the study, making it crucial for assessing the impact of Gaussian noise perturbations."}, {"fullname_first_author": "Xiangyu Qi", "paper_title": "Visual adversarial examples jailbreak aligned large language models", "publication_date": "2024-01-01", "reason": "This paper explores visual adversarial examples for jailbreaking VLMs, providing a key benchmark for evaluating the robustness of VLMs against attacks."}]}