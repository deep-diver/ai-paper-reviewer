{"importance": "This paper enhances VLM robustness against noise, mitigating vulnerabilities. By introducing a new dataset and defense mechanism, it paves the way for more reliable and secure multimodal AI systems. The findings are crucial for advancing VLM technology and addressing real-world deployment challenges, ensuring safety and utility in diverse applications. The research encourages further exploration to enhance VLM defenses against complex attacks.", "summary": "Robust-VLGuard: Fortifying VLMs against Gaussian noise attacks with multimodal safety dataset and noise-augmented training.", "takeaways": ["Vision-Language Models (VLMs) are vulnerable to even simple noise perturbations, disrupting both helpfulness and safety.", "Robust-VLGuard, a new dataset with aligned/misaligned image-text pairs, enhances VLM robustness against noise through noise-augmented fine-tuning.", "DiffPure-VLM, leverages diffusion models to convert adversarial perturbations into Gaussian-like noise, significantly mitigating adversarial attacks."], "tldr": "Vision-Language Models (**VLMs**) extend capabilities of Large Language Models by integrating visual and textual information. Despite advancements in training, **VLMs** remain susceptible to jailbreak attacks, particularly when handling noisy or corrupted images. Existing security measures often overlook vulnerabilities associated with noise-augmented visual inputs, leading to critical security gaps. Many **VLMs** are vulnerable to even simple perturbations like Gaussian noise, disrupting both the helpfulness and safety alignment of models.\n\nTo address these issues, the paper introduces **Robust-VLGuard**, a multimodal safety dataset with aligned/misaligned image-text pairs. It combines **Robust-VLGuard** with noise-augmented fine-tuning.  The paper also proposes **DiffPure-VLM**, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise.  Experiments demonstrate that the distribution-shifting property of diffusion models aligns well with fine-tuned **VLMs**, mitigating adversarial perturbations across varying intensities.", "affiliation": "University of Science and Technology of China", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.01308/podcast.wav"}