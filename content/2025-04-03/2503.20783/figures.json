[{"figure_path": "https://arxiv.org/html/2503.20783/x1.png", "caption": "Figure 1: Left: Dr.\u00a0GRPO introduces simple yet significant modifications to address the biases in GRPO\u00a0(Shao et\u00a0al., 2024), by removing the length and std normalization terms. Right: Our unbiased optimizer effectively prevents the model from generating progressively longer incorrect responses, thereby enhancing token efficiency.", "description": "Figure 1 demonstrates the improvements achieved by Dr. GRPO, a modified version of the GRPO algorithm.  The left panel displays the mathematical formula for Dr. GRPO, highlighting the removal of length and standard deviation normalization terms which were present in the original GRPO (Shao et al., 2024). This modification addresses biases in GRPO that led to the generation of excessively long, incorrect responses. The right panel shows a graph comparing the token efficiency of GRPO and Dr. GRPO during reinforcement learning. Dr. GRPO is shown to be significantly more efficient in terms of tokens used, preventing the model from generating increasingly long incorrect outputs.", "section": "Analysis on Reinforcement Learning"}, {"figure_path": "https://arxiv.org/html/2503.20783/x2.png", "caption": "Figure 2: Model performance comparison. Oat-Zero-7B is RL-tuned with our minimalist recipe described in Sec.\u00a01 (third paragraph). Please see App.\u00a0B for more results.", "description": "This figure compares the performance of various language models on several mathematical reasoning benchmarks.  The models include those trained with RL (Reinforcement Learning) methods and others without.  The model Oat-Zero-7B, trained using a minimalist RL approach described in section 1 of the paper, is highlighted.  The graph shows that Oat-Zero-7B achieves competitive results compared to other models, particularly on the AIME 2024 benchmark. For a more detailed breakdown of results across various benchmarks, please refer to Appendix B.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.20783/x3.png", "caption": "Figure 3: Model attributes across three aspects. Question-Answering Ability: the extent to which a pretrained language model provides a direct answer to a question rather than continuing or expanding upon it; Exploration Ability: pass@8 measures how well base models explore; Self-Reflection: counts are obtained through cross-validation between keyword-based detection and LLM-based detection, as detailed in Appendix C.", "description": "Figure 3 presents a comparison of different large language models (LLMs) across three key attributes: Question-Answering Ability, Exploration Ability, and Self-Reflection.  Question-Answering Ability shows the proportion of direct answers provided by each LLM versus those that continued or expanded upon the question without a clear answer. Exploration Ability is measured using \"pass@8\", indicating the model's ability to explore a variety of potential answers to a question. Self-Reflection is assessed by combining keyword-based and LLM-based detection methods (detailed in Appendix C) to identify instances where the model demonstrates reflection or self-correction in its responses. This figure provides a comprehensive view of the baseline capabilities of various LLMs before any reinforcement learning (RL) is applied.", "section": "Analysis on Base Models"}, {"figure_path": "https://arxiv.org/html/2503.20783/x4.png", "caption": "Figure 4: Cases showing that DeepSeek-V3-Base already exhibits \u201cAha moment\u201d even before RL tunning.", "description": "This figure showcases examples from the DeepSeek-V3-Base model demonstrating self-reflective behavior, a phenomenon known as the \"Aha moment,\"  even before reinforcement learning (RL) fine-tuning.  The examples illustrate the model's ability to pause, reconsider its approach, and even explicitly mention steps like verifying the problem's conditions or acknowledging overthinking. This demonstrates that the \"Aha moment\" is not solely a product of RL training but can also be present, to some degree, in well-pretrained base models.", "section": "2.3 Aha Moment Already Appears in Base Models Including DeepSeek-V3-Base"}, {"figure_path": "https://arxiv.org/html/2503.20783/x5.png", "caption": "Figure 5: Illustration of the biases in GRPO. Note that the effective advantage of GRPO ai,tsubscript\ud835\udc4e\ud835\udc56\ud835\udc61a_{i,t}italic_a start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT is equivalent to a reweighted version of the unbiased advantage A~i,t=R\u2062(\ud835\udc2a,\ud835\udc28i)\u2212mean\u2061(\ud835\udc11)subscript~\ud835\udc34\ud835\udc56\ud835\udc61\ud835\udc45\ud835\udc2asubscript\ud835\udc28\ud835\udc56mean\ud835\udc11\\tilde{A}_{i,t}=R({\\mathbf{q}},{\\mathbf{o}}_{i})-\\operatorname{mean}(\\mathbf{R})over~ start_ARG italic_A end_ARG start_POSTSUBSCRIPT italic_i , italic_t end_POSTSUBSCRIPT = italic_R ( bold_q , bold_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - roman_mean ( bold_R ). The terms std\u2061(\ud835\udc11)std\ud835\udc11\\operatorname{std}(\\mathbf{R})roman_std ( bold_R ) and |\ud835\udc28i|subscript\ud835\udc28\ud835\udc56|{\\mathbf{o}}_{i}|| bold_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | could bias the optimization by assigning different weights to different questions and responses, as denoted by the sizes of the blue circles and the lengths of the orange arrows.", "description": "Figure 5 illustrates how the Group Relative Policy Optimization (GRPO) algorithm, used in reinforcement learning for large language models (LLMs), introduces bias into the optimization process.  The effective advantage (a<sub>i,t</sub>) calculated by GRPO is a reweighted version of the true, unbiased advantage (\u00c3<sub>i,t</sub> = R(q, o<sub>i</sub>) - mean(R)).  This reweighting depends on the standard deviation of rewards (std(R)) and the length of the response (|o<sub>i</sub>|).  The figure visually represents these effects, showing that shorter correct responses and longer incorrect responses are given disproportionately high weight in the optimization, leading to a bias favoring longer responses.  The sizes of blue circles represent the magnitude of the unbiased advantage, and the length of orange arrows represents the effective advantage after weighting by standard deviation and response length.", "section": "Analysis on Reinforcement Learning"}, {"figure_path": "https://arxiv.org/html/2503.20783/x6.png", "caption": "Figure 6: Comparison of Dr.\u00a0GRPO and GRPO in terms of training dynamics (Top) and evaluation results (Bottom).", "description": "This figure compares the performance of Dr. GRPO and GRPO, two reinforcement learning optimization algorithms, during the training of a large language model for mathematical reasoning. The top part shows the training dynamics, illustrating how rewards, output length (overall, correct, and incorrect), change over training iterations for both algorithms. The bottom part displays the evaluation results on various mathematical reasoning benchmarks, showing the average benchmark scores achieved by each algorithm. The plots reveal that Dr. GRPO is more efficient in terms of token usage compared to GRPO and achieves comparable or better performance on the benchmarks.", "section": "Analysis on Reinforcement Learning"}, {"figure_path": "https://arxiv.org/html/2503.20783/x7.png", "caption": "Figure 7: The average benchmark accuracy of different {template, question set} combinations during RL training.", "description": "This figure displays the average accuracy across multiple benchmarks achieved during reinforcement learning (RL) training.  Different line graphs represent the performance using various combinations of question sets (containing different types and difficulty levels of math questions) and prompting templates (R1, Qwen-Math, and No Template). The x-axis represents the number of policy iterations during RL training, while the y-axis shows the average accuracy across the benchmarks. This allows for a comparison of how the choice of template and question set complexity impacts the effectiveness of the RL training process.", "section": "3.3 A Duet of Template and Question Set Coverage in RL dynamics"}, {"figure_path": "https://arxiv.org/html/2503.20783/x8.png", "caption": "Figure 8: Left: The average benchmark performance curves of different base models. Right: The comparison between Dr.\u00a0GRPO and GRPO with respect to reasoning accuracy (solid lines) and model response length (dashed lines).", "description": "Figure 8 presents a comparative analysis of different base models' performance in reinforcement learning (RL) for reasoning tasks, and a comparison of two RL optimization algorithms, Dr. GRPO and GRPO. The left panel shows the average benchmark accuracy across various models over the course of RL training. The right panel focuses on the difference between Dr. GRPO and GRPO, highlighting their impact on both reasoning accuracy and the length of model responses.  The solid lines represent reasoning accuracy, while dashed lines depict the response length. This visualization helps to understand the effects of different base models and optimization algorithms on the effectiveness and efficiency of RL fine-tuning for LLMs.", "section": "Analysis on Reinforcement Learning"}, {"figure_path": "https://arxiv.org/html/2503.20783/x9.png", "caption": "Figure 9: Count of keyword occurrences out of 40,000 responses (500 questions \u00d7\\times\u00d7 8 responses per question \u00d7\\times\u00d7 10 temperatures). y is in log scale.", "description": "Figure 9 presents a bar chart visualizing the frequency of self-reflection keywords across different language models.  The data encompasses 40,000 responses, resulting from 500 questions, each with 8 responses generated at 10 different temperature settings. Each bar represents a model and keyword, showing how many times each keyword appeared in that model's responses. The y-axis is logarithmic, highlighting the variation in keyword usage across models.", "section": "C Keyword-based Detection and LLM-Based Identification of Self-Reflection Behaviors"}, {"figure_path": "https://arxiv.org/html/2503.20783/x10.png", "caption": "Figure 10: Case (a): a false positive in keyword-based detection. Case (b): a false positive in LLM-based detection.", "description": "This figure shows two examples where automated methods for detecting self-reflection in large language model outputs fail.  Case (a) demonstrates a false positive from keyword-based detection, where the presence of keywords associated with self-reflection doesn't actually reflect true self-reflection in the response content. Case (b) illustrates a false positive from LLM-based detection, highlighting a situation where an LLM incorrectly identifies a response as demonstrating self-reflection despite the absence of such behavior.  These examples illustrate the challenges in accurately identifying self-reflection in LLM outputs using automated techniques.", "section": "C Keyword-based Detection and LLM-Based Identification of Self-Reflection Behaviors"}, {"figure_path": "https://arxiv.org/html/2503.20783/x11.png", "caption": "Figure 11: Comparison of keyword-based detection, LLM-based detection, and cross detection. Self-reflections are counted at the question level across 500 questions, where a question is marked as having self-reflection if at least one of its eight responses exhibits self-reflection.", "description": "Figure 11 displays a comparison of three methods used to detect self-reflection in large language model responses: keyword-based detection, LLM-based detection, and a cross-validation method combining the two.  The analysis focuses on 500 questions, each with eight responses generated at different temperature settings. A question is classified as exhibiting self-reflection if at least one of its eight responses shows evidence of self-reflection according to the chosen method. The figure visually represents how the count of questions identified as self-reflective varies across the different detection methods and across different temperature settings, allowing for a comparison of the effectiveness and limitations of each approach.", "section": "C Keyword-based Detection and LLM-Based Identification of Self-Reflection Behaviors"}, {"figure_path": "https://arxiv.org/html/2503.20783/x12.png", "caption": "Figure 12: Breakdown of response categories across difficulty levels in the MATH dataset for DeepSeek-V3-Base and DeepSeek-R1-Zero.", "description": "Figure 12 presents a comparison of response categories from the MATH dataset for two models: DeepSeek-V3-Base (a pre-trained model) and DeepSeek-R1-Zero (a model fine-tuned using reinforcement learning).  The figure displays the distribution of response types (correct, incorrect, unformatted, truncated) across various difficulty levels (levels 2-5) within the MATH dataset for both models. This visual comparison highlights the impact of R1-Zero training on response quality and the shift in response categories after reinforcement learning is applied.", "section": "D Comparison Between DeepSeek-V3-Base and DeepSeek-R1-Zero"}, {"figure_path": "https://arxiv.org/html/2503.20783/x13.png", "caption": "Figure 13: Accuracy difference between responses with and without self-reflection for each question (responses sampled from DeepSeek-R1-Zero).", "description": "This figure displays the accuracy difference between DeepSeek-R1-Zero model responses that contain self-reflection and those that do not, for each question in a dataset.  It illustrates the extent to which the presence of self-reflection in a response correlates with improved accuracy.  Each data point represents a question, and the y-axis value indicates the difference in accuracy between responses with and without self-reflection for that specific question.", "section": "D Comparison Between DeepSeek-V3-Base and DeepSeek-R1-Zero"}]