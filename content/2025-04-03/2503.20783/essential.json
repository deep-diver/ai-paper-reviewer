{"importance": "This paper critically examines **R1-Zero-like training**, offering a minimalist recipe that improves **token efficiency and performance** in LLMs. It\u2019s important for researchers aiming to enhance LLM reasoning capabilities efficiently, providing insights into **pretraining characteristics and optimization biases**.", "summary": "R1-Zero Training Critically Analyzed!", "takeaways": ["Base models' pretraining characteristics significantly influence RL performance.", "GRPO optimization has biases; Dr. GRPO addresses these, improving token efficiency.", "A minimalist R1-Zero recipe achieves state-of-the-art performance on AIME 2024 with a 7B model."], "tldr": "DeepSeek's R1-Zero uses Reinforcement Learning (RL) to boost Large Language Models (LLMs) reasoning without fine-tuning. This work analyzes R1-Zero-like training, focusing on base models and RL components. It finds that base models like DeepSeek-V3-Base already show Aha moment while Qwen2.5 models show reasoning skills sans prompt templates, hinting at pretraining biases. \n\nThe paper finds optimization bias in Group Relative Policy Optimization (GRPO), inflating response length. To fix this, they present Dr. GRPO which maintains reasoning but uses tokens more efficiently. With these insights, they make a minimalist R1-Zero setup, getting 43.3% accuracy on AIME 2024 using a 7B base model.", "affiliation": "Sea AI Lab", "categories": {"main_category": "Machine Learning", "sub_category": "Reinforcement Learning"}, "podcast_path": "2503.20783/podcast.wav"}