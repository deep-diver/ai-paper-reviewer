[{"figure_path": "https://arxiv.org/html/2504.01956/x2.png", "caption": "Figure 1: VideoScene enables one-step video generation of 3D scenes with strong structural consistency from just two input images. The top row shows the input sparse views and the following two rows show the output novel-view video frames.", "description": "This figure demonstrates VideoScene's ability to generate a short video of a 3D scene from only two input images.  The top row displays the two input images, which are sparse views of the scene. The subsequent rows show generated video frames from novel viewpoints, showcasing the model's capacity to create consistent 3D structure from limited information. The strong structural consistency between frames highlights the quality of the generated 3D scene.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.01956/x3.png", "caption": "Figure 2: Pipeline of VideoScene. Given input pair views, we first generate a coarse 3D representation with a rapid feed-forward 3DGS model (i.e., MVSplat\u00a0[17]), which enables accurate camera-trajectory-control rendering. The encoded rendering latent (\u201cinput\u201d) and encoded input pairs latent (\u201ccondition\u201d) are combined as input to the consistency model. Subsequently, a forward diffusion operation is performed to add noise to the video. Then, the noised \ud835\udc31n+1rsuperscriptsubscript\ud835\udc31\ud835\udc5b1\ud835\udc5f\\mathbf{x}_{n+1}^{r}bold_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT is sent to both the student and teacher model to predict videos \ud835\udc310p\u2062r\u2062e\u2062dsuperscriptsubscript\ud835\udc310\ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc51\\mathbf{x}_{0}^{pred}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p italic_r italic_e italic_d end_POSTSUPERSCRIPT of timestep tn+1subscript\ud835\udc61\ud835\udc5b1t_{n+1}italic_t start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT and \ud835\udc31^0\u03d5superscriptsubscript^\ud835\udc310italic-\u03d5\\hat{\\mathbf{x}}_{0}^{\\phi}over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_\u03d5 end_POSTSUPERSCRIPT of timestep tnsubscript\ud835\udc61\ud835\udc5bt_{n}italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. Finally, the student model and DDPNet are updated independently through distillation loss and DDP loss.", "description": "The figure illustrates the pipeline of VideoScene, a novel framework for one-step 3D scene generation from two input images. First, a feed-forward 3DGS model (MVSplat) generates a coarse 3D representation, enabling accurate camera trajectory control rendering.  The encoded renderings and input image pairs are then combined and fed into a consistency model. Next, a forward diffusion operation adds noise to the video. This noisy video is used to train both student and teacher models to predict videos at different timesteps. Finally, the student model and a dynamic denoising policy network (DDPNet) are updated independently using distillation loss and DDP loss to optimize video generation.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.01956/x4.png", "caption": "Figure 3: Qualitative comparison. We can observe that baseline models suffer from issues such as blurriness, frame skipping, excessive motion, and shifts in the relative positioning of objects, while our VideoScene achieves higher output quality and improved 3D coherence.", "description": "This figure compares the 3D scene generation results of VideoScene against three baseline methods: Stable Video Diffusion, DynamiCrafter, and CogVideoX.  Each method's output is shown for 1, 4, and 50 denoising steps. The comparison highlights VideoScene's superior quality, particularly in its ability to achieve high quality with only one step.  The baseline methods all exhibit artifacts such as blurriness, skipped frames, excessive motion, and inconsistent object positions, particularly pronounced in the one-step results.  VideoScene generates significantly clearer and more structurally consistent videos even in its single-step output, demonstrating improved 3D coherence and efficiency compared to the baselines.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.01956/x5.png", "caption": "Figure 4: Qualitative results in cross-dataset generalization. Models trained on the source dataset RealEstate10K are tested on ACID dataset. Fine-tuned models improve in 3D consistency but still fail with one-step.", "description": "This figure demonstrates the cross-dataset generalization capabilities of VideoScene and several baseline models.  Models were initially trained on the RealEstate10K dataset, then tested on the unseen ACID dataset to evaluate their ability to generalize to new, unseen data. The results show that while fine-tuning baseline models (DynamiCrafter and CogVideoX) on 3D data improves their 3D consistency, they still struggle to produce high-quality results with only one inference step. In contrast, VideoScene maintains high quality even with one inference step, demonstrating its strong generalization capabilities.", "section": "4.2 Comparison with Baselines"}, {"figure_path": "https://arxiv.org/html/2504.01956/x6.png", "caption": "Figure 5: Matching results comparison. Green represents high-quality matching results, while red represents discarded matching results. More green high-quality matches indicate a higher level of geometric consistency between the two views.", "description": "This figure compares the results of structure matching between pairs of images in generated videos. The green dots represent successful matches, signifying that corresponding points in the two frames have been correctly identified. Conversely, the red dots indicate failed matches, where the corresponding points could not be reliably identified.  A higher number of green dots, and thus a lower number of red dots, suggests a higher level of geometric consistency between the frames. This implies that the generated videos maintain accurate camera trajectories and consistent 3D structures. This is an important evaluation metric because it measures the reliability of depth estimation and camera pose inference that are crucial for accurate 3D scene reconstruction. ", "section": "4.2 Comparison with Baselines"}, {"figure_path": "https://arxiv.org/html/2504.01956/x7.png", "caption": "Figure 6: Visual results of ablation study. We ablate the design choices of 3D-aware leap flow distillation and dynamic denoising policy network (DDPNet).", "description": "This figure shows an ablation study on the VideoScene model, illustrating the impact of two key components: 3D-aware leap flow distillation and the dynamic denoising policy network (DDPNet).  By systematically removing each component, the figure visually demonstrates their individual contributions to the final video output quality.  The results showcase how both components are crucial for generating high-quality 3D-consistent videos, with the omission of either resulting in noticeable artifacts or lower quality in the final output.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.01956/x8.png", "caption": "Figure 7: Visual results of the generative ability. We highlight the generated regions in the red boxes in the novel generated views.", "description": "This figure showcases the model's ability to generate novel views of a scene, starting from only two input images.  The red boxes highlight the areas of the generated views that were not directly visible in the original input. This demonstrates the model's capability to infer and fill in missing information, creating plausible and realistic-looking scenes. This process highlights the effectiveness of the 3D-aware leap flow distillation approach in ensuring high-quality and visually consistent 3D scene generation from limited information.", "section": "3.3. 3D-Aware Leap Flow Distillation"}, {"figure_path": "https://arxiv.org/html/2504.01956/x9.png", "caption": "Figure 8: Quantitative comparison across steps. We evaluate the results of CogVideo, DynamiCrafter, Stable Video Diffusion (SVD), and VideoScene across 1, 10, 20, 30, 40, and 50 steps. VideoScene not only outperforms the other methods but also demonstrates remarkable consistency, with its 1-step results closely approximating its 50-step results, whereas other methods exhibit a significant decline in performance over fewer steps.", "description": "This figure displays a quantitative comparison of the performance of four different video generation models (CogVideo, DynamiCrafter, Stable Video Diffusion, and VideoScene) across varying numbers of steps (1, 10, 20, 30, 40, and 50).  The results show that VideoScene significantly outperforms the other models in terms of overall quality and notably maintains consistent performance even with only a single step, while the other models show a substantial drop in performance as the number of steps decreases.", "section": "4.2 Comparison with Baselines"}, {"figure_path": "https://arxiv.org/html/2504.01956/x10.png", "caption": "Figure 9: Comparisons with base renderings with severe artifacts.", "description": "This figure displays a comparison of video frames generated by VideoScene against base renderings.  The base renderings suffer from severe artifacts, highlighting the improvements in visual quality achieved by VideoScene.  The comparison emphasizes the effectiveness of VideoScene in mitigating common issues in video generation, such as blurriness, inconsistent geometry, and missing details.", "section": "8. More Qualitative Comparison Results"}, {"figure_path": "https://arxiv.org/html/2504.01956/x11.png", "caption": "Figure 10: Comparisons with 3D-aware diffusion model ViewCrafter.", "description": "This figure compares the performance of VideoScene against ViewCrafter, another 3D-aware diffusion model. It visually demonstrates the efficiency of VideoScene by showing that VideoScene achieves comparable results to ViewCrafter in just 3 seconds, while ViewCrafter takes 7 minutes.  This comparison highlights the significant speed advantage of VideoScene without sacrificing quality.", "section": "8. More Qualitative Comparison Results"}, {"figure_path": "https://arxiv.org/html/2504.01956/x12.png", "caption": "Figure 11: Comparisons with NeRF-based methods.", "description": "This figure compares the performance of VideoScene against other NeRF-based methods (PixelNeRF, MVSNeRF, and SparseNeRF) for novel view synthesis. It showcases the significant improvement in frame rate achieved by VideoScene compared to these methods, highlighting its efficiency and speed advantages in generating novel views.", "section": "8. More Qualitative Comparison Results"}, {"figure_path": "https://arxiv.org/html/2504.01956/x13.png", "caption": "Figure 12: Quantitative comparison across additional dimensions. We further evaluate the 1-step and 50-step results by incorporating additional dimensions from the VBench metrics.", "description": "Figure 12 presents a comprehensive quantitative comparison of VideoScene against several baseline models, using metrics from the VBench benchmark.  It visually displays the performance of 1-step and 50-step versions across various metrics, including Fr\u00e9chet Video Distance (FVD), Aesthetic Quality, Subject Consistency, Background Consistency, Motion Smoothness, Dynamic Degree, and Imaging Quality.  The figure helps illustrate VideoScene's efficiency by showing that its one-step performance is comparable to the 50-step results of other methods, which degrade significantly with fewer steps.", "section": "8.2 More Quantitative Comparison Results"}, {"figure_path": "https://arxiv.org/html/2504.01956/x14.png", "caption": "Figure 13: Qualitative comparison on Mip-Nerf 360 and Tank-and-Temples. With two sparse views as input, our method achieves much better reconstruction quality compared with baselines.", "description": "This figure compares the 3D reconstruction quality of VideoScene against several baseline methods (3DGS, SparseNeRF, DNGaussian, and InstantSplat) using the Mip-NeRF 360 and Tanks and Temples datasets.  The input to all methods consists of only two sparse views of a scene.  The results demonstrate that VideoScene produces significantly more accurate and detailed 3D reconstructions, highlighting its ability to generate high-quality 3D models from limited input data.", "section": "4.2 Comparison with Baselines"}, {"figure_path": "https://arxiv.org/html/2504.01956/x15.png", "caption": "Figure 14: Fail case of passing directly through the closed door.", "description": "In this failure case, the generated video incorrectly passes directly through a closed door instead of navigating around it. This demonstrates a limitation of the model in handling complex 3D scene understanding, specifically scenarios requiring accurate spatial reasoning and object interaction.", "section": "8.6 Failure Case"}]