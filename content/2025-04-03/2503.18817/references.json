{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP, a vision-language model that is foundational to the OoDD research discussed in the paper."}, {"fullname_first_author": "Xue Jiang", "paper_title": "Negative label guided ood detection with pretrained vision-language models", "publication_date": "2024-01-01", "reason": "This paper introduces NegLabel, a method that leverages negative labels to enhance OoDD performance in VLMs, and is used as a baseline for comparison in this paper."}, {"fullname_first_author": "Rui Huang", "paper_title": "Mos: Towards scaling out-of-distribution detection for large semantic space", "publication_date": "2021-01-01", "reason": "This paper introduces the MOS benchmark, used in this paper for evaluating OoDD performance."}, {"fullname_first_author": "Jingyang Zhang", "paper_title": "Openood v1. 5: Enhanced benchmark for out-of-distribution detection", "publication_date": "2023-01-01", "reason": "This paper introduces the OpenOOD v1.5 benchmark, which is used in this paper for evaluating OoDD performance in Near- and Far-OoD scenarios."}, {"fullname_first_author": "Sachin Goyal", "paper_title": "Finetune like you pretrain: Improved finetuning of zero-shot vision models", "publication_date": "2023-01-01", "reason": "This paper introduces FLYP, a fine-tuning method which mimics the CLIP pre-training scheme, and is used as a baseline for comparison in this paper."}]}