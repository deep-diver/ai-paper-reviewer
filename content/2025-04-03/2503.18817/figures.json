[{"figure_path": "https://arxiv.org/html/2503.18817/x1.png", "caption": "(a) Illustration of CMA", "description": "This figure illustrates the concept of Cross-Modal Alignment (CMA) in a hyperspherical embedding space.  Initially, the embedding space shows a bipartite separation between image and text embeddings.  Through CMA, in-distribution (ID) images and texts are brought closer together in the hypersphere while maintaining a clear separation from out-of-distribution (OoD) texts. This alignment enhances the discriminability of ID data from OoD data and negative examples, improving both in-distribution accuracy and out-of-distribution detection performance.", "section": "3. Methods"}, {"figure_path": "https://arxiv.org/html/2503.18817/x2.png", "caption": "(b) MOS benchmark: ID ACC vs. 1-FPR95", "description": "The figure shows a scatter plot comparing the in-distribution accuracy (ID ACC) against the out-of-distribution detection performance (1 - FPR95) on four different out-of-distribution datasets using the MOS benchmark.  Each point represents a different method. The top-right corner indicates superior performance (high ID accuracy and high OoD detection rate). The plot illustrates the trade-off between ID accuracy and OoD performance across various methods, including zero-shot, prompt learning, single-modality fine-tuning, and multi-modality fine-tuning approaches.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18817/x3.png", "caption": "Figure 1: (a) illustrates the hyperspherical embedding space and the corresponding cosine similarity values between the \u201cdog\u201d image and \u201cA photo of a <label>expectationlabel<\\textit{label}>< label >\u201d text embeddings. Initially, the embedding space shows a bipartite separation between images and texts (top)\u00a0[26, 35, 39]. Through CMA, ID images and texts are brought closer together while maintaining a clear separation from OoD texts (bottom). This alignment enhances the discriminability of ID data from negative concepts (i.e., OoD labels), thereby improving OoDD performance. In (b), uncolored shapes represent MCM, while colored shapes denote NegLabel. The arrows indicate the effect of NegLabel compared to MCM, demonstrating that our method enhances its effectiveness.\nPoints closer to the top right indicate better ID accuracy and OoDD performance.", "description": "Figure 1(a) shows how the hyperspherical embedding space changes before and after applying the proposed Cross-Modal Alignment (CMA) method.  Before CMA, image and text embeddings are distinctly separated (a phenomenon called modality gap). After CMA, in-distribution (ID) image and text embeddings (e.g., 'dog' image and 'A photo of a dog' text) are brought closer together, while out-of-distribution (OoD) text embeddings remain separated. This improved alignment enhances the model's ability to distinguish ID data from OoD data, leading to better out-of-distribution detection (OoDD) performance. Figure 1(b) compares different OoD detection methods (MCM and NegLabel). Our method (CMA) improves upon both, particularly improving NegLabel's effectiveness. The top-right corner represents high ID accuracy and OoDD performance, showing that CMA achieves state-of-the-art results.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.18817/x4.png", "caption": "(a) Zero-Shot", "description": "Figure 4 visualizes the embedding space learned by different models using PCA on ImageNet-1k data.  Subfigures (a) through (d) show the embedding space produced by Zero-Shot, FLYP, m\u00b2-mix, and the proposed CMA method, respectively.  In each subfigure, orange points represent ID image embeddings, and blue points represent ID text embeddings. The visualization helps to illustrate how the different methods position ID image and text embeddings in the embedding space and shows the effectiveness of the proposed CMA method in aligning image and text embeddings.", "section": "5. Analysis of Hyperspherical Embeddings"}, {"figure_path": "https://arxiv.org/html/2503.18817/x5.png", "caption": "(b) FLYP", "description": "This figure shows the visualization of image and text embeddings using Principal Component Analysis (PCA) on the ImageNet-1k dataset and negative texts.  The visualization specifically focuses on the results obtained using the FLYP method. Orange and blue points represent in-distribution (ID) image and text embeddings respectively, while red points depict the negative text embeddings.  The visualization helps illustrate how FLYP handles the multi-modal representation space and the separation between ID and OOD data.", "section": "5. Analysis of Hyperspherical Embeddings"}, {"figure_path": "https://arxiv.org/html/2503.18817/x6.png", "caption": "(c) m2superscript\ud835\udc5a2m^{2}italic_m start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT-mix", "description": "The figure displays the results of the m2-mix method on the hyperspherical embedding space.  It visually demonstrates how m2-mix, a multi-modal fine-tuning method, affects the arrangement of image and text embeddings. In particular, it likely shows the distribution of image and text embeddings in a hyperspherical space, highlighting the impact of modality gap reduction on out-of-distribution (OoD) detection performance.  The visualization may use techniques like t-SNE or PCA to reduce the dimensionality of the high-dimensional embedding space for easier interpretation.", "section": "4. Experiments"}]