[{"figure_path": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/1-teaser-v2.png", "caption": "Figure 1: We introduce DreamActor-M1, a DiT-based human animation framework, with hybrid guidance to achieve fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence.", "description": "Figure 1 showcases the capabilities of DreamActor-M1, a novel human image animation framework.  It uses a diffusion transformer (DiT) architecture and incorporates hybrid guidance to enable precise control over various aspects of the animation.  The image demonstrates the system's ability to produce realistic and expressive animations with fine-grained control over facial features and body movements.  Crucially, it shows the model's adaptability to different scales (from portrait to full body) and its ability to maintain coherence over extended time periods. The reference image on the left is compared to a generated human video sequence on the right, highlighting the realism and detail of the output.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/2-overview-v3.png", "caption": "Figure 2: Overview of DreamActor-M1. During the training stage, we first extract body skeletons and head spheres from driving frames and then encode them to the pose latent using the pose encoder. The resultant pose latent is combined with the noised video latent along the channel dimension. The video latent is obtained by encoding a clip from the input full video using 3D VAE. Facial expression is additionally encoded by the face motion encoder, to generate implicit facial representations. Note that the reference image can be one or multiple frames sampled from the input video to provide additional appearance details during training and the reference token branch shares weights of our DiT model with the noise token branch. Finally, the denoised video latent is supervised by the encoded video latent. Within each DiT block, the face motion token is integrated into the noise token branch via cross-attention (Face Attn), while appearance information of ref token is injected to noise token through concatenated self-attention (Self Attn) and subsequent cross-attention (Ref Attn).", "description": "DreamActor-M1's architecture uses a diffusion transformer (DiT) to generate human image animations.  The model takes a reference image and driving video as input.  During training, body skeletons and head spheres are extracted from the driving video and encoded into a pose latent. This pose latent is combined with a noised video latent (obtained from encoding a video clip with a 3D VAE) and a facial expression latent (generated from a face motion encoder). The reference image can be single or multiple frames, improving detail. The reference token branch shares weights with the noise token branch in the DiT. The model uses a multi-stage process with different tokens (noise, reference, face motion) integrated via cross-attention and self-attention within each DiT block. The denoised video latent is then compared with the encoded video latent for supervision.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/3-inference-v3.png", "caption": "Figure 3: Overview of our inference pipeline. First, we (optionally) generate multiple pseudo-references to provide complementary appearance information. Next, we extract hybrid control signals comprising implicit facial motion and explicit poses (head sphere and body skeleton) from the driving video. Finally, these signals are injected into a DiT model to synthesize animated human videos. Our framework decouples facial motion from body poses, with facial motion signals being alternatively derivable from speech inputs.", "description": "This figure illustrates the inference pipeline of the DreamActor-M1 model.  The process begins with an optional step to generate multiple pseudo-references, which enhances the model's ability to handle unseen regions and improves temporal consistency. Next, hybrid control signals are extracted from the driving video. These signals include implicit facial motion information (captured without explicit landmark use), and explicit pose information (represented by a 3D head sphere and a 3D body skeleton).  These signals are then fed into the DiT (Diffusion Transformer) model. The model synthesizes an animated human video based on these inputs. Importantly, the framework decouples facial motion from body poses, allowing for flexibility in control; facial movements can also be driven by audio input (speech) in addition to video.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/4-qualitative-pose-v1.png", "caption": "Figure 4: The comparisons with human image animation works, Animate Anyone\u00a0[14], Champ\u00a0[61], MimicMotion\u00a0[57] and DisPose\u00a0[20]. Our method demonstrates results with better fine-grained motions, identity preservation, temporal consistency and high fidelity.", "description": "Figure 4 presents a qualitative comparison of DreamActor-M1 against several state-of-the-art human image animation methods: AnimateAnyone [14], Champ [61], MimicMotion [57], and DisPose [20].  Each method is applied to the same reference image and driving video. The results showcase DreamActor-M1's superior performance in generating animations with finer details in the movements, better preservation of the subject's identity across frames, greater temporal consistency over longer sequences, and overall higher visual fidelity.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/4-qualitative-portrait-v1.png", "caption": "Figure 5: Our comparisons with portrait image animation works, LivePortrait\u00a0[12], X-Portrait\u00a0[49], Skyreels-A1\u00a0[30] and Runway Act-One\u00a0[33]. Our method demonstrates more accurate and expressive portrait animation capabilities.", "description": "Figure 5 presents a comparison of DreamActor-M1 with several state-of-the-art portrait animation methods: LivePortrait [12], X-Portrait [49], Skyreels-A1 [30], and Runway Act-One [33].  The results showcase DreamActor-M1's superior ability to generate more accurate and expressive portrait animations. Each column displays the reference image, driving image (used to animate the portrait), and the output from each method, including DreamActor-M1. This allows for a direct visual comparison of the generated animations, highlighting DreamActor-M1's improvements in detail and expressiveness.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.01724/extracted/6330760/figs/5-ablation.png", "caption": "Figure 6: Ablation study of 3D skeletons with bone length adjustment (BLA) and implicit face features.", "description": "This ablation study investigates the effects of using 3D body skeletons with bone length adjustment (BLA) and implicit facial features in the DreamActor-M1 model.  It compares the animation results when including both, only 3D skeletons with and without BLA, and only implicit face motions, highlighting the individual and combined contributions of these components to the overall quality and realism of the generated animations.", "section": "3.3 Complementary Appearance Guidance"}]