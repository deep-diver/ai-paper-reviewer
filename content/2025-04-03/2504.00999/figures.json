[{"figure_path": "https://arxiv.org/html/2504.00999/x1.png", "caption": "Figure 1: MergeVQ learning paradigms. (a) The MergeVQ Tokenizer extracts K\ud835\udc3eKitalic_K semantic tokens with decoupled positional information (retained in the source matrix) by ToMe\u00a0[7] while quantizing spatial details by LFQ\u00a0[49, 73], which will be recovered and reconstructed correspondingly. (b) MergeVQ with random-order Generator\u00a0[51] generates K\ud835\udc3eKitalic_K discrete tokens with associated position instructions while trained Source Prediction and decoder restore position details.\n(c) MergeAR Generator predicts L\ud835\udc3fLitalic_L tokens efficiently in a raster-order with tailored KV Cache compression to remove the redundancy within Next-token Prediction (NTP)\u00a0[57].", "description": "Figure 1 illustrates the MergeVQ framework's three learning paradigms. (a) shows the pre-training stage, where the MergeVQ Tokenizer uses ToMe to extract key semantic tokens, decoupling positional information stored in a source matrix, while LFQ handles spatial detail quantization. The decoder reconstructs the image using both these tokens and the positional information from the matrix. (b) depicts the random-order generation, where a random-order generator produces tokens along with position instructions.  The decoder reconstructs the image using these positional cues. (c) presents MergeAR's efficient raster-order generation. It leverages KV cache compression to eliminate redundancy in next-token prediction, thereby speeding up the process.", "section": "3. MergeVQ Learning Paradigm"}, {"figure_path": "https://arxiv.org/html/2504.00999/x2.png", "caption": "Figure 2: Overview of MergeVQ framework, which contains two stages and three groups of subtasks (Sec.\u00a03.1). (a) As for representation learning (Sec.\u00a03.2), K\ud835\udc3eKitalic_K semantic tokens are extracted by the encoder with self-attention and token merging\u00a0[7], which can be aligned globally with a pre-trained teacher while learning contextual information by predicting the source matrix.\n(b) As for reconstruction (Sec.\u00a03.3), taking K\ud835\udc3eKitalic_K merged and quantized tokens as the input, the positional information can be retained by the Source Recovery module, and then high-quality details will be reconstructed. (c) As for generation (Sec.\u00a04), we utilize the source matrix to construct a causal mask for training and leverage the KV cache to prune repeated tokens during inference for efficient generation.", "description": "MergeVQ is a two-stage framework for visual representation learning and generation. Stage 1 (a) uses an encoder with self-attention and a token merge module [7] to extract K semantic tokens, which are then aligned globally with a pre-trained teacher.  The encoder also predicts a source matrix that retains positional information.  Stage 2 has two parts: reconstruction (b) and generation (c). In reconstruction, K merged and quantized tokens are used as input to the decoder, and positional information is recovered using the source matrix to reconstruct high-quality image details. In generation, the source matrix is used to create a causal mask for training and a KV cache to remove duplicate tokens for more efficient inference.", "section": "3. MergeVQ Learning Paradigm"}, {"figure_path": "https://arxiv.org/html/2504.00999/x3.png", "caption": "Figure 3: Analysis of kept tokens in reconstruction and representation learning. Three MergeVQ tokenizers are trained with 128128128128 resolution for 30 epochs on ImageNet-1K.\nThey keep 256, 144, and 36 tokens with ToMe\u00a0[7] in the encoder during training. In inference, we evaluate rFID and linear probing top-1 accuracy with diverse merge ratios to show the trade-off between generation and representation. Please view Sec.\u00a05 and Appendix\u00a0B for details.", "description": "This figure analyzes the impact of the number of tokens retained after the ToMe [7] token merging module on both image reconstruction quality (measured by rFID) and representation learning performance (measured by linear probing top-1 accuracy).  Three variations of the MergeVQ tokenizer were trained on ImageNet-1K at a resolution of 128x128, each retaining a different number of tokens (256, 144, and 36) during the training process. The experiment evaluates the reconstruction and representation capabilities of these models by varying the number of tokens kept during inference. This allows for analysis of the trade-off between the generative (reconstruction) and discriminative (representation learning) capabilities of the model as influenced by the token count.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.00999/x4.png", "caption": "Figure 4: Visualization of MergeVQ (G+R) reconstruction. With the kept tokens varying from 64 to 256, clustering maps of ToMe Attention indicate that MergeVQ can extract discriminative semantic tokens while recovering contextual positions and details.", "description": "This figure visualizes the reconstruction capabilities of the MergeVQ (G+R) model.  It shows the results of reconstructing images using different numbers of kept tokens (64, 144, and 256).  The clustering maps generated by the ToMe Attention module are displayed alongside the reconstructed images. These maps highlight how MergeVQ effectively extracts key semantic tokens while simultaneously preserving detailed spatial information from the original image.  The increasing realism of the reconstructed images as the number of kept tokens increases demonstrates the model's ability to balance efficient compression with high-fidelity reconstruction.", "section": "3.2 Harmonize Representation and Generation"}, {"figure_path": "https://arxiv.org/html/2504.00999/x5.png", "caption": "Figure 5: Distribution of merge ratios sampling in training. (a) With 256 tokens in total, MergeVQ (R) and (G+R) sample the square number as kept token numbers in [36,100]36100[36,100][ 36 , 100 ] and [121,225]121225[121,225][ 121 , 225 ] with exponential and Gaussian distributions for stage-1 training, while the G+R version sampling from [144,256]144256[144,256][ 144 , 256 ] for stage-2 training.\n(b) With 1024 tokens in total, MergeVQ (G) samples the square kept number in [225,400]225400[225,400][ 225 , 400 ] and [256,1024]2561024[256,1024][ 256 , 1024 ] with Gaussian and exponential distributions in both stage-1 and stage-2 training.", "description": "This figure illustrates the distribution of merge ratios used during the training of the MergeVQ model.  Two subfigures are shown. (a) shows the distribution for models with a total of 256 tokens.  For stage 1 training, the MergeVQ (R) and (G+R) models sample a square number of tokens to keep, ranging from 36 to 100 and 121 to 225, respectively, with the distribution following an exponential and a Gaussian pattern.  In stage 2 training, the (G+R) model samples square numbers between 144 and 256. (b) shows the distribution for models with 1024 tokens.  The MergeVQ (G) model samples square numbers of tokens to keep in stage 1 between 225 and 400, and in stage 2 between 256 and 1024, using Gaussian and exponential distributions.", "section": "3.3 Token Recovery and Reconstruction"}, {"figure_path": "https://arxiv.org/html/2504.00999/x6.png", "caption": "Figure 6: Illustration of MergeAR pipeline. (a) MergeAR training with the source matrix and K\ud835\udc3eKitalic_K-sparse target sequences from the MergeVQ tokenizer to build a causal mask with duplicated tokens masked out, taking a class token and a merge instruction token as the starting conditions.\n(b) MergeAR inference that generates L\ud835\udc3fLitalic_L tokens in the raster order with duplicated tokens detected and removed in the position and KV Caches.", "description": "Figure 6 illustrates the MergeAR pipeline, a method for efficient visual generation.  Panel (a) shows the training process.  The input consists of the source matrix (encoding positional information from the MergeVQ tokenizer) and a K-sparse target sequence (representing the desired output with K tokens).  Duplicate tokens are masked out, creating a causal mask. A class token and merge instruction token initiate the generation. Panel (b) details inference.  The process generates L tokens in raster order. A position cache and KV cache identify and remove duplicate tokens, improving efficiency. ", "section": "4. MergeVQ for Efficient Generation"}, {"figure_path": "https://arxiv.org/html/2504.00999/x7.png", "caption": "Figure A1: Visualization of tokenizer reconstruction on ImageNet-1K.\nWe conducted reconstruction experiments with our G version using 1024, 576, 400, 256, and 144 tokens and with our G+R version using 256, 196, 144, 100, 64, and 36 tokens. The reconstruction results are shown in the figure. As the number of retained tokens increases, the reconstruction becomes more realistic.", "description": "This figure visualizes the reconstruction quality of the MergeVQ tokenizer on ImageNet-1K.  It shows the reconstruction results for four different images, comparing the performance of the MergeVQ 'G' and 'G+R' versions using different numbers of retained tokens (1024, 576, 400, 256, 144 for 'G'; 256, 196, 144, 100, 64, 36 for 'G+R').  The results demonstrate that as the number of tokens retained increases, the quality of the reconstructed images improves, becoming more realistic.", "section": "More Experiment Results"}, {"figure_path": "https://arxiv.org/html/2504.00999/x8.png", "caption": "Figure A2: Visualization of class conditional generation with MergeVQ variants on ImageNet-1K. The G version performs generation on 256 tokens, and the G+R version performs generation on 144 tokens.", "description": "This figure visualizes the results of class-conditional image generation using two variants of the MergeVQ model: MergeVQ(G) and MergeVQ(G+R).  MergeVQ(G), designed primarily for generation, utilizes 256 tokens, while MergeVQ(G+R), balancing generation and representation learning, uses 144 tokens. The figure displays generated images for several classes from ImageNet-1K, showcasing the visual quality and diversity achieved by each model variant with different token counts.  The comparison highlights the trade-off between model complexity (number of tokens) and the quality/diversity of generated images.", "section": "B. More Experiment Results"}]