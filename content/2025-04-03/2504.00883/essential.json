{"importance": "This paper is important for researchers because it **addresses the critical gap in visual-spatial reasoning capabilities of MLLMs**, a key area for AI agents operating in the physical world. The introduction of vsGRPO training and the VSI-100k dataset **provides a valuable new resource** for the community, potentially **spurring further research** into more effective training methods.", "summary": "vsGRPO training boosts visual-spatial reasoning in MLLMs, outperforming fine-tuning and surpassing GPT-40 on challenging tasks.", "takeaways": ["Small- to medium-sized Qwen2-VL models struggle with visual-spatial reasoning even with Chain of Thought prompting.", "vsGRPO training significantly improves visual-spatial reasoning in Qwen2-VL models, even outperforming larger models.", "Maintaining the KL penalty during GRPO training is essential for stable and effective learning."], "tldr": "Multi-modal large language models(MLLMs) are enhanced with video-based visual-spatial intelligence (VSI), which is key for AI agents acting in the real world. Current models often fall short in VSI. This work first identifies that small- to medium-sized Qwen2-VL models can't activate VSI with Chain of Thought (CoT) prompts. The key issue is that these models don't effectively trade inference FLOPs to improve visual-spatial reasoning. \n\nTo address this, the paper conducts a study on improving visual-spatial reasoning of MLLMs using R1-Zero-like training. They introduce vsGRPO training using the VSI-100k dataset, following DeepSeek-R1-Zero. The vsGRPO-2B model outperforms the base model and surpasses GPT-40. vsGRPO-7B achieves similar performance to LLaVA-NeXT-Video-72B, with strong performance superiority over supervised fine-tuning.", "affiliation": "Shanghai Jiao Tong University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.00883/podcast.wav"}