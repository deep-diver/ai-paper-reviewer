[{"figure_path": "https://arxiv.org/html/2504.00883/", "caption": "Figure 1: Comparison between the  vanilla-mode and  think-mode predictions.", "description": "This figure displays a comparison of the model's responses to a visual question using two different prompting methods: 'vanilla-mode' and 'think-mode'.  The 'vanilla-mode' prompt is a simple request for an answer, while the 'think-mode' prompt encourages the model to explicitly detail its reasoning process step-by-step before providing an answer.  The figure shows that, even with a step-by-step reasoning process, the 'think-mode' prediction is incorrect. This highlights the difficulty of the model to accurately interpret and answer complex visual questions, even when prompted to provide a detailed reasoning process. The image demonstrates the model's struggle with visual-spatial reasoning capabilities, specifically its inability to correctly identify all objects within the scene.", "section": "Can Visual-spatial Reasoning Capacities Be Activated by Prompting?"}, {"figure_path": "https://arxiv.org/html/2504.00883/x2.png", "caption": "Figure 2: Illustrations of the constructed dataset.", "description": "Figure 2 shows example questions and answers from the VSI-100k dataset, illustrating the six question types used to evaluate visual-spatial reasoning.  These include object counting, relative direction, relative distance, object size, room size, and absolute distance. The image provides visual context for each question, showcasing the diversity of scenarios and types of spatial reasoning required.", "section": "3.1 Training Data Construction"}]