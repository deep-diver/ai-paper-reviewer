[{"figure_path": "https://arxiv.org/html/2503.23368/x1.png", "caption": "Figure 1: Existing commercial closed-source VDMs fail to generate physically plausible motion, whereas our video generation framework is able to achieve this by incorporating external physical prior knowledge.", "description": "The figure shows examples of video generation results from several video diffusion models (VDMs), including existing commercial closed-source models and the authors' proposed model.  The examples demonstrate scenarios with simple physics, like two balls colliding, water being poured, or a basketball falling. The commercial VDMs produce unrealistic motions that violate basic physical laws, while the authors' model generates videos with more physically plausible movements. The improvement is attributed to the incorporation of external physical prior knowledge into their framework.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.23368/x2.png", "caption": "Figure 2: The illustration of our physically plausible image-to-video generation pipeline. Our pipeline consists of two stages. In the first stage, the VLM generates a coarse-grained, physically plausible motion trajectory based on the provided input conditions. In the second stage, We simulate a synthetic video using the predicted trajectory to provide the motion condition. We then extract the optical flow from this video and convert it into structured noise. These conditions are fed into a motion controllable image-to-video diffusion model, and ultimately generates a physically plausible video.", "description": "This figure illustrates a two-stage pipeline for generating physically plausible videos.  Stage one uses a Vision-Language Model (VLM) to plan a coarse-grained, physically realistic motion trajectory based on an input image and text prompt.  Stage two simulates a video using the planned trajectory. Optical flow is extracted from this synthetic video and transformed into structured noise. This structured noise, along with the initial frame and prompt, is fed into a motion-controllable image-to-video diffusion model, resulting in a physically plausible final video.", "section": "3. Generation"}, {"figure_path": "https://arxiv.org/html/2503.23368/x3.png", "caption": "Figure 3: The illustration of chain-of-thought reasoning in the VLM for generating a coarse-grained motion trajectory. First off, the VLM determines the corresponding physical laws and its context for the given scene. Then, the VLM performs step-by-step reasoning to predict the physically plausible motions of objects in image space, leveraging physical context and chain-of-thought prompting. Finally, the VLM predicts bounding boxes according to real-world physics.", "description": "This figure illustrates the chain-of-thought reasoning process within the Vision-Language Model (VLM) for predicting coarse-grained motion trajectories.  The process begins with the VLM identifying the relevant physical laws and contextual information from the scene description.  Next, through a step-by-step reasoning process guided by chain-of-thought prompting and informed by the physical context, the VLM predicts the physically plausible movements of objects within the image space.  Finally, the VLM outputs a sequence of bounding boxes representing the predicted object positions, reflecting real-world physics.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.23368/x4.png", "caption": "Figure 4: Visual comparisons of physically plausible video generation results from our framework, CogVideoX-I2V-5B\u00a0[60], LTX-Video-I2V\u00a0[14] and SVD-XT\u00a0[5].", "description": "This figure presents a qualitative comparison of video generation results between four different methods: the proposed framework, CogVideoX-I2V-5B, LTX-Video-I2V, and SVD-XT. Each row shows the results for a different video generation task (e.g., a ball falling, pouring water).  The figure aims to visually demonstrate the superior ability of the proposed framework in generating physically plausible videos that accurately depict real-world physical phenomena, in contrast to the other methods which exhibit artifacts or physically implausible behavior.", "section": "4. Empirical Analysis and Discussion"}, {"figure_path": "https://arxiv.org/html/2503.23368/x5.png", "caption": "Figure 5: Visual comparisons of physically plausible video generation results from our framework, CogVideoX-I2V-5B, LTX-Video-I2V, SVD-XT and SG-I2V\u00a0[35] in the Physics-IQ dataset.", "description": "Figure 5 presents a qualitative comparison of video generation results produced by five different methods on the Physics-IQ benchmark dataset.  The methods compared include the authors' proposed framework and four existing models: CogVideoX-I2V-5B, LTX-Video-I2V, SVD-XT, and SG-I2V. Each row in the figure corresponds to a different physical scenario from the Physics-IQ dataset, with the input frame shown at the beginning of each row. Subsequent frames illustrate the video generated by each method. This allows for visual comparison of the physical plausibility and quality of the generated videos, highlighting the strengths and weaknesses of each approach.", "section": "4. Empirical Analysis and Discussion"}, {"figure_path": "https://arxiv.org/html/2503.23368/x6.png", "caption": "Figure 6: Physics-Aware Reasoning Template for Rigid Body Motion", "description": "This figure details the step-by-step reasoning process employed by the Vision Language Model (VLM) when predicting the motion of a rigid body, such as a ball.  It shows how the VLM analyzes the caption describing the physical scenario (e.g., a wooden ball dropping onto a table), identifies the relevant physical laws (Newton's laws of motion, conservation of momentum), and reasons through the impact of these laws on the object's motion over time.  The output is a sequence of predicted bounding box coordinates for the object in subsequent frames, representing the ball's trajectory as it falls, impacts the table, and bounces.", "section": "3. Generation based on VLMs Planning"}, {"figure_path": "https://arxiv.org/html/2503.23368/x7.png", "caption": "Figure 7: Physics-Aware Reasoning Template for fluid dynamics and thermodynamics.", "description": "Figure 7 presents a detailed template for physics-aware reasoning, specifically focusing on fluid dynamics and thermodynamics.  It shows a step-by-step process where a vision-language model (VLM) analyzes a caption describing a scenario involving fluids (e.g., pouring tea), identifies relevant physical laws (e.g., fluid mechanics), reasons through the implications of these laws for the given scenario, and then predicts the bounding box coordinates for each object over a series of frames. This allows the VLM to generate a coarse-grained motion trajectory that adheres to the principles of fluid dynamics and thermodynamics before feeding it to the video generation model.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.23368/x8.png", "caption": "Figure 8: More examples of generated videos related to fluid dynamics and thermodynamics.", "description": "This figure displays several example videos generated by the proposed model, focusing on scenarios involving fluid dynamics and thermodynamics.  The videos showcase a range of complex physical phenomena.  For example, there are examples of liquid pouring, resulting in ripples and changes in water level, and phase transitions of substances due to temperature changes, like melting butter or a candle wax.  The examples highlight the model's capability to generate realistic and physically plausible video sequences that adhere to the laws of physics in these areas.", "section": "4.4 Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.23368/x9.png", "caption": "Figure 9: More examples of generated videos related to thermodynamics.", "description": "Figure 9 presents additional examples of videos generated by the proposed model, focusing on scenarios related to thermodynamics.  The figure visually demonstrates the model's ability to produce realistic depictions of thermodynamic processes, such as the melting of butter and the burning of a candle. These examples showcase the model's capability to simulate changes in state due to heat, including changes in texture, form, and appearance, while respecting the physical properties of the materials involved.", "section": "4.4. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.23368/x10.png", "caption": "Figure 10: More examples of generated videos related to fluid dynamics.", "description": "Figure 10 presents a visual comparison of video generation results related to fluid dynamics from various models: Ours, CogVideoX, LTX-Video, and SVD-XT.  Each row showcases a specific scene: the first involves a stone gently placed on water, while the second involves a wooden block placed on water. For each scenario, the generated video frames are displayed across different models, highlighting variations in how accurately each model simulates the fluid dynamics (e.g., ripples, displacement of water) of the objects interacting with the water.", "section": "4. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.23368/x11.png", "caption": "Figure 11: More examples of generated videos related to optics.", "description": "This figure showcases several examples of videos generated by different models, focusing on scenarios related to optics.  It visually compares the outputs of the proposed method against existing state-of-the-art techniques (CogVideoX, LTX-Video, and SVD-XT). The comparisons highlight the ability of the proposed method to generate more realistic and physically plausible videos, particularly concerning phenomena related to reflection and refraction of light, as seen in the presented examples of oil poured into water and sulfuric acid reacting with cotton.", "section": "4. Qualitative Evaluation"}]