{"references": [{"fullname_first_author": "Anthropic", "paper_title": "Claude", "publication_date": "2024-01-01", "reason": "This is one of the representative closed-source MLLMs that is benchmarked against in this study."}, {"fullname_first_author": "Shuai Bai", "paper_title": "Qwen2.5-vl technical report", "publication_date": "2025-02-28", "reason": "This is one of the most important reference papers because it integrates robust video understanding and fine-grained visual grounding modules."}, {"fullname_first_author": "Zhe Chen", "paper_title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling", "publication_date": "2024-12-06", "reason": "This is one of the important references as it represents open-source MLLMs."}, {"fullname_first_author": "Google", "paper_title": "Gemini", "publication_date": "2023-01-01", "reason": "This is one of the most important reference papers because it represents one of the families of models that is compared against in the study."}, {"fullname_first_author": "Aaron Hurst", "paper_title": "Gpt-4o system card", "publication_date": "2024-10-28", "reason": "This is one of the most important reference papers as it represents another one of the models that is compared against in the study."}]}