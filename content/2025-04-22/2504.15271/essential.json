{"importance": "**Eagle 2.5** boosts long-context understanding in VLMs, matching closed-source models with enhanced training & a novel dataset. It offers new research directions for efficient & versatile VLMs in real-world applications by providing comprehensive details of training strategies and data recipes.", "summary": "Eagle 2.5: Better long-context vision-language via boosted post-training!", "takeaways": ["Eagle 2.5 significantly improves long-context multimodal understanding, especially for long videos and high-resolution images.", "The framework incorporates innovative techniques like Automatic Degrade Sampling and Image Area Preservation.", "The introduction of Eagle-Video-110K offers a valuable resource for long-video understanding, featuring story-level and clip-level annotations."], "tldr": "Current Vision-Language Models(VLMs) struggle with long-context tasks like video comprehension, facing hurdles in dataset creation and architecture. Existing methods often add overhead or fall short of top models. Optimal training is unclear due to complex factors. Therefore, there is a need to improve performance and increase visual input. \n\n**Eagle 2.5**, a versatile multimodal model, efficiently processes extensive contextual information, improving with longer inputs. It uses information-first sampling (Image Area Preservation and Automatic Degradation Sampling) and progressive training. The approach enhances information density, maintains performance across input types and embraces a diversity-first principle with **Eagle-Video-110K** dataset", "affiliation": "NVIDIA", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.15271/podcast.wav"}