[{"figure_path": "https://arxiv.org/html/2504.14603/x1.png", "caption": "Figure 1. A comparison of (a) existing CUAs and (b) desktop AgentOS UFO2.", "description": "This figure compares the architecture of existing Computer-Using Agents (CUAs) with the architecture of the novel desktop AgentOS UFO2.  Panel (a) illustrates the shallow OS integration and reliance on screenshot-based interaction of traditional CUAs. These systems lack deep access to OS-level functionalities and APIs, which limits their robustness and scalability. In contrast, panel (b) highlights the key features of UFO2, including deep OS integration, the use of application-specific APIs alongside GUI actions, and the hybrid control detection pipeline that fuses Windows UI Automation (UIA) and vision-based parsing.  The unified GUI-API action layer, HoSTAGENT for task decomposition and coordination, and the collection of application-specialized APPAGENTS are also shown. The Picture-in-Picture user interface is also depicted as a feature that enhances usability.  Overall, the figure visually demonstrates the significant architectural advancements of UFO2 compared to existing CUAs.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.14603/x2.png", "caption": "Figure 2. An overview of the architecture of UFO2.", "description": "The figure illustrates the architecture of UFO2, a multiagent AgentOS for Windows desktops.  It highlights the centralized HostAgent, responsible for task decomposition and coordination, and the application-specialized AppAgents which utilize native APIs and a unified GUI-API action layer for robust task execution.  The diagram also shows the interaction between the HostAgent, AppAgents, and Windows applications, emphasizing the system-level integration and modularity of the UFO2 design.", "section": "3 System Design of UFO2"}, {"figure_path": "https://arxiv.org/html/2504.14603/x3.png", "caption": "Figure 3. High-level architecture of HostAgent as a control-plane orchestrator.", "description": "The HostAgent is the central control plane of UFO2, responsible for managing the entire automation process.  It receives user requests (natural language), decomposes them into a series of subtasks, and dispatches these subtasks to specialized AppAgents.  The HostAgent maintains a persistent state machine that tracks the progress of each subtask and the overall workflow. It uses a two-pronged approach to understand the system's state: a visual layer (screenshots) for layout and context, and a semantic layer (Windows UI Automation APIs) for structural information about applications and controls.  The HostAgent uses this information to manage AppAgent instantiation, task scheduling, and the overall execution plan, facilitating communication and coordination among the AppAgents through a shared blackboard.", "section": "3 System Design of UFO2"}, {"figure_path": "https://arxiv.org/html/2504.14603/x4.png", "caption": "Figure 4. Control-state transitions managed by HostAgent.", "description": "This figure illustrates the finite-state machine (FSM) that governs the HostAgent's behavior.  The HostAgent manages the workflow and coordination of multiple AppAgents, moving through different states to handle tasks and their execution.  The states include ASSIGN (assigning a subtask to an AppAgent), PENDING (waiting for user input or clarification), CONTINUE (main execution loop), FINISH (all subtasks complete), and FAIL (an unrecoverable error occurred).  The transitions between these states are shown, indicating the flow of the automation process.", "section": "3 System Design of UFO2"}, {"figure_path": "https://arxiv.org/html/2504.14603/x5.png", "caption": "Figure 5. Architecture of an AppAgent, the per-application execution runtime in UFO2.", "description": "This figure details the architecture of an AppAgent within the UFO2 system.  AppAgents are responsible for executing individual subtasks within specific Windows applications. The diagram shows how an AppAgent uses visual input (GUI screenshots), semantic metadata (extracted from Windows UI Automation APIs), and symbolic annotations (from Set-of-Mark techniques) to understand the application state. This information feeds into an action execution module, which utilizes a hybrid approach, employing application-specific APIs when available and falling back to traditional GUI actions when necessary.  The APPAGENT incorporates a knowledge integration layer, drawing upon application documentation and prior execution experiences, to improve task reliability and efficiency. Finally, the AppAgent's operations are managed by a finite-state machine, ensuring proper control flow and error handling.", "section": "3.3 APPAGENT: Application-Specialized Execution Runtime"}, {"figure_path": "https://arxiv.org/html/2504.14603/x6.png", "caption": "Figure 6. Control-state transitions for an AppAgent runtime.", "description": "This figure illustrates the finite state machine governing the execution lifecycle of an individual AppAgent within UFO2.  The AppAgent starts in the CONTINUE state, iteratively performing perception, planning, and action. If a sensitive operation is needed, it transitions to PENDING, requiring user confirmation.  Successful task completion leads to the FINISH state, while failures result in the FAIL state. This FSM ensures robust and safe execution even in the context of a dynamic desktop environment.", "section": "3.3 APPAGENT: Application-Specialized Execution Runtime"}, {"figure_path": "https://arxiv.org/html/2504.14603/x7.png", "caption": "Figure 7. The hybrid control detection approach employed in UFO2.", "description": "This figure illustrates UFO2's hybrid control detection method, which combines the strengths of both Windows UI Automation (UIA) APIs and vision-based grounding models. UIA provides structured information about GUI elements, but might miss custom-rendered controls. In contrast, vision-based grounding offers pixel-level perception for a more complete understanding of the interface, including the identification of controls that are not accessible through UIA APIs. Therefore, UFO2 leverages a hybrid approach, integrating both methods to ensure reliable control grounding, even in complex or non-standard interfaces.  The figure shows how UIA and OmniParser-v2 (a vision-based model) detect controls, and how these detections are merged and refined to create a comprehensive control graph, representing the GUI state for an APPAGENT.", "section": "3.4 Hybrid Control Detection"}, {"figure_path": "https://arxiv.org/html/2504.14603/x8.png", "caption": "Figure 8. Puppeteer serves as a unified execution engine that harmonizes GUI actions and native API calls.", "description": "The Puppeteer is a module within the UFO2 architecture that acts as an intermediary between the APPAGENT and the target application.  It analyzes the task and decides whether it's more efficient to use the application's native APIs or traditional GUI actions (mouse clicks and keyboard inputs). The goal is to ensure tasks are completed quickly and accurately, prioritizing efficient API calls whenever possible, but resorting to GUI actions if APIs are unavailable or unsuitable.", "section": "3.5 Unified GUI-API Action Orchestrator"}, {"figure_path": "https://arxiv.org/html/2504.14603/x9.png", "caption": "Figure 9. Example API registration for Excel.", "description": "This figure shows a code snippet demonstrating how to register a custom API function, `select_table_range`, for use with Excel within the UFO2 framework.  The code uses Python decorators to define the function's name and behavior, making it easily discoverable and usable by the APPAGENT responsible for interacting with Excel. This allows the agent to perform actions such as selecting a table range directly through an API call, instead of relying on slower and less reliable GUI-based actions.  The example highlights the modularity and extensibility of UFO2's approach to application control.", "section": "3.5 Unified GUI-API Action Orchestrator"}, {"figure_path": "https://arxiv.org/html/2504.14603/x10.png", "caption": "Figure 10. Overview of the knowledge substrate in UFO2, combining static documentation with dynamic execution history.", "description": "The figure illustrates how UFO2 leverages a hybrid knowledge base for enhanced automation.  It combines static knowledge, such as help documents and user manuals (Help Document Database), with dynamic execution history, including successful task trajectories (Demonstrated Examples) and logs of previous actions (Logs).  This combined knowledge is used by the LLM summarizer to generate concise guidance for AppAgents, improving automation reliability and adaptability.", "section": "3 System Design of UFO2"}, {"figure_path": "https://arxiv.org/html/2504.14603/x11.png", "caption": "Figure 11. Speculative multi-action execution in UFO2: batched inference with online validation.", "description": "This figure illustrates the process of speculative multi-action execution in UFO2.  Instead of executing actions one at a time (requiring a separate LLM inference for each), UFO2 predicts multiple actions in a batch, significantly reducing the LLM overhead.  However, to ensure accuracy, each action is validated online (using Windows UI Automation) before execution. If validation fails for any action, the execution stops early and UFO2 replan based on the current UI state. This approach balances efficiency and robustness.", "section": "3.7 Speculative Multi-Action Execution"}, {"figure_path": "https://arxiv.org/html/2504.14603/x12.png", "caption": "Figure 12. The Picture-in-Picture interface: a virtual desktop window for non-disruptive automation.", "description": "The Picture-in-Picture (PiP) interface in UFO2 allows agents to operate in an isolated virtual desktop environment. This prevents interference with the user's primary desktop session, ensuring non-disruptive automation.  The PiP window shows a separate virtual desktop where the agents perform their tasks. The user can view this PiP window alongside their active work without interruption.  This approach ensures that automation is running without affecting the main workspace.", "section": "Picture-in-Picture Interface"}, {"figure_path": "https://arxiv.org/html/2504.14603/x13.png", "caption": "Figure 13. The interactive Session model in UFO2 supports multi-round refinement.", "description": "The figure illustrates the multi-round interactive session model in UFO2.  A user provides an initial request, and the system processes it, resulting in a series of actions (Round 1). The system presents the results, and the user can then provide further refinements or additional requests based on the initial results (Round 2). This iterative process continues until the user is satisfied with the outcome (Round n), demonstrating the system's ability to support multi-round task refinement and user interaction.", "section": "System Design of UFO2"}, {"figure_path": "https://arxiv.org/html/2504.14603/x14.png", "caption": "Figure 14. The safeguard mechanism employed in UFO2.", "description": "This figure illustrates the safeguard mechanism implemented in UFO2 to prevent potentially harmful actions.  The safeguard mechanism is designed to detect actions that meet predefined risk criteria, such as deleting files or terminating applications. When such an action is detected, the system pauses execution and prompts the user for confirmation. This ensures that users maintain control and can prevent unintended consequences, enhancing the safety and security of the automation process. The safeguard mechanism is highly customizable, allowing users and system administrators to define what constitutes a risky action based on their organization's specific needs.", "section": "Implementation and Specialized Engineering Design"}, {"figure_path": "https://arxiv.org/html/2504.14603/x15.png", "caption": "Figure 15. The agent registry supports seamless wrapping of third-party components into the AppAgent framework.", "description": "This figure illustrates the UFO2 agent registry's ability to integrate external tools and libraries into its framework.  The registry acts as a central point for managing and orchestrating diverse agents, enabling the system to leverage both custom-developed APPAGENTS and pre-existing third-party tools for enhanced automation flexibility and capability. The seamless integration allows for easy expansion of UFO2's functionality without requiring substantial modifications to the core system architecture.", "section": "System Design of UFO2"}, {"figure_path": "https://arxiv.org/html/2504.14603/x16.png", "caption": "Figure 16. The client-server deployment model used in AgentOS-as-a-Service.", "description": "The figure illustrates the client-server architecture of AgentOS-as-a-Service.  A lightweight client resides on the user's machine, responsible for capturing screenshots and OS/app state data. This data is sent to a cloud-hosted server (the AgentOS server) via a REST API. The server houses the core components of the AgentOS, including the HOSTAGENT and the APPAGENTS' state machines, which handle task execution, orchestration and data processing. Results and action commands are sent back to the client to interact with the user's applications.", "section": "5 Implementation and Specialized Engineering Design"}, {"figure_path": "https://arxiv.org/html/2504.14603/x17.png", "caption": "Figure 17. An illustration of the markdown-formatted log viewer and debugging tool in UFO2.", "description": "This figure showcases two key tools within the UFO2 framework: a markdown-formatted log viewer and a debugging tool.  The log viewer provides a structured, human-readable summary of each automated task's execution, including inputs (user requests, prompts), outputs (agent responses, actions), the sequence of steps taken (task trajectory), and any relevant screenshots.  The debugging tool allows for step-by-step analysis of past sessions, allowing developers to pinpoint where failures occurred, inspect intermediate results, and experiment with alternative approaches.  Both tools contribute to the overall transparency and debuggability of the UFO2 system.", "section": "5 Implementation and Specialized Engineering Design"}, {"figure_path": "https://arxiv.org/html/2504.14603/x18.png", "caption": "Figure 18. The LLM-based task evaluator applies CoT reasoning to structured session logs.", "description": "This figure illustrates the automated task evaluation process within the UFO2 system.  The system uses a large language model (LLM) and chain-of-thought (CoT) reasoning to analyze structured session logs, which contain information about the tasks performed, the steps taken, and their outcomes. The LLM evaluates these logs to determine whether the task was completed successfully, partially, or failed, providing insights into the performance of the automation. This automated process provides feedback that facilitates ongoing improvement and refinement of the UFO2 system.", "section": "5 Implementation and Specialized Engineering Design"}, {"figure_path": "https://arxiv.org/html/2504.14603/x19.png", "caption": "Figure 19. Error analysis of UFO2-base (GPT-4o) on the two benchmarks.", "description": "This figure presents a breakdown of error types encountered by the UFO2-base model (using GPT-40) when tested on two different benchmarks, WAA and OSWorld-W.  It visually represents the proportion of failures attributed to three categories: Plan Errors (incorrect high-level task understanding), Execution Errors (flawed execution despite a correct plan), and Control Detection Failures (inability to detect essential GUI controls). This breakdown helps to identify the strengths and weaknesses of the model and pinpoint areas needing further improvement.", "section": "6.2 Success Rate Comparison"}, {"figure_path": "https://arxiv.org/html/2504.14603/x20.png", "caption": "Figure 20. The number of detected controls of different approaches.", "description": "This bar chart compares the number of GUI controls detected by three different methods: using only UI Automation (UIA) APIs, using only OmniParser-v2 (a vision-based grounding model), and using a hybrid approach that combines both UIA and OmniParser-v2.  The chart shows the number of controls detected for each method on two benchmark datasets: WAA and OSWorld-W. The hybrid approach consistently detects a larger number of controls than either UIA or OmniParser-v2 alone, highlighting its effectiveness in comprehensively detecting GUI controls across a range of application interfaces.", "section": "3.4 Hybrid Control Detection"}, {"figure_path": "https://arxiv.org/html/2504.14603/x21.png", "caption": "Figure 21. A case study comparing the completion of the same task using GUI-only actions vs. GUI + API actions.", "description": "This figure shows a side-by-side comparison of completing the same task using two different methods: one relying solely on Graphical User Interface (GUI) actions and the other incorporating Application Programming Interface (API) calls.  The GUI-only approach demonstrates a multi-step process involving several clicks and menu selections, while the GUI+API method shows a simplified, single-step execution using an API call. This comparison highlights the efficiency and robustness gains achievable through API integration.", "section": "3.5 Unified GUI-API Action Orchestrator"}, {"figure_path": "https://arxiv.org/html/2504.14603/x22.png", "caption": "Figure 22. A case study of the successful speculative multi-action execution.", "description": "This figure showcases a successful example of UFO2's speculative multi-action execution.  The user requests to center-align a heading in a Word document.  Instead of executing this task as a sequence of individual actions (selecting the heading, clicking the Center button), the system uses a single LLM inference to predict and execute both steps simultaneously. This demonstrates the efficiency gains achieved by speculative execution, reducing the number of required LLM calls and thus the overall latency.", "section": "3.7 Speculative Multi-Action Execution"}, {"figure_path": "https://arxiv.org/html/2504.14603/x23.png", "caption": "Figure 23. Comparison of Operator vs. UFO2 + Operator on WAA and OSWorld-W.", "description": "This figure compares the success rates of Operator, a state-of-the-art Computer Using Agent (CUA), against UFO2 + Operator on the Windows Agent Arena (WAA) and OSWorld-W benchmarks.  UFO2 + Operator refers to running the Operator CUA within the UFO2 AgentOS framework. The chart visually represents the improvement in success rates achieved by integrating Operator into the UFO2 system, showcasing UFO2's ability to enhance the performance of existing CUAs.  The x-axis represents the benchmark (WAA and OSWorld-W), and the y-axis represents the success rate.  The bars depict the success rates for Operator and UFO2 + Operator in each benchmark.", "section": "6 Evaluation"}, {"figure_path": "https://arxiv.org/html/2504.14603/x24.png", "caption": "Figure 24. Average time cost per-stage of a single execution step.", "description": "This figure is a bar chart that shows the breakdown of the average time spent on each stage of a single execution step in the UFO2 system.  The stages include screenshot capture, control detection, prompt preparation (including knowledge retrieval), LLM inference, and action execution.  The chart allows for comparison of the time spent in each phase across various configurations and helps illustrate which steps are the most time-consuming in the process.", "section": "6.8 Efficiency Analysis"}]