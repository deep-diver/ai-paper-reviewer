{"importance": "This paper introduces a novel framework that advances GUI agents, addressing their limitations in reasoning and planning. The **Actor2Reasoner framework** and the **InfiGUI-R1 agent** offer a promising approach for developing more capable and robust automated systems, potentially impacting how humans interact with technology.", "summary": "InfiGUI-R1: GUI agents evolve from reactive to reasoning, using Actor2Reasoner framework to enhance MLLM-based interaction & planning.", "takeaways": ["The Actor2Reasoner framework effectively transitions GUI agents from reactive actors to deliberative reasoners.", "Spatial Reasoning Distillation improves the integration of visual-spatial understanding into textual reasoning for GUI agents.", "InfiGUI-R1 achieves state-of-the-art performance in cross-platform GUI grounding and strong results in complex task execution."], "tldr": "Current GUI agents often lack robust reasoning and planning, relying on manual templates or simple reactions, hindering their adaptability in complex environments. To solve the above issues, the paper presents **Actor2Reasoner framework**, a two-stage training approach to evolve GUI agents based on Multimodal Large Language Models (MLLMs) from Reactive Actors to Deliberative Reasoners.\n\nThe framework uses Reasoning Injection to establish basic reasoning by employing Spatial Reasoning Distillation, transferring spatial reasoning skills to MLLMs. Then it uses Deliberation Enhancement to refine agents using Reinforcement Learning with Sub-goal Guidance and Error Recovery Scenario Construction. The result, **InfiGUI-R1**, shows strong performance in both GUI grounding and trajectory tasks.", "affiliation": "Zhejiang University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2504.14239/podcast.wav"}