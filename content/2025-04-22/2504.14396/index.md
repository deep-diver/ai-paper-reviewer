---
title: "SphereDiff: Tuning-free Omnidirectional Panoramic Image and Video Generation via Spherical Latent Representation"
summary: "SphereDiff: Seamless 360° panorama generation via spherical latent space, no fine-tuning needed!"
categories: ["AI Generated", "🤗 Daily Papers"]
tags: ["Computer Vision", "Image Generation", "🏢 Korea Advanced Institute of Science and Technology (KAIST)",]
showSummary: true
date: 2025-04-19
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2504.14396 {{< /keyword >}}
{{< keyword icon="writer" >}} Minho Park et el. {{< /keyword >}}
 
{{< keyword >}} 🤗 2025-04-22 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2504.14396" target="_self" >}}
↗ arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2504.14396" target="_self" >}}
↗ Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2504.14396/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Generating high-quality 360° panoramic content is challenging due to distortions introduced by equirectangular projection(ERP). Existing methods require fine-tuning diffusion models or still rely on ERP latent representations, leading to discontinuities. To solve this, SphereDiff is introduced for seamless panoramic content generation without tuning.



SphereDiff ensures uniform distribution across perspectives by defining a spherical latent representation. It extends MultiDiffusion to spherical latent space and introduces spherical latent sampling. It also has distortion-aware weighted averaging. Experiments show this method generates high-quality 360° panoramic content with high fidelity.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} SphereDiff introduces a spherical latent representation for seamless 360° panoramic image and video generation. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} The method leverages state-of-the-art diffusion models without requiring additional fine-tuning. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} SphereDiff outperforms existing approaches in generating high-quality, distortion-free panoramic content. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This paper introduces a novel tuning-free method for 360° panorama generation, **addressing distortion issues without extensive training**. It offers a more accessible and efficient approach, **potentially impacting AR/VR content creation and opening new research directions** in panoramic media generation.

------
#### Visual Insights



![](https://arxiv.org/html/2504.14396/x2.png)

> 🔼 This figure showcases a 360-degree panoramic video generated using the SphereDiff model.  The video displays a dynamic scene, likely an outdoor cityscape at night, with various elements like buildings, rivers, and fireworks, offering an immersive, omnidirectional view. The caption encourages viewers to click on the image to view the animation, recommending Acrobat Reader for optimal viewing experience. This visual demonstrates the model's capability to produce high-quality, seamless panoramic video content, highlighting the absence of distortions often present in other methods near the poles.
> <details>
> <summary>read the caption</summary>
> Figure 1: 360-degree panoramic video generated by SphereDiff. Click to play the animation clips. Best viewed with Acrobat Reader.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.8.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.8.8.9.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.8.8.9.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.8.8.9.1.2">Panorama Creteria</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.8.8.9.1.3">Image Creteria</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.8.8.9.1.4">Text Adherence</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T1.8.8.9.1.5">Video Creteria</th>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S4.T1.8.8.8.9">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1">Distortion <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.2.2.2.2">End Continuity <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.3.3">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.3.3.3.3.1">
<tr class="ltx_tr" id="S4.T1.3.3.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.3.3.3.1.1.1">Image</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.3.3.3.1.2.1">Quality</td>
</tr>
</table> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.4.4.4.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.4.4.4.4.1">
<tr class="ltx_tr" id="S4.T1.4.4.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.4.4.4.1.1.1">Aesthetic</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.4.4.4.1.2.1">Appearance</td>
</tr>
</table> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.4.4.4.4.m1.1"><semantics id="S4.T1.4.4.4.4.m1.1a"><mo id="S4.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T1.4.4.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.5.5.5.5">Scene <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.5.5.5.5.m1.1"><semantics id="S4.T1.5.5.5.5.m1.1a"><mo id="S4.T1.5.5.5.5.m1.1.1" stretchy="false" xref="S4.T1.5.5.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.5.5.5.5.m1.1b"><ci id="S4.T1.5.5.5.5.m1.1.1.cmml" xref="S4.T1.5.5.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.5.5.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.5.5.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.6.6.6.6">CLIP-Score <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.6.6.6.6.m1.1"><semantics id="S4.T1.6.6.6.6.m1.1a"><mo id="S4.T1.6.6.6.6.m1.1.1" stretchy="false" xref="S4.T1.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.6.6.6.6.m1.1b"><ci id="S4.T1.6.6.6.6.m1.1.1.cmml" xref="S4.T1.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.6.6.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.6.6.6.6.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.7.7.7.7">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.7.7.7.7.1">
<tr class="ltx_tr" id="S4.T1.7.7.7.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.7.7.7.1.1.1">Motion</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7.7.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.7.7.7.1.2.1">Smoothness</td>
</tr>
</table> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.7.7.7.7.m1.1"><semantics id="S4.T1.7.7.7.7.m1.1a"><mo id="S4.T1.7.7.7.7.m1.1.1" stretchy="false" xref="S4.T1.7.7.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.7.7.7.7.m1.1b"><ci id="S4.T1.7.7.7.7.m1.1.1.cmml" xref="S4.T1.7.7.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.7.7.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.7.7.7.7.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.8.8.8.8">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.8.8.8.8.1">
<tr class="ltx_tr" id="S4.T1.8.8.8.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.8.8.8.8.1.1.1">Temporal</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.8.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.8.8.8.8.1.2.1">Flickering</td>
</tr>
</table> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.8.8.8.8.m1.1"><semantics id="S4.T1.8.8.8.8.m1.1a"><mo id="S4.T1.8.8.8.8.m1.1.1" stretchy="false" xref="S4.T1.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.8.8.8.8.m1.1b"><ci id="S4.T1.8.8.8.8.m1.1.1.cmml" xref="S4.T1.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.8.8.8.8.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.8.8.8.8.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.8.8.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.8.8.10.1.1">360 LoRA</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.2">2.027</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.3">3.423</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.4">2.965</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.5">3.492</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.10.1.6.1">0.2875</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.7">26.40</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.8">-</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.10.1.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.8.8.11.2.1">Text2Light <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.14396v1#bib.bib5" title=""><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.2">2.381</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.3">3.454</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.4">2.415</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.5">2.777</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.6">0.0000</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.7">21.03</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.11.2.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.8.8.12.3.1">PanFusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.14396v1#bib.bib30" title=""><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.2">1.965</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.3">3.696</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.4">2.819</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.5">3.450</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.6">0.2125</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.7">25.70</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.12.3.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.13.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.8.8.13.4.1">DynamicScaler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.14396v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.13.4.2.1">2.854</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.13.4.3.1">3.985</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.13.4.4.1">4.496</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.13.4.5.1">4.577</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.6">0.2750</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.13.4.7.1">26.63</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.13.4.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.14.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.8.8.14.5.1">SphereDiff (Ours)</th>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.2"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.14.5.2.1">3.238</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.3"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.14.5.3.1">4.892</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.4"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.14.5.4.1">4.496</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.14.5.5.1">4.685</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.6"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.14.5.6.1">0.5875</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.7"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.14.5.7.1">28.65</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.8">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.14.5.9">-</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.15.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.8.8.15.6.1">360 LoRA <span class="ltx_text" id="S4.T1.8.8.15.6.1.1" style="font-size:90%;">(+ AnimateDiff <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.14396v1#bib.bib8" title=""><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite>)</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.2">1.939</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.15.6.3.1">3.482</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.4"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.15.6.4.1">3.179</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.15.6.5.1">3.571</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.6">0.2914</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.7">26.34</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.8">0.9908</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.8.8.15.6.9">0.9847</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.16.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.8.8.16.7.1">360DVD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.14396v1#bib.bib24" title=""><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.16.7.2.1">2.086</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.3">3.246</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.4">2.929</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.5">3.396</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.6">0.0719</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.7">23.13</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.8">0.9843</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.16.7.9">0.9777</td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.17.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.8.8.17.8.1">DynamicScaler <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2504.14396v1#bib.bib18" title=""><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.2">1.971</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.3">2.971</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.4">2.711</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.5">3.236</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.17.8.6.1">0.4836</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.17.8.7.1">26.89</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.17.8.8.1">0.9943</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.17.8.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.17.8.9.1">0.9918</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.8.8.18.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.8.8.18.9.1">SphereDiff (Ours)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.2"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.18.9.2.1">2.579</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.3"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.18.9.3.1">4.496</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.8.8.18.9.4.1">3.050</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.5"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.18.9.5.1">3.593</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.6"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.18.9.6.1">0.5703</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.7"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.18.9.7.1">27.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.8"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.18.9.8.1">0.9956</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.8.8.18.9.9"><span class="ltx_text ltx_font_bold" id="S4.T1.8.8.18.9.9.1">0.9941</span></td>
</tr>
</tbody>
</table>{{< /table-caption >}}

> 🔼 This table presents a quantitative comparison of SphereDiff against other state-of-the-art methods for 360-degree panoramic image and video generation.  Several metrics are used to evaluate performance across different aspects including distortion, continuity, image quality, aesthetic appeal, text adherence, motion smoothness, and temporal flickering.  The best performing method for each metric is shown in bold, while the second best is underlined. The results demonstrate that SphereDiff outperforms existing methods across most metrics, achieving the top spot in several key areas of evaluation. The only exception is video generation's image quality, where SphereDiff achieves second place.
> <details>
> <summary>read the caption</summary>
> Table 1:  Quantitative comparison with the best results in bold and the second best underlined. SphereDiff outperforms existing methods across all metrics except image quality for video generation, where it ranks second.
> </details>





### In-depth insights


#### Sphere Latent
The concept of a "Sphere Latent" representation, as implied by the SphereDiff paper, suggests a significant departure from traditional 2D latent spaces used in generative models for panoramic images.  Instead of relying on equirectangular projection (ERP), which introduces distortions, particularly near the poles, a spherical latent space offers a more **uniform and distortion-aware** representation. This is crucial because diffusion models trained on standard perspective images often struggle with the distribution shift caused by ERP. The key insight is to map each latent vector to a point on the sphere, ensuring that all perspectives are treated equally.  This avoids the uneven sampling density inherent in ERP, where high-latitude regions are overrepresented.  Further, a spherical latent space naturally lends itself to seamless panorama generation, as it inherently captures the cyclical nature of 360-degree views.  The challenge then lies in effectively interfacing this spherical latent space with existing diffusion models, which typically operate on 2D grids.  Methods for sampling and projecting the spherical latent features onto a 2D plane, while preserving spatial relationships and minimizing distortions, become paramount. The use of spherical harmonics or other spherical basis functions could be explored to represent the latent space in a compact and efficient manner.  A distortion-aware loss function during training would further encourage the model to learn a representation that minimizes artifacts in the final panoramic image.  The advantage of "Sphere Latent" is the better **spatial awareness and seamlessness**.

#### Tuning-Free 360
The concept of a "Tuning-Free 360" approach represents a significant leap in panoramic image and video generation. **Traditional methods often require extensive fine-tuning** of pre-trained models or rely on complex architectures to handle the distortions inherent in equirectangular projection (ERP). A truly tuning-free method bypasses these limitations, allowing for **seamless integration with existing, state-of-the-art diffusion models** without the need for specialized training data or architectural modifications. This is achieved by focusing on novel latent space representations and projection techniques that inherently minimize distortion and ensure continuity, leading to robust and high-quality panoramic content generation. By eliminating the need for task-specific tuning, tuning-free approach offers **greater flexibility, adaptability, and efficiency** in creating immersive 360-degree experiences.

#### Spherical Diff
It seems the author intends to present a novel diffusion framework, perhaps termed "SphereDiff," operating on a **spherical latent representation**. This likely addresses limitations of existing methods in omnidirectional image/video generation that suffer from distortions due to equirectangular projection (ERP). By working directly in a spherical space, SphereDiff aims to achieve **seamless and distortion-free panoramic content**. The key idea may be to leverage the **uniformity of a sphere** to avoid the pole singularities inherent in ERP. This could involve mapping standard latent diffusion models to this spherical space, with careful consideration of sampling and projection techniques to maintain coherence and avoid artifacts. A potential benefit is improved performance in areas that suffer from ERP distortion. 

#### Distortion Aware
**Distortion awareness** is crucial in panoramic image generation, especially for 360-degree views. Traditional methods using equirectangular projection (ERP) suffer from severe distortions, particularly near the poles. To address this, techniques like spherical latent representations are employed. These ensure a **uniform distribution** across all perspectives, mitigating distortions inherent in ERP. Furthermore, distortion-aware weighted averaging is used to improve generation quality by adjusting per-pixel weights, emphasizing the center of the field of view (FoV) where distortion is minimal, ensuring seamless content generation.

#### MultiDiffusion+
**MultiDiffusion+** likely represents an enhanced version of the MultiDiffusion framework, aimed at improving image generation. The '+' suggests enhancements in several aspects, such as **handling complex scenes**, improving **consistency across multiple views**, or increasing **generation speed**. The improvements might address limitations like blending artifacts or computational costs.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2504.14396/x3.png)

> 🔼 Figure 2 illustrates the limitations of existing methods for generating 360-degree panoramic images.  Fine-tuning approaches, while using an equirectangular projection (ERP) dataset, struggle to create continuous scenes, especially near the poles due to distortions and limited training data. Similarly, tuning-free methods relying on ERP latent representations also fail to produce seamless images because of the inherent distortions in ERP.
> <details>
> <summary>read the caption</summary>
> Figure 2: Motivation. Previous finetuning approaches [15, 30] often fail to generate continuous scenes near the pole due to the limited ERP dataset. The tuning-free approach [18] also fails to generate a seamless frame due to the ERP latent representation.
> </details>



![](https://arxiv.org/html/2504.14396/x4.png)

> 🔼 SphereDiff uses a novel approach to generate 360-degree panoramic images and videos.  It starts with uniform spherical latents, which represent the scene's information in a distortion-free way. Then, for each denoising step in the diffusion process, perspective latents are extracted for multiple views using dynamic latent sampling.  These perspective latents are denoised individually using a pre-trained diffusion model. Finally, the denoised latents are fused together using MultiDiffusion and distortion-aware weighted averaging to produce a seamless, distortion-free panoramic output. This entire process is tuning-free, meaning it does not require any additional training of the diffusion model.
> <details>
> <summary>read the caption</summary>
> Figure 3: Overall Pipeline. We initialize uniform spherical latents and extract perspective latents for multiple views at each denoising step using dynamic latent sampling. These latents are then denoised and fused using the MultiDiffusion [2] with distortion-aware weighted averaging. This process enables seamless and distortion-free 360-degree panoramic image and video generation in a tuning-free manner.
> </details>



![](https://arxiv.org/html/2504.14396/x5.png)

> 🔼 The figure compares the spatial distribution of latent features in two different representations: equirectangular projection (ERP) and the proposed spherical representation.  ERP, a common method for representing 360-degree images on a 2D plane, suffers from distortions, especially near the poles, leading to uneven distribution of latent features.  The image shows that the density of ERP latents is much higher near the poles, while the density of latents in the spherical representation is relatively uniform across all viewing angles. This demonstrates the advantage of using a spherical representation for generating seamless panoramic images.
> <details>
> <summary>read the caption</summary>
> Figure 4:  Comparison of ERP and Spherical Latent Representations. When changing perspective, the ERP latent representation shows significant density variations in latent density depending on position, especially near the poles, while our spherical representation maintains a nearly uniform density across all perspectives.
> </details>



![](https://arxiv.org/html/2504.14396/x8.png)

> 🔼 Figure 5 illustrates two methods for sampling spherical latents: nearest sampling and dynamic sampling.  Nearest sampling simply selects the nearest latent to each grid point, potentially leading to some latents being selected multiple times and others missed entirely.  This results in uneven sampling and can cause artifacts, especially near the poles where latents are less densely packed.  In contrast, dynamic sampling addresses these issues by starting at the center and working outward, ensuring a more uniform sampling of latents.  This method ensures a more comprehensive representation of the spherical surface while minimizing the omission of crucial data points.
> <details>
> <summary>read the caption</summary>
> Figure 5:  Comparison of Nearest and Dynamic Sampling. Nearest sampling often resamples the selected latents or omits central ones, while dynamic sampling selects latents from the center outward, discarding only the outermost ones.
> </details>



![](https://arxiv.org/html/2504.14396/x9.png)

> 🔼 Figure 6 presents a qualitative evaluation of the SphereDiff model's performance.  It showcases the model's ability to generate seamless and high-quality 360° panoramic images and videos. The figure displays visualization results of a generated scene using the equirectangular projection (ERP) representation, a standard format for panoramic images, alongside views from three different perspectives at various elevations. These multiple perspective views offer a comprehensive understanding of the model's generation quality and consistency across different viewpoints.  The supplementary material contains additional results not shown in this figure.
> <details>
> <summary>read the caption</summary>
> Figure 6: Qualitative results of our method. Visualization results for the entire scene using the ERP representation and 3 perspectives views across various elevation multiple perspective images or frames. Additional results are available in the supplementary materials.
> </details>



![](https://arxiv.org/html/2504.14396/x10.png)

> 🔼 Figure 7 presents a qualitative comparison of various methods for generating 360-degree panoramic images and videos.  The figure displays multiple examples, each showing perspective views from top to bottom. This allows for easy visual assessment of end-to-end continuity and the presence of distortion or artifacts. The authors highlight the shortcomings of existing baselines, noting issues such as split seams, severe distortions (especially near the poles), blurriness, and random spots. In contrast, the proposed method (SphereDiff) is shown to generate seamless, high-quality panoramic content free of these artifacts.
> <details>
> <summary>read the caption</summary>
> Figure 7: Qualitative comparison of all image and video baselines. Each sample presents perspective images from the top view to the bottom view, highlighting end-to-end continuity and distortion. Other methods exhibit noticeable artifacts, such as split seams, severe distortions near the poles, blurriness, or spots due to inadequate handling of these issues. In contrast, our method generates seamless, high-quality panoramic content without such artifacts.
> </details>



![](https://arxiv.org/html/2504.14396/x11.png)

> 🔼 Figure 8 demonstrates the effects of different latent sampling methods (nearest neighbor vs. dynamic) and the use of weighted averaging on the quality of generated 360° panoramic images.  Nearest neighbor sampling suffers from a lack of information exchange between different viewpoints, resulting in inconsistencies and noticeable overlap artifacts due to undersampling.  In contrast, dynamic sampling improves information sharing, leading to more coherent and integrated outputs.  Applying weighted averaging enhances the seamlessness of the results for both sampling methods; however, even with weighted averaging, nearest neighbor sampling still struggles to maintain full connectivity between adjacent regions, producing discontinuities.
> <details>
> <summary>read the caption</summary>
> Figure 8: Ablation on latent sampling and weighted averaging. Nearest sampling lacks information exchange between views, leading to inconsistencies and visible overlap artifacts caused by undersampling problem. In contrast, dynamic sampling facilitates information sharing, resulting in more integrated outputs. With weighted averaging, both sampling methods improve seamlessness. However, nearest sampling still fails to maintain connectivity between adjacent regions, leading to discontinuities.
> </details>



![](https://arxiv.org/html/2504.14396/x12.png)

> 🔼 This figure compares the similarity distribution of positional embeddings generated using discrete versus continuous positional representations.  The visualizations show that when using discrete positional embeddings (a), high similarity is concentrated within rows and columns of the grid.  However, with continuous positional embeddings (b), similarity decreases rapidly as the distance between positions increases, even for small positional offsets. This difference highlights the impact of discretization on latent similarity and explains why a continuous latent representation is insufficient for creating a seamless 360-degree panorama without additional methods. 
> <details>
> <summary>read the caption</summary>
> Figure 9: Similarities between positional embeddings calculated with discrete and continuous positions. The small squares represent the similarity distribution when the central pixel is used as a query. As shown in the figure, when discretization is applied, within the same row or column, resulting in high similarity. However, when positions vary slightly (as in the continuous case), the similarity drops significantly.
> </details>



![](https://arxiv.org/html/2504.14396/x13.png)

> 🔼 Figure 10 compares the image generation results of SphereDiff with those of a baseline method (DynamicScaler) using both spherical and equirectangular projection (ERP) latent representations. The comparison highlights the impact of the number of view directions on the generation quality, particularly around the polar regions. When fewer view directions are used, more latents remain unsampled and unprocessed during denoising, resulting in blurry artifacts near the poles, as indicated by the red boxes.  The figure demonstrates that SphereDiff's use of spherical latent representation is more robust to a reduction in view directions and produces significantly higher quality images.
> <details>
> <summary>read the caption</summary>
> Figure 10: Comparison with spherical and ERP. The red box highlights blurry artifacts that appear near the polar regions. As the number of view directions decreases, more latents remain unsampled and unprocessed during denoising, making the issue more severe.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S4.T1.3.3.3.3.1">
<tr class="ltx_tr" id="S4.T1.3.3.3.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.3.3.3.1.1.1">Image</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.3.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.3.3.3.1.2.1">Quality</td>
</tr>
</table>{{< /table-caption >}}
> 🔼 This table presents the results of a user study comparing the performance of different methods for generating 360-degree panoramic images and videos.  Participants used a multiple-alternative forced-choice survey to evaluate image and video quality across several criteria: distortion, end continuity, image quality, text alignment, motion smoothness, and temporal flickering. The results show that the proposed SphereDiff method is significantly preferred by users, especially in terms of minimizing distortion and ensuring seamless transitions between image sections.
> <details>
> <summary>read the caption</summary>
> Table 2: User study results. All images and videos are presented as perspective images, and the evaluation is conducted using a multiple-alternative forced-choice survey. Our method demonstrates the highest preference in most performance aspects, with particularly in distortion and end continuity.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S4.T1.4.4.4.4.1">
<tr class="ltx_tr" id="S4.T1.4.4.4.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.4.4.4.1.1.1">Aesthetic</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.4.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.4.4.4.4.1.2.1">Appearance</td>
</tr>
</table>{{< /table-caption >}}
> 🔼 This table compares different approaches for generating 360-degree panoramic images and videos.  It highlights key differences in the latent space used (equirectangular, cube map, or spherical), whether the methods require fine-tuning of pre-trained models, and whether the code is open-source. The table shows that most existing methods use equirectangular projections, requiring fine-tuning.  In contrast, the proposed method uses spherical latent representations and does not require fine-tuning, offering a novel approach to this task.
> <details>
> <summary>read the caption</summary>
> Table 3: Comparison of 360-degree panorama generation approach. Most existing panoramic generation models perform denoising in the equirectangular latent space, except for CubeDiff, which utilizes a cube map representation. In contrast, our method leverages spherical latents. Among the compared methods, only DynamicScaler and ours support tuning-free generation.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_align_middle" id="S4.T1.7.7.7.7.1">
<tr class="ltx_tr" id="S4.T1.7.7.7.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.7.7.7.1.1.1">Motion</td>
</tr>
<tr class="ltx_tr" id="S4.T1.7.7.7.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.7.7.7.7.1.2.1">Smoothness</td>
</tr>
</table>{{< /table-caption >}}
> 🔼 This table details the evaluation prompt used to assess the quality of 360-degree panoramic image generation.  The evaluation is based on four key criteria: Image Quality (overall quality), Aesthetic Appeal (visual pleasantness), Distortion Level (presence of distortions), and Connectivity (absence of noticeable breaks or discontinuities). Each criterion is scored on a five-point scale (Excellent, Good, Fair, Poor, Awful) with a brief explanation of the reasoning provided for each score.  Evaluators assess one image at a time, providing a comprehensive evaluation based on these four aspects of image quality.
> <details>
> <summary>read the caption</summary>
> Table 4: Evaluation prompt used for assessing 360-degree panoramic generation quality based on four criteria.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2504.14396/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2504.14396/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}