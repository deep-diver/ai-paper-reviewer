{"importance": "This paper introduces a flexible reward optimization technique, adaptable to various media, that can help researchers fine-tune their models. The introduced DRAGON framework provides a novel approach to reward function design, optimizing media quality and bridging the gap between model-based and human-perceived metrics.", "summary": "DRAGON: Distributional Rewards Optimize Diffusion Generative Models", "takeaways": ["DRAGON is a versatile framework for fine-tuning generative models, optimizing towards a desired outcome.", "The framework introduces a novel method for constructing reward functions by selecting an embedding extractor and a set of examples to represent an exemplar distribution.", "DRAGON improves human-perceived music quality with sparse or no human preference annotations."], "tldr": "Diffusion models excel at content generation, but aligning them with human preferences remains challenging. Existing methods like RLHF or DPO require expensive human feedback and struggle with distributional objectives. They are also disconnected from generative model evaluation metrics. It makes it difficult to adapt these approaches to new objectives. \n\nThis paper introduces **DRAGON**, a framework for fine-tuning generative models. **DRAGON** optimizes a spectrum of rewards. By constructing reward functions using encoders and reference examples, it can optimize instance-wise, instance-to-distribution and distribution-to-distribution rewards, improving human-perceived quality without extensive human annotations.", "affiliation": "UC Berkeley", "categories": {"main_category": "Speech and Audio", "sub_category": "Music Generation"}, "podcast_path": "2504.15217/podcast.wav"}