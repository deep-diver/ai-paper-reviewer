[{"figure_path": "https://arxiv.org/html/2504.15270/x2.png", "caption": "Figure 1: Quicksviewer involves a cubing network that partitions a video into nonuniform cubes, followed by a 3D resampler to gather a fixed number of visual tokens per cube. This efficiency enables Large Receptive Field (420 frames) with High Compression Rate (64\u00d7\\times\u00d7) during all training stages, and scaling laws on extended frames in inference.", "description": "Quicksviewer processes video by first dividing it into non-uniform cubes using a cubing network.  The size of each cube is determined by the video's inherent temporal information density; areas with rapid changes are represented by smaller cubes, while slower-paced sections are in larger cubes.  A 3D resampler then processes these cubes, extracting a consistent number of visual tokens from each, regardless of the cube's original size. This approach allows Quicksviewer to handle videos with varying levels of activity efficiently. The result is a system with a large receptive field (420 frames) and a high compression rate (64x). This efficiency is beneficial throughout training and allows for scaling laws to be applied when processing longer videos during inference.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.15270/x3.png", "caption": "Figure 2: Left: The network architecture of Quicksviewer, that performs unified understanding of videos and images through visual tokens from cascaded modules. Right: The cubing network, that partitions an online video into nonuniform cubes based on Gumbel Softmax.", "description": "This figure illustrates the architecture of Quicksviewer, a novel Large Multimodal Model (LMM) designed for efficient video understanding. The left panel depicts the overall network architecture. It shows how the model processes both images and videos, unifying them through visual tokens generated by cascaded modules.  These modules include a visual encoder, a cubing network (detailed in the right panel), a resampling layer, and finally an LLM. The right panel zooms in on the cubing network, the core innovation of Quicksviewer.  This network dynamically partitions online video streams into non-uniform cubes, each with a varying number of frames, according to temporal density and semantic differences between frames. This partitioning relies on the Gumbel-Softmax method, allowing the model to focus on areas of higher information density and compress the input video efficiently.", "section": "Approach"}, {"figure_path": "https://arxiv.org/html/2504.15270/x4.png", "caption": "Figure 3: (a) Left: Performance of Quicksviewer on particular domains and categories of Video-MME. (b) Right: Distribution of cube lengths across Video-MME videos.", "description": "This figure is a composite showing two subfigures related to the performance of Quicksviewer on the Video-MME benchmark dataset.  (a) Left: A bar chart displays the accuracy of Quicksviewer across various domains and categories within Video-MME.  This illustrates the model's performance consistency and potential weaknesses across different video types. (b) Right: A histogram depicts the distribution of cube lengths generated by Quicksviewer's cubing mechanism for videos in the Video-MME dataset. This shows the frequency of various cube durations that the model employed.", "section": "4.2 Experiments on Video Understanding"}, {"figure_path": "https://arxiv.org/html/2504.15270/x5.png", "caption": "Figure 4: The \"Visual Lag\" phenomenon occurring during the model\u2019s cube-based segmental comprehension, where current cubes incorporate terminal frames from preceding event scenes to enable retrospective understanding.", "description": "Figure 4 illustrates the \"Visual Lag\" phenomenon observed in the model's cube-based video comprehension.  Instead of processing each video segment in strict temporal order, the model incorporates frames from the end of previous segments into the current segment's processing.  This means that information from earlier events 'lags' and influences the understanding of subsequent events. The figure visually shows examples of this phenomenon, demonstrating how the model's understanding of a current scene is informed by contextual frames from previous scenes, leading to a more comprehensive, retrospective interpretation of the events.", "section": "4.3 Analysis of the Cubes Partitioning"}, {"figure_path": "https://arxiv.org/html/2504.15270/x6.png", "caption": "Figure 5: (a) Left: Gumbel noise progressively anneals to 0.001 following the decaying learning rate with cosine scheduler. (b) Right: Compared to non-annealed training (cyan curve), adding Gumbel noise annealing (purple curve) yields more stable and superior loss convergence.", "description": "Figure 5 presents two graphs illustrating the impact of Gumbel noise annealing during training. The left graph (a) shows how Gumbel noise decreases from 1.0 to 0.001 over the training steps, following a cosine annealing schedule synchronized with the learning rate decay.  This controlled reduction of randomness ensures that exploration is emphasized in early training stages, while later stages focus on exploitation of already learned patterns. The right graph (b) compares the loss convergence of models trained with and without Gumbel noise annealing. It reveals that using the annealing technique leads to more stable and improved loss convergence compared to models without it. The smoother, faster descent in the annealed case (purple curve) indicates that the controlled randomness from the annealing process helps avoid oscillations and promotes more efficient learning and generalization.", "section": "4.4 Analysis of the Annealing Strategy"}, {"figure_path": "https://arxiv.org/html/2504.15270/extracted/6378287/imgs/train-loss-wo-anneal.png", "caption": "Figure 6: Qualitative analysis showns that Quicksviewer effectively understands lengthy documentary and sports videos, as well as informative single and multiple images.", "description": "Figure 6 presents qualitative results showcasing Quicksviewer's capabilities in understanding various types of visual content.  The examples demonstrate the model's ability to accurately comprehend and respond to questions about lengthy documentary videos (a penguin encounter), sports videos (a lacrosse match), and informative single and multi-image inputs (various diagrams and photos).  It highlights the model's proficiency in identifying key events, extracting relevant details, and providing coherent answers across diverse video and image types.", "section": "Qualitative Analysis"}]