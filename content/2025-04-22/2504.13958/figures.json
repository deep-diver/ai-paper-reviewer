[{"figure_path": "https://arxiv.org/html/2504.13958/extracted/6367558/figures/introduction.png", "caption": "Figure 1: SFT on distilled deep-thinking trajectories suffers from overthinking and limited generalization.", "description": "This figure uses an example to compare the performance of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) methods in handling tool-integrated reasoning tasks.  The example involves determining the distance between two cities using a tool designed for calculating date differences. The SFT model, initialized with a distilled deep-thinking trajectory, overthinks the problem, attempting to force an inappropriate tool to work.  In contrast, the RL model, trained with the proposed Group Relative Policy Optimization (GRPO) approach, correctly identifies the tool's inadequacy and rejects it, demonstrating improved generalization and avoidance of overthinking.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.13958/x1.png", "caption": "Figure 2: Main results (left) and reward trends over training steps for GRPO Cold Start across four models (right). GRPO Cold Start, equipped with our proposed reward design, consistently achieves the highest performance, with reward curves showing a rapid increase during training.", "description": "This figure shows a comparison of different model training approaches on three benchmark datasets. The left panel displays the overall accuracy of each method on the BFCL, Bamboogle, and API-Bank datasets. The right panel shows the trends of format and correctness rewards during training for each of the four models (Qwen-1.5B, Qwen-2.5B, Qwen-7B, LLaMa-3.2B) using the GRPO Cold Start method with the proposed reward design.  The results clearly demonstrate that the GRPO Cold Start approach consistently achieves the highest performance across all datasets, exhibiting a sharp increase in reward values throughout the training process. This highlights the effectiveness of the proposed reward design in improving the tool-use capabilities of LLMs.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13958/extracted/6367558/figures/reward.png", "caption": "Figure 3: Illustration of TIR rollout and calculation of format and correctness reward.", "description": "This figure illustrates the process of Tool-Integrated Reasoning (TIR) rollout and how the format and correctness rewards are calculated.  The TIR rollout starts with a user query and available tools. The LLM generates a reasoning trace, which may include tool calls and responses.  Each step involves a <think> segment, which shows the model's reasoning process. If a tool is used, a <tool_call> segment, containing the tool name and parameters, is generated; this is followed by an <obs> segment that provides the observation after tool execution. The LLM may also generate a <response> to directly answer the query without invoking a tool.  The final reward is the sum of the format and correctness rewards. The format reward is binary (1 if the format is correct, 0 otherwise), indicating whether the generated sequence of <think>, <tool_call>, <obs>, and <response> follows the expected structure. The correctness reward is based on a comparison between predicted and ground truth tool calls. It assesses the accuracy in three aspects: matching tool names, parameter names, and parameter contents. Each aspect produces a score, and these scores are aggregated to compute the final correctness reward.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2504.13958/x2.png", "caption": "Figure 4: The system prompt used for TIR\u2019s rollout.", "description": "This figure shows the system prompt used during the rollout phase of Tool-Integrated Reasoning (TIR). The prompt instructs the large language model (LLM) on how to interact with provided tools, emphasizing the use of special tokens (<think>, <tool_call>, <response>) to structure its reasoning process. It specifies the steps for each turn, including recalling context, deciding on tool usage, and generating a response. The output format section provides details on how to structure the tool calls in JSON format. Finally, it includes some important notes to guide the LLM's behavior. This structured prompt ensures consistent and interpretable LLM outputs during the rollout and training phases.", "section": "3 Method"}]