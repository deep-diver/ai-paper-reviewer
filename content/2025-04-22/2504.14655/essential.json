{"importance": "This work introduces a **high-quality, contamination-free dataset** and an evaluation framework, advancing code LLM research by providing a testbed that addresses reasoning abilities. It enables fair evaluation and efficient training, impacting future benchmarks and training strategies for code generation.", "summary": "New LeetCodeDataset enables robust code LLM evaluation and efficient training with temporal splits and high-quality problems.", "takeaways": ["LeetCodeDataset offers a high-quality benchmark for code generation models with temporal splits and rich metadata.", "Reasoning models significantly outperform non-reasoning models in competitive programming tasks.", "Supervised fine-tuning with a small, model-generated dataset can achieve performance comparable to much larger, human-written datasets."], "tldr": "Code generation critically needs better benchmarks and training resources for large language models (LLMs). Current benchmarks lack accurate reasoning assessments, while training resources often miss live updates and tools for reinforcement learning. Addressing these gaps is essential for advancing research and applications in code generation.\n\nThis paper introduces the **LeetCodeDataset**, a high-quality resource for evaluating and training code-generation models. It provides a contamination-free dataset with temporal splits and rich metadata, enabling fair and efficient supervised fine-tuning. Experiments reveal that reasoning models outperform non-reasoning ones, and small model-generated datasets can match the performance of larger counterparts.", "affiliation": "newfacade@163.com", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2504.14655/podcast.wav"}