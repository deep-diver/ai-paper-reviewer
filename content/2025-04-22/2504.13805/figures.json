[{"figure_path": "https://arxiv.org/html/2504.13805/extracted/6372901/images/teaser-final.drawio.png", "caption": "Figure 1. \nThe LearnAct Framework and LearnGUI Benchmark focus on addressing the long-tail challenges in mobile GUI agent performance through demonstration-based learning.\nFrom rule-based automation to LLM-powered agents, mobile GUI automation has evolved significantly, yet still struggles with long-tail scenarios due to interface diversity. Our LearnAct framework introduces demonstration-based learning to effectively handle these challenges, outperforming existing methods in both offline and online evaluations.", "description": "This figure illustrates the LearnAct framework and the LearnGUI benchmark dataset. LearnAct is a novel framework that uses demonstration-based learning to improve the performance of mobile GUI agents.  It addresses the limitations of traditional rule-based and LLM-based methods, which struggle with the diversity of mobile interfaces and long-tail scenarios (uncommon or unexpected tasks). LearnGUI, a benchmark dataset, is specifically designed to support evaluating demonstration-based learning for mobile GUI agents.  The figure visually compares different approaches to mobile GUI automation and highlights LearnAct's superior performance in both offline and online settings.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2504.13805/x1.png", "caption": "Figure 2. \nA toy example for demonstration learning on mobile GUI Agent.\nWe build a benchmark named LearnGUI for demonstration learning on Mobile GUI Agent, which provides different few-shot task combinations and offers multi-dimensional metrics including task similarity, UI similarity, and action similarity between support tasks and query tasks.", "description": "This figure demonstrates a toy example of demonstration-based learning for a mobile GUI agent.  Two sample tasks are shown: a support task and a query task.  The support task involves checking the temperature in the living room and adjusting the windows and air conditioner. The query task involves checking the humidity in the bedroom and adjusting the humidifier and windows.  The goal is to use the demonstration of the support task to improve the agent's ability to solve the query task. The LearnGUI benchmark uses this type of setup to provide various few-shot task combinations, measuring task similarity, UI similarity, and action similarity between support and query tasks to evaluate performance.", "section": "INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2504.13805/extracted/6372901/images/ui_action_similarity_scatter.drawio.png", "caption": "Figure 3. \nJoint distribution of UI similarity and action similarity in LearnGUI-Offline.\nThe scatter plot shows the relationship between UI and action similarity measures across task pairs. The quadrant divisions represent our categorization of tasks into four profiles: UISHActSH, UISHActSL, UISLActSH, and UISLActSL, enabling analysis of how different similarity combinations affect learning transfer.", "description": "This scatter plot illustrates the relationship between UI (user interface) and action similarity in the LearnGUI-Offline dataset.  Each point represents a pair of tasks, with its horizontal position indicating the UI similarity and its vertical position indicating the action similarity between the two tasks.  The plot is divided into four quadrants, representing four profiles of task pairs: \n\n1. **UISHActSH:** High UI similarity and high action similarity.\n2. **UISHActSL:** High UI similarity and low action similarity.\n3. **UISLActSH:** Low UI similarity and high action similarity.\n4. **UISLActSL:** Low UI similarity and low action similarity.\n\nBy analyzing the distribution of task pairs across these quadrants, we can understand how different combinations of UI and action similarity affect the transfer of knowledge when learning from demonstrations.", "section": "3 LEARNGUI DATASET"}, {"figure_path": "https://arxiv.org/html/2504.13805/extracted/6372901/images/learnact-pipline.drawio.png", "caption": "Figure 4. \nIllustration of the overall framework of LearnAct.\nArchitecture diagram showing the three main components (DemoParser, KnowSeeker, ActExecutor) and their interconnections within the LearnAct system, including data flow from human demonstrations to execution.", "description": "LearnAct is a multi-agent framework for mobile GUI automation that leverages demonstration learning.  This figure provides a detailed architecture diagram illustrating LearnAct's three main components and their interactions: 1) DemoParser extracts knowledge from user demonstrations; 2) KnowSeeker retrieves relevant knowledge from the knowledge base generated by DemoParser; and 3) ActExecutor uses the retrieved knowledge, along with user instructions and current screen observations, to perform the task. The diagram shows the flow of data from human demonstrations through the three components and finally into the mobile environment for task execution.", "section": "4 METHOD: LEARNACT"}, {"figure_path": "https://arxiv.org/html/2504.13805/x2.png", "caption": "Figure 5. \nPipeline of DemoParser Agent.\nInput instructions and corresponding actions and screenshots; output low-level action descriptions and create knowledge database.\nThis process transforms high-level user instructions into precise operation sequences while building a reusable domain knowledge base to improve mobile interface interaction automation efficiency.", "description": "The DemoParser Agent takes high-level user instructions and corresponding actions and screenshots as input.  It processes this information to generate low-level, semantically descriptive action descriptions. These descriptions, which are more precise and detailed than the original input, are stored in a knowledge base. This knowledge base is reusable and helps to improve the efficiency of mobile interface interaction automation.  The figure illustrates the pipeline of this process, showing how the agent transforms high-level commands into a structured knowledge base that can be used to automate tasks.", "section": "LEARNGUI DATASET"}, {"figure_path": "https://arxiv.org/html/2504.13805/x3.png", "caption": "Figure 6. \nPipeline of KnowSeeker Agent.\nThe KnowSeeker Agent converts demo trajectories from the knowledge base into a vector database. When executing user tasks, KnowSeeker retrieves the top-k relevant demos from the vector database for subsequent use. This approach enables efficient retrieval of similar demonstrations to assist with new task execution.", "description": "The KnowSeeker Agent is a crucial component of the LearnAct framework, responsible for efficiently retrieving relevant demonstration knowledge to guide task execution.  It takes as input a user instruction and searches a vector database containing embeddings of demonstrations from the knowledge base (created by the DemoParser). The KnowSeeker employs a sentence transformer model (all-MiniLM-L6-v2) to generate dense vector embeddings of both user instructions and demonstration instructions.  These embeddings are used to calculate cosine similarity scores, enabling the retrieval of the top-k most similar demonstrations to the current user task.  This approach ensures that the most relevant and helpful prior demonstration experiences are available to the ActExecutor, optimizing the process of handling unseen task scenarios.", "section": "4.2 KnowSeeker"}, {"figure_path": "https://arxiv.org/html/2504.13805/x4.png", "caption": "Figure 7. \nPipeline of ActExecutor Agent.\nThe ActExecutor Agent executes the low-level action descriptions generated by the Action Planner Agent. It uses the KnowSeeker Agent to retrieve relevant demonstrations from the knowledge base and execute the actions in the demonstrations. This approach enables efficient execution of low-level actions to assist with new task execution.", "description": "The ActExecutor Agent is a key component of the LearnAct framework, responsible for executing the low-level actions needed to complete a task.  It receives action descriptions generated by the Action Planner. To execute these actions effectively, it leverages the KnowSeeker Agent to find relevant demonstrations from a knowledge base.  By mimicking actions from these demonstrations, the ActExecutor efficiently and accurately performs the actions required, making it easier to handle complex or unfamiliar tasks.", "section": "4.3 ActExecutor"}, {"figure_path": "https://arxiv.org/html/2504.13805/extracted/6372901/images/similarity_distributions_continuous.drawio.png", "caption": "Figure 8. \nDistribution of instruction, UI, and action similarity scores in LearnGUI-Offline.\nThe histograms show the distribution of similarity scores across three dimensions: instruction similarity (top), UI similarity (middle), and action similarity (bottom). These distributions enable systematic analysis of how different types of similarity between demonstration and query tasks affect learning efficacy.", "description": "Figure 8 displays the distributions of instruction, UI, and action similarity scores within the LearnGUI-Offline dataset.  Three histograms illustrate the distributions for each similarity type.  The x-axis of each histogram represents the similarity score (ranging from 0 to 1, where 1 indicates perfect similarity), and the y-axis represents the frequency or count of task pairs exhibiting that particular similarity score.  This figure allows for a visual analysis of the relationships between different types of similarity and how those relationships impact the effectiveness of demonstration-based learning in mobile GUI agents. By comparing the distributions, one can assess if high instruction similarity typically co-occurs with high UI or action similarity and the implications of those relationships for learning transfer.", "section": "3.3 Dataset Statistics"}, {"figure_path": "https://arxiv.org/html/2504.13805/x5.png", "caption": "Figure 9. \nPrompt template for intermediate action descriptions.\nThe template guides DemoParser to generate standardized descriptions for intermediate actions, including detailed rules for memory annotations that capture important information observed during task execution.", "description": "Figure 9 shows the prompt template used by the DemoParser agent to generate descriptions of intermediate actions within a mobile GUI task.  The prompt provides detailed instructions to the language model, specifying the format for the output JSON, acceptable action types (clicks, swipes, typing, etc.), and rules for including 'memory' annotations in the description. These memory annotations are used to record task-relevant information that the model should remember for future steps in the task, helping it to handle complex, multi-step processes.  The prompt emphasizes precise formatting, limitations on word counts, and examples to ensure the model produces consistently structured and useful knowledge representations.", "section": "4.1 DemoParser"}, {"figure_path": "https://arxiv.org/html/2504.13805/x6.png", "caption": "Figure 10. \nPrompt templates for terminal action descriptions.\nThe templates provide specific formats for both standard task completion and information retrieval tasks, ensuring consistent output structure across different task types.", "description": "This figure shows prompt templates used by the DemoParser agent to generate terminal action descriptions.  These templates enforce a consistent output structure for task completion, regardless of whether the task requires simply reporting completion or returning a specific answer.  The templates provide clear guidelines for the format and content of the terminal action description, which are crucial for maintaining data consistency within the LearnGUI dataset.", "section": "3.2 Data Collection"}, {"figure_path": "https://arxiv.org/html/2504.13805/x7.png", "caption": "Figure 11. \nTask execution prompt template.\nThis comprehensive prompt directs ActExecutor to generate actions based on current observations, action history, and retrieved demonstrations, with explicit formatting requirements to ensure consistent action outputs.", "description": "Figure 11 shows the prompt template used by the ActExecutor agent in the LearnAct framework.  This prompt guides the large language model within ActExecutor to make decisions about the next action to take in a mobile GUI automation task. It combines various pieces of information to formulate a decision: the user's original instructions for the task, visual data from the current screen displayed on the phone (screenshot), a history of the actions already performed, and relevant information extracted from previously-seen demonstrations. The structure of the prompt is carefully designed to ensure consistency in the outputs produced by the language model, thus facilitating effective task completion.  The prompt specifies the allowed action types (e.g., click, swipe, type, etc.) and enforces a particular output format for each action taken. For instance, when the task is completed, the prompt requires reporting the outcome of the task. When the action requires a specific response or the system needs to remember information for later steps, the prompt provides instructions for including these items.", "section": "4.2 KnowSeeker"}, {"figure_path": "https://arxiv.org/html/2504.13805/x8.png", "caption": "Figure 12. \nDetailed performance comparison of Qwen2-VL-7B with and without LearnAct on LearnGUI-Online.\nThe figure shows the task success rates of Qwen2-VL-7B baseline versus Qwen2-VL-7B enhanced with LearnAct across different task dimensions in the LearnGUI-Online benchmark.", "description": "This figure presents a detailed comparison of the performance of the Qwen2-VL-7B model with and without the LearnAct framework on the LearnGUI-Online benchmark.  It shows the task success rates, broken down by various task categories (e.g., complex UI understanding, data entry, information retrieval) to illustrate the impact of LearnAct on different types of tasks. By comparing the baseline Qwen2-VL-7B performance against the enhanced version incorporating LearnAct, the figure visually demonstrates LearnAct's effectiveness in improving task success across multiple task dimensions within the LearnGUI-Online benchmark.  The bar chart format facilitates easy comparison of success rates between the baseline and LearnAct-enhanced models for each task category.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.13805/x9.png", "caption": "Figure 13. \nDetailed performance comparison of UI-TARS-7B-SFT with and without LearnAct on LearnGUI-Online.\nThe figure presents a comprehensive breakdown of task success rates for UI-TARS-7B-SFT baseline versus UI-TARS-7B-SFT enhanced with LearnAct across multiple task dimensions in the LearnGUI-Online benchmark.", "description": "This figure displays a detailed comparison of the performance of the UI-TARS-7B-SFT model with and without the LearnAct framework on the LearnGUI-Online benchmark.  It shows the success rates for various tasks categorized by different dimensions (e.g., task complexity, type of interaction, etc.).  The bars visually represent the improvement in task success rate achieved by incorporating the LearnAct framework.", "section": "5 EXPERIMENTS"}]