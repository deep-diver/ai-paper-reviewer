{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-00-00", "reason": "This paper introduces Latent Diffusion Models (LDMs), a foundational approach for efficient high-resolution image synthesis, making it a central reference for the paper's investigation into end-to-end tuning of VAEs with diffusion models."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-00-00", "reason": "This paper explores diffusion models with transformers, showing the scalability benefits, and is important because the current paper uses and builds upon these transformer-based architectures (DiT/SiT)."}, {"fullname_first_author": "Sihyun Yu", "paper_title": "Representation alignment for generation: Training diffusion transformers is easier than you think", "publication_date": "2024-00-00", "reason": "This paper introduces REPA loss, used in the current paper as REPA-E, which allows both VAE and diffusion model to be jointly tuned during training process."}, {"fullname_first_author": "Nanye Ma", "paper_title": "Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers", "publication_date": "2024-00-00", "reason": "The paper introduces SiT, a scalable interpolant transformer, which is used in the current paper as generative model for evaluating performance."}, {"fullname_first_author": "Patrick Esser", "paper_title": "Taming transformers for high-resolution image synthesis", "publication_date": "2021-00-00", "reason": "This work focuses on training transformers for high-resolution image synthesis and provides an alternative training setup for the LDM that the current paper uses."}]}