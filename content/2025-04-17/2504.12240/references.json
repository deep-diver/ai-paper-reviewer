{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-08-01", "reason": "This paper introduces Latent Diffusion Models, which have become a foundational technique for image generation and are widely used in subsequent works on image colorization and manipulation."}, {"fullname_first_author": "Ashish Vaswani", "paper_title": "Attention Is All You Need", "publication_date": "2017-06-12", "reason": "This paper introduces the Transformer architecture, which is a fundamental component of many modern deep learning models, including those used for image colorization, due to its ability to model long-range dependencies."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-02-26", "reason": "This paper introduces CLIP (Contrastive Language-Image Pre-training), which is used for evaluating the perceptual similarity between images and text and in this paper for generating the training data and evaluating the final results."}, {"fullname_first_author": "Junhao Zhuang", "paper_title": "ColorFlow: Retrieval-Augmented Image Sequence Colorization", "publication_date": "2024-12-19", "reason": "As this paper is a continuation of the ColorFlow work, it is a crucial reference point, especially for comparative analysis and understanding the advancements made by Cobra."}, {"fullname_first_author": "Junsong Chen", "paper_title": "PixArt-\u03b1: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis", "publication_date": "2023-10-01", "reason": "Cobra is trained based on the pretrained PixArt-Alpha model and this work is a foundational paper in photorealistic text-to-image synthesis, providing a fast and efficient training methodology for Diffusion Transformers."}]}