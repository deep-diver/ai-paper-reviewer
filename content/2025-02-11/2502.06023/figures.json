[{"figure_path": "https://arxiv.org/html/2502.06023/x1.png", "caption": "Figure 1: Sample images generated by different methods on the HPSv2, Geneval, and Pickscore benchmarks. After fine-tuning SD 2.1 with SFTChosensubscriptSFTChosen\\text{SFT}_{\\text{Chosen}}SFT start_POSTSUBSCRIPT Chosen end_POSTSUBSCRIPT, Diffusion-DPO, MaPO, and DCPO on Pick-a-Picv2 and Pick-Double Caption datasets, DCPO produces images with notably higher preference and visual appeal (See more examples in Appendix H).", "description": "Figure 1 showcases a comparison of image generation results from various methods, including Stable Diffusion 2.1 (SD 2.1), SFTChosen, Diffusion-DPO, MaPO, and the novel DCPO method.  The models were fine-tuned using the Pick-a-Pic v2 and Pick-Double Caption datasets. Each model was given the same text prompts, and the resulting images are displayed, demonstrating the relative quality and adherence to the prompt.  The figure highlights that DCPO generates images with superior preference and visual appeal compared to the other techniques.  More examples are available in Appendix H.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06023/x2.png", "caption": "Figure 2: The overview of our 3 Dual Preference Optimization (DCPO) pipelines: DCPO-c, DCPO-p, and DCPO-h, all of which require a duo of a captioned preferred image (x0w,zw)subscriptsuperscript\ud835\udc65\ud835\udc640superscript\ud835\udc67\ud835\udc64(x^{w}_{0},z^{w})( italic_x start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT ) and a captioned less-preferred image (x0l,zl)subscriptsuperscript\ud835\udc65\ud835\udc590superscript\ud835\udc67\ud835\udc59(x^{l}_{0},z^{l})( italic_x start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ). DCPO-c (Top Left): We use a captioning model to generate distinctive captions respectively for images x0wsubscriptsuperscript\ud835\udc65\ud835\udc640x^{w}_{0}italic_x start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT and x0lsubscriptsuperscript\ud835\udc65\ud835\udc590x^{l}_{0}italic_x start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT given the shared prompt c\ud835\udc50citalic_c. DCPO-p (Bottom Left): We take prompt c\ud835\udc50citalic_c as the caption for image x0wsubscriptsuperscript\ud835\udc65\ud835\udc640x^{w}_{0}italic_x start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, then we use a Large Language Model (LLM) to generate a semantically perturbed prompt zplsuperscriptsubscript\ud835\udc67\ud835\udc5d\ud835\udc59z_{p}^{l}italic_z start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT given prompt c\ud835\udc50citalic_c as the caption for image x0lsubscriptsuperscript\ud835\udc65\ud835\udc590x^{l}_{0}italic_x start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. DCPO-h (Right):  A hybrid method where the generated caption zlsuperscript\ud835\udc67\ud835\udc59z^{l}italic_z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT is now perturbed into zplsuperscriptsubscript\ud835\udc67\ud835\udc5d\ud835\udc59z_{p}^{l}italic_z start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT for image x0lsubscriptsuperscript\ud835\udc65\ud835\udc590x^{l}_{0}italic_x start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. Our Pick-Double Caption Dataset discussed in Section 3.1 is constructed with the DCPO-c pipeline.", "description": "Figure 2 illustrates the three Dual Caption Preference Optimization (DCPO) pipelines: DCPO-c, DCPO-p, and DCPO-h. Each pipeline uses a pair of images: a preferred image with its caption and a less preferred image with its caption.  DCPO-c uses a captioning model to generate distinct captions for the preferred and less-preferred images, given a shared prompt.  DCPO-p uses the shared prompt as the caption for the preferred image, and employs a Large Language Model (LLM) to create a semantically perturbed caption for the less-preferred image. DCPO-h combines both strategies, starting with a generated caption for the less preferred image which is then further perturbed. The Pick-Double Caption Dataset (discussed in Section 3.1) is created using the DCPO-c pipeline.", "section": "2 METHOD"}, {"figure_path": "https://arxiv.org/html/2502.06023/x3.png", "caption": "Figure 3: The conflict distribution issue in the Pick-a-Pic v2 dataset. \u03bclsuperscript\ud835\udf07\ud835\udc59\\mu^{l}italic_\u03bc start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT and \u03bcwsuperscript\ud835\udf07\ud835\udc64\\mu^{w}italic_\u03bc start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT represent the average CLIPscore of preferred and less preferred images for prompt c\ud835\udc50citalic_c, respectively. Also, \u0394\u2062\u03bc\u0394\ud835\udf07\\Delta\\muroman_\u0394 italic_\u03bc shows the difference between the distributions.", "description": "Figure 3 illustrates the issue of overlapping distributions in the Pick-a-Pic v2 dataset, which is a key challenge in preference optimization for diffusion models. The figure displays the distribution of CLIP scores for both preferred and less preferred images generated from the same prompt. The average CLIP score for preferred images is represented by \u03bcw (mu-w), and the average CLIP score for less preferred images is represented by \u03bcl (mu-l). The difference between these two averages, \u0394\u03bc (delta-mu), highlights the extent of the overlap.  A smaller difference indicates a greater overlap, making it more challenging to distinguish between preferred and less preferred images using only the prompt as input for the optimization process.", "section": "2.1 THE CHALLENGES"}, {"figure_path": "https://arxiv.org/html/2502.06023/x4.png", "caption": "Figure 4: Effect of the perturbation method on semantic distributions in terms of CLIPScore. (a) shows the distributions that feature the captions zwsuperscript\ud835\udc67\ud835\udc64z^{w}italic_z start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT and zlsuperscript\ud835\udc67\ud835\udc59z^{l}italic_z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT generated by the LLaVA model, while (b), (c), and (d) represent different levels of perturbation on top of the caption zlsuperscript\ud835\udc67\ud835\udc59z^{l}italic_z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT. The figure demonstrates that as the level of perturbation increases, the distance between the distributions of captions zwsuperscript\ud835\udc67\ud835\udc64z^{w}italic_z start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT and zlsuperscript\ud835\udc67\ud835\udc59z^{l}italic_z start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT increases. For more details on the perturbation method, refer to Appendix E.", "description": "Figure 4 illustrates the impact of different perturbation levels on the semantic similarity between captions associated with preferred and less preferred images, as measured by CLIPScore.  Panel (a) displays the CLIPScore distributions for captions generated by the LLaVA model without perturbation. Subsequent panels (b), (c), and (d) show how these distributions change with increasing levels of perturbation applied to the caption of the less preferred image. The key observation is that greater perturbation leads to a larger separation between the CLIPScore distributions of the preferred and less preferred image captions, indicating that the perturbation method effectively increases the semantic difference between the two types of captions.", "section": "2.2 DCPO: Dual Caption Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2502.06023/x5.png", "caption": "Figure 5: Performance comparison of DCPO-c and DCPO-h on different perturbation levels. We plotted regression lines for the four models, showing that as \u0394\u2062\u03bc\u0394\ud835\udf07\\Delta\\muroman_\u0394 italic_\u03bc increases, performance improves but drops after a threshold t\ud835\udc61titalic_t (orange boundary).", "description": "This figure shows the performance comparison between DCPO-c (baseline) and three variants of DCPO-h with different levels of perturbation.  The x-axis represents the difference in CLIPScore (\u0394\u03bc) between preferred and less preferred image captions. The y-axis shows the performance metrics (Pickscore, HPSv2.1, ImageReward, CLIPscore, and GenEval). Regression lines are plotted for each model to illustrate the trend.  The results indicate that increasing \u0394\u03bc improves performance up to a certain threshold (represented by the orange vertical line), after which performance begins to decrease. This suggests that an optimal level of perturbation exists for maximizing the effectiveness of dual caption preference optimization.", "section": "3.3 ABLATION STUDIES AND ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2502.06023/x6.png", "caption": "Figure 6: Comparison of DCPO-h performance on in-distribution and out-of-distribution data.", "description": "Figure 6 presents a comparative analysis of the Dual Caption Preference Optimization (DCPO-h) model's performance when trained on both in-distribution and out-of-distribution data. The figure displays bar charts showing the performance metrics (Pickscore, HPSv2.1, ImageReward, CLIPscore, and GenEval) achieved by the DCPO-h model trained on in-distribution data (LLaVA-in, Emu2-in) versus the model trained on out-of-distribution data (LLaVA-out, Emu2-out).  The results visually demonstrate the impact of data distribution on the model's ability to generate high-quality images. This comparison highlights the robustness and generalizability of DCPO-h.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06023/x7.png", "caption": "Figure 7: DCPO-h performance comparison across various \u03b2\ud835\udefd\\betaitalic_\u03b2 values, evaluated on HPSv2.1 and GenEval.", "description": "Figure 7 displays the performance of the DCPO-h model across various values of the hyperparameter \\(\\beta\\).  The results are shown for two key evaluation benchmarks: HPSv2.1 and GenEval.  The graph likely shows how the model's performance (e.g., measured by a specific score) changes as \\(\\beta\\) is varied. This helps to illustrate the impact of this hyperparameter on the effectiveness of the DCPO-h method.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2502.06023/x8.png", "caption": "Figure 8: (Left) PartiPrompts benchmark results for three evaluation questions, as voted by GPT-4o. (Right) Qualitative comparison between DCPO-h and Diffusion-DPO fine-tuned on SD 2.1. DCPO-h shows better prompt adherence and realism, with outputs that align more closely with human preferences, emphasizing high contrast, vivid colors, fine detail, and well-focused composition.", "description": "Figure 8 presents a comparative analysis of the performance of DCPO-h and Diffusion-DPO, both fine-tuned on the Stable Diffusion 2.1 model. The left panel displays the quantitative results of the PartiPrompts benchmark evaluation using GPT-4 as the evaluator.  Three specific questions were posed to GPT-4: General Preference (which image is preferred?), Visual Appeal (which image is more visually appealing?), and Prompt Alignment (which image better matches the text prompt?).  The bar chart shows the win rates for DCPO-h versus Diffusion-DPO for each question. The right panel provides a qualitative comparison of images generated by both methods in response to three sample prompts. The images generated by DCPO-h exhibit higher visual fidelity, richer detail, more vibrant colors, superior contrast, and better compositional focus, indicating enhanced realism and overall quality that more closely aligns with human preference compared to images produced by Diffusion-DPO.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06023/x9.png", "caption": "Figure 9: Token distribution of original prompt.", "description": "This histogram displays the distribution of prompt lengths (measured in tokens) from the original Pick-a-Pic v2 dataset.  The x-axis represents the number of tokens in a prompt, and the y-axis shows the frequency or count of prompts with that length. The distribution appears to be somewhat right-skewed, indicating that many prompts are relatively short but some are quite long, with a peak around 15 tokens.", "section": "3.1 PICK-DOUBLE CAPTION DATASET"}, {"figure_path": "https://arxiv.org/html/2502.06023/x10.png", "caption": "Figure 10: Examples of Pick-Double Caption dataset.", "description": "Figure 10 shows examples from the Pick-Double Caption dataset, which is a modified version of the Pick-a-Pic v2 dataset.  The dataset includes pairs of images, one preferred and one less preferred, for the same prompt.  Critically, each image is accompanied by two distinct captions: one generated using a conditional prompt (that is, a caption that explicitly uses the original prompt for guidance) and one generated using an unconditional prompt (a caption that describes the image without any mention of the original prompt).  This dual-caption approach addresses the 'conflict distribution' problem observed in previous datasets where preferred and less-preferred images share similar visual characteristics for the same prompt. The figure highlights this by illustrating paired images with their corresponding conditional and unconditional captions.  This approach provides more information to the model during training, enabling it to more effectively differentiate between preferred and less-preferred images.", "section": "3.1 PICK-DOUBLE CAPTION DATASET"}, {"figure_path": "https://arxiv.org/html/2502.06023/x11.png", "caption": "Figure 11: Comparison of DCPO-c performance on in-distribution and out-of-distribution data.", "description": "This figure compares the performance of the DCPO-c method (Dual Caption Preference Optimization using the captioning method) on two types of data: in-distribution and out-of-distribution. In-distribution data refers to data that closely resembles the data used to train the model, while out-of-distribution data is significantly different.  The figure likely shows a bar chart or similar visualization, displaying metrics like Pickscore, HPSv2.1, ImageReward, CLIPscore, and GenEval for both in-distribution and out-of-distribution data sets, allowing for a direct comparison of the model's performance under different data conditions. The comparison aims to demonstrate how well the model generalizes to unseen data. The results will likely show that the model performs better on in-distribution data than on out-of-distribution data.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2502.06023/x12.png", "caption": "Figure 12: Additional generated outcomes using prompts from HPSv2 benchmark.", "description": "Figure 12 presents a qualitative comparison of image generation results across five different methods: Stable Diffusion 2.1 (baseline), SFTChosen, Diffusion-DPO, MaPO, and the proposed DCPO-h. Each method was used to generate images from three prompts sourced from the HPSv2 benchmark.  The figure visually demonstrates the differences in image quality, style, and adherence to the prompt between the methods. This comparison highlights the improvements achieved by DCPO-h in terms of image coherence, visual appeal, and relevance to the given prompts.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06023/x13.png", "caption": "Figure 13: Additional generated outcomes using prompts from Pickscore benchmark.", "description": "Figure 13 presents a qualitative comparison of image generation results from several different methods: SD 2.1 Base, SFTChosen, Diffusion-DPO, MaPO, and DCPO-h (the authors' method).  Each method was used to generate images based on prompts from the Pickscore benchmark. The figure displays the generated images for four different prompts.  By visually comparing the outputs, one can assess the relative strengths and weaknesses of each approach in terms of image quality, adherence to the prompt, and stylistic consistency.  The DCPO-h method, in particular, appears to produce high-quality images that align well with the intended prompt, compared to the other methods.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06023/x14.png", "caption": "Figure 14: Additional generated outcomes using prompts from GenEval benchmark.", "description": "Figure 14 presents several images generated using different models (SD 2.1 Base, SFTChosen, Diffusion-DPO, MaPO, and DCPO-h) based on prompts from the GenEval benchmark.  The prompts focus on everyday objects and scenes and the resulting images show variations in quality and adherence to the prompt across the different models.  This figure is a qualitative comparison to demonstrate the visual improvements achieved by the proposed DCPO-h model in comparison to the baseline and other existing methods.", "section": "3 Experiments"}]