{"references": [{"fullname_first_author": "Y. Bengio", "paper_title": "Estimating or propagating gradients through stochastic neurons for conditional computation", "publication_date": "2013-08-13", "reason": "This paper introduces the straight-through estimator (STE), a crucial technique used in MatQuant for propagating gradients through the quantization operator during backpropagation."}, {"fullname_first_author": "B. Jacob", "paper_title": "Quantization and training of neural networks for efficient integer-arithmetic-only inference", "publication_date": "2018-00-00", "reason": "This is a foundational paper on Quantization Aware Training (QAT), a key learning-based quantization method that MatQuant builds upon."}, {"fullname_first_author": "W. Shao", "paper_title": "OmniQuant: Omnidirectionally calibrated quantization for large language models", "publication_date": "2023-00-00", "reason": "MatQuant uses OmniQuant as a base quantization method, and this paper details its efficient and accurate quantization technique."}, {"fullname_first_author": "A. Kusupati", "paper_title": "Matryoshka representation learning", "publication_date": "2022-00-00", "reason": "This paper introduces the Matryoshka-style training, which is the core concept behind MatQuant's multi-scale training approach."}, {"fullname_first_author": "T. Dettmers", "paper_title": "GPT3.int8(): 8-bit matrix multiplication for transformers at scale", "publication_date": "2022-00-00", "reason": "This paper discusses the challenges and methods for quantizing LLMs to low-precision, which is directly relevant to the motivation and goals of MatQuant."}]}