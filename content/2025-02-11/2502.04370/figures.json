[{"figure_path": "https://arxiv.org/html/2502.04370/x1.png", "caption": "Figure 1: Overview of our method.\nDreamDPO first constructs pairwise examples, then compares their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D presentation with a preference-driven loss function. The loss function pulls the win example \ud835\udc31twinsuperscriptsubscript\ud835\udc31\ud835\udc61win\\mathbf{x}_{t}^{\\text{win}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT win end_POSTSUPERSCRIPT closer and pushes the lose example \ud835\udc31tlosesuperscriptsubscript\ud835\udc31\ud835\udc61lose\\mathbf{x}_{t}^{\\text{lose}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT lose end_POSTSUPERSCRIPT away.\nAs a piecewise objective, it selectively pushes \ud835\udc31tlosesuperscriptsubscript\ud835\udc31\ud835\udc61lose\\mathbf{x}_{t}^{\\text{lose}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT lose end_POSTSUPERSCRIPT only when the preference score gap sgapsubscript\ud835\udc60gaps_{\\text{gap}}italic_s start_POSTSUBSCRIPT gap end_POSTSUBSCRIPT exceeds a threshold \u03c4\ud835\udf0f\\tauitalic_\u03c4, preventing chaotic gradients from overly similar \ud835\udc31tlosesuperscriptsubscript\ud835\udc31\ud835\udc61lose\\mathbf{x}_{t}^{\\text{lose}}bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT lose end_POSTSUPERSCRIPT.", "description": "DreamDPO is an optimization-based framework. It consists of three steps: (1) Pairwise Example Construction: constructing pairs of 3D model examples by adding different Gaussian noise to the initial model; (2) Pairwise Comparison: comparing the alignment of the pairs with human preferences using reward models or large multimodal models; (3) Preference-Guided Optimization: optimizing the 3D representation using a preference-driven loss function that pulls the preferred example closer and pushes away the less preferred example only when their preference scores differ significantly. This prevents chaotic gradients and allows for more fine-grained control.", "section": "3 Method"}, {"figure_path": "https://arxiv.org/html/2502.04370/x2.png", "caption": "Figure 2: \nQualitative comparisons on the benchmark of GPTEval3D\u00a0[25].\nExisting methods struggle with text matching, as marked in red.\nDreamDPO improves text matching, which provides better human preference results.\n(Zoom in to see the details.)", "description": "Figure 2 presents a qualitative comparison of different text-to-3D generation models on the GPTEval3D benchmark.  The figure visually demonstrates the strengths and weaknesses of various methods in aligning generated 3D models with the input text prompt.  Models that struggled to accurately capture elements described in the prompt are highlighted in red, illustrating the challenges that existing methods face in correctly interpreting and representing textual descriptions in a 3D space.  In contrast, DreamDPO, the method proposed in the paper, shows significantly improved text matching, which is reflected in improved alignment with human preferences, as the generated 3D models more closely resemble the textual descriptions.", "section": "4.2 Comparison with Prior Methods"}, {"figure_path": "https://arxiv.org/html/2502.04370/x3.png", "caption": "Figure 3: \nQualitative comparisons with MVDream\u00a0[7].\nDreamDPO performs well across short to long prompts, offering better human preference results, marked in red.\u00a0(Zoom in to see the details.)", "description": "Figure 3 presents a qualitative comparison of 3D models generated by DreamDPO and MVDream for various text prompts ranging in length and complexity.  The figure showcases DreamDPO's improved ability to generate 3D models that align more closely with human preferences as indicated by the red markings highlighting superior results in text matching, detail, and overall quality.  Zooming in reveals finer details of the 3D models and their respective text prompts.", "section": "4.2 Comparison with Prior Methods"}, {"figure_path": "https://arxiv.org/html/2502.04370/x4.png", "caption": "Figure 4: The analysis of backbone.\nWe present the results of DreamDPO using Stable Diffusion v2.1 (SD2.1)\u00a0[17].\nDreamDPO demonstrates effective performance with SD2.1, highlighting its potential to leverage more advanced backbone diffusion models for further improvements.", "description": "Figure 4 showcases an ablation study analyzing the impact of the backbone diffusion model used in DreamDPO.  The results compare DreamDPO's performance when using the Stable Diffusion v2.1 model (SD2.1) against other backbones, demonstrating that DreamDPO achieves effective results even with this model.  This highlights the model's adaptability and its potential for improvement when utilizing more advanced diffusion models.", "section": "4.3 More Analyses and Justifications"}, {"figure_path": "https://arxiv.org/html/2502.04370/x5.png", "caption": "Figure 5: The analysis of reward models.\nWe present the results of DreamDPO using ImageReward\u00a0[26].\nDreamDPO demonstrates effective performance with ImageReward, highlighting its potential to leverage stronger reward models to further enhance generation quality.", "description": "This figure showcases the results of experiments conducted using DreamDPO with the ImageReward model for human preference evaluation.  The results demonstrate that DreamDPO consistently performs well when integrated with ImageReward, suggesting that using more robust reward models could further improve the quality of 3D asset generation.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04370/x6.png", "caption": "Figure 6: \nThe analysis of the score gap threshold \u03c4\ud835\udf0f\\tauitalic_\u03c4.\nWe conduct 2D toy experiments with \u03c4\ud835\udf0f\\tauitalic_\u03c4 ranging from 0.010.010.010.01 to 00.\nThe results indicate that a small but non-zero \u03c4\ud835\udf0f\\tauitalic_\u03c4 effectively filters out overly similar lose examples, leading to more detailed outputs.", "description": "This figure shows the impact of the score gap threshold (\u03c4) on the quality of generated images in 2D toy experiments.  The experiments varied \u03c4 from 0.01 down to 0. The results demonstrate that a small, non-zero value of \u03c4 effectively removes very similar 'lose' examples from the optimization process. This results in more detailed and refined final image outputs because the optimization isn't distracted by near-identical samples.", "section": "4.3 More Analyses and Justifications"}, {"figure_path": "https://arxiv.org/html/2502.04370/x7.png", "caption": "Figure 7: \nQualitative comparisons with DreamReward\u00a0[12].\nDreamDPO improves both text matching (marked in red) and geometric/texture details.", "description": "Figure 7 presents a qualitative comparison of 3D model generation results between DreamReward and the proposed DreamDPO method.  Each row shows a text prompt and the 3D models generated by each method.  Elements where DreamDPO shows improvement over DreamReward in terms of accurately representing details described in the text prompt are highlighted in red. This demonstrates DreamDPO's improved ability to align generated 3D models with textual descriptions and produce superior geometric and textural details.", "section": "4.2 Comparison with Prior Methods"}, {"figure_path": "https://arxiv.org/html/2502.04370/x8.png", "caption": "Figure 8: \nThe generation results of DreamDPO with large multi-modal models (LMMs).\nWe explore the potential of our method to leverage LMMs, such as QwenVL\u00a0[41] for explicit guidance in correcting the number and attribute of 3D assets. The left corner shows the details of pairwise comparisons using the LMM, including the question and win/lose criteria.\nBy carefully designing the question, DreamDPO can leverage both win and lose examples to guide optimization. (Zoom in to see the details.)", "description": "Figure 8 showcases DreamDPO's ability to integrate Large Multimodal Models (LMMs) like QwenVL for enhanced 3D asset generation.  The experiment demonstrates how carefully crafted questions posed to the LMM can provide pairwise comparisons, guiding the optimization process by indicating preferred (\"win\") and less preferred (\"lose\") 3D renderings. This allows DreamDPO to refine the generated 3D models, correcting issues like incorrect object counts or attributes.  The left corner of the figure details the question-answer process and win/lose labels used in the LMM evaluation. Zooming in reveals more detail.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04370/x9.png", "caption": "Figure 9: The analysis of pairwise example construction.\nWe compare (1) different noises: adding different Gaussian noises with the same timesteps, and (2) difference timesteps: adding the same Gaussian noise with different timesteps.", "description": "Figure 9 investigates the impact of the pairwise example construction method on the model's performance.  The figure compares two approaches: (1) using different Gaussian noise at the same timestep to generate the pairs and (2) using the same Gaussian noise but different timesteps.  This comparison helps understand how different methods of introducing variation in the input affect the model's ability to learn and generate high-quality 3D assets guided by preferences.", "section": "4.3 More Analyses and Justifications"}, {"figure_path": "https://arxiv.org/html/2502.04370/x10.png", "caption": "Figure 10: The further application of DreamDPO.\nWe conduct toy experiments on text-to-avatar generation by combining DreamDPO with Gaussian-based avatar generation framework\u00a0[48]. More details can be checked in Appendix\u00a0B.3.", "description": "This figure demonstrates an application of the DreamDPO method to text-to-avatar generation.  It shows a comparison between avatars generated by a prior method (MVDream) and DreamDPO. The results indicate that DreamDPO, when combined with a Gaussian-based avatar generation framework [48], produces improved results. More details of this experiment and implementation are provided in Appendix B.3.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2502.04370/x11.png", "caption": "Figure 11: More qualitative results using DreamDPO.", "description": "This figure displays qualitative results obtained using the DreamDPO method.  It presents several 3D model renderings generated by DreamDPO, compared to models created by the MVDream method.  The comparison highlights DreamDPO's superior ability to generate models that more closely align with textual descriptions.  For each prompt, the two resulting images are displayed side by side to visually showcase the differences.", "section": "4.2 Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2502.04370/x12.png", "caption": "Figure 12: More qualitative results using DreamDPO.", "description": "This figure displays additional qualitative results generated by DreamDPO, showcasing its ability to produce high-fidelity 3D models that accurately reflect the given text prompts.  Each row presents a text prompt along with comparative 3D renderings from both DreamDPO and a baseline method (MVDream).  The comparisons demonstrate that DreamDPO generates more realistic 3D models with enhanced alignment to the input text, improved text matching, and better geometric and texture details compared to the baseline method. Specific examples include an improved rendering of a cat magician with a white dove, a more accurate depiction of a beagle in a detective outfit, and a more realistic rendition of plants in a workshop.  This visual comparison provides further evidence of DreamDPO's superior performance.", "section": "4.2 Comparison with Prior Methods"}]