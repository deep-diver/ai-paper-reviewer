[{"figure_path": "https://arxiv.org/html/2502.06788/x1.png", "caption": "Figure 1: Overview of (1) diverse vision construction inside existing VLMs and (2) potential architecture variants of Encoder-Free VLMs.", "description": "This figure provides a comprehensive overview of vision-language models (VLMs).  Part (1) illustrates different methods used by existing VLMs to incorporate visual information. This includes using pre-trained vision encoders to extract visual features, which are then fed to large language models (LLMs); using cross-attention between vision encoders and LLMs at different layers; and more monolithic approaches that merge vision and language information directly within a single model. Part (2) explores various architecture designs for encoder-free VLMs, focusing on how to effectively construct visual perception within the LLM without relying on pre-trained vision encoders. These architectures range from simpler models with single visual layers to more complex designs that use re-parameterization techniques or a Mixture-of-Experts approach to manage vision-language interaction.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06788/x2.png", "caption": "Figure 2: Preliminary analyses across various VLMs\u2019 scaling efficiency during pre-training or fine-tuning (More details in\u00a0Appendix\u00a0A). Notably, VE / DT / EVE apply varying image downsampling rates (142 / 82 / 322). For fairness, we choose slightly different resolutions that yield relatively balanced token counts of 576 / 1024 / 625 tokens per image. Besides, we quantify weight changes between LLMs and VLMs by averaging absolute value variation within specific layer number or type. We report accuracy on GQA\u00a0[30], SEED\u00a0[26], TextVQA\u00a0[60], and SQA\u00a0[53] to examine VLMs\u2019 capabilities across general in-domain, open-domain, OCR-related, and text-related knowledge tasks.", "description": "This figure compares the scaling efficiency of different vision-language models (VLMs) during pre-training and fine-tuning.  Three types of VLMs are compared: those using vision encoders (VE), discrete tokenizers (DT), and encoder-free models (EVE).  The models use different image downsampling rates, but the resolutions are adjusted to maintain a relatively similar number of tokens per image for fair comparison. The figure shows how the performance of each model type changes as the amount of training data increases.  Weight changes between the LLMs and VLMs are quantified by averaging the absolute value of variations within specific layers.  The final performance is evaluated on four benchmark datasets (GQA, SEED, TextVQA, and SQA) that test the models' abilities across various domains, including general, open-domain, OCR-related, and text-related tasks.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.06788/x3.png", "caption": "Figure 3: Overview of our proposed EVEv2.0 framework. We first adopt a patch embedding layer to encode images losslessly, and then concatenate visual and textual tokens into a unified decoder-only vision-language model. Here, it extends the standard autoregressive transformer by incorporating modality-specific weights for each multi-head self-attention layer, feed-forward layer, and layer normalization.", "description": "Figure 3 illustrates the architecture of EVEv2.0, a novel encoder-free vision-language model.  Unlike traditional methods that rely on pre-trained vision encoders, EVEv2.0 processes images directly using a lossless patch embedding layer.  This layer converts the image into a sequence of tokens, which are then combined with text tokens and fed into a unified decoder-only transformer.  A key innovation is the use of modality-specific weights for various components of the transformer, including self-attention, feed-forward layers, and layer normalization. This design helps to reduce interference between the vision and language modalities and improve the model's overall efficiency and performance. The figure visually details this process, showcasing the patch embedding layer, the concatenation of visual and text tokens, and the modality-specific components within the transformer.", "section": "3.2. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2502.06788/x4.png", "caption": "Figure 4: Overview of training procedure. PEL/WEL denotes patch/word embedding layer. We begin by training the patch embedding layer to establish initial alignment across modalities. Afterward, we only update vision layers within the LLM to enhance visual perception progressively. Notably, we gradually increase the image resolutions from 800\u00d7\\times\u00d7800 to 1600\u00d7\\times\u00d71600 and keep the original image aspect ratio. Finally, we train the entire model via QA and instruction data to strengthen cross-modality correspondence and complex understanding.", "description": "This figure illustrates the four-stage training process for the EVEv2.0 model. Stage 1 focuses on initial alignment of vision and language modalities by training only the patch embedding layer.  Stage 2 progressively enhances visual perception by selectively updating vision layers within the pre-trained language model (LLM), while keeping other layers frozen. Image resolution is gradually increased from 800x800 to 1600x1600 while maintaining the aspect ratio to further improve model's adaptability to different image sizes.  In Stage 3, the entire model is fine-tuned using multi-task datasets comprising question-answering and instruction-following data.  This stage aims to strengthen cross-modal correspondence and improve overall model understanding. The diagram visually represents each stage's training data, trainable modules, and maximum image resolution. ", "section": "3.3. Training Procedure"}, {"figure_path": "https://arxiv.org/html/2502.06788/x7.png", "caption": "Figure 5: Training loss curve and evaluation results in Stage 2. We adopt various EVE variants based on Qwen-2.5\u00a0[70] as the baseline. We first train the patch embedding layer using EVE-recap-10M in Stage 1, and further unfreeze vision layers except LLM layers in Stage 2.", "description": "This figure displays the training loss curves and evaluation results for different variants of the EVE model during Stage 2 of the training process.  The EVE models are all based on the Qwen-2.5 (LLM) model as a baseline.  The training begins in Stage 1 by training only the patch embedding layer using the EVE-recap-10M dataset.  In Stage 2, the vision layers (excluding the LLM layers) are unfrozen, and training continues. The figure visually compares the performance of different model variants (EVEv1.0, EVEv1.2, EVEv1.5, and EVEv2.0) across various metrics, demonstrating the impact of different design choices on the model's ability to learn visual representations and generalize to downstream tasks.", "section": "3.2. Model Architecture"}, {"figure_path": "https://arxiv.org/html/2502.06788/x8.png", "caption": "Figure 6: Evaluation results of different data sources and caption engines. We utilize EVEv1.0 based on Vicuna-7B\u00a0[16] as the baseline.\nHere \u201d*-raw\u201c, \u201d*-cap\u201c, or \u201d*-recap\u201c denote noisy web image captions, the samples annotated by both LLaVA-1.5 (13B) and Emu2 (17B), or modified DenseFusion++ (7B), respectively. Note that \u201dL.O.S.\u201c represents the mixture of LAION\u00a0[59], OpenImages\u00a0[35], and SAM\u00a0[34].", "description": "Figure 6 presents a comparative analysis of the performance of EVEv1.0 (a vision-language model) trained on different image caption datasets.  It compares the model's accuracy on various vision-language benchmarks (GQA, SEED-ALL, SQA-IMG, TextVQA) when trained using different captioning methods: noisy web image captions (*-raw), captions generated by LLaVA-1.5 and Emu2 (*-cap), and captions generated by the modified DenseFusion++ (*-recap).  The figure also includes results when training data is a mixture of LAION, OpenImages, and SAM datasets (*-LOS).  The x-axis represents the amount of training data used (in millions), and the y-axis shows the model's accuracy on each benchmark. This visual representation allows for a direct comparison of the impact of data source and caption quality on the performance of the EVEv1.0 model.", "section": "3.1. Preliminary"}, {"figure_path": "https://arxiv.org/html/2502.06788/x9.png", "caption": "Figure 7: Evaluation results of mixed data ratio. We adopt EVEv1.0 with Vicuna-7B\u00a0[16] for validation. Note that x:y:z denote the proportion of synthesized data : language-only data : web data.", "description": "This figure displays the performance of EVEv1.0 (an encoder-free vision-language model) trained on various ratios of synthesized data, language-only data, and web data. The model is validated using the Vicuna-7B language model.  The x-axis represents the training data composition (synthesized:language-only:web), and the y-axis shows the average accuracy across several benchmark datasets. The graph illustrates how different proportions of these data types impact the model's performance on tasks requiring understanding of vision and language.", "section": "4.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.06788/x10.png", "caption": "Figure 8: Evaluation results of image settings.\nWe use EVEv1.0 with Vicuna-7B\u00a0[16].\n\u201cAnyRatio_maxL\u201d: longest image edge as 800,\n\u201cAnyRatio_LD\u201d: fixed image area as 8002,\n\u201cAnyRatio_HD\u201d: fixed image area as 16002,\n\u201cAnyResolution\u201d: arbitrary resolution.", "description": "This figure displays the performance of EVEv1.0 (a vision-language model) on various vision-language benchmarks under different image input settings.  Four distinct image processing strategies are compared: \n1. **AnyRatio_maxL:** Images are resized such that the longest edge is 800 pixels.\n2. **AnyRatio_LD:** Images are resized to maintain a fixed area of 800x800 pixels, preserving aspect ratio.\n3. **AnyRatio_HD:** Similar to AnyRatio_LD but with a larger fixed area of 1600x1600 pixels.\n4. **AnyResolution:** Images are processed at their original, arbitrary resolutions. The results show how the model's performance varies with different image scaling and whether maintaining aspect ratio or using a fixed area is beneficial. The underlying language model used is Vicuna-7B.", "section": "4.3. Ablation Studies"}]