[{"figure_path": "https://arxiv.org/html/2503.12532/x1.png", "caption": "Figure 1: Windows File Explorer task completion rate of different computer-use agents:\n(i) Our powerful GUI grounding model achieves the current best task completion rate, setting a promising upper bound for computer-use agent finetuning.\n(ii) Using STEVE, our step verification pipeline, we are able to train\nour agents with KTO (red), which consistently outperforms (iii) the supervised finetuning (SFT).\nNotably, with increased computer operating time (x-axis), our 7B KTO agent is able to outperform the OmniParser with the GPT-4o planner.", "description": "This figure displays the task completion rate for different computer-use agents on a Windows File Explorer task.  The y-axis represents the accuracy or task completion rate, while the x-axis shows the amount of computer time used for training (in thousands of hours). Three key observations are highlighted: (i) The authors' GUI grounding model achieves the highest task completion rate, suggesting an upper bound for performance achievable with fine-tuning methods; (ii) Their agent trained using STEVE and the KTO optimization (red line) consistently outperforms an agent trained using supervised fine-tuning (SFT, blue line); (iii) With increased training time, their 7B KTO agent surpasses the OmniParser, which is a state-of-the-art agent using a GPT-40 planner.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.12532/x2.png", "caption": "Figure 2: Datasets we collected for UI-grounding model training, including open-source datasets and an additional private Windows OS dataset created by ourselves to enhance the model\u2019s performance on Windows.", "description": "Figure 2 details the datasets used to train the UI-grounding model.  The model's training leveraged a combination of publicly available datasets (WebUI, Seeclick, AITW, Allava) and a new, privately collected dataset focusing on the Windows OS.  This private dataset was created to specifically improve the model's performance and accuracy when interacting with Windows interfaces, supplementing the more general UI data from the open-source datasets. The table shows the name of each dataset, the type of annotation provided (DOM, OCR, caption, general QA), the number of images, and the number of UI elements included in each.", "section": "3.1 UI-grounding Model"}, {"figure_path": "https://arxiv.org/html/2503.12532/x3.png", "caption": "Figure 3: \nOverview of STEVE, the step verification pipeline.\nWe first create a large number of feasible tasks from the seed tasks to scale up the quality and diversity of agent tasks. Then we deploy our computer-use agent in desktop environments to sample trajectory data. A GPT-4o judge is used to verify the quality of each step in the trajectory, resulting in a large process reward dataset for agent training.", "description": "The figure illustrates the STEVE (Step Verification Pipeline) process.  It begins with a small set of seed tasks, which are expanded into a larger, more diverse set of feasible tasks. A computer-use agent then executes these tasks in a simulated desktop environment, generating a set of trajectories (sequences of actions). Each step in these trajectories is then evaluated by a GPT-40 model, acting as a judge to determine its correctness based on visual feedback (before and after screenshots). The result of this step-by-step verification is a large dataset of trajectories annotated with process-level rewards, which are used to train the agent.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.12532/x4.png", "caption": "Figure 4: \nPercentage consistency between human judges and the GPT-4o step verifier. We split all the positive and negative actions into early (step ID \u22647absent7\\leq 7\u2264 7) and late (step ID >7absent7>7> 7) groups, resulting in four bars in the figure. For example, 92.3%percent92.392.3\\%92.3 % for the Early Pos. bar means the GPT-4o judge agrees with humans for 92.3%percent92.392.3\\%92.3 % of the early positive actions.", "description": "This figure illustrates the agreement between GPT-4 and human evaluators on the correctness of agent actions within a task.  The x-axis categorizes actions as early (steps 1-7) or late (steps after 7) in a task sequence, and whether the action was deemed positive (successful) or negative (unsuccessful). The y-axis represents the percentage of agreement between GPT-4's assessment and human evaluators' assessment for each category.  The high percentage agreement in the early steps and a slight decrease in agreement for later steps suggests GPT-4's ability to evaluate action correctness diminishes as the complexity of a task increases.", "section": "3.3 Step Verifier for Trajectory Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.12532/x5.png", "caption": "(a) Task success rate on the File Explorer split.", "description": "This figure shows a bar chart comparing the task success rates of different computer-use agents on the File Explorer task split within the WinAgentArena benchmark.  The chart allows for a comparison of the performance of various agents, including OmniParser, an SFT (Supervised Finetuning) agent, and multiple KTO (Kahneman & Tversky Optimization) agents across different training rounds.  The x-axis represents the different agents, and the y-axis represents the task success rate.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12532/x6.png", "caption": "(b) Task success rate on the Web Browser split.", "description": "This figure shows the task success rate achieved by different computer-use agent models on web browser-specific tasks within the WinAgentArena benchmark.  The models are compared across multiple training rounds to evaluate the effectiveness of different training methodologies (Supervised Finetuning (SFT) and Kahneman & Tversky Optimization (KTO)).  The results highlight the improved performance of KTO-trained agents over SFT and a baseline model across iterative training rounds.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12532/x7.png", "caption": "(c) Task success rate on the VsCode split.", "description": "This figure shows a bar chart comparing the task success rates of different computer-use agents on the VsCode split of the WinAgentArena benchmark.  The agents compared include OmniParser, an agent trained using supervised finetuning (SFT), and three agents trained using the Kahneman & Tversky Optimization (KTO) method across multiple training rounds (R1, R2, R3).  The chart visually represents the relative performance of each agent in successfully completing tasks within the VsCode application.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12532/x8.png", "caption": "Figure 5: We show an ablation study of OmniParser, the SFT agent, and three KTO agents at three iterative rounds (SFT, R1, R2, and R3). The results are evaluated on three distinct task domains from the WinAgentArena benchmark. Yellow bars in the figures indicate that GPT-4o is employed as the task planner. The reported outcomes represent the average performance over five experimental runs.", "description": "This ablation study compares the performance of different agent training methods on three distinct tasks from the WinAgentArena benchmark: File Explorer, Web Browser, and VsCode.  The methods compared are OmniParser (a baseline), supervised fine-tuning (SFT), and the Kahneman & Tversky Optimization (KTO) approach.  For KTO, results are shown for three iterative training rounds (R1, R2, R3).  The yellow bars highlight results where GPT-40 was used as a task planner.  Each bar represents the average performance across five experimental runs.  The figure visually demonstrates the improvement in task success rates achieved by the KTO approach over multiple training iterations.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.12532/x9.png", "caption": "Figure 6: Zoom in visualization of UI localization performance of different models on four target GUIs: example.txt, Design tab, Cached image check box, and Title of PPT slide (left to right). The UI-Grounding Model\u2019s performance is shown in green (top row), the SFT-trained agent in red (middle row), and the KTO-trained agent in blue (bottom row).", "description": "Figure 6 presents a zoomed-in visualization comparing the UI localization performance of three different models on four distinct Graphical User Interface (GUI) elements.  These elements include: 'example.txt' (a text file), a 'Design' tab (likely within an application), a 'Cached image check box' (a specific UI element), and the 'Title' of a PowerPoint slide. Each model's performance is color-coded and displayed in separate rows: the UI-Grounding model (green, top row), the Supervised Finetuning (SFT)-trained agent (red, middle row), and the Kahneman & Tversky Optimization (KTO)-trained agent (blue, bottom row). The figure allows for a detailed comparison of the precision and accuracy of each model's localization capability, highlighting the differences in their ability to accurately identify and locate specific GUI elements within a complex screen.", "section": "4.1 GUI Grounding Evaluation"}, {"figure_path": "https://arxiv.org/html/2503.12532/x10.png", "caption": "Figure 7: \nThe reward margin (vertical axis) between the chosen and rejected samples consistently improve during the KTO training.", "description": "This figure shows a line graph illustrating the reward margin's improvement over training steps in the Kahneman & Tversky Optimization (KTO) algorithm. The reward margin represents the difference between rewards assigned to chosen (positive) samples and rejected (negative) samples during training.  A consistent upward trend in the reward margin indicates the algorithm's effectiveness in distinguishing between positive and negative examples.  The graph visually demonstrates that as KTO training progresses, the model becomes increasingly better at identifying and utilizing beneficial samples.", "section": "3.4 KTO Training with Stepwise Rewards"}, {"figure_path": "https://arxiv.org/html/2503.12532/x11.png", "caption": "Figure 8: The trajectories of our STEVE-KTO-7B agent for the chrome tasks from the WinAgentArena\u00a0[4] with ID bb5e4c0d-f964-439c-97b6-bdb9747de3f4-wos (up) and b070486d-e161-459b-aa2b-ef442d973b92-wos (bottom). We display a simplified action for each step and plot the target UI localization results with a red bounding box in each screenshot. For high-resolution screenshots/videos, full model responses with screen analysis, multi-step planning, and python code blocks, please refer to the corresponding attachments.", "description": "This figure visualizes two example task trajectories generated by the STEVE-KTO-7B agent within the WinAgentArena environment.  Each trajectory corresponds to a distinct task ID and shows a sequence of steps the agent took to complete the task.  Simplified actions are displayed for each step, highlighting the interaction with the user interface.  The target UI elements for each action are indicated by red bounding boxes.  For a detailed understanding, including high-resolution screenshots, videos of the full interactions, the agent's complete reasoning process (including screen analysis and multi-step planning), and the Python code used to control the actions, please refer to the supplementary materials accompanying the research paper.", "section": "C. WinAgentArena Examples"}, {"figure_path": "https://arxiv.org/html/2503.12532/x12.png", "caption": "Figure 9: The trajectories of our STEVE-KTO-7B agent for the file explorer tasks from the WinAgentArena\u00a0[4] with ID 7c70e16b-e14f-4baa-b046-3e022b2d0305-WOS (up) and 5316686e-5688-4115-be24-052037df599f-WOS (bottom). We display a simplified action for each step and plot the target UI localization results with a red bounding box in each screenshot. For high-resolution screenshots/videos, full model responses with screen analysis, multi-step planning, and python code blocks, please refer to the corresponding attachments.", "description": "Figure 9 presents two example task trajectories generated by the STEVE-KTO-7B model within the WinAgentArena environment.  Each trajectory visualizes a multi-step process of interacting with a file explorer window in Windows OS. The top trajectory (ID: 7c70e16b-e14f-4baa-b046-3e022b2d0305-WOS) shows the steps involved in sorting files by modification date.  The bottom trajectory (ID: 5316686e-5688-4115-be24-052037df599f-WOS) demonstrates how the model updates file explorer settings to display hidden and system files.  For each step, the figure displays a simplified action taken by the agent and highlights the target UI elements that were interacted with using red bounding boxes. The high-resolution screenshots and additional details, like the full model's reasoning process (screen analysis, multi-step planning, and Python code), are available in supplementary material.", "section": "3.3 Step Verifier for Trajectory Evaluation"}]