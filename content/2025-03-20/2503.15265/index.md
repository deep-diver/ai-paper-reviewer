---
title: "DeepMesh: Auto-Regressive Artist-mesh Creation with Reinforcement Learning"
summary: "DeepMesh: RL-guided auto-regressive creation of artist-quality 3D meshes, enhanced by tokenization & DPO for human-aligned aesthetics."
categories: ["AI Generated", "ğŸ¤— Daily Papers"]
tags: ["Computer Vision", "3D Vision", "ğŸ¢ Tsinghua University",]
showSummary: true
date: 2025-03-19
draft: false
---

<br>

{{< keywordList >}}
{{< keyword icon="fingerprint" >}} 2503.15265 {{< /keyword >}}
{{< keyword icon="writer" >}} Ruowen Zhao et el. {{< /keyword >}}
 
{{< keyword >}} ğŸ¤— 2025-03-20 {{< /keyword >}}
 
{{< /keywordList >}}

{{< button href="https://arxiv.org/abs/2503.15265" target="_self" >}}
â†— arXiv
{{< /button >}}
{{< button href="https://huggingface.co/papers/2503.15265" target="_self" >}}
â†— Hugging Face
{{< /button >}}



<audio controls>
    <source src="https://ai-paper-reviewer.com/2503.15265/podcast.wav" type="audio/wav">
    Your browser does not support the audio element.
</audio>


### TL;DR


{{< lead >}}

Triangle meshes are essential in 3D applications. Existing auto-regressive methods, which generate structured meshes by predicting vertex tokens, often face limitations in face counts and mesh completeness. Moreover, these methods struggle with aligning outputs with human aesthetic preferences, leading to geometric inaccuracies and a lack of artistic refinement. These challenges hinder the creation of high-quality, artist-like 3D meshes.



To address these issues, this paper presents a framework that optimizes mesh generation through two key innovations. First, an efficient pre-training strategy incorporates a novel tokenization algorithm. Second, Reinforcement Learning (RL) is introduced into 3D mesh generation, achieving human preference alignment via Direct Preference Optimization (DPO). With a scoring standard combining human evaluation and 3D metrics, the framework generates detailed, precise meshes, outperforming existing methods in precision and quality.

{{< /lead >}}


#### Key Takeaways

{{< alert "star" >}}
{{< typeit speed=10 lifeLike=true >}} Introduces an efficient pre-training strategy that incorporates a novel tokenization algorithm. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=1000 lifeLike=true >}} Pioneers the adaptation of Direct Preference Optimization (DPO) for 3D auto-regressive models, aligning model outputs with human preference. {{< /typeit >}}
{{< /alert >}}

{{< alert "star" >}}
{{< typeit speed=10 startDelay=2000 lifeLike=true >}} Achieves state-of-the-art performance in both precision and quality for generating intricate and precise 3D meshes. {{< /typeit >}}
{{< /alert >}}

#### Why does it matter?
This research introduces a novel approach to **3D mesh generation with human-aligned artistry**, offering new methods for geometric detail & visual appeal. It opens avenues for exploring RL in generative modeling & artistic mesh creation.

------
#### Visual Insights



![](https://arxiv.org/html/2503.15265/x1.png)

> ğŸ”¼ This figure showcases a variety of 3D meshes generated by the DeepMesh model.  Each mesh is conditioned on a given point cloud as input, demonstrating the model's ability to efficiently create aesthetically pleasing, artist-quality meshes from this sparse input data. The variety of the meshes highlights DeepMesh's capabilities in producing diverse, high-fidelity results.
> <details>
> <summary>read the caption</summary>
> Figure 1: Gallery of DeepMeshâ€™s generation results. DeepMesh efficiently generates aesthetic, artist-like meshes conditioned on the given point cloud.
> </details>





{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.3.3.4">Metrics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1">C.Dist. <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.2.2">H.Dist. <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3">User Study <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.m1.1d">â†‘</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.4.1.1">MeshAnythingv2Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.15265v1#bib.bib6" title=""><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.4.1.2">0.1249</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.4.1.3">0.2991</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T1.3.4.1.4">10%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.5.2.1">BPTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2503.15265v1#bib.bib67" title=""><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.2">0.1425</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.2.3">0.2796</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.5.2.4">19%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.6.3.1">Ours w/o DPO</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.2">0.1001</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.3.3">0.1861</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.3.6.3.4">34%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.3.7.4.1">Ours w DPO</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.7.4.2"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.4.2.1">0.0884</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.7.4.3"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.4.3.1">0.1708</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T1.3.7.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.3.7.4.4.1">37%</span></td>
</tr>
</tbody>
</table>{{< /table-caption >}}

> ğŸ”¼ This table presents a quantitative comparison of DeepMesh against other state-of-the-art baselines for 3D mesh generation.  The metrics used likely assess the geometric accuracy (e.g., Chamfer Distance, Hausdorff Distance) and the visual quality (e.g., user study scores) of the generated meshes.  Lower Chamfer and Hausdorff distances indicate better geometric fidelity to the ground truth. Higher user study scores represent better subjective visual appeal. The results demonstrate DeepMesh's superior performance in both geometric accuracy and visual quality compared to the competing methods.
> <details>
> <summary>read the caption</summary>
> Table 1: Quantitative comparison with other baselines. Our method outperforms other baselines in generated geometry and visual quality.
> </details>





### In-depth insights


#### Mesh Tokenization
**Mesh tokenization** is crucial for enabling transformer-based models to process 3D meshes. Efficient tokenization schemes aim to reduce sequence length, thereby decreasing computational cost and memory requirements. The key challenge involves balancing compression ratio with preserving geometric details and maintaining manageable vocabulary size. **Effective algorithms** typically employ techniques like face traversal, vertex quantization, and hierarchical indexing to represent meshes as discrete tokens suitable for auto-regressive modeling. Optimizing tokenization enhances training efficiency and enables the generation of high-resolution, artist-like meshes, addressing limitations in face counts and mesh completeness often encountered in existing methods.

#### DPO for 3D Mesh
**Direct Preference Optimization (DPO)** has emerged as a promising technique for aligning generative models with human preferences, showing success in language models and vision-language models.  Applying DPO to 3D mesh generation could significantly improve the **aesthetic quality** and **geometric accuracy** of generated meshes. This involves collecting preference data where humans evaluate pairs of meshes and select the preferred one based on visual appeal and geometric correctness. A key challenge is designing a **scoring standard** that effectively captures human preferences for 3D meshes, potentially combining subjective visual assessments with objective 3D metrics like Chamfer distance and mesh quality metrics. Overcoming these challenges could lead to a new state-of-the-art in 3D mesh generation.

#### Pre-Training Refined
**Refining pre-training** is crucial for auto-regressive models. A refined pre-training phase could involve several key aspects. Firstly, it's about the data: **curating a high-quality dataset** is essential, filtering out noisy or incomplete meshes and prioritizing those with clean geometry and topology. Then there's the tokenization process. An efficient tokenization scheme could **reduce sequence length** without sacrificing geometric detail, leading to faster training and reduced memory consumption. Efficient tokenization allows us to use high-resolution meshes for training. A refined pre-training strategy ensures better initial weights, leading to faster convergence and improved final performance in downstream tasks like generating artist-like meshes.

#### Data Curation Vital
Data curation is vital because the **quality of training data fundamentally governs model performance**. In the context of 3D mesh generation, existing datasets often exhibit **high variability in quality**, including irregular topology, excessive fragmentation, or extreme geometric complexity. Addressing these issues through data curation is essential to mitigate potential problems such as unstable training, loss spikes, and compromised model generalization. A well-designed data curation strategy can filter out poor-quality meshes based on geometric structure and visual fidelity. This ensures that the model is trained on a **high-quality dataset**, leading to more stable training and superior mesh generation results. By focusing on clean, representative data, the model can learn more effectively and avoid being misled by noisy or incomplete examples. This enhances the overall reliability and performance of the 3D mesh generation process.

#### Scalable Artist Mesh
The concept of "Scalable Artist Mesh" is intriguing, suggesting a method to create 3D meshes that maintain artistic quality while scaling efficiently. This could involve novel mesh representations, tokenization, or procedural generation techniques. A key challenge would be balancing artistic control and computational cost, possibly using **hierarchical structures** or **adaptive mesh refinement**. Another aspect involves training data - the quality and size of artist-created meshes significantly impact the model's ability to learn and generate aesthetically pleasing results. **Reinforcement learning** to align outputs with human preferences could play a role, as could **data curation techniques** to improve the quality of training examples.


### More visual insights

<details>
<summary>More on figures
</summary>


![](https://arxiv.org/html/2503.15265/x2.png)

> ğŸ”¼ This figure illustrates the DeepMesh architecture and training process.  DeepMesh uses an autoregressive transformer with self-attention and cross-attention layers to generate meshes.  The model is first pre-trained using a novel tokenization algorithm on discrete mesh tokens. Then, a scoring system that combines 3D metrics with human evaluation is used to create 5000 preference pairs. Finally, the model is fine-tuned with Direct Preference Optimization (DPO) using these preference pairs to better align the generated meshes with human preferences.
> <details>
> <summary>read the caption</summary>
> Figure 2: An overview of our method. DeepMesh is an auto-regressive transformer composed of both self-attention and cross-attention layers. The model is pre-trained on discrete mesh tokens generated by our improved tokenization algorithm. To further enhance the quality of results, we propose a scoring standard that combines 3D metrics with human evaluation. With this standard, we annotate 5,000 preference pairs and then post-train the model with DPO to align its outputs with human preferences.
> </details>



![](https://arxiv.org/html/2503.15265/x3.png)

> ğŸ”¼ This histogram displays the distribution of polygon face counts within the DeepMesh training dataset.  The x-axis represents the number of faces per mesh, and the y-axis shows the frequency of meshes with that face count.  The dataset comprises approximately 500,000 meshes, with an average of 8,000 faces per mesh. This visualization highlights the prevalence of high-polygon meshes in the dataset, which is important to note for model training.
> <details>
> <summary>read the caption</summary>
> Figure 3: Distribution of face count in training dataset. We present the distribution of face counts in our training dataset. Our dataset size is approximately 500k, with an average face count of 8k.
> </details>



![](https://arxiv.org/html/2503.15265/x4.png)

> ğŸ”¼ This figure displays examples from a dataset of mesh preference pairs used in training the DeepMesh model.  Each row shows a pair of generated meshes, one preferred and one rejected.  The preference annotations are based on three key aspects: how complete the mesh geometry is (geometry completeness), the level of detail in the mesh's surface (surface details), and how clean and well-organized the mesh's wireframe is (wireframe structure).  This dataset helps train the model to generate more aesthetically pleasing meshes that satisfy human preferences.
> <details>
> <summary>read the caption</summary>
> Figure 4: Some examples of the collected preference pairs. We annotate the preferred meshes based on their geometry completeness, surface details and wireframe structure.
> </details>



![](https://arxiv.org/html/2503.15265/x5.png)

> ğŸ”¼ Figure 5 presents a qualitative comparison of point cloud-conditioned mesh generation results between DeepMesh and several baseline methods.  The figure shows that DeepMesh generates meshes with superior geometric accuracy and preservation of fine details compared to other methods.  A key observation is that DeepMesh produces meshes with a significantly higher number of faces, suggesting a greater level of detail and complexity.
> <details>
> <summary>read the caption</summary>
> Figure 5: Qualitative comparison on point cloud conditioned generation between DeepMesh and baselines. DeepMesh outperforms baselines in both generated geometry and preservation of fine-grained details. The meshes generated by ours have much more faces than others.
> </details>



![](https://arxiv.org/html/2503.15265/x6.png)

> ğŸ”¼ This figure showcases examples of 3D meshes generated by the DeepMesh model when conditioned on input images.  The results demonstrate the model's capability to generate high-fidelity meshes that accurately reflect the details and overall structure present in the original images, showcasing a strong alignment between the input image and generated 3D mesh.
> <details>
> <summary>read the caption</summary>
> Figure 6: Image-conditioned generation results of our method. Our method can generate high-fidelity meshes aligned with the input images.
> </details>



![](https://arxiv.org/html/2503.15265/x7.png)

> ğŸ”¼ Given the same input point cloud, DeepMesh demonstrates its ability to generate a variety of meshes with different appearances, showcasing its capacity for creative and diverse output.
> <details>
> <summary>read the caption</summary>
> Figure 7: Diversity of generations. DeepMesh can generate meshes with diverse appearance given the same point cloud.
> </details>



![](https://arxiv.org/html/2503.15265/x8.png)

> ğŸ”¼ This ablation study compares mesh generation results with and without Direct Preference Optimization (DPO).  Both methods produce meshes with good geometric accuracy. However, the image clearly shows that meshes generated with DPO are aesthetically more pleasing and visually appealing, demonstrating the effectiveness of DPO in enhancing the visual quality of the generated meshes.
> <details>
> <summary>read the caption</summary>
> Figure 8: Ablation study on the effectiveness of DPO. We can observe that while both approaches yield excellent geometry, the results generated using DPO are more visually appealing.
> </details>



![](https://arxiv.org/html/2503.15265/x9.png)

> ğŸ”¼ This figure details a novel mesh tokenization algorithm.  The process begins by traversing the mesh faces, grouping them into connected patches. Each vertex within these patches is then quantized into one of 512 discrete values. The entire coordinate system is divided into a three-level hierarchy of blocks, and each quantized coordinate is represented as an offset within its block.  Finally, to further reduce the length of the token sequence, identical offsets for neighboring vertices are merged.
> <details>
> <summary>read the caption</summary>
> Figure 9: Details of our tokenization algorithm. We first traverse mesh faces by dividing them into patches according to their connectivity and quantize each vertex of faces into rğ‘Ÿritalic_r bins (in our setting r=512ğ‘Ÿ512r=512italic_r = 512).Then we partition the whole coordinate system into three hierarchical levels of blocks and index the quantized coordinates as offsets within each block. We merge the index of neighbor vertices if they have the identical values.
> </details>



![](https://arxiv.org/html/2503.15265/x10.png)

> ğŸ”¼ This figure compares the training efficiency of DeepMesh's novel tokenization algorithm against existing methods (AMT, EdgeRunner, BPT).  The experiment involved integrating each algorithm into the same model architecture and training on a dataset containing 80 meshes per face count category (10K, 20K, 30K, 40K faces). The results demonstrate that DeepMesh's tokenization method consistently achieves the fastest training times across all face count categories, highlighting its superior training efficiency.
> <details>
> <summary>read the caption</summary>
> Figure 10: Comparison with other tokenization algorithms in training effciency. We integrate all tokenization algorithms into our model architecture and train them on a dataset of 80 meshes for each face count category (10K, 20K, 30K, 40K). Our method achieves the fastest training time across all face count categories, demonstrating superior training efficiency.
> </details>



![](https://arxiv.org/html/2503.15265/x11.png)

> ğŸ”¼ The figure shows the training loss curves before and after data curation. Before data curation, the loss curve exhibits frequent spikes, indicating instability in the training process due to low-quality data samples.  After data curation, where low-quality samples were removed, the training loss curve becomes significantly more stable, highlighting the positive effect of data curation on the training process.
> <details>
> <summary>read the caption</summary>
> (a) Before data curation
> </details>



![](https://arxiv.org/html/2503.15265/x12.png)

> ğŸ”¼ This figure shows the impact of data curation on the training process of the DeepMesh model. The graph displays the training loss over time, comparing the original training data (before curation) with the curated dataset (after curation). The graph demonstrates that the training process is significantly more stable after the data curation step, as indicated by the reduced frequency and magnitude of loss spikes in the loss curve.
> <details>
> <summary>read the caption</summary>
> (b) After data curation
> </details>



![](https://arxiv.org/html/2503.15265/x13.png)

> ğŸ”¼ This figure shows a comparison of training loss curves before and after data curation. The graph on the left (before data curation) displays frequent and significant spikes in the loss, indicating instability in the training process. This instability likely results from the inclusion of noisy or low-quality data in the training set. The graph on the right (after data curation) shows a much smoother and stable loss curve. The data curation process removed problematic data points, leading to a more consistent and stable model training process.  The smoother curve indicates that the model is learning more effectively and consistently without the disruptions caused by the poor-quality data.
> <details>
> <summary>read the caption</summary>
> Figure 11: Training loss before and after data curation. Before data curation, we observe frequent loss spikes. After data curation, preâ€‘training becomes significantly more stable.
> </details>



![](https://arxiv.org/html/2503.15265/x14.png)

> ğŸ”¼ Figure 12 showcases additional examples of 3D meshes generated by the DeepMesh model.  These examples highlight the model's ability to generate high-fidelity, detailed meshes across a range of object categories and styles. The variety of objects demonstrates the model's versatility and capacity to handle complex geometries. Each mesh is presented as a wireframe rendering, clearly showing the intricate detail and topology of the generated models.
> <details>
> <summary>read the caption</summary>
> Figure 12: More results of DeepMesh. We present more high-fidelity results generated by our method.
> </details>



![](https://arxiv.org/html/2503.15265/x15.png)

> ğŸ”¼ Figure 13 showcases additional high-quality 3D meshes generated by the DeepMesh model.  These examples demonstrate the model's ability to create detailed and aesthetically pleasing meshes across a variety of object categories, highlighting its capacity for generating complex and intricate 3D structures.
> <details>
> <summary>read the caption</summary>
> Figure 13: More results of DeepMesh. We present more high-fidelity results generated by our method.
> </details>



![](https://arxiv.org/html/2503.15265/x16.png)

> ğŸ”¼ This figure showcases high-resolution renderings of 3D meshes generated by the DeepMesh model.  The detailed view highlights the model's capability to create intricate and realistic surface details, demonstrating the effectiveness of the proposed method in generating high-quality, artist-like 3D assets.
> <details>
> <summary>read the caption</summary>
> Figure 14: High resolution results of our generated meshes.
> </details>



</details>




<details>
<summary>More on tables
</summary>


{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.3.4.1.1">Metrics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.4.1.2">AMT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.4.1.3">EdgeRunner</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.4.1.4">BPT</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.3.4.1.5">Ours</th>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.1.1.1">Comp Ratio <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.m1.1.1" stretchy="false" xref="S4.T2.1.1.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">â†“</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.2">0.46</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.3">0.47</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.1">0.26</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.5">0.28</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.2.2.1">Vocal Size <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.2.2.1.m1.1"><semantics id="S4.T2.2.2.1.m1.1a"><mo id="S4.T2.2.2.1.m1.1.1" stretchy="false" xref="S4.T2.2.2.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.1.m1.1d">â†“</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.1">512</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.3"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.3.1">512</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.2.2.4">40960</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.5">4736</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.3.3.1">Time (s) <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.1.m1.1"><semantics id="S4.T2.3.3.1.m1.1a"><mo id="S4.T2.3.3.1.m1.1.1" stretchy="false" xref="S4.T2.3.3.1.m1.1.1.cmml">â†“</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><ci id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1">â†“</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.m1.1d">â†“</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.2">816</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.3.3.4">540</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.5.1">480</span></td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> ğŸ”¼ This table compares the performance of four different mesh tokenization algorithms:  the proposed method and three existing methods (AMT, EdgeRunner, BPT).  The comparison focuses on three key metrics: compression ratio (lower is better, indicating more efficient data representation), vocabulary size (smaller is better, implying faster training), and training time (shorter is better, representing higher computational efficiency). The results demonstrate that the proposed algorithm significantly outperforms the existing methods in all three metrics, achieving the lowest compression ratio, the smallest vocabulary size, and the fastest training time, showcasing its superior efficiency and compactness for mesh processing.
> <details>
> <summary>read the caption</summary>
> Table 2: Quantitative comparison with other tokenization algorithms. Our improved tokenization algorithm achieves a low compression ratio, a small vocabulary size, and the highest computational efficiency, making it both compact and highly efficient for mesh processing.
> </details>

{{< table-caption >}}
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.4.5.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A2.T3.4.5.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T3.4.5.1.2">Small scale</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T3.4.5.1.3">Large scale</td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.6.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T3.4.6.2.1">Parameter count</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.4.6.2.2">500 M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.4.6.2.3">1.1B</td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.7.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.4.7.3.1">Batch Size</th>
<td class="ltx_td ltx_align_center" id="A2.T3.4.7.3.2">9</td>
<td class="ltx_td ltx_align_center" id="A2.T3.4.7.3.3">5</td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.4.8.4.1">Layers</th>
<td class="ltx_td ltx_align_center" id="A2.T3.4.8.4.2">21</td>
<td class="ltx_td ltx_align_center" id="A2.T3.4.8.4.3">20</td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.9.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.4.9.5.1">Heads</th>
<td class="ltx_td ltx_align_center" id="A2.T3.4.9.5.2">10</td>
<td class="ltx_td ltx_align_center" id="A2.T3.4.9.5.3">14</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.1.1.1"><math alttext="d_{model}" class="ltx_Math" display="inline" id="A2.T3.1.1.1.m1.1"><semantics id="A2.T3.1.1.1.m1.1a"><msub id="A2.T3.1.1.1.m1.1.1" xref="A2.T3.1.1.1.m1.1.1.cmml"><mi id="A2.T3.1.1.1.m1.1.1.2" xref="A2.T3.1.1.1.m1.1.1.2.cmml">d</mi><mrow id="A2.T3.1.1.1.m1.1.1.3" xref="A2.T3.1.1.1.m1.1.1.3.cmml"><mi id="A2.T3.1.1.1.m1.1.1.3.2" xref="A2.T3.1.1.1.m1.1.1.3.2.cmml">m</mi><mo id="A2.T3.1.1.1.m1.1.1.3.1" xref="A2.T3.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A2.T3.1.1.1.m1.1.1.3.3" xref="A2.T3.1.1.1.m1.1.1.3.3.cmml">o</mi><mo id="A2.T3.1.1.1.m1.1.1.3.1a" xref="A2.T3.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A2.T3.1.1.1.m1.1.1.3.4" xref="A2.T3.1.1.1.m1.1.1.3.4.cmml">d</mi><mo id="A2.T3.1.1.1.m1.1.1.3.1b" xref="A2.T3.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A2.T3.1.1.1.m1.1.1.3.5" xref="A2.T3.1.1.1.m1.1.1.3.5.cmml">e</mi><mo id="A2.T3.1.1.1.m1.1.1.3.1c" xref="A2.T3.1.1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A2.T3.1.1.1.m1.1.1.3.6" xref="A2.T3.1.1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.m1.1b"><apply id="A2.T3.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T3.1.1.1.m1.1.1.1.cmml" xref="A2.T3.1.1.1.m1.1.1">subscript</csymbol><ci id="A2.T3.1.1.1.m1.1.1.2.cmml" xref="A2.T3.1.1.1.m1.1.1.2">ğ‘‘</ci><apply id="A2.T3.1.1.1.m1.1.1.3.cmml" xref="A2.T3.1.1.1.m1.1.1.3"><times id="A2.T3.1.1.1.m1.1.1.3.1.cmml" xref="A2.T3.1.1.1.m1.1.1.3.1"></times><ci id="A2.T3.1.1.1.m1.1.1.3.2.cmml" xref="A2.T3.1.1.1.m1.1.1.3.2">ğ‘š</ci><ci id="A2.T3.1.1.1.m1.1.1.3.3.cmml" xref="A2.T3.1.1.1.m1.1.1.3.3">ğ‘œ</ci><ci id="A2.T3.1.1.1.m1.1.1.3.4.cmml" xref="A2.T3.1.1.1.m1.1.1.3.4">ğ‘‘</ci><ci id="A2.T3.1.1.1.m1.1.1.3.5.cmml" xref="A2.T3.1.1.1.m1.1.1.3.5">ğ‘’</ci><ci id="A2.T3.1.1.1.m1.1.1.3.6.cmml" xref="A2.T3.1.1.1.m1.1.1.3.6">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.m1.1c">d_{model}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.1.1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_m italic_o italic_d italic_e italic_l end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="A2.T3.1.1.2">1280</td>
<td class="ltx_td ltx_align_center" id="A2.T3.1.1.3">1792</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.2.2.1"><math alttext="d_{FFN}" class="ltx_Math" display="inline" id="A2.T3.2.2.1.m1.1"><semantics id="A2.T3.2.2.1.m1.1a"><msub id="A2.T3.2.2.1.m1.1.1" xref="A2.T3.2.2.1.m1.1.1.cmml"><mi id="A2.T3.2.2.1.m1.1.1.2" xref="A2.T3.2.2.1.m1.1.1.2.cmml">d</mi><mrow id="A2.T3.2.2.1.m1.1.1.3" xref="A2.T3.2.2.1.m1.1.1.3.cmml"><mi id="A2.T3.2.2.1.m1.1.1.3.2" xref="A2.T3.2.2.1.m1.1.1.3.2.cmml">F</mi><mo id="A2.T3.2.2.1.m1.1.1.3.1" xref="A2.T3.2.2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A2.T3.2.2.1.m1.1.1.3.3" xref="A2.T3.2.2.1.m1.1.1.3.3.cmml">F</mi><mo id="A2.T3.2.2.1.m1.1.1.3.1a" xref="A2.T3.2.2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="A2.T3.2.2.1.m1.1.1.3.4" xref="A2.T3.2.2.1.m1.1.1.3.4.cmml">N</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.1.m1.1b"><apply id="A2.T3.2.2.1.m1.1.1.cmml" xref="A2.T3.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.T3.2.2.1.m1.1.1.1.cmml" xref="A2.T3.2.2.1.m1.1.1">subscript</csymbol><ci id="A2.T3.2.2.1.m1.1.1.2.cmml" xref="A2.T3.2.2.1.m1.1.1.2">ğ‘‘</ci><apply id="A2.T3.2.2.1.m1.1.1.3.cmml" xref="A2.T3.2.2.1.m1.1.1.3"><times id="A2.T3.2.2.1.m1.1.1.3.1.cmml" xref="A2.T3.2.2.1.m1.1.1.3.1"></times><ci id="A2.T3.2.2.1.m1.1.1.3.2.cmml" xref="A2.T3.2.2.1.m1.1.1.3.2">ğ¹</ci><ci id="A2.T3.2.2.1.m1.1.1.3.3.cmml" xref="A2.T3.2.2.1.m1.1.1.3.3">ğ¹</ci><ci id="A2.T3.2.2.1.m1.1.1.3.4.cmml" xref="A2.T3.2.2.1.m1.1.1.3.4">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.1.m1.1c">d_{FFN}</annotation><annotation encoding="application/x-llamapun" id="A2.T3.2.2.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_F italic_F italic_N end_POSTSUBSCRIPT</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="A2.T3.2.2.2">5120</td>
<td class="ltx_td ltx_align_center" id="A2.T3.2.2.3">7168</td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.4.4.3">Learning rate</th>
<td class="ltx_td ltx_align_center" id="A2.T3.3.3.1"><math alttext="1e-4" class="ltx_Math" display="inline" id="A2.T3.3.3.1.m1.1"><semantics id="A2.T3.3.3.1.m1.1a"><mrow id="A2.T3.3.3.1.m1.1.1" xref="A2.T3.3.3.1.m1.1.1.cmml"><mrow id="A2.T3.3.3.1.m1.1.1.2" xref="A2.T3.3.3.1.m1.1.1.2.cmml"><mn id="A2.T3.3.3.1.m1.1.1.2.2" xref="A2.T3.3.3.1.m1.1.1.2.2.cmml">1</mn><mo id="A2.T3.3.3.1.m1.1.1.2.1" xref="A2.T3.3.3.1.m1.1.1.2.1.cmml">â¢</mo><mi id="A2.T3.3.3.1.m1.1.1.2.3" xref="A2.T3.3.3.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A2.T3.3.3.1.m1.1.1.1" xref="A2.T3.3.3.1.m1.1.1.1.cmml">âˆ’</mo><mn id="A2.T3.3.3.1.m1.1.1.3" xref="A2.T3.3.3.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.3.3.1.m1.1b"><apply id="A2.T3.3.3.1.m1.1.1.cmml" xref="A2.T3.3.3.1.m1.1.1"><minus id="A2.T3.3.3.1.m1.1.1.1.cmml" xref="A2.T3.3.3.1.m1.1.1.1"></minus><apply id="A2.T3.3.3.1.m1.1.1.2.cmml" xref="A2.T3.3.3.1.m1.1.1.2"><times id="A2.T3.3.3.1.m1.1.1.2.1.cmml" xref="A2.T3.3.3.1.m1.1.1.2.1"></times><cn id="A2.T3.3.3.1.m1.1.1.2.2.cmml" type="integer" xref="A2.T3.3.3.1.m1.1.1.2.2">1</cn><ci id="A2.T3.3.3.1.m1.1.1.2.3.cmml" xref="A2.T3.3.3.1.m1.1.1.2.3">ğ‘’</ci></apply><cn id="A2.T3.3.3.1.m1.1.1.3.cmml" type="integer" xref="A2.T3.3.3.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.3.3.1.m1.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="A2.T3.3.3.1.m1.1d">1 italic_e - 4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="A2.T3.4.4.2"><math alttext="1e-4" class="ltx_Math" display="inline" id="A2.T3.4.4.2.m1.1"><semantics id="A2.T3.4.4.2.m1.1a"><mrow id="A2.T3.4.4.2.m1.1.1" xref="A2.T3.4.4.2.m1.1.1.cmml"><mrow id="A2.T3.4.4.2.m1.1.1.2" xref="A2.T3.4.4.2.m1.1.1.2.cmml"><mn id="A2.T3.4.4.2.m1.1.1.2.2" xref="A2.T3.4.4.2.m1.1.1.2.2.cmml">1</mn><mo id="A2.T3.4.4.2.m1.1.1.2.1" xref="A2.T3.4.4.2.m1.1.1.2.1.cmml">â¢</mo><mi id="A2.T3.4.4.2.m1.1.1.2.3" xref="A2.T3.4.4.2.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A2.T3.4.4.2.m1.1.1.1" xref="A2.T3.4.4.2.m1.1.1.1.cmml">âˆ’</mo><mn id="A2.T3.4.4.2.m1.1.1.3" xref="A2.T3.4.4.2.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.4.4.2.m1.1b"><apply id="A2.T3.4.4.2.m1.1.1.cmml" xref="A2.T3.4.4.2.m1.1.1"><minus id="A2.T3.4.4.2.m1.1.1.1.cmml" xref="A2.T3.4.4.2.m1.1.1.1"></minus><apply id="A2.T3.4.4.2.m1.1.1.2.cmml" xref="A2.T3.4.4.2.m1.1.1.2"><times id="A2.T3.4.4.2.m1.1.1.2.1.cmml" xref="A2.T3.4.4.2.m1.1.1.2.1"></times><cn id="A2.T3.4.4.2.m1.1.1.2.2.cmml" type="integer" xref="A2.T3.4.4.2.m1.1.1.2.2">1</cn><ci id="A2.T3.4.4.2.m1.1.1.2.3.cmml" xref="A2.T3.4.4.2.m1.1.1.2.3">ğ‘’</ci></apply><cn id="A2.T3.4.4.2.m1.1.1.3.cmml" type="integer" xref="A2.T3.4.4.2.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.4.4.2.m1.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="A2.T3.4.4.2.m1.1d">1 italic_e - 4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.10.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.4.10.6.1">LR scheduler</th>
<td class="ltx_td ltx_align_center" id="A2.T3.4.10.6.2">Cosine</td>
<td class="ltx_td ltx_align_center" id="A2.T3.4.10.6.3">Cosine</td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.11.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.4.11.7.1">Weight decay</th>
<td class="ltx_td ltx_align_center" id="A2.T3.4.11.7.2">0.1</td>
<td class="ltx_td ltx_align_center" id="A2.T3.4.11.7.3">0.1</td>
</tr>
<tr class="ltx_tr" id="A2.T3.4.12.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T3.4.12.8.1">Gradient Clip</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T3.4.12.8.2">1.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T3.4.12.8.3">1.0</td>
</tr>
</tbody>
</table>{{< /table-caption >}}
> ğŸ”¼ This table details the architecture and training hyperparameters used for both the small-scale and large-scale versions of the DeepMesh model.  It shows the parameter count, batch size, number of layers, number of heads, hidden dimension size (d_model), feed-forward network dimension size (d_FFN), learning rate, learning rate scheduler, weight decay, and gradient clipping used during training.  These specifications provide a comprehensive overview of the model's configuration and training process.
> <details>
> <summary>read the caption</summary>
> Table 3: Deepmeshâ€™s architectural and training details.
> </details>

</details>




### Full paper

{{< gallery >}}
<img src="https://ai-paper-reviewer.com/2503.15265/1.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/2.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/3.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/4.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/5.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/6.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/7.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/8.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/9.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/10.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/11.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/12.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/13.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/14.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/15.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/16.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/17.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/18.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/19.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
<img src="https://ai-paper-reviewer.com/2503.15265/20.png" class="grid-w50 md:grid-w33 xl:grid-w25" />
{{< /gallery >}}