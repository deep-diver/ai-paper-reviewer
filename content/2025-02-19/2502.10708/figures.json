[{"figure_path": "https://arxiv.org/html/2502.10708/x1.png", "caption": "Figure 1: Illustration of Growth Trends in Domain-Specific Knowledge Injection into LLMs.\nThe chart displays the cumulative number of papers published between October 2022 and December 2024.\nDifferent colors and border styles represent various injection methods and domains, such as blue with a solid border denoting dynamic injection in the biomedical field.", "description": "Figure 1 illustrates the growth of research on injecting domain-specific knowledge into Large Language Models (LLMs) from October 2022 to December 2024.  The graph shows the cumulative number of published papers over time.  Different colors and border styles represent the various knowledge injection methods (dynamic injection, static embedding, modular adapters, and prompt optimization) and the specific domains of application (biomedicine, materials science, finance, human-centered science, etc.). For example, blue with a solid border indicates dynamic knowledge injection within the biomedical field.  This visualization helps to understand trends and the relative popularity of different approaches within various domains.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.10708/x2.png", "caption": "Figure 2: \nFour knowledge injection paradigms for LLMs.\n(a) Dynamic Knowledge Injection retrieves external knowledge during inference for enhanced reasoning.\n(b) Static Knowledge Injection embeds external knowledge into model parameters during fine-tuning.\n(c) Modular Knowledge Adapters use plug-and-play modules to dynamically adapt to tasks or updates.\n(d) Prompt Optimization utilizes precise prompts to guide the LLM without altering its parameters.", "description": "Figure 2 illustrates four main methods of incorporating external knowledge into Large Language Models (LLMs). (a) Dynamic Knowledge Injection:  External knowledge is accessed during the LLM's reasoning process, enhancing its performance on a given task. This approach requires a retrieval mechanism to select relevant information. (b) Static Knowledge Injection: External knowledge is integrated into the model's parameters during training or fine-tuning. This permanently alters the LLM, making it specialized for a particular domain.  (c) Modular Knowledge Adapters: Small, trainable modules are added to the LLM to incorporate domain expertise without modifying its base parameters. These modules can be easily swapped or updated to adapt the LLM to different tasks or changing knowledge bases.  (d) Prompt Optimization:  Carefully designed prompts guide the LLM to utilize its existing internal knowledge without any model parameter changes, relying solely on optimized input phrasing.", "section": "3 Paradigms of Knowledge Injection"}]