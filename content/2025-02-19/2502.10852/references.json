{"references": [{"fullname_first_author": "Aakanksha Chowdhery", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2023", "reason": "This paper introduces PaLM, a large language model that significantly advanced multilingual capabilities, providing a benchmark for the current state-of-the-art in the field."}, {"fullname_first_author": "Alexis Conneau", "paper_title": "Unsupervised cross-lingual representation learning at scale", "publication_date": "2020", "reason": "This paper introduces XLM-R, a multilingual language model that serves as the foundation for the proposed method in the current paper, showcasing the significance of unsupervised cross-lingual learning."}, {"fullname_first_author": "Yinhan Liu", "paper_title": "Multilingual denoising pre-training for neural machine translation", "publication_date": "2020", "reason": "This paper introduces mBART, a key multilingual machine translation model, which the current work adapts to improve text generation in low-resource languages and is directly compared with."}, {"fullname_first_author": "Linting Xue", "paper_title": "mt5: A massively multilingual pre-trained text-to-text transformer", "publication_date": "2021", "reason": "This paper introduces mT5, another important multilingual text-to-text transformer model that is compared against the proposed model, highlighting the advancements in multilingual capabilities."}, {"fullname_first_author": "Chen Zhang", "paper_title": "Mc\u00b2: Towards transparent and culturally-aware NLP for minority languages in china", "publication_date": "2024", "reason": "This paper introduces MC2, a crucial multilingual corpus of minority languages in China that provides the primary pretraining data for the models in this paper, significantly impacting the experiments."}]}