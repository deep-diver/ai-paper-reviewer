{"references": [{"fullname_first_author": "Peter Anderson", "paper_title": "Greenback bears and fiscal hawks: Finance is a jungle and text embeddings must adapt", "publication_date": "2024-11-01", "reason": "This paper is cited as showcasing the need for domain-specific embedding models in the finance domain and for the evaluation framework of those models."}, {"fullname_first_author": "Elliot Bolton", "paper_title": "BioMedLM: A 2.7 b parameter language model trained on biomedical text", "publication_date": "2024-03-01", "reason": "This paper is cited as an example of a successful domain-specific language model, demonstrating the trend towards specialized models in various fields."}, {"fullname_first_author": "Jing Chen", "paper_title": "The BQ corpus: A large-scale domain-specific Chinese corpus for sentence semantic equivalence identification", "publication_date": "2018-11-01", "reason": "This paper provides one of the datasets used in the benchmark, showcasing the importance and diversity of resources used for the evaluation."}, {"fullname_first_author": "Jacob Devlin", "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "publication_date": "2019-06-01", "reason": "BERT is a foundational model in the field of NLP and its impact is reflected in the performance of several models compared in the paper."}, {"fullname_first_author": "Niklas Muennighoff", "paper_title": "MTEB: Massive Text Embedding Benchmark", "publication_date": "2022-10-01", "reason": "This paper introduces the MTEB benchmark which FinMTEB is modeled after, making it a key reference for understanding the broader context of embedding model evaluation."}]}