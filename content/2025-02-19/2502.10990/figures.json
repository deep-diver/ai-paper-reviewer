[{"figure_path": "https://arxiv.org/html/2502.10990/x1.png", "caption": "Figure 1: Word cloud visualization of Fin-E5\u2019s training data, contain common financial terms.", "description": "This word cloud shows the most frequent terms present in the training data used to develop the Fin-E5 model.  The size of each word reflects its frequency, illustrating the prevalence of various financial concepts within the training dataset.  This visualization helps to highlight the domain-specific vocabulary learned by Fin-E5, illustrating its focus on finance-related terms and concepts.", "section": "4 Fin-E5: Finance-Adapted Text Embedding Model"}, {"figure_path": "https://arxiv.org/html/2502.10990/x2.png", "caption": "Figure 2: An overview of tasks and datasets used in FinMTEB. All the dataset descriptions and examples are provided in the Appendix A.", "description": "This figure provides a visual overview of the tasks and datasets included in the FinMTEB benchmark.  It's organized into seven categories representing different natural language processing tasks: Clustering, Reranking, Retrieval, Pair Classification, Classification, Summarization, and Semantic Textual Similarity (STS).  Each task category lists the specific datasets used within FinMTEB for that task, showing the breadth of financial text data types covered by the benchmark (e.g., financial news, annual reports, etc.).  The figure highlights the diversity of tasks and datasets designed to comprehensively evaluate the performance of embedding models in the financial domain. More detailed information on each dataset is available in Appendix A.", "section": "3 The FinMTEB Benchmark"}, {"figure_path": "https://arxiv.org/html/2502.10990/x3.png", "caption": "Figure 3: Distribution analysis of 5000 randomly sampled training data showing the breakdown of Tasks and Person Types. Left: Persona distribution. Right: Task distribution.", "description": "This figure presents a breakdown of the data used to train the Fin-E5 model.  The left pie chart visualizes the distribution of different personas (e.g., financial analyst, investor, trader) represented in the training data, illustrating the diversity of user perspectives. The right pie chart shows the distribution of various financial tasks (e.g., market analysis, risk assessment, financial planning) covered by the dataset.  Both charts offer insights into the comprehensiveness and balance of the training data, demonstrating its ability to capture the nuances of financial language across various roles and tasks.", "section": "4 Fin-E5: Finance-Adapted Text Embedding Model"}, {"figure_path": "https://arxiv.org/html/2502.10990/x4.png", "caption": "Figure 4: Semantic similarity across all the datasets in FinMTEB benchmark.", "description": "This heatmap visualizes the pairwise semantic similarity between the 64 datasets within the FinMTEB benchmark.  Each cell represents the cosine similarity between the average embeddings of two datasets, calculated using the all-MiniLM-L6-v2 model. Darker blues indicate higher similarity, revealing relationships between datasets with similar semantic content. The figure highlights the semantic diversity of the FinMTEB datasets, showing that many have low similarity scores, demonstrating the benchmark's comprehensive coverage of distinct financial text types.", "section": "3.2 Characteristics of FinMTEB"}]