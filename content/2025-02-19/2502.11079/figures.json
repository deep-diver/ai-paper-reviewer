[{"figure_path": "https://arxiv.org/html/2502.11079/x1.png", "caption": "Figure 1: \nSubject-consistent generation examples using our method, with reference images and corresponding video frames (text prompts omitted). The last three rows show multiple reference subjects.", "description": "This figure showcases examples of videos generated using the PHANTOM method.  Each set of images displays a reference image followed by several frames from the generated video. The top rows demonstrate single-subject video generation, where the video focuses on a single subject from the provided reference image. The bottom rows demonstrate the method's capability to handle multiple subjects within the generated video, showing multiple reference images and their corresponding video frames.  The text prompts used to generate each video are not included in the figure.", "section": "1 INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2502.11079/x2.png", "caption": "Figure 2: \nRelationship in cross-modal video generation tasks.", "description": "This figure illustrates the relationships between different cross-modal video generation tasks. It shows how text-to-video, image-to-video, and subject-to-video generation methods are interconnected and build upon each other.  The central concept is 'alignment', showing how these methods strive to align visual content with textual descriptions, and how subject-to-video generation aims to achieve a balance between the two to maintain subject consistency.", "section": "2 PHANTOM"}, {"figure_path": "https://arxiv.org/html/2502.11079/x3.png", "caption": "Figure 3: Data processing pipeline for cross-modal video generation", "description": "This figure illustrates the data processing pipeline used in the PHANTOM model for cross-modal video generation.  It details how image, text, and video data are combined and prepared for training. The process begins with retrieving and matching data sources, filtering and captioning the data.  Then, the pipeline separates the data into in-paired and cross-paired data sets, which are further segmented based on application scenarios such as multi-person interactions or human-object interactions.  These steps aim to achieve a balance between visual consistency and textual control during the generation process, preventing over-reliance on simple image copying while incorporating the richness provided by text prompts.", "section": "2.1 DATA PIPELINE"}, {"figure_path": "https://arxiv.org/html/2502.11079/x4.png", "caption": "Figure 4: Overview of the Phantom architecture", "description": "This figure presents a detailed architecture of the Phantom model, which is a unified video generation framework designed for subject-consistent video generation.  It shows the model's input processing, including the way text and image prompts are encoded and combined with video features. The core components are highlighted, such as the triplet data input head, vision and text branches, and the MMDiT block. The figure also depicts how the model utilizes a Variational Autoencoder (VAE) and CLIP for image feature extraction and integration within the DiT module. This detailed illustration clarifies the model's flow of processing text and visual information to produce subject-consistent videos.", "section": "2.2 FRAMEWORK"}, {"figure_path": "https://arxiv.org/html/2502.11079/x5.png", "caption": "Figure 5: Video quality evaluation (left) and user study results for multi-subject consistency (right).", "description": "This figure presents a comparative analysis of video generation quality and multi-subject consistency. The left panel displays a radar chart summarizing the video quality assessment across several metrics, including clarity, aesthetic quality, structure, background consistency, and dynamic degree.  Each metric is scored against the video generation results of the Phantom model, compared to various benchmark systems.  The right panel presents the results of a user study focusing on multi-subject consistency where participants rate different models including Phantom, Vidu 2.0, Pika 2.1, and Keling 1.6. The user ratings reflect preferences for different methods in terms of subject consistency and prompt adherence.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2502.11079/x6.png", "caption": "Figure 6: Comparative results of single reference subject-to-video generation", "description": "This figure displays a comparison of video generation results from four different methods (Ours, Vidu2.0, Pika2.1, and Keling1.6) given a single reference image and corresponding text prompt.  Each row showcases a different scenario, with the reference image shown at the far left, followed by video frames generated by each method.  The objective is to evaluate the methods\u2019 ability to maintain subject consistency (i.e., how well they generate videos that accurately reflect the subject and content from the reference image and text prompt). The caption highlights that the comparisons shown in this image are based on single-subject videos.", "section": "3.3 QUALITATIVE RESULTS"}, {"figure_path": "https://arxiv.org/html/2502.11079/x7.png", "caption": "Figure 7: Comparative results of multi-reference subject-to-video generation", "description": "This figure showcases comparative results from different video generation methods when using multiple reference images as input.  The goal is to generate videos that are consistent with multiple subjects depicted in the provided reference images.  Each row displays a different scenario with different subjects, using the same prompts. The results from four different models (Ours, Keling1.6, Pika2.1, and Vidu2.0) are shown side-by-side to illustrate the differences in the generated video quality and subject consistency.", "section": "3.3 Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2502.11079/x8.png", "caption": "Figure 8: Comparative results of video generation for face ID preservation", "description": "Figure 8 presents a comparison of video generation results focusing on face ID preservation.  It showcases examples generated by different methods, including the authors' method (Phantom), ConsisID, and Hailuo, all applied to the same set of input images. The figure's purpose is to visually demonstrate the relative strengths and weaknesses of each method in maintaining the identity of the individuals across the generated video frames, highlighting differences in visual quality, motion clarity, and overall faithfulness to the input image. Each set of generated video frames is accompanied by a brief text description of the scenario.", "section": "3.3 QUALITATIVE RESULTS"}]