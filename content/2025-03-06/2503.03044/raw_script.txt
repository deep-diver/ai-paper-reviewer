[{"Alex": "Hey everyone, and welcome to the podcast! Today, we\u2019re diving headfirst into the wild world of AI translation, specifically how it impacts the real heroes of the translation world: human post-editors! Think of it as AI stepping into the translator's office \u2013 is it helping or just making things more complicated? I'm Alex, your host, and I\u2019m thrilled to have Jamie with us, ready to unravel this fascinating topic.", "Jamie": "Hey Alex, thanks for having me! AI translation, huh? Sounds like a recipe for either revolution or disaster! I'm eager to dig in and see what you've found."}, {"Alex": "Exactly! And that's what this research paper, *QE4PE: Word-level Quality Estimation for Human Post-Editing,* is all about. Basically, it explores how we can use AI to highlight potential errors in machine translations to make human post-editors' lives easier. Think of it like giving them a super-powered error-detecting tool.", "Jamie": "Okay, so it\u2019s about spotting the AI's mistakes before the human even gets to them. Clever! But why focus on *word-level* errors? Seems pretty granular."}, {"Alex": "Great question, Jamie! While segment-level, broader approaches to quality estimation are common, word-level QE allows for a more interpretable and fine-grained analysis, aligning well with how modern MT systems are used. We're really focusing on the details.", "Jamie": "Hmm, so it's about getting into the nitty-gritty. But how do you actually *do* word-level quality estimation? What methods are you using?"}, {"Alex": "That's where it gets interesting! We tested a few different methods, including a supervised model called XCOMET, which is trained on human error annotations, and an unsupervised method that leverages the MT model's own uncertainty. We also included something we called 'oracle' error spans, derived from the consensus of human post-editors, as a kind of best-case scenario.", "Jamie": "Oracle error spans? That sounds\u2026 intense. So, it's like having the perfect answer key to compare against. Makes sense. And how do these different highlighting methods stack up?"}, {"Alex": "Well, that's the million-dollar question! We had 42 professional translators post-edit MT outputs in English to both Italian and Dutch, using interfaces that highlighted potential errors based on those different QE methods. Then we tracked everything: how long it took them, what they changed, and even surveyed them about their experience.", "Jamie": "Wow, that\u2019s a pretty comprehensive setup. So you were basically measuring their productivity, the quality of their edits, and how *useful* they found the highlights? Seems like a lot of data!"}, {"Alex": "Absolutely. We wanted to go beyond just accuracy metrics and really understand the impact on real-world translation workflows. We even introduced some intentionally incorrect translations \u2013 critical errors, as we called them \u2013 to see if the highlights helped catch the really important mistakes.", "Jamie": "So, you were basically setting traps for them! Sneaky! Did these highlights help catch those errors, or did the translators fall for it anyway?"}, {"Alex": "Some traps were easily spotted regardless of the error highlighting. And while most highlighting types did a decent job catching those critical errors -- highlighting over Unsupervised -- translators did manage to edit more critical errors in highlighted modes vs. just standard No Highlights version.", "Jamie": "Interesting... so even if the AI wasn't perfect, it still nudged the human in the right direction for critical mistakes. That tells me it can serve as a good failsafe when a translator might be getting tired."}, {"Alex": "That's the idea. Although our participants reported that although the work was helpful, the highlighting was not directly helpful.", "Jamie": "That's super interesting. So the numbers look promising, but real folks don't feel as if the AI actually helps? Why could that be?"}, {"Alex": "That's the key takeaway: they said that, at best, the highlighting was not quite accurate and, at worst, a distraction. Perhaps the cognitive load of processing highlights and figuring out if it really is an issue outweighs the potential benefits. Also, the domain and language also played a big role in all of this.", "Jamie": "So there is a gap between accuracy and real usability. In terms of domain and language being influential, what do you mean?"}, {"Alex": "We found that English-to-Italian translators, for example, were more inclined to act on highlighted spans than their English-to-Dutch counterparts. And when it came to the text domain, style errors in social media content may have been simpler to fix than the complex technical terms in biomedicine.", "Jamie": "Gotcha. So, cultural nuances and the difficulty level of content influence whether the AI is helpful or not. Hmmm."}, {"Alex": "Exactly. The type of error really matters. If there's a cultural error in translation, those errors are also spotted and corrected easily too.", "Jamie": "So, what does that tell us about AI's role in all of this? Is it just adding noise to the process, or is there still potential?"}, {"Alex": "There's definitely potential! Our research suggests that improvements can be made in the field of QA, but should primarily focus on usability -- or how the AI highlights things. A new assistive approach for post-editing should not only strive to increase productivity, but reduce cognitive burden associated with it.", "Jamie": "Ah, so less cognitive load to free humans up for creative work... Any ideas on where this should be applied next?"}, {"Alex": "I think focusing on other demographics could prove fruitful. So, perhaps QA could focus on non-professional translators or language learners. Or, perhaps this can be combined with edits to justify presence of error spans.", "Jamie": "Great point: by adding edit suggestions or showing a confidence interval, you can push it even further."}, {"Alex": "Great point!", "Jamie": "So, where do we go from here? What's the big takeaway for people working with machine translation?"}, {"Alex": "The research highlights that QA improvements can be made, but the role of the domain, language and speed of the editor are influential. For professionals, more gains in QA might not make up for the losses of cognitive function.", "Jamie": "Okay, but this all applies to professionals. What if someone is just casually translating stuff? It's all just business as usual?"}, {"Alex": "The research is limited to professional use, but the point about balancing usability and accuracy still stands. For example, if the AI incorrectly flags an expression in another language, the user might spend more time on edits. In QA and translation, this is a double-edged sword.", "Jamie": "Well put. AI can't be a panacea to this."}, {"Alex": "Well, speaking of the study limitations, these results are limited to professional translators as mentioned and the language that they know. This also relies on the data put out by the WMT23 machine learning, so QA can still be improved.", "Jamie": "I see... so, if QA can be improved and the data was based on the WMT23, a non-QA benchmark, how do we measure the validity of this data set?"}, {"Alex": "In that case, the study focused on error span detection at a more granular level, focusing also on errors. We then rely on previous work done that builds on the Direct Assessment in QE in the past.", "Jamie": "So, while I can't implement QA on a small scale to confirm the study results, I can verify with this info."}, {"Alex": "That's the point! It's all open data and reproducible, so you're free to do so! But there's more to the point, of course. There's ethics and responsibility too. We need to consider how this impacts people's livelyhood.", "Jamie": "Definitely. The human element is still critical, and we need to design tools that empower translators rather than replace them."}, {"Alex": "And it's a team sport too. As the report says, humans augment AI. AI augments human. No one replaces one or the other, but works in synergy. Well, that's all the time we have for today. Thank you, Jamie, for your insightful questions!", "Jamie": "Thanks for having me, Alex! It's been fascinating!"}]