[{"figure_path": "https://arxiv.org/html/2503.02951/x5.png", "caption": "Figure 1: This figure demonstrates the pipeline for generating KodCode-V1. Our approach follows a three-step pipeline: Coding Question Synthesis, Solution & Test Generation, and Post-training Data Synthesis. The final KodCode-V1 dataset contains 447K verified question-solution-test triplets. The distribution of each subset is demonstrated on the right.", "description": "This figure illustrates the three-step pipeline used to create the KodCode-V1 dataset.  First, coding questions are synthesized using diverse methods. Second, solutions and corresponding unit tests are generated, undergoing a self-verification process to ensure correctness.  Finally, post-training data synthesis expands the dataset by rewriting questions and creating chain-of-thought responses via a reasoning model. The resulting dataset contains 447,000 question-solution-test triplets, with the distribution of each subset shown on the right side of the figure.", "section": "2 KODCODE : Synthesizing Diverse, Challenging, and Verifiable Correct Post-Training Data for Code"}, {"figure_path": "https://arxiv.org/html/2503.02951/x6.png", "caption": "Figure 2: Statistics on pass rates via self-verification in Step 2 by subset with varing number of attempts.", "description": "This figure visualizes the success rate of the self-verification process in Step 2 of the KODCODE dataset generation pipeline.  It displays the pass rate for each subset of coding questions (Prefill, LeetCode, Codeforces, etc.) with varying numbers of self-verification attempts (1, 5, and 10).  Higher pass rates indicate that the questions in that subset were generally easier to solve and verify, while lower pass rates may reflect increased difficulty. The graph allows assessment of the effectiveness of increasing verification attempts in generating verifiable solutions, showing how many more successful solutions are produced with more attempts.", "section": "Pipeline Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02951/x7.png", "caption": "Figure 3: Distribution of token lengths for questions and responses, along with unit test counts across different subsets.", "description": "This figure presents three distinct histograms visualizing the distribution of token lengths for questions and their corresponding responses across various subsets of the KODCODE dataset.  A supplementary bar chart displays the average number of unit tests associated with each question in these subsets. This allows for a comparative analysis of the textual length characteristics of questions and answers, and the associated test suite complexity across different question categories within the dataset. This analysis helps to understand the diversity and challenge level within the dataset.", "section": "3.1 Pipeline Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02951/extracted/6242597/figs_v1.1/tsne_kodcode_v1.1.jpg", "caption": "Figure 4: Comparison of t-SNE visualization between KodCode\u00a0 (by subset) and baseline datasets (OSS Instruct, ACECoder, Educational Instruct, and Package Instruct), with 2,000 sampled instructions per dataset.", "description": "This figure uses t-distributed Stochastic Neighbor Embedding (t-SNE) to visualize the high-dimensional vector representations of coding instructions from KODCODE and four other datasets: OSS Instruct, ACECoder, Educational Instruct, and Package Instruct.  Each point represents a coding instruction, and the proximity of points indicates semantic similarity. The visualization reveals KODCODE's superior diversity compared to the baseline datasets, as its instructions span a wider range of the embedding space. This indicates that KODCODE covers a broader range of coding topics and styles than the other datasets.", "section": "3.1 Pipeline Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02951/x8.png", "caption": "Figure 5: Difficulty distribution across subsets measured by pass rates.", "description": "This figure shows the distribution of question difficulty across different subsets of the KODCODE dataset.  The difficulty is measured by the pass rate of the self-verification process for each question, using the number of successful attempts out of a maximum of 10.  The subsets are categorized into four difficulty levels: easy (pass rate > 2/3), medium (1/3 to 2/3), hard (<1/3), and fail (all failures). The chart displays the percentage of questions in each difficulty level for every subset, providing a visual representation of the distribution of question difficulty in the dataset.", "section": "3.1 Pipeline Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02951/x9.png", "caption": "Figure 6: Contamination analysis between KodCode\u00a0 subsets and existing benchmarks. (a) Confusion matrix showing the percentage and absolute number (in parentheses) of contaminated samples with cosine similarity > 0.95. (b) Distribution of maximum cosine similarity scores across different KodCode\u00a0subsets, with horizontal lines indicating subset averages.", "description": "Figure 6 presents a detailed analysis of potential overlaps between the KODCODE dataset and existing code benchmarks.  Part (a) is a confusion matrix that visually represents the number of KODCODE samples exhibiting high similarity (cosine similarity > 0.95) to each benchmark. The percentages and raw counts are provided within the matrix cells, illustrating the extent of potential contamination for each KODCODE subset. Part (b) provides a supplementary histogram showing the distribution of maximum cosine similarity scores within each KODCODE subset. Horizontal lines on the histogram highlight the average maximum similarity for each subset, offering a clearer picture of the overall similarity levels.", "section": "3.2 Dataset Statistics and Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02951/x10.png", "caption": "Figure 7: Data flow visualization through our pipeline: from initial subsets (left) through deduplication (middle) to final filtered sets after reject sampling (right). Red paths indicate discarded instances.", "description": "This figure illustrates the data flow within the KODCODE dataset creation pipeline.  Starting from twelve initial subsets of coding questions (left), the pipeline proceeds through a deduplication step to remove redundant questions (middle).  The remaining questions then undergo a reject sampling process where solutions and tests are generated, and those that fail verification are removed (right).  The red paths highlight questions removed during the deduplication and reject sampling phases. The final dataset is considerably smaller than the initial number of coding questions indicating the effectiveness of the data cleaning steps.", "section": "2 KODCODE : Synthesizing Diverse, Challenging, and Verifiable Correct Post-Training Data for Code"}, {"figure_path": "https://arxiv.org/html/2503.02951/x11.png", "caption": "Figure 8: Task categories of the collected Magpie Coding data before filtering.", "description": "This figure shows a pie chart illustrating the distribution of various coding task categories within the Magpie Coding dataset *before* any filtering was applied.  The pie chart visually represents the proportion of questions in each category, providing an overview of the dataset's initial composition before any refinement or selection processes were implemented. This helps illustrate the diversity of tasks addressed in the original Magpie data and the impact that subsequent filtering steps had on the final dataset composition.", "section": "2 KODCODE : Synthesizing Diverse, Challenging, and Verifiable Correct Post-Training Data for Code"}, {"figure_path": "https://arxiv.org/html/2503.02951/x12.png", "caption": "Figure 9: Failed Task 511.", "description": "This figure shows a coding task (Task 511) from the MBPP dataset that failed the self-verification process during the KODCODE dataset generation. The task requires writing a Python function to find the minimum sum of factors for a given number.  The figure displays the GPT-40 generated response (incorrect), the correct solution, and the associated unit tests.", "section": "Pipeline Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02951/x13.png", "caption": "Figure 10: Failed Task 525.", "description": "This figure shows a failed example from the MBPP validation dataset (Task 525).  The task is to write a Python function to determine if two lines are parallel.  The function should accept two lines represented as tuples, handling both two-element (a,b) and three-element (a,b,c) tuples. The GPT-40 response's solution incorrectly assumes all input lines are represented by three-element tuples (a,b,c), resulting in a failed unit test when only two-element tuples are provided. This highlights a limitation of the GPT-40 model in handling edge cases and generalizing to different input formats.", "section": "2.2 Step 2: Solution & Test Generation"}, {"figure_path": "https://arxiv.org/html/2503.02951/x14.png", "caption": "Figure 11: Example 1 of Contaminated Data", "description": "This figure shows an example of data contamination found in the KODCODE dataset.  It highlights a question from KODCODE that is nearly identical to a question from the MBPP benchmark dataset. Specifically, it demonstrates a KODCODE question about counting uppercase letters in a string and its almost exact equivalent from MBPP. This illustrates a potential limitation in the dataset generation process, where some synthetically generated questions closely resemble pre-existing questions from other established benchmarks.", "section": "3.2 Dataset Statistics and Analysis"}]