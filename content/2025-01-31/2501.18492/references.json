{"references": [{"fullname_first_author": "Achiam, J.", "paper_title": "Gpt-4 technical report", "publication_date": "2023-03-08", "reason": "This paper is a technical report describing GPT-4, a large language model that the current paper's GuardReasoner model is compared against."}, {"fullname_first_author": "Han, S.", "paper_title": "Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of LLMs", "publication_date": "2024-06-18", "reason": "This paper introduces WildGuard, a benchmark dataset used in the current paper's experiments to evaluate guardrail models, making it a crucial reference for evaluating the proposed GuardReasoner."}, {"fullname_first_author": "Ghosh, S.", "paper_title": "Aegis: Online adaptive AI content safety moderation with ensemble of LLMs experts", "publication_date": "2024-04-05", "reason": "This paper introduces Aegis, another benchmark dataset used in the current work to assess guardrail models, which is directly compared with the performance of the proposed GuardReasoner."}, {"fullname_first_author": "Inan, H.", "paper_title": "Llama guard: LLM-based input-output safeguard for human-AI conversations", "publication_date": "2023-12-06", "reason": "This paper introduces LLaMA Guard, a prior approach to LLM safeguards that the current paper's GuardReasoner aims to improve upon, making it a key comparative reference."}, {"fullname_first_author": "Wei, J.", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-11-28", "reason": "This paper explores the chain-of-thought prompting technique, which is foundational to the reasoning-based approach adopted by GuardReasoner, and thus is highly relevant to the core methodology of the current paper."}]}