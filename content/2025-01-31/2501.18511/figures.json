[{"figure_path": "https://arxiv.org/html/2501.18511/extracted/6167093/figures/spider_plot_v2.png", "caption": "Figure 1: Re-Wild\u00a0outperforms strong baselines, on average, across nine benchmarks.  In particular, it exhibits strong performance on generalist chat and instruction following benchmarks. MT Bench scores here are divided by 10, so that the scale is similar to our other evaluations. For the exact numeric scores for all models, please refer to our GitHub repository. Figure best viewed in color.", "description": "This figure presents a comparative analysis of the performance of different supervised fine-tuning (SFT) models on various benchmarks.  The main finding is that RE-WILD, a novel data mix developed by the authors, outperforms existing strong baselines, particularly in generalist chat and instruction-following tasks. The chart visualizes these results across nine benchmarks, with MTBench scores scaled down for consistent representation.  The data used to create the chart and the scores for each model can be found in the paper's GitHub repository.", "section": "3. SFT Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18511/x1.png", "caption": "Figure 2: Data scaling improves SFT performance.  The effect is, however, somewhat dependent on SDQ \u2013 for DGMs such as GPT 3.5, the benefits taper off relatively quickly, but for the other three DGMs we consider, they continue to increase. Avg is the average performance over (MixEval, AlpacaEval2-LC, MTBench / 10, OpenLLM LB 2).", "description": "This figure shows the impact of increasing the size of the training dataset on the performance of Supervised Fine-Tuning (SFT).  The x-axis represents the size of the training dataset, while the y-axis shows the average performance across various benchmarks (MixEval, AlpacaEval2-LC, MTBench/10, and OpenLLM LB2). Different colored bars represent different Data Generating Models (DGMs). The results indicate that larger datasets generally lead to improved SFT performance; however, the degree of improvement varies depending on the quality of the synthetic data generated by the DGM. For models like GPT-3.5, performance plateaus relatively quickly with increased data size, while other DGMs show consistent improvement with larger datasets.  This highlights that data quality and the model used for data generation significantly influence the effectiveness of data scaling in SFT.", "section": "3.2. Key Findings"}, {"figure_path": "https://arxiv.org/html/2501.18511/extracted/6167093/figures/key_words_common_llama.png", "caption": "Figure 3: Key words more common in L8B : L70 judgments.  The more negative tone of these judgments emphasizes words like clearer (as in, \u201ccould have been clearer\u201d), lacks, convoluted and repetitive.", "description": "Figure 3 is a word cloud showing the words that appeared most frequently in the human evaluator's feedback when comparing the performance of two fine-tuned language models (LLMs): L8B:L70 (Llama-3.1 8B Base fine-tuned on Llama-3.3-70B responses) and L8B:Q72 (Llama-3.1 8B Base fine-tuned on Qwen-2.5-72B responses). The word cloud visualizes the negative sentiment expressed in the evaluations of L8B:L70.  The prominent words, such as \"lacks,\" \"convoluted,\" and \"repetitive,\" highlight the perceived shortcomings of L8B:L70 in comparison to L8B:Q72. The presence of \"clearer\" indicates that the judges desired more clarity in L8B:L70's outputs. The word cloud provides a concise visual summary of the qualitative aspects of model performance.", "section": "3.2 Key Findings"}, {"figure_path": "https://arxiv.org/html/2501.18511/extracted/6167093/figures/key_words_common_qwen.png", "caption": "Figure 4: Key words more common in L8B : Q72 judgments.  These judgments tended to be more positive; emphasis was placed on words like appropriate, necessary, comprehensive and accurate.", "description": "Figure 4 is a word cloud visualizing the words that frequently appeared in the LLM judge's feedback when comparing the performance of Llama-3.1-8B-Base fine-tuned on Qwen-2.5-72B responses against other models.  The word cloud shows that the judges' comments were generally more positive for this model. Words like \"appropriate,\" \"necessary,\" \"comprehensive,\" and \"accurate\" were prominently featured, indicating that the model's responses were perceived as well-suited to the task and of high quality.", "section": "3.2. Key Findings"}]