[{"figure_path": "https://arxiv.org/html/2501.18512/x1.png", "caption": "Figure 1: Streaming DiLoCo: each replica trains independently for dozen of inner optimization steps, and then synchronize a single fragment during outer optimization. In this figure, there are M=4\ud835\udc404M=4italic_M = 4 replicas with p={1,2,3}\ud835\udc5d123p=\\{1,2,3\\}italic_p = { 1 , 2 , 3 } fragments. Each fragment can be made of several transformer layers. Note that this figure only showcases the streaming partial updates (subsection\u00a02.2) and not the quantized communication overlapping (subsection 2.3 and 2.4).", "description": "The figure illustrates the Streaming DiLoCo algorithm.  It depicts four replicas (M=4), each independently processing their data for a specified number of inner optimization steps. After these steps, instead of synchronizing the entire model parameters, they only synchronize a single fragment (p={1,2,3}) of the parameters. This process repeats iteratively. Each fragment is a subset of layers in the transformer model. Importantly, the diagram only visualizes the streaming partial updates (section 2.2), excluding the quantized communication and overlapping techniques (sections 2.3 and 2.4).", "section": "2. Streaming partial updates"}, {"figure_path": "https://arxiv.org/html/2501.18512/x2.png", "caption": "Figure 2: Streaming pattern: sequential (left) and strided (right). Colors denotes the fragment. A different fragment is synchronized each time.", "description": "This figure illustrates two different methods for partitioning model parameters into fragments during training.  The left panel depicts a sequential pattern where consecutive layers of the model are grouped into a single fragment. The right panel shows a strided pattern where layers are interleaved across different fragments. In both cases, only one fragment is synchronized at a time, improving communication efficiency by reducing the amount of data transferred during each synchronization step. The color-coding visually distinguishes different fragments within the model architecture. Each fragment is synchronized independently at a time, and different fragments get synchronized at each step.", "section": "2. Streaming partial updates"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_1b.png", "caption": "Figure 3: Simulation of a schedule interleaving forward passes (in blue), backward passes w.r.t. activations and parameters (resp. in light and dark green), and (outer) gradient reduction (in purple).", "description": "This figure displays a schedule where forward and backward passes are interleaved, along with outer gradient reduction. This interleaving is done for computation and communication overlap, a key technique in the paper. Forward passes are shown in blue, backward passes w.r.t. activations are in light green, backward passes w.r.t. parameters are in dark green, and gradient reduction is in purple. Each color represents a different stage in the process, and the interleaving helps to improve compute utilization by overlapping communication with computation.", "section": "2.3. Overlapping communication with computation"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_10b.png", "caption": "(a) 1B parameters model.", "description": "This figure shows the compute utilization simulated across a range of bandwidth for a model with 1 billion parameters.  Compute utilization is a measure of the percentage of time spent on computation versus communication. A higher compute utilization indicates more efficient use of resources. The figure illustrates how the compute utilization varies as WAN (Wide Area Network) bandwidth increases. It also compares different methods (Data-Parallel, vanilla DiLoCo, Streaming DiLoCo, and Streaming DiLoCo with overlapping communication). This allows for an assessment of the communication efficiency of each method in relation to the available bandwidth.  The graph shows that Streaming DiLoCo (with overlapping communication) has significantly higher compute utilization across the bandwidth range compared to the other methods, indicating that it uses resources more efficiently.", "section": "3.1. Compute utilization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_100b.png", "caption": "(b) 10B parameters model", "description": "This figure shows the compute utilization for a 10 billion parameter model across various bandwidths. Compute utilization is a measure of how much time is spent on actual computation versus communication.  A higher compute utilization indicates better efficiency, with more time spent on productive computation and less time waiting for data transfer. The graph displays how the compute utilization improves as the available bandwidth increases, reaching nearly 95% utilization in the optimal bandwidth range.", "section": "3.1. Compute utilization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/scaling_loss.png", "caption": "(c) 100B parameters model", "description": "This figure displays the compute utilization for a 100 billion parameter model across a range of bandwidth.  Compute utilization is a measure of the percentage of time spent in computation versus communication.  A higher compute utilization indicates that the model is spending more time performing computations and less time waiting for communication to complete. The graph shows how different methods (Data-Parallel, Vanilla DiLoCo, and variations of Streaming DiLoCo) achieve different compute utilization rates as the bandwidth changes.", "section": "3.1. Compute utilization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/scaling_hellaswag.png", "caption": "Figure 4: Compute Utilization simulated across a range of bandwidth. A compute utilization of 0.8 means 80% of the time is spent in computation, and 20% in communication. Our best method reaches a compute utilization of 95% for models 1B, 10B, and 100B with a bandwidth roughly constant between 1 and 5 Gbit/s. Data-Parallel on the other hand requires 100, 200, and 300Gbit/s.", "description": "This figure displays the compute utilization results for various deep learning model training methods across a range of bandwidths. Compute utilization represents the proportion of time dedicated to computation versus communication.  A higher compute utilization indicates greater efficiency.  The results show that the proposed 'Streaming DiLoCo' method achieves significantly higher compute utilization (approximately 95%) compared to the data-parallel baseline, especially at larger model sizes (1B, 10B, and 100B parameters).  Importantly, Streaming DiLoCo achieves this high utilization with a considerably lower bandwidth (between 1 and 5 Gbit/s), whereas Data-Parallel requires much higher bandwidths (100, 200, and 300 Gbit/s). This demonstrates the effectiveness of Streaming DiLoCo in reducing communication overhead while maintaining computational efficiency.", "section": "3.1. Compute utilization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/fragment_size_loss.png", "caption": "(a) Evaluation loss on C4", "description": "This figure shows the evaluation loss on the C4 dataset for different models and training methods. The x-axis represents the total number of training flops, and the y-axis shows the evaluation loss.  It compares the performance of Data-Parallel, DiLoCo with 30 inner steps, Streaming DiLoCo with 30 inner steps, and Streaming DiLoCo with 100 inner steps across various model sizes. The plot visualizes how the loss decreases as the amount of computation (flops) increases for each method and illustrates the relative performance of each approach in large-scale language model training.", "section": "3.2 LLM Scaling Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/fragment_size_bandwidth.png", "caption": "(b) HellaSwag accuracy", "description": "This figure shows the HellaSwag accuracy for different compression methods applied to the outer gradients.  It compares the performance using different levels of compression, including various forms of value dropping (FedDropout, Dare, and Top-k) and lower-precision floating-point numbers (fp4, fp8, and bf16). The x-axis represents the level of compression, and the y-axis shows the HellaSwag accuracy. The graph allows for the comparison of accuracy loss across different compression techniques and helps to determine the optimal trade-off between bandwidth reduction and model performance.", "section": "3.3.3. Ablating the quantized communication"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_100b_stride.png", "caption": "Figure 5: Scaling models from 35M (1.49e17 flops) to 4B parameters (2e21 flops) on C4.", "description": "This figure shows the results of scaling experiments on the C4 dataset, training language models with sizes ranging from 35 million parameters to 4 billion parameters.  The x-axis represents the total number of training FLOPs (floating point operations), which is a measure of computational work. The y-axis of the left-hand plot shows the evaluation loss on the C4 dataset, while the y-axis of the right-hand plot shows the HellaSwag accuracy. Lower evaluation loss and higher HellaSwag accuracy indicate better model performance. The plot demonstrates how different methods (Data-Parallel, DiLoCo, and Streaming DiLoCo) perform as model size increases.  It illustrates the scaling behavior and the relative performance of the methods.", "section": "3.2 LLM Scaling Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/num_overlap.png", "caption": "(a) C4 eval loss", "description": "This figure shows the effect of varying the number of layers per fragment on the evaluation loss for the C4 dataset.  The x-axis represents the number of layers per fragment, and the y-axis represents the evaluation loss.  Two different fragment patterns (sequential and strided) are compared, showing the trade-off between performance and peak bandwidth reduction.  The figure helps to determine the optimal fragment size for balancing these factors.", "section": "3.3.1. Ablating the streaming synchronization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_100b_async.png", "caption": "(b) Peak bandwidth reduction", "description": "This figure shows the peak bandwidth reduction achieved by varying the fragment size in the Streaming DiLoCo model.  Smaller fragment sizes lead to lower peak bandwidth requirements because synchronization happens more frequently, but on smaller subsets of parameters.  The trade-off is explored, showing the impact of fragment size on bandwidth reduction.", "section": "3.3.1. Ablating the streaming synchronization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/num_overlap_async_between_workers.png", "caption": "Figure 6: The fragment\u2019s size will determine the peak bandwidth but also the learning dynamics. We choose in practice 3 layers per fragment across all model scales.", "description": "This figure shows the effect of different fragment sizes on both peak bandwidth and model performance.  The fragment size refers to the number of transformer blocks included in a single fragment during synchronization.  Smaller fragment sizes reduce peak bandwidth but can impact learning dynamics.  After experimentation across various model sizes, the authors determined that a fragment size of 3 layers provided an optimal balance between bandwidth efficiency and model performance. This size was consistently used across all model scales in subsequent experiments.", "section": "3.3.1. Ablating the streaming synchronization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/compression_loss.png", "caption": "Figure 7: Compute utilization profile of sequential vs strided pattern for a 100 billion parameters model.", "description": "This figure compares the compute utilization of two different fragment patterns (sequential and strided) in the Streaming DiLoCo algorithm for a 100 billion parameter model. Compute utilization represents the percentage of time spent on computation versus communication. The x-axis represents the bandwidth, and the y-axis represents the compute utilization. The plot shows how compute utilization changes with varying bandwidth for both sequential and strided patterns. The strided pattern generally demonstrates better compute utilization, especially at higher bandwidths. This signifies that the strided fragment pattern leads to more efficient use of computing resources during training compared to the sequential pattern, particularly in bandwidth-rich environments.", "section": "3.1. Compute utilization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/compression_hellaswag.png", "caption": "Figure 8: Varying the number of overlapped inner steps \u03c4\ud835\udf0f\\tauitalic_\u03c4 for \u03b1={0,0.5}\ud835\udefc00.5\\alpha=\\{0,0.5\\}italic_\u03b1 = { 0 , 0.5 }. A larger \u03c4\ud835\udf0f\\tauitalic_\u03c4 requires a significantly lower bandwidth, see also Figure\u00a09.", "description": "This figure shows the impact of varying the number of inner steps that overlap with communication (\u03c4) on the model's evaluation loss. Two scenarios are tested: \u03b1 = 0 (no merging of local and global parameters) and \u03b1 = 0.5 (averaging local and global parameters).  The results show that increasing \u03c4 leads to a significant reduction in the required bandwidth for communication, as also shown in Figure 9.", "section": "3.3.2. Ablating the communication overlap"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/num_replicas.png", "caption": "Figure 9: Estimated compute utilization for a 100B model when increasing \u03c4\ud835\udf0f\\tauitalic_\u03c4, the number of inner steps which overlap with communication.", "description": "This figure shows how compute utilization changes for a 100-billion parameter model as the number of inner optimization steps (\u03c4) overlapped with communication increases.  The x-axis represents the WAN bandwidth, and the y-axis represents the compute utilization.  Different lines represent different numbers of overlapped steps (\u03c4). The figure aims to demonstrate the impact of overlapping communication and computation on the efficiency of the training process by showing how much time is spent on computation vs. communication.", "section": "3.1. Compute utilization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/num_replicas_bsz.png", "caption": "Figure 10: Varying the number of overlapped inner steps \u03c42subscript\ud835\udf0f2\\tau_{2}italic_\u03c4 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for the second worker while keeping \u03c41=1subscript\ud835\udf0f11\\tau_{1}=1italic_\u03c4 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 1. For all data points, \u03b1=0.5\ud835\udefc0.5\\alpha=0.5italic_\u03b1 = 0.5. Training is very robust for values of \u03c42subscript\ud835\udf0f2\\tau_{2}italic_\u03c4 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT less than 5.", "description": "This figure shows the impact of overlapping communication with computation on model training.  Specifically, it investigates how varying the number of inner steps (\u03c42) for the second worker, while keeping the number of inner steps for the first worker (\u03c41) constant at 1, affects the model's loss.  The alpha parameter (\u03b1) is set to 0.5, which represents a mean between using only locally computed updates and globally shared updates for parameter synchronization. The results demonstrate that the model's performance (loss) is robust to increasing \u03c42, showing minimal degradation when \u03c42 is less than 5.", "section": "3.3.2 Ablating the communication overlap"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/num_inner_steps.png", "caption": "(a) C4 evaluation loss", "description": "The figure shows the evaluation loss on the C4 dataset for various model sizes, ranging from 35 million to 4 billion parameters.  Different training methods are compared: Data-Parallel, DiLoCo, and Streaming DiLoCo with different numbers of inner optimization steps (H) and inner communication overlap (\u03c4).  The x-axis represents the total number of training FLOPs, illustrating the scaling behavior of each method. The y-axis shows the evaluation loss, indicating the model's performance in terms of prediction error.", "section": "3.2. LLM Scaling Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_100b_1s.png", "caption": "(b) HellaSwag accuracy", "description": "This figure shows the HellaSwag accuracy results for different compression methods applied to the outer gradients during communication.  It compares the performance of using lower-precision floating-point numbers (fp4, fp8) and value-dropping methods (FedDropout, Dare, Top-k) to the baseline of using full-precision (fp32). The x-axis represents the compression rate, and the y-axis represents the HellaSwag accuracy.  The results indicate the impact of different compression techniques on the model's performance in a downstream task, demonstrating that reducing the precision of communication does not significantly affect accuracy at the billion-scale parameter model size.", "section": "3.3.3. Ablating the quantized communication"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_100b_5s.png", "caption": "Figure 11: Compressing the outer gradients with either value dropping (FedDropout, Dare) or using lower-precision floating point numbers.", "description": "This figure displays the results of ablating the effect of compressing outer gradients by either dropping values (using FedDropout and Dare methods) or using lower-precision floating point numbers (FP4, FP8, BF16).  The left panel shows the evaluation loss on the C4 dataset, while the right panel displays the HellaSwag accuracy.  The different compression techniques are compared against the baseline of using full-precision (FP32) numbers to highlight the impact of compression on both loss and accuracy.", "section": "3.3.3. Ablating the quantized communication"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_100b_10s.png", "caption": "(a) Keeping the global batch size constant, and thus decreasing the local per-replica batch size.", "description": "This figure shows the impact of varying the number of replicas on the evaluation loss, while keeping the global batch size constant.  As the number of replicas increases, the local per-replica batch size decreases.  This experiment helps to understand the effect of the local batch size on model performance in a distributed training setting. The x-axis shows the number of replicas, and the y-axis represents the evaluation loss on the C4 dataset. Two lines are shown, one for Streaming DiLoCo and one for DiLoCo, illustrating the performance difference between the two methods.", "section": "3.1 Compute utilization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_llama405b.png", "caption": "(b) Keeping the local per-replica batch size constant, and thus increasing the global batch size.", "description": "This figure shows the effect of scaling the number of replicas while keeping the local batch size constant.  Increasing the number of replicas leads to a larger global batch size. The experiment demonstrates how the evaluation loss on C4 changes with the increase in the number of replicas, while maintaining a consistent local batch size.  This visualization helps analyze the impact of distributed training on model performance under different scaling scenarios.", "section": "LLM Scaling Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/bandwidth_deepseekV3.png", "caption": "Figure 12: Scaling the number of DiLoCo replicas M\ud835\udc40Mitalic_M from M=2\ud835\udc402M=2italic_M = 2 to M=4\ud835\udc404M=4italic_M = 4. For all experiments, the token budget is kept constant.", "description": "This figure shows the impact of increasing the number of DiLoCo replicas while keeping the total token budget constant. It consists of two subfigures. Subfigure (a) keeps the global batch size constant and reduces the local per-replica batch size as the number of replicas increases, while subfigure (b) keeps the local per-replica batch size constant and increases the global batch size. Both subfigures compare the performance of DiLoCo and Streaming DiLoCo under these different scaling scenarios.", "section": "3.2 LLM Scaling Experiments"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/cos_fragments.png", "caption": "Figure 13: Varying the number of inner steps H\ud835\udc3bHitalic_H for DiLoCo and Streaming DiLoCo while keeping the total number of steps constants. A lower H\ud835\udc3bHitalic_H means more communication rounds to be done.", "description": "This figure shows the effect of changing the number of inner steps (H) in DiLoCo and Streaming DiLoCo on the evaluation loss. The total number of training steps is kept constant, so reducing H increases the number of communication rounds.  The results demonstrate the trade-off between communication cost and model performance. A lower H implies more frequent communication but may also result in noisy gradient updates, while a higher H reduces the communication frequency but increases the risk of replicas drifting apart.", "section": "3.3. Ablating the streaming synchronization"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/cos_emb.png", "caption": "Figure 14: Simulation of a schedule interleaving forward passes (in blue), backward passes w.r.t. activations and weights (resp. in light and dark green), and (outer) gradient reduction (in purple) for Streaming DiLoCo, respectively with a sequential and strided pattern.", "description": "This figure visualizes the scheduling of computations and communication in Streaming DiLoCo, comparing sequential and strided fragment patterns. The blue bars represent forward passes, light and dark green bars represent backward passes (w.r.t. activations and weights, respectively), and purple bars represent outer gradient reduction.  The illustration shows how computations and communication are interleaved, allowing for overlapping and improved efficiency.  The difference in scheduling between sequential and strided fragments highlights the impact of the chosen strategy on resource utilization.", "section": "2.3. Overlapping communication with computation"}, {"figure_path": "https://arxiv.org/html/2501.18512/extracted/6166994/figures/cosine_across_scales.png", "caption": "(a) 1s step time", "description": "This figure shows the compute utilization for a 100 billion parameters model when the computation time for one step is set to 1 second. Compute utilization represents the percentage of time spent on computation versus communication.  The graph displays compute utilization across a range of bandwidth, highlighting the impact of different optimization methods.  Data-parallel, vanilla DiLoCo, and various streaming DiLoCo methods are compared to show the efficiency gains achieved by overlapping communication with computation and using lower-precision gradients.", "section": "3.1. Compute utilization"}]