{"references": [{"fullname_first_author": "D. Guo", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This paper introduces DeepSeek-R1, a key model in the comparative analysis of the paper, directly impacting the study's core findings."}, {"fullname_first_author": "T. Xie", "paper_title": "Sorry-bench: Systematically evaluating large language model safety refusal behaviors", "publication_date": "2024-06-01", "reason": "This paper provides a benchmark dataset and methodology for evaluating LLM safety, which is directly relevant to the paper's safety assessment."}, {"fullname_first_author": "Z. Zhang", "paper_title": "Safetybench: Evaluating the safety of large language models with multiple choice questions", "publication_date": "2023-09-01", "reason": "This paper offers another approach to LLM safety evaluation that the authors contrast their methodology against, providing context and comparison."}, {"fullname_first_author": "M. Ugarte", "paper_title": "Astral: Automated safety testing of large language models", "publication_date": "2025-01-01", "reason": "This is the authors' own previous work which introduces the ASTRAL tool used for the current study, making it a foundational reference."}, {"fullname_first_author": "A. Arrieta", "paper_title": "Early external safety testing of openai's o3-mini: Insights from the pre-deployment evaluation", "publication_date": "2025-01-01", "reason": "This is another work by the authors that offers a preliminary safety evaluation of the OpenAI o3-mini model used in this paper, offering a baseline for comparison."}]}