[{"figure_path": "https://arxiv.org/html/2502.06589/x1.png", "caption": "Figure 1: Training paradigms of LLM agents.\nPrompting alone fails to introduce new knowledge and capabilities, while heavy fine-tuning can hinder generalization and degrade performance in non-agent use cases, potentially suppressing the original base model capabilities.", "description": "This figure illustrates four different approaches to training large language model (LLM) agents.  (A) Prompting uses only prompts to guide the LLM, without adding new knowledge or capabilities.  This is the simplest but least effective approach. (B) Fine-tuning uses task-specific data to train the LLM. Although this improves performance on the specific task, it can hinder generalization to other tasks and negatively affect performance in non-agent contexts. (C) Instruction fine-tuning improves on fine-tuning by using instruction data for training.  While this improves generalization, the limitations similar to (B) still apply. (D) The proposed approach (Hephaestus-Forge) uses continual pre-training on a multi-source corpus designed to enhance the fundamental capabilities of the LLM agents, leading to better generalization and performance across a range of tasks. The figure highlights that continual pre-training is superior to other approaches in terms of generalization and retaining the original model's capabilities.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06589/x2.png", "caption": "(a) Hephaestus-Forge", "description": "This figure shows a pie chart illustrating the composition of the Hephaestus-Forge dataset.  It breaks down the dataset into its constituent parts: API documentation, function calls, code, multi-modality data, reasoning data, coding data, and various text sources (e.g. Arxiv, Wikipedia, web crawls).  The proportions of each data type indicate the relative emphasis placed on different aspects of agent capabilities during pre-training.", "section": "4 Hephaestus-Forge"}, {"figure_path": "https://arxiv.org/html/2502.06589/x3.png", "caption": "(b) Tool Data", "description": "This figure shows the seed data used to create the Hephaestus-Forge pre-training corpus.  It specifically details the \"Tool Data\" component, illustrating the various sources of information included. These sources represent diverse types of agent-related data and include API trajectories, tool documentation from multiple sources (e.g., StarCoder APIs, API-Pack, ToolBench, etc.), code examples, and natural language planning data.", "section": "4 Hephaestus-Forge"}, {"figure_path": "https://arxiv.org/html/2502.06589/x4.png", "caption": "(c) Retrieved Data", "description": "This figure shows a t-SNE visualization of the retrieved web data used in the Hephaestus-Forge dataset.  The visualization plots the data points in a two-dimensional space, where proximity indicates semantic similarity. The colorful points represent the seed data from various sources, while the black points represent the retrieved data. The gray points represent the general text data.  The visualization demonstrates that the retrieved data is more semantically similar to the seed data than to the general text, indicating successful retrieval of relevant agent data from the web.", "section": "4 Hephaestus-Forge"}, {"figure_path": "https://arxiv.org/html/2502.06589/x5.png", "caption": "(d) t-SNE: Retrieved Data", "description": "This t-SNE plot visualizes the semantic relationships between different data sources used in the Hephaestus-Forge dataset.  Seed data (colorful points) from various sources like API documentation, code, and web crawls are shown, along with the retrieved web data (black points) and general text data (gray points). The proximity of the retrieved data points to the seed data points illustrates the effectiveness of the semantic matching process in retrieving relevant web data for pre-training.", "section": "4 Hephaestus-Forge"}, {"figure_path": "https://arxiv.org/html/2502.06589/x6.png", "caption": "Figure 2: Data composition of (a) the entire Hephaestus-Forge, (b) seed data collection (\u00a7\u00a04.1), and (c) retrieved agent data from the open web (\u00a7\u00a04.2). A t-SNE visualization (d) depicts seed data (colorful points, with each color representing different data sources), retrieved data (black), and general text (gray) within the semantic space, where retrieved data is closer to the selected seed data than to the general text. Detailed data sources are in \u00a7\u00a0A.1.", "description": "Figure 2 illustrates the composition of the Hephaestus-Forge dataset.  Panel (a) shows an overview of the entire dataset, highlighting the diverse data sources. Panel (b) details the seed data collection process, outlining the different sources and their relative contributions. Panel (c) shows the retrieved agent data obtained from the open web, expanding the dataset further. Finally, panel (d) provides a t-SNE visualization of the semantic space, demonstrating that the retrieved data points cluster closely to the seed data points and are clearly distinct from general text.  This highlights the quality and relevance of the retrieved data.", "section": "4 Hephaestus-Forge"}, {"figure_path": "https://arxiv.org/html/2502.06589/x7.png", "caption": "Figure 3: Scaling law of the relationship between agent data mixing ratio (%percent\\%%) and benchmark loss.", "description": "This figure shows the results of experiments investigating the optimal ratio of agent data within a pre-training corpus for large language models (LLMs).  The x-axis represents the percentage of agent data in the training corpus, while the y-axis displays the benchmark loss (a measure of model performance).  Multiple lines represent different benchmarks, demonstrating how the loss changes as the proportion of agent data varies. The figure demonstrates that an optimal mixing ratio exists, where including too little or too much agent data negatively impacts performance.", "section": "5 Scaling Laws for Data Composition"}, {"figure_path": "https://arxiv.org/html/2502.06589/x8.png", "caption": "Figure 4: \nOverview of the pre-training (Stages I & II) and instruction fine-tuning (III) framework in Hephaestus.", "description": "This figure illustrates the three-stage training process for the Hephaestus model. Stage I involves injecting general agent knowledge through pre-training on a large dataset containing diverse data sources like general text, code, and API documentation. In Stage II, the model further enhances its agent-specific capabilities using a high-quality seed dataset of agent-relevant data. Finally, Stage III includes instruction fine-tuning using instruction-completion datasets to align the model with specific task requirements and user preferences.", "section": "6 Hephaestus"}, {"figure_path": "https://arxiv.org/html/2502.06589/x9.png", "caption": "Figure 5: Training and benchmark loss. (a) Training loss of Hephaestus during continual pre-training and instruction fine-tuning. (b) Benchmark loss at periodic training checkpoints and (c) a comparison across base models.", "description": "Figure 5 illustrates the training and evaluation results for the Hephaestus model. Panel (a) shows the training loss curve during both continual pre-training (with two distinct stages) and subsequent instruction fine-tuning.  Panel (b) displays the benchmark loss at various checkpoints during the training process, demonstrating performance improvement over time. Finally, panel (c) provides a comparative analysis of the benchmark loss across different base models, highlighting Hephaestus's superior performance.", "section": "7. Main Experiments"}, {"figure_path": "https://arxiv.org/html/2502.06589/x10.png", "caption": "Figure 6: \nExamples of different task formats in Hephaestus-Forge, including tool documentation, action trajectory (w/ environmental feedback), and code data.", "description": "Figure 6 presents example task formats from the Hephaestus-Forge dataset, illustrating the diversity of data included.  It showcases three key aspects: Tool documentation provides structured information about APIs, detailing parameters and functionalities. Action trajectories, often with environmental feedback, show sequences of actions taken by an agent to solve a problem, including observations and resulting responses. Code data includes actual code examples that are relevant to the tasks and contribute to the model's understanding of both problem-solving strategies and implementation details.", "section": "4 Hephaestus-Forge"}]