{"references": [{"fullname_first_author": "Aky\u00fcrek, A. F.", "paper_title": "R14f: Generating natural language feedback with reinforcement learning for repairing model outputs", "publication_date": "2023-05-00", "reason": "This paper is highly relevant because it directly addresses the challenge of generating effective feedback for model improvement using reinforcement learning, a key aspect of the current work."}, {"fullname_first_author": "Huang, J.", "paper_title": "Large language models cannot self-correct reasoning yet", "publication_date": "2023-10-00", "reason": "This paper highlights the limitations of self-improvement mechanisms in LLMs without appropriate external feedback, providing a strong motivation for the current work's focus on developing effective critics."}, {"fullname_first_author": "Gao, L.", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2023-00-00", "reason": "This paper is crucial because it discusses the challenges of using reward models for LLM self-improvement, particularly the issue of overoptimization, which the current work seeks to address through a different approach."}, {"fullname_first_author": "Shao, Z.", "paper_title": "Group Relative Policy Optimization", "publication_date": "2024-00-00", "reason": "This paper is highly relevant as the current work utilizes GRPO to address the variance issue in training the critic model. This method is a key technical contribution enabling stable training"}, {"fullname_first_author": "Li, Y.", "paper_title": "Competition-level code generation with alpha-code", "publication_date": "2022-00-00", "reason": "This paper introduces a challenging benchmark (CodeContests) used for evaluation in the current study, providing a strong basis for comparing the effectiveness of different critique-revision methods."}]}