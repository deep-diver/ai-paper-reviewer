[{"figure_path": "https://arxiv.org/html/2501.14492/x1.png", "caption": "Figure 1: Caption", "description": "The figure presents a benchmark comparison of self-critique and cross-critique abilities across various Large Language Models (LLMs).  It showcases the performance differences between classical LLMs and advanced reasoning-based LLMs, highlighting the superior performance of the o1-mini model in both self and cross-critique scenarios.  The bar chart visually demonstrates the average performance improvement (or decline) after critique for each model.  The results indicate that while classical LLMs show little to no improvement in self-critique and modest gains in cross-critique, the o1-mini model exhibits significant performance increases.  This underscores the importance of advanced reasoning capabilities for effective critique generation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.14492/x2.png", "caption": "Figure 2: TODO.", "description": "This figure presents two examples to illustrate the limitations of the CriticBench benchmark in evaluating critique quality.  CriticBench assesses critique quality solely based on the accuracy of the critique's verdict (correct or incorrect) regarding the input solution's correctness. The figure showcases that CriticBench may incorrectly classify low-quality critiques as high-quality because the critique correctly predicted the solution's inaccuracy, even though the critique itself was flawed and didn't effectively guide the correction process. In contrast, the RealCritic approach, which measures critique quality based on its effectiveness in improving the solution, accurately identifies the high-quality critique, demonstrating that the evaluation methodology is crucial for a robust benchmark.", "section": "Toward Effectiveness-Driven Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.14492/x3.png", "caption": "Figure 3: Caption.", "description": "This figure compares the evaluation methods used in CriticBench and RealCritic. CriticBench uses an open-loop approach, evaluating critiques in isolation based on the correctness of their verdicts (good/bad).  RealCritic, in contrast, employs a closed-loop methodology.  It assesses critique quality by measuring the accuracy of the solution that results from applying the critique and subsequent correction. The closed-loop approach is represented in the figure as the correction feeding back into the system.", "section": "3 Our Evaluation Framework"}, {"figure_path": "https://arxiv.org/html/2501.14492/x4.png", "caption": "Figure 4: Caption", "description": "This figure illustrates the process of constructing datasets for evaluating the critique abilities of large language models (LLMs).  The process involves two main steps. First, it begins with a pool of candidate datasets encompassing diverse question types (open-ended mathematical problems and multiple-choice questions) and difficulty levels.  These datasets are then filtered based on the capabilities of several LLMs to ensure that the chosen datasets provide a balanced representation of difficulty and challenge model capabilities appropriately.  The second step shows that solutions from several different LLMs are generated for all questions in the filtered dataset. Then, the solutions are filtered to ensure a balance of correct and incorrect solutions for evaluation. An example of a question with the associated LLM response, score and source dataset is also shown.", "section": "3.3 Dataset Collection"}, {"figure_path": "https://arxiv.org/html/2501.14492/x5.png", "caption": "Figure 5: Caption", "description": "Figure 5 presents a comparative analysis of the performance of different LLMs across self-critique and cross-critique tasks.  The charts visualize the percentage change in solution correctness after critique, categorized into two scenarios: I\u2192C (improving incorrect solutions) and C\u2192I (degrading correct solutions). The results highlight the asymmetry between a model's ability to improve incorrect solutions versus maintain the correctness of already correct solutions.  This asymmetry is observed across various LLMs on both self- and cross-critique tasks.", "section": "4.2.3 Comparative Analysis of Self-Critique and Cross-Critique"}, {"figure_path": "https://arxiv.org/html/2501.14492/x6.png", "caption": "Figure 6: Caption", "description": "Figure 6 presents the performance of iterative critique, averaging results across eight tasks.  The iterative critique process involves multiple rounds of critique and correction, allowing for the refinement of initial solutions.  The figure shows how performance, measured by the change in accuracy compared to the baseline, changes over successive rounds.  Different models exhibit varied trajectories.  Some models show a consistent decline, others maintain consistent improvement, and still others initially improve before declining. These trends highlight the varying dynamics and effectiveness of iterative critique across different models.", "section": "4.2.4 Impact of Iterative Critique"}]