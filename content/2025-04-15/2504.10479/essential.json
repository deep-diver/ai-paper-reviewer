{"importance": "InternVL3 is crucial for researchers as it sets a new standard among **open-source MLLMs**, rivaling proprietary models in performance. Publicly available training data and model weights will accelerate advancements. It offers a strong foundation for future multimodal AI innovations.", "summary": "InternVL3: A new open-source multimodal model achieves state-of-the-art results, offering advanced training and test-time recipes.", "takeaways": ["InternVL3 employs native multimodal pre-training, jointly learning language and vision.", "It achieves state-of-the-art performance among open-source MLLMs on MMMU benchmark.", "The training data and model weights are publicly released."], "tldr": "Multimodal Large Language Models(MLLMs) are advancing artificial intelligence. However, most models adapt text-only models through multi-stage pipelines, leading to alignment challenges. This paper introduces InternVL3, improving on the InternVL series with native multimodal pre-training, addressing complexities of conventional training. This unified approach enables simultaneous acquisition of linguistic and multimodal competencies. \n\nInternVL3 incorporates variable visual position encoding (V2PE) for extended contexts, along with advanced post-training and optimized infrastructure. It surpasses predecessors across tasks and is competitive with proprietary models. The 78B model achieves 72.2 on MMMU, setting a new open-source standard. Training data and weights will be released.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.10479/podcast.wav"}