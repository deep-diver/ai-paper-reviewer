[{"figure_path": "https://arxiv.org/html/2504.08791/x1.png", "caption": "Figure 1: Piped-ring parallelism. In this case, 6 devices handle a 36-layer model. With a layer window size of 2, the model is splitted into 18 segments, which are assigned to 6 devices in a ring order, so each device needs 3 rounds to predict one token.", "description": "The figure illustrates the concept of piped-ring parallelism in a distributed inference system.  A 36-layer language model is divided into 18 segments (layer window size of 2).  Six devices are arranged in a ring, each responsible for processing a subset of the segments. To predict a single token, each device executes three rounds, passing its results to the next device in the ring for further processing. This approach allows for the efficient distribution of the computational load across multiple devices, hiding latency and improving overall performance.", "section": "3.1 Piped-ring Parallelism with Prefetching"}, {"figure_path": "https://arxiv.org/html/2504.08791/", "caption": "Figure 2: Normalized token latency over k\ud835\udc58kitalic_k.", "description": "This figure shows how the normalized token latency changes with different values of k (the number of rounds to predict one token in piped-ring parallelism).  It demonstrates the impact of piped-ring parallelism on latency, particularly in scenarios with insufficient memory.  The results indicate that smaller values of k are more efficient when sufficient memory is available, while larger k values become necessary and beneficial when memory is limited, effectively leveraging disk offloading and preventing memory overload.", "section": "4.2 Ablation Study on Prefetching, Halda and Piped-ring Parallelism"}, {"figure_path": "https://arxiv.org/html/2504.08791/x3.png", "caption": "Figure 3: Illustration of model layers loaded into memory in pipeline parallelism with prefetching. In this case, the device handles 6 model layers but its available memory can only hold 3. The green blocks show the layers loaded into memory, while white blocks indicate those not yet loaded.", "description": "This figure illustrates the prefetch-release effect in pipeline parallelism.  A device with limited memory (only 3 layers) is attempting to process 6 layers using prefetching.  Prefetching loads layers in anticipation of computation, but because the device's memory is constrained, it releases already loaded layers to make space for new ones. The effect is that layers need to be loaded multiple times, negating the benefits of prefetching and causing extra disk I/O latency. Green blocks represent layers currently in memory, white blocks represent layers not yet loaded.", "section": "3.1 Piped-ring Parallelism with Prefetching"}, {"figure_path": "https://arxiv.org/html/2504.08791/x4.png", "caption": "Figure 4: Illustration of model layers loaded into memory in piped-ring parallelism with fast disk.", "description": "In piped-ring parallelism with a fast disk, prefetching completes before computation begins.  A device loads two layers (window size =2) before starting computations. After completing the computation of those two layers, it immediately begins prefetching the next two.  This overlap continues until the inference is complete. This prevents the issue of page faults and improves overall efficiency.", "section": "3.1 Piped-ring Parallelism with Prefetching"}, {"figure_path": "https://arxiv.org/html/2504.08791/x5.png", "caption": "Figure 5: Illustration of model layers loaded into memory in piped-ring parallelism with slow disk.", "description": "This figure illustrates the memory management in piped-ring parallelism when the disk is slow. Unlike fast disk scenarios, where prefetching can load all required layers before computation, slow disks cause page faults as layers are released by the OS before they're used. This leads to a cycle of prefetching, page faults, and reloading, which adds unnecessary disk overhead. The figure visually depicts this process, highlighting how the slow disk I/O impacts the efficiency of the system.", "section": "3.1 Piped-ring Parallelism with Prefetching"}, {"figure_path": "https://arxiv.org/html/2504.08791/x6.png", "caption": "Figure 6: Timeline of (a,b) piped-ring parallelism on homogeneous devices with fast and slow disks; (c,e) piped-ring parallelism on heterogeneous devices with same and different window sizes; and (d,e) vanilla pipeline parallelism on heterogeneous devices with and without prefetching.", "description": "This figure illustrates different parallelization strategies for large language model (LLM) inference across multiple devices.  Panels (a) and (b) show piped-ring parallelism on devices with uniform hardware specifications, but differing disk speeds. Panels (c) and (e) demonstrate piped-ring parallelism on heterogeneous devices with varying processing capabilities and the same or different layer processing windows.  Panels (d) and (e) compare vanilla pipeline parallelism on heterogeneous devices, with and without prefetching enabled.", "section": "3 Prima.cpp: Parallel Architecture and Scheduler Design"}, {"figure_path": "https://arxiv.org/html/2504.08791/x7.png", "caption": "Figure 7: Comparison of token latency and TTFT for llama.cpp, exo, dllama, and prima.cpp.", "description": "This figure compares the performance of four different large language model (LLM) inference systems: llama.cpp, exo, dllama, and prima.cpp.  The comparison is made across a range of model sizes, showing token latency (the time it takes to process a single token) and time-to-first-token (TTFT) (the time it takes to generate the first token).  The results illustrate the relative speed and efficiency of each system in handling different sized LLMs.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.08791/x8.png", "caption": "Figure 8: Layer assignment and token latency over different number of devices.", "description": "This figure illustrates how the assignment of model layers to devices and the resulting token latency change as the number of devices in the cluster varies.  It demonstrates the impact of adding more or fewer devices on performance, showing that more devices are not always better if those devices have limited computational resources. It specifically examines a Llama 3-70B model and highlights how layer assignment adapts across different numbers of devices, reflecting the strategies used by the prima.cpp system to optimize inference speed.", "section": "4.5 Select Devices to Build the Most Powerful Cluster"}, {"figure_path": "https://arxiv.org/html/2504.08791/x9.png", "caption": "(a)", "description": "This figure shows the memory usage of each device for llama.cpp when running different sizes of Llama models.  It visually represents the RAM and VRAM consumption on each device (D1-D3) for Llama models ranging from 8B to 70B parameters. The graph allows for a comparison of memory usage across different model sizes, highlighting potential memory pressure issues.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.08791/x10.png", "caption": "(b)", "description": "This figure displays the memory usage of each device when running different sized Llama models using the exo inference system.  It shows the breakdown of memory usage for CPUs and GPUs on each device, highlighting the differences in memory consumption across various models and devices. The bars represent the amount of memory used in GiB (Gigabytes), categorized by device and model size, distinguishing between CPU-bound and GPU-bound processes. This visualization helps in understanding the memory efficiency of exo and how different models and devices impact resource utilization.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.08791/x11.png", "caption": "(c)", "description": "This figure shows the memory usage on each device for the dllama model across different model sizes (Llama 3-8B to Llama 3-70B).  The bars represent the memory usage for each device's CPU and GPU, while the horizontal line indicates the maximum available memory for that device. It helps visualize memory pressure and potential for out-of-memory (OOM) errors across the cluster of different devices.", "section": "4.2 Ablation Study on Prefetching, Halda and Piped-ring Parallelism"}, {"figure_path": "https://arxiv.org/html/2504.08791/x12.png", "caption": "(d)", "description": "This figure displays memory usage for each device across different Llama model sizes when using prima.cpp.  It contrasts with Figures 9a, 9b, and 9c, showing prima.cpp's efficient memory management.  The bars represent the RAM usage (in GiB) for each device (D1-D4), broken down by CPU and GPU usage.  'Bound' indicates the memory that is not reclaimable. The low memory usage demonstrates prima.cpp's ability to prevent out-of-memory (OOM) errors even with large models.", "section": "A.6 Efficient Workload Distribution and Memory Usage"}]