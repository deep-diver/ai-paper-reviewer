{"references": [{"fullname_first_author": "Yupeng Chang", "paper_title": "A survey on evaluation of large language models.", "publication_date": "2024-01-01", "reason": "This survey provides a comprehensive overview of methods for evaluating large language models, which is relevant to the benchmark proposed in the paper."}, {"fullname_first_author": "Qiguang Chen", "paper_title": "Towards reasoning era: A survey of long chain-of-thought for reasoning large language models.", "publication_date": "2025-03-01", "reason": "This survey explores the techniques and applications of long chain-of-thought reasoning, a crucial aspect of the LRMs evaluated in the paper."}, {"fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.", "publication_date": "2022-01-01", "reason": "This work introduces the chain-of-thought prompting technique, which is foundational to the development and understanding of large reasoning models."}, {"fullname_first_author": "Daya Guo", "paper_title": "DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning.", "publication_date": "2025-01-01", "reason": "This paper introduces DeepSeek-R1, an important LRM that serves as a baseline model and analysis tool in the current work, showcasing techniques in training LRMs."}, {"fullname_first_author": "Lianmin Zheng", "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena.", "publication_date": "2023-01-01", "reason": "This paper is relevant as it describes MT-bench, which is a widely used benchmark, as the current research attempts to provide a benchmark of S1 thinking, highlighting and contrasting its goals and methodology with MT-bench."}]}