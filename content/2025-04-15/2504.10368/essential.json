{"importance": "This paper offers insights into **LRMs' cognitive limitations** in simple tasks, urging researchers to develop models with **balanced dual-system thinking**. It opens new avenues for exploring LRM efficiency and adaptability, crucial for real-world applications. ", "summary": "S1-Bench: A new benchmark reveals that Large Reasoning Models (LRMs) exhibit inefficiency in simple tasks, favoring System 1 thinking.", "takeaways": ["Current LRMs are inefficient and lack System 1 thinking capabilities, requiring significantly longer outputs than smaller models.", "LRMs often identify correct answers early but continue unnecessary reasoning, increasing content similarity without improving accuracy.", "LRMs demonstrate an ability to prejudge question simplicity, yet still exhibit inefficiencies, highlighting a need for dual-system compatibility."], "tldr": "Large Reasoning Models (LRMs) excel at complex tasks using deliberate System 2 thinking, but a research gap exists in understanding their System 1 thinking abilities for simple, intuitive tasks. Current benchmarks are often adversarial or require reasoning, not aligning with intuitive processing. Thus, a benchmark to evaluate LRMS system 1 thinking is still lacking. \n\nTo address this, the paper introduces **S1-Bench**, a novel benchmark to evaluate LRM's performance on simple tasks. It reveals that LRMs exhibit lower efficiency, with longer outputs than small LLMs. LRMs often find correct answers early but continue unnecessary reasoning, leading to accuracy degradation on simple questions. The research highlights the need to achieve balanced dual-system thinking in LRMs.", "affiliation": "Institute of Information Engineering, Chinese Academy of Sciences", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.10368/podcast.wav"}