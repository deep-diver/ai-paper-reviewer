[{"figure_path": "https://arxiv.org/html/2504.10068/x1.png", "caption": "Figure 1: (a) Sparse sampling, which remains the high resolution but loses many details in the unsampled frames; (b) Dense sampling with low resolution, which understands the videos from a large number of frames but would confuse on the low-resolution content; (c) Dense sampling with token compression, which keeps the key tokens on the main characters but suffers from hallucinations owing to the missing of visual tokens; (d) Our Mavors, balancing the demands of resolution and number of frames. Though all these approaches could perform similarly on Video-MME, Mavors significantly improves the caption capability on complex scenes. Note that the words in red and green denote incorrect and correct details, respectively.", "description": "Figure 1 compares four different video processing approaches used in long-context video understanding models. (a) shows sparse sampling, where only a few frames are selected, preserving high resolution but losing many details and temporal information. (b) depicts dense sampling with low resolution, which captures more temporal information but at the cost of significant spatial detail loss. (c) illustrates dense sampling with token compression, where key information is retained while losing finer details and resulting in hallucinations. (d) presents the Mavors approach, achieving a balance between spatial and temporal detail preservation by utilizing dense sampling with high resolution and chunk-level processing. While all methods perform similarly on the Video-MME benchmark, Mavors demonstrates improved captioning capabilities for complex scenes, as indicated by the correct (green) and incorrect (red) captions provided in the example.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.10068/x2.png", "caption": "(a) Video-MME", "description": "This figure shows the results of the Video-MME benchmark. Video-MME is a multiple-choice question answering task based on video content. It requires a good understanding of the temporal relationships between different video frames. The results show the performance of different video large language models (LLMs) in answering questions based on video content. The methods being compared include sparse sampling, dense sampling with low resolution, dense sampling with token compression, and the proposed multi-granularity video representation. Mavors outperforms other methods, showing the importance of preserving both spatial fidelity and temporal coherence.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.10068/x3.png", "caption": "(b) Dream1K", "description": "The figure shows the impact of the resolution of frames on the performance of two video MLLMs, namely Qwen2.5-VL-7B and Oryx-1.5-7B, across two benchmark datasets: Video-MME and DREAM-1K.  The x-axis represents the resolution of the frames, and the y-axis represents the score achieved by each model on each benchmark.  The graph indicates that higher resolution frames lead to improved performance, particularly on the DREAM-1K dataset, which involves open-ended video captioning requiring a detailed understanding of the video content.", "section": "3.1. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2504.10068/x4.png", "caption": "Figure 2: The impact of the number of frames (720P).", "description": "This figure shows the effect of the number of frames in a video on the performance of two video LLMs (Qwen2.5-VL-7B and Oryx-1.5-7B) on two video understanding benchmarks: Video-MME and DREAM-1K.  The x-axis represents the number of frames, and the y-axis represents the score on each benchmark.  The plot demonstrates that increasing the number of frames generally improves the performance on both benchmarks, particularly on DREAM-1K which involves open-ended video captioning requiring finer-grained spatiotemporal understanding. Video-MME shows less improvement with increasing frames, suggesting that understanding fine spatial details is not as critical for that specific task.", "section": "3.1. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2504.10068/x5.png", "caption": "(a) Video-MME", "description": "The figure shows the performance of different video understanding models on the Video-MME benchmark.  Video-MME is a multiple-choice question answering benchmark that tests the ability of models to understand long videos by answering questions about the events depicted. The figure compares Mavors to other methods, including sparse sampling, dense sampling with low-resolution, and token compression, highlighting that Mavors achieves superior performance by balancing computational efficiency and retaining fine-grained spatio-temporal details.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.10068/x6.png", "caption": "(b) Dream1K", "description": "The figure shows the impact of frame resolution on the performance of two video MLLMs (Qwen2.5-VL-7B and Oryx-1.5-7B) for the DREAM-1K benchmark.  DREAM-1K is an open-ended video captioning task that requires detailed descriptions of the main events.  The graph shows that as the frame resolution increases, the performance of both models significantly improves on this benchmark. This indicates that high-resolution inputs are important for accurately capturing and describing detailed events within the videos.", "section": "3.1 Preliminaries"}, {"figure_path": "https://arxiv.org/html/2504.10068/x7.png", "caption": "Figure 3: The impact of the resolution of frames (64 frames).", "description": "This figure displays the performance of two video-based large language models (Qwen2.5-VL-7B and Oryx-1.5-7B) on two video understanding benchmarks (Video-MME and DREAM-1K) at different resolutions while maintaining a fixed number of frames (64).  The x-axis represents the resolution of the video frames, and the y-axis represents the corresponding benchmark scores. The plot visualizes how the models' performance changes with varying resolutions, illustrating the impact of resolution on the models' ability to capture spatial details. This is particularly important for tasks that require a nuanced understanding of visual details within the video.", "section": "3.1. Preliminaries"}, {"figure_path": "https://arxiv.org/html/2504.10068/extracted/6359558/figures/mm_training_stage_v4.png", "caption": "Figure 4: The architecture of Mavors.", "description": "This figure presents a detailed illustration of the Mavors architecture, highlighting its core components and workflow.  Mavors processes raw video content (or images treated as single-frame videos) in a two-tier approach.  First, an Intra-chunk Vision Encoder (IVE) extracts high-resolution spatial features from localized video segments using 3D convolutions and Vision Transformers. These features are then passed to an Inter-chunk Feature Aggregator (IFA) which uses temporal transformers with chunk-level rotary position encodings (C-ROPE) to establish temporal coherence across chunks. The unified visual representation is then projected to a unified dimension (via an MLP Projector) before entering the Language Model (LLM) for final understanding. The figure also shows the preprocessing steps for both video and image inputs, including sub-image division for images, and the use of chunk-level processing for videos.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.10068/x8.png", "caption": "Figure 5: The dynamic resolution strategy in Mavors.", "description": "Mavors processes images and videos with varying resolutions by dynamically resizing them to match the input resolution of the Vision Transformer (ViT) used in the model.  This maintains the original aspect ratio of the images or video frames, avoiding distortion from fixed-shape resizing. The figure illustrates the process, starting with the original image and proceeding through steps of thumbnail creation, optimal partitioning into sub-images, flattening of the sub-images into chunks, and finally duplicating the sub-images to form the final input. This dynamic resolution strategy allows Mavors to handle high-resolution images and videos efficiently while preserving spatial details and avoiding information loss.", "section": "3.5. Preprocessing"}, {"figure_path": "https://arxiv.org/html/2504.10068/x9.png", "caption": "Figure 6: Training paradigm of different stages.", "description": "This figure illustrates the multi-stage training process of the Mavors model. Stage 1 focuses on modality alignment, aligning the semantic spaces of the vision encoder and the LLM. Stage 1.5 enhances temporal understanding by training on various computer vision tasks. Stage 2 performs multitask instruction tuning, adapting the model to diverse multimodal tasks. Finally, Stage 3 refines the model using direct preference optimization (DPO), addressing issues like overly concise responses or inappropriate generation terminations.", "section": "3. Training Paradigm"}, {"figure_path": "https://arxiv.org/html/2504.10068/x10.png", "caption": "Figure 7: Performance with different numbers of frames in a video chunk.", "description": "This figure displays the impact of varying the number of frames within each video chunk on the performance of the Mavors model across six different benchmark tasks.  The x-axis represents the different benchmark tasks (MMMU, MathVista, CapsBench, Video-MME, VinoGround, and DREAM-1K), and the y-axis shows the corresponding performance scores.  Four different settings for the number of frames (F) per chunk are compared: F=4, F=8, F=16, and F=32.  The bars visually represent the performance for each task and frame setting, allowing for a direct comparison of how the choice of F affects performance on various benchmark tasks.", "section": "3.2 Overview of Mavors"}, {"figure_path": "https://arxiv.org/html/2504.10068/x11.png", "caption": "Figure 8: Performance with different token compression ratios.", "description": "This figure demonstrates the effect of different token compression ratios on the performance of the Mavors model.  The x-axis represents the compression ratio, ranging from 0% (no compression) to around 75%.  The y-axis shows the model's scores on various benchmark tasks (Video-MME, MLVU, DREAM-1K). The lines in the graph display how performance varies across different tasks under various compression levels. The graph shows that the performance on video question answering (Video-MME) is quite stable, even under high compression ratios. However, video captioning tasks (MLVU, DREAM-1K) show a more significant performance decline as the compression rate increases, suggesting that token compression may compromise the model's ability to generate detailed captions, particularly at higher compression rates.", "section": "5.3 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2504.10068/x12.png", "caption": "Figure 9: The dynamic of training losses across different stages for Mavors.", "description": "This figure shows the training loss curves for the Mavors model across its three training stages: Modality Alignment, Temporal Understanding Enhancement, and Instruction Tuning & DPO Training.  The x-axis represents the training steps, and the y-axis represents the loss value. The plot visualizes how the loss decreases over the course of training within each stage, indicating model convergence and performance improvement. Observing the loss curves provides insight into the effectiveness of each training stage in optimizing the model and suggests areas where training might be further refined.", "section": "4. Training Paradigm"}, {"figure_path": "https://arxiv.org/html/2504.10068/x13.png", "caption": "Figure 10: Comparison of generated video captions from Qwen2.5-VL-7B and Mavors-7B.", "description": "Figure 10 presents a qualitative comparison of video captioning results between Qwen2.5-VL-7B and Mavors-7B on a complex scene from the DREAM-1K benchmark.  The figure highlights Mavors-7B's superior ability to capture fine-grained details and generate accurate captions, whereas Qwen2.5-VL-7B misses crucial elements and generates less detailed, and in some cases, inaccurate descriptions.  Specifically, words in red indicate errors or omissions by Qwen2.5-VL-7B, while words in green highlight details correctly captured only by Mavors-7B.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.10068/x14.png", "caption": "Figure 11: Results of NIAH of Mavors with at most 60 video chunks.", "description": "This figure shows the results of the Needle in a Haystack (NIAH) test performed on the Mavors model.  The NIAH test evaluates the model's ability to accurately identify a target frame (the 'needle') when it is inserted into various video chunks of different lengths. The x-axis represents the index of the chunk where the needle is placed, while the y-axis represents the total number of chunks in the video. The heatmap illustrates the model's accuracy; darker colors indicate higher accuracy. The test demonstrates the model's ability to maintain accuracy even as the video length increases, up to 60 chunks (or 960 frames).", "section": "5.3. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2504.10068/x15.png", "caption": "Figure 12: Comparison of the generated image captions from Qwen2.5-VL-7B and Mavors-7B. The text in red contains wrong content, and the text in green marks the detailed descriptions only appear in Mavors.", "description": "Figure 12 presents a comparison of image captions generated by two different large language models (LLMs): Qwen2.5-VL-7B and Mavors-7B.  Two example images are shown, each with captions generated by both models.  The captions highlight Mavors-7B's superior performance in generating more detailed and accurate descriptions.  Specifically, text in red indicates errors or omissions in the Qwen2.5-VL-7B captions, while text in green highlights details that are only present in the Mavors-7B captions. This demonstrates Mavors-7B's improved ability to identify proper nouns, capture nuanced details, and understand human interactions and emotions within the images.", "section": "5. Showcases of Mavors in Image Captioning"}]