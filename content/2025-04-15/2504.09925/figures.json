[{"figure_path": "https://arxiv.org/html/2504.09925/x1.png", "caption": "Figure 1: Performance comparison of FUSION with leading MLLM models across 18 benchmark dimensions. With only 630 vision tokens, our model (FUSION-X) significantly outperforms Cambrian-1 and Florence-VL, achieving overall parity with LLaVA-OneVision, while maintaining a minimal performance gap with top-tier models such as InternVL2 and Qwen2VL. Furthermore, even when the number of vision tokens is reduced to 300, our model (FUSION-L) preserves 95% of its original performance, remaining on par with Florence-VL.", "description": "This figure presents a performance comparison among several leading Multimodal Large Language Models (MLLMs), including the authors' model FUSION, across 18 benchmark tasks.  The key finding is that FUSION, even with a limited number of vision tokens (630), substantially outperforms competitors like Cambrian-1 and Florence-VL. Notably, FUSION achieves similar results to LLaVA-OneVision and stays competitive with top performers (InternVL2 and Qwen2VL) using only 630 visual tokens.  Reducing the visual tokens further to 300 (FUSION-L) maintains almost the same level of performance as Florence-VL, demonstrating impressive efficiency.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.09925/x2.png", "caption": "Figure 2: Visualization of modality alignment and integration. At pixel-level, we compute attention maps between image regions and question-relevant keywords within the vision encoder. At space-level, we measure the cosine similarity between vision tokens projected into the LLM embedding space and corresponding keywords. At question-level, we visualize attention maps from question keywords to vision tokens during LLM decoding. The results indicate that our model achieves consistent and progressively enhanced cross-modal alignment throughout the processing pipeline.", "description": "Figure 2 visualizes the model's cross-modal alignment and integration process at three levels: pixel-level, space-level, and question-level.  At the pixel level, attention maps highlight the relationships between image regions and question keywords processed within the vision encoder, showing how the model focuses on relevant image parts.  At the space level, cosine similarity is calculated between vision tokens (after projection into the LLM's embedding space) and corresponding question keywords.  This demonstrates the semantic alignment between visual and textual features. Finally, at the question level, the attention maps between question keywords and vision tokens during LLM decoding are shown, illustrating the model's fine-grained semantic integration during the generation process.  The overall visualization showcases that the model progressively strengthens its cross-modal alignment through each stage of processing.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.09925/x3.png", "caption": "Figure 3: Illustration of our Text-Guided Unified Vision Encoding and Dual-Supervised Semantic Mapping Loss. Given an input image, the corresponding question is first projected into the vision feature space and processed jointly with the image. The extracted visual features are then mapped into the text space and fed into LLM. To ensure the reliability of the mapping MLP, we reconstruct the text and image tokens by reusing the encoded tokens and projecting them back into their original feature spaces, then compute the similarity between the reconstructed and raw tokens to encourage structural alignment between the two spaces.", "description": "This figure illustrates the FUSION model's architecture for text-guided vision encoding and dual-supervised semantic mapping.  An input image and its corresponding question are initially processed.  The question is first projected into the image's feature space (vision space), effectively incorporating textual context into the visual representation. This joint visual and textual information is then encoded. The resulting encoded visual features are subsequently mapped into the text feature space (text space) and fed to a large language model (LLM). To ensure that the feature mapping (MLP) is reliable, the model uses a reconstruction loss.  This loss compares the original image and text tokens with their reconstructed counterparts, which are created by using the encoded features and projecting them back to the original spaces. Minimizing this loss encourages structural alignment between the two feature spaces. This method promotes more accurate and effective integration of visual and linguistic features during the process of generating a response.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.09925/x4.png", "caption": "Figure 4: Illustration of our Context-Aware Recursive Alignment Decoding. For each set of question tokens (highlighted in yellow), we prepend a set of context-aware latent tokens (highlighted in green). Additional interaction layers are introduced between decoding layers, where vision tokens interact with both latent tokens and question tokens at a question-level granularity (e.g., Group 1, Group 2, \u2026).", "description": "This figure illustrates the Context-Aware Recursive Alignment Decoding process.  The decoding process uses interaction layers between decoding layers to recursively refine alignment of visual and textual information. Each set of question tokens (yellow) is prepended by context-aware latent tokens (green) to create context-aware query representations. These are then used in cross-attention mechanisms within the interaction layers.  This enables vision tokens to interact with both latent and question tokens at a granular level for each question (Group 1, Group 2, etc.), resulting in fine-grained, question-level semantic alignment.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2504.09925/x5.png", "caption": "Figure 5: Overview of our Text-Centered QA Dataset framework. Our approach shifts the focus from visual content to textual richness by leveraging high-quality captions, enriching them with LLMs, and using them as the foundation for both image generation via diffusion models and diverse QA pair construction.", "description": "Figure 5 illustrates the framework for creating a text-centered QA dataset.  The approach prioritizes detailed textual descriptions over visual content. High-quality captions are first selected and enhanced with LLMs to generate rich, nuanced descriptions. These enhanced descriptions then serve as prompts for a diffusion model to generate corresponding images.  The same descriptions are also used by the LLM to create a diverse set of QA pairs. This method focuses on textual richness to improve the model's understanding of images and generate diverse QA data.", "section": "3.4. Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x6.png", "caption": "(a) Model performance under varying numbers of latent vision tokens.", "description": "This figure displays a graph illustrating how the model's performance changes across various vision-centric benchmarks as the number of latent vision tokens is altered.  It showcases the model's robustness across different tasks, even with varying numbers of latent tokens. This is useful in understanding the model's efficiency and effectiveness under different computational resource constraints.", "section": "5. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.09925/x7.png", "caption": "(b) Model performance under varying numbers of global vision tokens.", "description": "This figure displays the performance of the FUSION model across various benchmarks as the number of global vision tokens used is altered.  It compares the model's performance with other leading models (Cambrian-1, Florence-VL) to show the impact of changing the number of visual tokens.  The graph likely illustrates that FUSION maintains strong performance even with a significantly reduced number of visual tokens compared to competitors. ", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.09925/x11.png", "caption": "Figure 6: Comparative analysis of performance with different latent and global vision token configurations.", "description": "This figure displays comparative performance results across different model configurations, varying the number of latent and global vision tokens.  It shows how the model's performance changes on several benchmark tasks (General, Vision-centric, Knowledge-Based, and OCR & Chart) as the number of visual tokens (both latent and global) is increased. This allows for an assessment of the model's efficiency and robustness concerning visual token usage.", "section": "5. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.09925/x12.png", "caption": "Figure 7: FUSION-10M and FUSION-12M. We collected a total of 10M samples for pretraining (Stage 1) and 12M samples for fine-tuning (Stage 1.5 and Stage 2). The circle illustrates the data distribution, with the right side of the circle showing all the utilized data and their respective proportions.", "description": "This figure shows the composition of the datasets used to train the FUSION model.  The left circle represents the 10 million samples used in the pretraining stage (Stage 1), and the right circle shows the 12 million samples used in the fine-tuning stages (Stage 1.5 and Stage 2). Each circle is divided into segments representing different sources of data, including publicly available datasets (like LLaVA, PixelProse, ShareGPT4V), existing benchmarks (e.g., URSA), and synthetic data generated by the authors. The size of each segment is proportional to the amount of data from that source.", "section": "A. Data Collection"}, {"figure_path": "https://arxiv.org/html/2504.09925/x13.png", "caption": "Figure 8: Comparison of Modality Alignment with traditional MLLMs. We conduct a comparative analysis of modality alignment across three different levels using FUSION, LLaVA, and LLaVA-NeXT. To ensure a fair evaluation, we adopt consistent visualization and augmentation strategies across all models.", "description": "This figure compares the modality alignment capabilities of FUSION, LLaVA, and LLaVA-NeXT across pixel, space, and question levels.  Heatmaps visualize the attention mechanisms of each model, revealing how effectively each model integrates visual and textual information during different processing stages. Consistent visualization and data augmentation techniques are applied to all three models to ensure a fair and objective comparison.  The analysis demonstrates the superior cross-modal alignment achieved by FUSION across all three levels compared to the baseline models.", "section": "C. Comparison and Deeper Exploration of Model Architectures"}, {"figure_path": "https://arxiv.org/html/2504.09925/x14.png", "caption": "Figure 9: Illustration of our model\u2019s multiturn dialogue", "description": "This figure showcases the model's ability to engage in multi-turn conversations about images.  It demonstrates the model's capacity to answer diverse questions about image content,  incorporating detailed visual and contextual information in its responses. The model processes complex prompts and generates detailed, coherent answers over multiple turns, showcasing its conversational understanding.  The images used are examples from different scenarios.", "section": "5. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.09925/x15.png", "caption": "Figure 10: Illustration of our SynthConvShort", "description": "Figure 10 shows examples from the SynthConvShort dataset.  The dataset contains short, multi-turn conversations between a human and an AI. The human provides questions about images, and the AI provides concise, factual answers directly based on the image content.  The questions cover a wide range of topics including object identification, color, counting, location, and simple comparisons between images. Each QA pair shows an example image, the human's question, and the AI's answer.", "section": "3.4. Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x16.png", "caption": "Figure 11: Illustration of our SynthConvLong", "description": "This figure showcases an example of a multi-turn conversation generated using the SynthConvLong method.  The conversation centers around a photograph of Taylor Swift performing on stage. The AI responds to several user questions, demonstrating its ability to provide detailed descriptions of the image's content and contextual information.  The questions cover aspects such as Taylor Swift's attire, the stage setting, and the elements displayed on the background screen. The AI's responses are comprehensive and align directly with what is visible in the image.", "section": "3.4 Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x17.png", "caption": "Figure 12: Illustration of our SynthContrastShort", "description": "Figure 12 presents two image pairs from the SynthContrastShort dataset, highlighting the differences between them. Each pair shows subtle variations in terms of visual elements like color schemes, object counts, and scene details.  The top row shows images of industrial power plants, the bottom row shows kitchen scenes. For both pairs, the left image has warmer colors and simpler composition, while the right image features cooler tones and more elements.", "section": "3.4. Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x18.png", "caption": "Figure 13: Illustration of our SynthContrastLong", "description": "This figure showcases a detailed comparison of two images generated using the SynthContrastLong method. It highlights the differences between the images by comparing visual elements such as object types (partially constructed boat vs. completed boat), their quantities, arrangement, colors (silver and bright orange vs. bronze and faded denim), and sizes.  The images are accompanied by a set of questions and answers that emphasize the visual contrast between them, including questions about the number of objects, their relative positions, colors, and sizes.", "section": "3.4. Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x19.png", "caption": "Figure 14: Illustration of our SynthMultiChoice", "description": "Figure 14 shows examples from the Synthesized Language-Driven QA Dataset, specifically showcasing the SynthMultiChoice category.  It presents several image-question-answer triplets where the questions are multiple choice. The questions target various aspects of the images, including object identification, counting, activity recognition, and spatial reasoning. The answers provided are concise and directly relate to the visual information in the corresponding image.", "section": "3.4. Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x20.png", "caption": "Figure 15: Illustration of our SynthColor", "description": "The image showcases a bedroom with soft white windows, warm beige walls, and rich wood floors.  This image exemplifies the use of color in scene design to evoke a particular mood or atmosphere, which is a key aspect of the SynthColor dataset discussed in the paper. The emphasis on color selection and its impact on the overall scene, rather than individual objects, is consistent with the paper's focus on a language-driven approach to data generation.", "section": "3.4. Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x21.png", "caption": "Figure 16: Illustration of our SynthCount", "description": "Figure 16 shows examples from the SynthCount dataset.  The dataset focuses on generating images with varying counts of objects. The caption's brevity is intended to reflect the simplicity of the core concept. The figure provides visual examples of images generated with different counts of objects (e.g., seagulls, flowers), showcasing the variation in object quantities in the generated images.", "section": "3.4 Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x22.png", "caption": "Figure 17: Illustration of our SynthScene", "description": "Figure 17 shows examples from the Synthesized Language-Driven QA dataset. Specifically, it showcases images generated based on text prompts related to various scenes (vintage lantern and Eiffel Tower, Mount Fuji and snow machines, etc.). These examples highlight the ability of the language-driven approach to generate high-quality, diverse images grounded in rich textual descriptions.", "section": "3.4 Synthesized Language-Driven QA Dataset"}, {"figure_path": "https://arxiv.org/html/2504.09925/x23.png", "caption": "Figure 18: Illustration of our SynthSpatial", "description": "Figure 18 shows two example image-question pairs generated using the SynthSpatial data synthesis method described in the paper.  SynthSpatial focuses on creating high-quality image-text pairs where the spatial relationships between objects are accurately reflected.  The top example shows a woman standing behind a counter with glasses and a bottle visible. The question probes the concise description of the scene. The bottom example showcases a home in the foreground, a car parked to its left, and a teddy bear on the seat; the associated question requests a concise image description.  These examples illustrate the dataset's ability to generate images and questions centered around spatial relationships.", "section": "3.4 Synthesized Language-Driven QA Dataset"}]