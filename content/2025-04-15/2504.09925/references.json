{"references": [{"fullname_first_author": "Marah Abdin", "paper_title": "Phi-3 technical report: A highly capable language model locally on your phone", "publication_date": "2024-04-14", "reason": "This paper details the language model architecture used in FUSION, highlighting its local processing capabilities."}, {"fullname_first_author": "Shuai Bai", "paper_title": "Qwen2.5-vl technical report", "publication_date": "2025-01-02", "reason": "This paper details the language model Qwen2.5-vl and it's technical report for vision language modeling capabilities."}, {"fullname_first_author": "Jiuhai Chen", "paper_title": "Florence-vl: Enhancing vision-language models with generative vision encoder and depth-breadth fusion", "publication_date": "2024-01-08", "reason": "This paper introduces Florence-VL, a vision-language model using a generative vision encoder, serving as a key comparison point for FUSION's performance."}, {"fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4v: Improving large multi-modal models with better captions", "publication_date": "2023-11-12", "reason": "This paper discusses ShareGPT4V, improving large multi-modal models with better captions, which contributed to the dataset used for FUSION's training."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This paper presents LLaVA, a visual instruction tuning approach, and it is used as a baseline for comparison in the component-wise analysis of modality fusion."}]}