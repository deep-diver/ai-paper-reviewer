[{"heading_title": "LLM-SRBench", "details": {"summary": "**LLM-SRBench** appears to be a novel benchmark designed to rigorously evaluate the equation discovery capabilities of Large Language Models (LLMs). This is crucial because existing benchmarks often rely on common equations susceptible to memorization, leading to inflated performance metrics. LLM-SRBench aims to address this by constructing problem sets that avoid trivial recitation, yet leverage the embedded scientific priors within LLMs, simulating conditions closer to actual scientific discovery. The benchmark incorporates two main categories of problems: **LSR-Transform**, focusing on transforming known scientific problems into less common mathematical representations, and **LSR-Synth**, introducing synthetic, discovery-driven problems requiring data-driven reasoning. This dual approach ensures a comprehensive assessment, targeting both reasoning beyond memorized forms and the ability to uncover new relationships within data. By providing a challenging and realistic evaluation environment, LLM-SRBench has the potential to significantly advance the field of LLM-based scientific equation discovery, fostering the development of innovative methods capable of true scientific insight."}}, {"heading_title": "No Memorization", "details": {"summary": "The paper addresses the critical issue of **memorization by Large Language Models (LLMs)** in scientific equation discovery. Existing benchmarks often contain equations that LLMs may have already encountered during their training, leading to artificially inflated performance metrics that don't reflect true discovery capabilities. LLM-SRBench, a new benchmark introduced in this work, aims to mitigate this by including two main categories of problems designed to **challenge LLMs' reasoning abilities** beyond simple recall. LSR-Transform transforms common physical models into less familiar mathematical representations, while LSR-Synth introduces synthetic, discovery-driven problems requiring data-driven reasoning. This approach ensures a more rigorous evaluation of LLMs' ability to generate scientific hypotheses, and to discern true knowledge rather than rote learning, ensuring that the focus is on genuine understanding and novel scientific insight instead of **reliance on pre-existing knowledge**. In essence, LLM-SRBench is intended to **prevent LLMs from simply regurgitating** known equations."}}, {"heading_title": "LSR-Transform", "details": {"summary": "The LSR-Transform category in LLM-SRBench is designed to **evaluate if LLM-based methods can discover equations in less common forms**, avoiding reliance on memorization. It leverages a **transformation of existing problems**, like Feynman equations, into different mathematical representations of the same physical problem. This tests LLMs' data-driven scientific reasoning capabilities. By switching input-output variables and altering equations symbolically, the benchmark creates less familiar mathematical forms. This **challenges LLMs to generalize** beyond direct recall of common equations. Solvability and complexity constraints ensure feasibility and relevance. The goal is to assess whether LLMs can validate discoveries from **non-trivial data transformations**, rather than discovering new equations."}}, {"heading_title": "Data Fidelity", "details": {"summary": "The 'Data Fidelity' section focuses on evaluating how well discovered equations fit the observed data, going beyond just finding symbolic matches. **Accuracy to Tolerance (Acc)** and **Normalized Mean Squared Error (NMSE)** are key metrics, offering complementary perspectives. Acc assesses whether predictions stay within a defined error range, crucial for situations demanding consistent precision. NMSE provides a continuous measure of overall prediction quality, scaled to enable comparisons. Critically, 'Data Fidelity' incorporates **out-of-distribution (OOD) evaluation**, testing extrapolation ability. This is vital as scientific laws must generalize beyond training data. The difference in performance between in-domain and OOD sets (\u0394NMSE and \u0394Acc) then reveals how well the discovered equations actually generalize. This more thorough approach offers valuable insights into overall data fitness."}}, {"heading_title": "OOD Generalize", "details": {"summary": "This section emphasizes **generalization to unseen data** as a core requirement for scientific laws, a feature often neglected in equation discovery benchmarks. By introducing **explicit out-of-domain (OOD) assessment**, the study addresses this gap. The observed performance degradation in OOD settings underscores the importance of robust methods that truly capture underlying principles. The analysis also reveals **domain-specific variations in generalization**, with challenges differing across scientific fields, which highlights the need for future research to develop more robust approaches for different scientific disciplines."}}]