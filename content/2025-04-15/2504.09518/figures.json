[{"figure_path": "https://arxiv.org/html/2504.09518/x1.png", "caption": "Figure 1. Conceptual homepage figure for 3D CoCa, highlighting its architecture (left) and performance (right). Left: The 3D CoCa model unifies contrastive learning and multimodal captioning in one framework. Right:Radar chart comparison of 3D CoCa and previous methods Scan2Cap\u00a0(Chen et\u00a0al., 2021a), 3DJCG\u00a0(Cai et\u00a0al., 2022), 3D-VLP\u00a0(Zhang et\u00a0al., 2024a), Vote2Cap-DETR\u00a0(Chen et\u00a0al., 2023b), Vote2Cap-DETR++\u00a0(Chen et\u00a0al., 2024b) on the ScanRefer\u00a0(Chen et\u00a0al., 2020a) benchmark.", "description": "Figure 1 is a two-part figure illustrating the 3D CoCa model. The left panel shows a schematic diagram of the model's architecture, highlighting its unified framework that integrates contrastive learning and multimodal captioning.  The right panel presents a radar chart comparing the performance of 3D CoCa against several other state-of-the-art 3D captioning methods (Scan2Cap, 3DJCG, 3D-VLP, Vote2Cap-DETR, and Vote2Cap-DETR++) on the ScanRefer benchmark dataset.  The radar chart visually represents the relative strengths and weaknesses of each model across multiple evaluation metrics.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.09518/x2.png", "caption": "Figure 2. Illustration of a multi-modal Transformer architecture for 3D vision-language understanding. The input point cloud and textual description are processed by CLIP Vision and Text Encoders, respectively. Cross-attention mechanisms fuse these features within a Multi-Modal Decoder, enabling the generation of descriptive captions. The model training is guided by contrastive and captioning losses, promoting effective alignment between visual and textual modalities.", "description": "Figure 2 illustrates the 3D CoCa model architecture, a multi-modal Transformer designed for 3D vision-language understanding.  The process begins with a point cloud and a textual description as inputs.  These are separately processed by CLIP Vision and Text Encoders to extract relevant features.  A cross-attention mechanism within a Multi-Modal Decoder then integrates these visual and textual features. This fusion allows the model to generate descriptive captions that accurately reflect the 3D scene.  The model's training is optimized using both contrastive and captioning loss functions to ensure effective alignment between the visual and textual modalities.", "section": "3 The Proposed Method"}, {"figure_path": "https://arxiv.org/html/2504.09518/x3.png", "caption": "Figure 3. A visual comparison on the ScanRefer\u00a0(Chen et\u00a0al., 2020a) dataset showcasing indoor scenes described by Vote2Cap-DETR++\u00a0(Chen et\u00a0al., 2024b), our method (Ours), and the ground truth (GT), highlighting differences in descriptive accuracy and style.", "description": "Figure 3 presents a qualitative comparison of 3D scene descriptions generated by three different methods: Vote2Cap-DETR++, the proposed 3D CoCa model, and the ground truth.  The figure showcases several indoor scenes from the ScanRefer dataset, with each scene accompanied by three captions: one generated by Vote2Cap-DETR++, one generated by the authors' model (labeled \"Ours\"), and the ground truth caption. This visual comparison helps highlight the differences in descriptive accuracy and style between the models and underscores the ability of the proposed 3D CoCa method to generate more comprehensive and accurate descriptions than the baseline method.", "section": "4.5 Qualitative Results"}]