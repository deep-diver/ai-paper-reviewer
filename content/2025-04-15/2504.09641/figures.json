[{"figure_path": "https://arxiv.org/html/2504.09641/x1.png", "caption": "Figure 1: A case of TinyLLaVA-Video-R1 on video understanding data, sourced from MVBench. The model demonstrates the ability to perceive video scenes and analyze options, while also exhibiting reflective and backtracking behavior (highlighted in blue).", "description": "This figure showcases TinyLLaVA-Video-R1's performance on a video understanding task from the MVBench dataset.  The example demonstrates the model's ability to not only correctly identify the action in a video (a person picking up an object), but also to articulate its reasoning process. The model's thought process is presented step-by-step, showing its ability to analyze the visual scene, consider multiple options, and ultimately arrive at a confident and accurate answer.  The blue highlighting emphasizes the model's reflective and backtracking behavior, indicating a more advanced reasoning process than simply recognizing objects and actions; it actively evaluates its own thinking and adjusts its approach if necessary.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2504.09641/x2.png", "caption": "Figure 2: A case of TinyLLaVA-Video-R1 on video reasoning data, sourced from MMVU. The model demonstrates comprehensive video content understanding and the capability to derive correct answers through analytical reasoning.", "description": "Figure 2 presents an example of TinyLLaVA-Video-R1's performance on a video reasoning task from the MMVU dataset.  The figure showcases the model's ability to not only understand the visual content of the video but also to analyze the provided options and arrive at the correct answer through a step-by-step reasoning process. This demonstrates the model's advanced capabilities in comprehending video content and using analytical reasoning to solve problems, which is a key contribution of the TinyLLaVA-Video-R1 model.", "section": "3 Methods"}, {"figure_path": "https://arxiv.org/html/2504.09641/x3.png", "caption": "Figure 3: Cases of \u201daha moment\u201d, where the model demonstrates reflection and backtracking during its reasoning process (highlighted in blue). The cases are from MVBench and MMVU respectively.", "description": "This figure showcases two instances where the TinyLLaVA-Video-R1 model exhibits 'aha moment' behavior.  'Aha moment' refers to the model's ability to reflect on its initial reasoning process and potentially revise its answer after further deliberation or identifying flaws in its initial approach.  The examples highlight the model's ability to backtrack and reconsider its thought process before arriving at a final answer. The left example is from the MVBench dataset, while the right one is from MMVU.", "section": "4.2 Main Results and Aha Moment"}, {"figure_path": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/completion_length_smoothed.png", "caption": "(a) Evolution in completion length.", "description": "The figure shows the evolution of the response length during the training process of the TinyLLaVA-Video-R1 model.  The x-axis represents the training step, and the y-axis shows the average length of the model's generated responses. The plot demonstrates a general upward trend, indicating that the model learns to produce longer responses as it trains.  This is a key observation because longer responses often correlate with more comprehensive and detailed reasoning.  The plot might also include comparisons between different reward strategies or training settings to show the impact on response length.", "section": "4.2 Main Results and Aha Moment"}, {"figure_path": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/accuracy_reward_smoothed.png", "caption": "(b) Evolution in accuracy reward.", "description": "This figure shows a graph illustrating the change in accuracy reward over the course of training the TinyLLaVA-Video-R1 model. The x-axis represents the training step, and the y-axis represents the accuracy reward. The graph likely shows an upward trend, indicating that the model's performance improves as training progresses. The plot might include both raw data points and a smoothed curve to better visualize the overall trend. This improvement likely reflects the model's increasing ability to correctly answer video reasoning questions.", "section": "4.2 Main Results and Aha Moment"}, {"figure_path": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/format_reward_smoothed.png", "caption": "(c) Evolution in format reward.", "description": "This figure shows the evolution of the format reward during the training process of the TinyLLaVA-Video-R1 model. The format reward is a component of the overall reward function used to incentivize the model to generate responses that adhere to a specific format, including the use of <think> and </think> tags for the reasoning process and <answer> and </answer> tags for the final answer. The graph plots the format reward against the training step, showing how the reward changes over time.  A smoothed version of the data is shown alongside the raw data, which helps to reveal the overall trend and reduce the noise.", "section": "4.2 Main Results and Aha Moment"}, {"figure_path": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/comparison_length.png", "caption": "Figure 4: Evolution in key metrics during the training of TinyLLaVA-Video-R1. Under our reward rule settings, both the response length and rewards of TinyLLaVA-Video-R1 gradually increased during training.", "description": "Figure 4 presents a detailed view of the training progress of the TinyLLaVA-Video-R1 model, focusing on three key metrics: response length, accuracy reward, and format reward.  The graphs visually demonstrate how these metrics evolve over training steps.  The response length steadily increases, indicating the model is generating more comprehensive answers.  Similarly, both the accuracy and format rewards exhibit upward trends, suggesting the model is not only providing correct answers but also adhering to the desired response format. The smooth curves result from applying exponential smoothing to the raw data, highlighting the general trends.  Overall, the figure confirms the model's gradual improvement in its ability to generate longer, accurate, and well-formatted responses during training, confirming the effectiveness of the reward rule settings.", "section": "4.2 Main Results and Aha Moment"}, {"figure_path": "https://arxiv.org/html/2504.09641/extracted/6357976/sec/fig/ablation.png", "caption": "Figure 5: The variation in response length during training under different settings.", "description": "Figure 5 illustrates the impact of different reward mechanisms on the length of model responses generated during training.  The x-axis represents the training step, while the y-axis shows the response length. Three lines are plotted, each representing a distinct reward configuration: 1) using only the format reward; 2) adding a continuous length reward to the format reward; and 3) incorporating an additional penalty for incorrect answers along with the format and length rewards. The figure demonstrates how the addition of these components progressively influences the length of model responses over the course of training.", "section": "4.3 Ablation Study"}]