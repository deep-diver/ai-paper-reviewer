{"importance": "This paper is crucial for researchers constrained by computational resources, demonstrating effective video reasoning with smaller models. It offers practical insights and a reproducible setup (**TinyLLaVA-Video-R1**), fostering innovation in resource-efficient AI and opening avenues for exploring reasoning in compact multimodal systems.", "summary": "TinyLLaVA-Video-R1: Smaller LMMs achieve boosted video reasoning via reinforcement learning and 'aha moments'!", "takeaways": ["Small-scale models can exhibit strong reasoning capabilities after reinforcement learning.", "Reinforcement learning enhances both reasoning and thinking abilities in video-QA tasks.", "The model exhibits 'aha moments,' demonstrating reflection and backtracking in its reasoning process."], "tldr": "Improving reasoning in Large Multimodal Models(LMMs) via reinforcement learning shows progress, but often relies on large-scale models and intensive datasets, limiting accessibility for researchers with fewer resources. Existing video reasoning models struggle with reasoning-intensive data scarcity. Moreover, it is equally meaningful to have models explain reasoning on general question-answering datasets.\n\nThis paper introduces **TinyLLaVA-Video-R1**, a small-scale video reasoning model, based on TinyLLaVA-Video. It enhances reasoning via reinforcement learning on general Video-QA datasets, demonstrating improved thinking and reasoning with emergent \"aha moments\". The authors also share experimental findings for video reasoning exploration in small-scale models.", "affiliation": "Beihang University", "categories": {"main_category": "Computer Vision", "sub_category": "Video Understanding"}, "podcast_path": "2504.09641/podcast.wav"}