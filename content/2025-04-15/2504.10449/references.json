{"references": [{"fullname_first_author": "Junxiong Wang", "paper_title": "The mamba in the llama: Distilling and accelerating hybrid models", "publication_date": "2024-08-15", "reason": "This work directly inspires the current paper's distillation approach of a Transformer model into a Mamba model, forming the basis of their M1 model."}, {"fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "publication_date": "2023-12-01", "reason": "Mamba architecture is used as the building block of the proposed model and this is the original paper introducing the Mamba architecture."}, {"fullname_first_author": "DeepSeek-AI", "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning", "publication_date": "2025-01-01", "reason": "This work presents a model that the paper compares against, also serving as inspiration for using reinforcement learning to enhance reasoning capabilities."}, {"fullname_first_author": "Charlie Snell", "paper_title": "Scaling llm test-time compute optimally can be more effective than scaling model parameters", "publication_date": "2024-08-01", "reason": "This paper explores the idea of improving performance with more compute at test time, a key idea that M1 leverages with efficient inference."}, {"fullname_first_author": "Jason Wei", "paper_title": "Self-consistency improves chain of thought reasoning in language models", "publication_date": "2023-03-11", "reason": "This paper shows the effectiveness of self-consistency in improving chain-of-thought reasoning, and the current paper employs self-consistency voting at test time."}]}