{"importance": "This paper is important for researchers because it introduces **a new approach to scaling test-time compute for reasoning models**, potentially leading to more efficient and accurate solutions. The **hybrid Mamba reasoning model** offers a compelling alternative to transformer-based models and **paves the way for future research in efficient reasoning architectures**.", "summary": "M1: A scalable Mamba reasoning model that matches transformer performance with 3x faster inference, enabling better test-time compute scaling.", "takeaways": ["M1, a hybrid linear RNN reasoning model built on the Mamba architecture, achieves comparable performance to state-of-the-art transformer models on math benchmarks.", "M1 offers a 3x speedup in inference compared to transformer models of similar size, demonstrating its potential for efficient large-batch generation.", "The improved efficiency of M1 enables more effective test-time compute scaling through self-consistency and longer sequence generation."], "tldr": "Large language models (LLMs) improve reasoning by scaling test-time computation, but transformers are limited by quadratic complexity and memory. The paper addresses these limitations by introducing a novel reasoning model, **M1**, based on the Mamba architecture, enabling memory-efficient inference. M1 leverages distillation from existing models and reinforcement learning to enhance capabilities. The results demonstrate the model not only outperforms existing models but also matches the state-of-the-art transformer while significantly reducing cost. \n\nThe core contribution lies in demonstrating **strong reasoning models** can be derived by transferring knowledge from large transformer models, using the **Mamba architecture**. Through distillation, supervised fine-tuning, and reinforcement learning, M1 achieves a 3x speedup compared to same-sized transformers. The model also shows a higher throughput enables increased accuracy when using methods such as self-consistency voting. Overall, the model has a potential to scale test-time generation using long chain of thought.", "affiliation": "TogetherAI", "categories": {"main_category": "Machine Learning", "sub_category": "Deep Learning"}, "podcast_path": "2504.10449/podcast.wav"}