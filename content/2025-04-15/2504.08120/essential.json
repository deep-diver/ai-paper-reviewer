{"importance": "This paper pioneers assessing reasoning LLMs for NLG evaluation, revealing architecture-specific performance. It highlights the **critical role of architecture and implementation**, paving the way for targeted reasoning-enhanced evaluation approaches and addressing limitations in current NLG metrics. Offers **new avenues for enhancing evaluation quality**.", "summary": "Reasoning LLMs: Do they enhance MT & summarization evaluation? It depends on the model!", "takeaways": ["The effectiveness of reasoning in LLMs for NLG evaluation is highly architecture-dependent.", "Distilling reasoning capabilities in smaller LLMs for NLG evaluation leads to a substantial performance drop.", "Summarization and machine translation evaluation may require distinct reasoning strategies."], "tldr": "Reasoning-enabled large language models (LLMs) have shown promise in complex tasks, yet their effectiveness in evaluating natural language generation (NLG) remains unclear. This study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI 03) with non-reasoning counterparts in machine translation (MT) and text summarization (TS) evaluation tasks. The team evaluated eight models across architectures, using WMT23 and SummEval benchmarks, to understand the benefits of reasoning. \n\nThe paper uncovers that the advantages of reasoning capabilities depend heavily on the model and task. OpenAI 03-mini models improved with reasoning intensity, DeepSeek-R1 generally underperformed its non-reasoning variant. Distillation maintained performance in medium-sized models but degraded in smaller ones. The research provides insight on when to use reasoning LLMs for NLG evaluation and offers practical guidelines.", "affiliation": "University of Mannheim", "categories": {"main_category": "Natural Language Processing", "sub_category": "Machine Translation"}, "podcast_path": "2504.08120/podcast.wav"}