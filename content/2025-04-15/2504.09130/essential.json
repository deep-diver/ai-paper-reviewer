{"importance": "This paper introduces a novel framework which enhances reasoning in vision-language models, showing SOTA performance in spatial reasoning tasks. This innovation opens new avenues for test-time scaling in multimodal learning.", "summary": "VisuoThink: Multimodal Tree Search enhances LVLM reasoning by dynamically integrating visual-textual cues, achieving state-of-the-art spatial reasoning.", "takeaways": ["Introduces a multimodal tree search for enhanced reasoning.", "Demonstrates significant gains in geometry and spatial reasoning.", "Extends test-time scaling to the visual domain."], "tldr": "Large Vision-Language Models (LVLMs) have limitations in complex reasoning tasks, particularly those requiring visual aids. Current methods don't capture the interleaved nature of human visual-verbal reasoning, creating a \"visual blind spot.\" They either rely on textual reasoning or rudimentary visual assistance, failing to fully utilize visual information throughout the reasoning process. This gap limits their ability to solve geometric and spatial problems effectively.\n\nTo address these limitations, this work introduces **VisuoThink**, a novel framework that integrates visual and linguistic domains for multimodal slow thinking. It enables progressive visual-textual reasoning and incorporates test-time scaling through look-ahead tree search. **VisuoThink** enhances reasoning capabilities by enabling progressive visual-textual reasoning and test-time scaling. Experiments show significant gains in reasoning, achieving state-of-the-art performance in geometry and spatial reasoning tasks.", "affiliation": "Fudan University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2504.09130/podcast.wav"}