[{"figure_path": "https://arxiv.org/html/2504.09643/extracted/6358040/2024-10-10_15.21.45.jpg", "caption": "Figure 1: Iterative Self-Training Workflow for RewardRanker. The process starts with supervised fine-tuning (A), followed by training the RewardRanker model (B). A PPO-based model (C) is then trained, generating new examples that are evaluated to produce both positive and hard negative samples (D). These samples are fed back into the process for further refinement and retraining (E), completing the iterative loop.", "description": "This figure illustrates the iterative self-training workflow of the RewardRanker model.  The process begins with supervised fine-tuning of a code generation model (A). This model is then used to generate code samples, which are reranked by the RewardRanker model (B) trained using proximal policy optimization (PPO).  A PPO-based model (C) is then trained, producing new code samples that are evaluated to identify positive and difficult negative examples (D). These new samples are then added to the training dataset (E) which allows for further refinement and retraining of the RewardRanker model, thus completing the iterative refinement loop.", "section": "1 Introduction"}]