{"importance": "This paper is crucial for GUI agent researchers as it **tackles the data scarcity problem**, offering a practical mid-training approach using readily available datasets. It **reveals effective knowledge transfer strategies** and **optimizes performance**, opening new avenues for cross-domain learning in GUI automation.", "summary": "GUI agent learns to generalize tasks better by training on reasoning-intensive tasks, breaking the data barrier for improved performance.", "takeaways": ["Task generalization is highly effective for GUI agents, with multimodal math reasoning leading to substantial improvements.", "GUI perception data has a comparatively limited impact on final performance.", "Optimized mixture datasets yield significant absolute performance gains on WebArena and AndroidWorld."], "tldr": "GUI agents automate digital tasks but are limited by data scarcity. VLMs are trained on data-rich, reasoning-intensive tasks during a mid-training phase. Tasks include GUI perception, multimodal reasoning, and textual reasoning. Experiments across 11 tasks showed task generalization is highly effective, with multimodal math reasoning boosting AndroidWorld by 6.3%. Text-only math data improves GUI web agent performance, showing cross-modal generalization from text to visual domains. \n\nThis paper introduces a mid-training stage to enhance agentic capabilities before GUI-specific fine-tuning. The most effective mid-training tasks are identified and optimized mixture datasets are curated. It achieves performance gains of 8.0% on WebArena and 12.2% on AndroidWorld. Contrary to prior assumptions, GUI perception data has a limited impact. GUIMid, a 300k dataset, combines best-performing domains. GUIMid achieves SOTA on AndroidWorld in pure-visual settings and improves Qwen2-VL to GPT4-o levels.", "affiliation": "Zhejiang University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2504.10127/podcast.wav"}