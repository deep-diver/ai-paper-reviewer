[{"figure_path": "https://arxiv.org/html/2504.10127/x1.png", "caption": "Figure 1: Overview of the mid-training and fine-tuning process. Left: We first train the GUI agent on mid-training data, primarily from non-GUI domains, to investigate whether the enhanced capabilities can generalize to GUI agent tasks; Right: We perform post-training on GUI trajectory data.", "description": "This figure illustrates the two-stage training process for GUI agents. The left side shows the mid-training phase, where the model is trained on data from non-GUI domains (like multimodal reasoning, text-based tasks, etc.) to enhance its general capabilities.  These enhanced capabilities are then tested for generalization to GUI tasks. The right side depicts the fine-tuning phase, where the model is further trained on GUI trajectory data for specific GUI task adaptation and improved performance. This approach aims to address the limited availability of high-quality GUI trajectory data by leveraging abundant data from related areas.", "section": "3 Breaking the Data Barrier via Mid-Training"}, {"figure_path": "https://arxiv.org/html/2504.10127/x2.png", "caption": "Figure 2: A case illustrating the performance of the Model w/o Mid-Training and the Model w/ Mid-Training under the same task. The middle text shows the model\u2019s thought process and the action taken, while the screenshots on the left and right represent the screen states before and after the action, respectively. The model with middle training (bottom) successfully reflects on errors and generates correct actions from error states, while the model without (top) middle training fails to recover from such states.", "description": "This figure showcases a comparative analysis of two models performing the same task: one trained with mid-training (Model w/ Mid-Training) and one without (Model w/o Mid-Training). The central text details the thought process and chosen actions for each model.  The left and right screenshots display the screen's state before and after the action, respectively. The key takeaway is that the model incorporating mid-training demonstrates error recovery and corrective action generation, unlike the model trained without mid-training, which fails to recover from its initial errors.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2504.10127/x3.png", "caption": "Figure 3: Performance of models trained on GUIMidwith different scales.", "description": "This figure shows the performance of models trained on the GUIMid dataset with varying scales of training data.  The left graph displays the success rate and progress rate on the AndroidWorld benchmark, while the right graph shows the same metrics on the WebArena benchmark. The x-axis represents the amount of training data used (in thousands), while the y-axis represents the success rate (left) and progress rate (right). The results demonstrate the effectiveness of the GUIMid dataset and the scaling behavior with respect to the amount of training data.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.10127/extracted/6359059/figures/loss_curves_grid.png", "caption": "Figure 4: Comparison of training loss between two training strategies: (a) and (c) show the mixture of GUI trajectory data during mid-training, while (b) and (d) are not.", "description": "Figure 4 illustrates the effect of incorporating GUI trajectory data into the mid-training phase on the training loss.  Panels (a) and (c) depict the training loss curves when GUI trajectory data is mixed with other mid-training data, demonstrating smoother learning dynamics. In contrast, panels (b) and (d) show the training loss when no GUI trajectory data is mixed in, resulting in less stable training with potential issues like sharp fluctuations or even gradient vanishing. This comparison highlights the importance of data mixing for mitigating the domain gap between mid-training and fine-tuning data, thereby enhancing model stability during training.", "section": "4.4 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.10127/extracted/6359059/figures/annoate_tools_final.png", "caption": "Figure 5: The annotation UI for VisualWebArena.", "description": "Figure 5 shows the user interface (UI) for annotating data in the VisualWebArena dataset.  This UI allows annotators to specify actions to be taken by a web agent within the context of a given task. It displays the current webpage as a screenshot, along with tools to select elements on the page and define actions (such as clicks, typing, scrolling). The interface also includes fields for entering details about the actions, like element IDs or coordinates, and facilitates the annotation of the agent's thought process and planned actions.  This careful annotation process is crucial for creating high-quality training data for web agents.", "section": "2 Vision-based GUI Agent Framework"}]