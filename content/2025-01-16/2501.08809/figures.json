[{"figure_path": "https://arxiv.org/html/2501.08809/x1.png", "caption": "Figure 1: The architectural overview of our XMusic framework. It contains two essential components: XProjector and XComposer. XProjector parses various input prompts into specific symbolic music elements. These elements then serve as control signals, guiding the music generation process within the Generator of XComposer. Additionally, XComposer includes a Selector that evaluates and identifies high-quality generated music. The Generator is trained on our large-scale dataset, XMIDI, which includes precise emotion and genre labels.", "description": "The XMusic framework consists of two main components: XProjector and XComposer.  XProjector takes various inputs (images, text, humming, etc.) and translates them into symbolic musical elements representing aspects like genre, emotion, and rhythm. These elements act as control signals for the XComposer.  The XComposer's Generator creates music based on these signals, using the XMIDI dataset for training.  The XComposer also contains a Selector, which uses a multi-task learning model to evaluate the generated music for quality, emotion, and genre, selecting only high-quality outputs.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x2.png", "caption": "Figure 2: Illustration of the proposed XMusic, which supports flexible (a) X-Prompts to guide the generation of high-quality symbolic music. The XProjector analyzes these prompts, mapping them to symbolic music elements within the (b) Projection Space. Subsequently, the (c) Generator of XComposer transforms these symbolic music elements into token sequences based on our enhanced representation. It employs a Transformer Decoder as the generative model to predict successive events iteratively, thereby creating complete musical compositions. Finally, the (d) Selector of XComposer utilizes a Transformer Encoder to encode the complete token sequences and employs a multi-task learning scheme to evaluate the quality of the generated music.", "description": "Figure 2 illustrates the XMusic framework's architecture and workflow.  (a) shows the diverse input prompts (images, videos, text, tags, humming) accepted by the system.  These are analyzed by the XProjector (a), which maps them into a common symbolic music representation space (b) comprised of emotions, genres, rhythms, and notes. This symbolic representation is then used by the XComposer's Generator (c) \u2013 a Transformer Decoder \u2013 to generate musical compositions.  The Generator iteratively predicts musical events using the enhanced symbolic music representation. Finally, the XComposer's Selector (d) \u2013 a Transformer Encoder \u2013 evaluates the quality of generated music using a multi-task learning approach (emotion, genre, quality assessment).", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x3.png", "caption": "Figure 3: Comparison between our representation and Compound Word (CP) \u00a0[10] representation. The dotted boxes represent our new tokens in comparison with those of the CP representation.", "description": "Figure 3 illustrates the difference between the proposed music representation in the XMusic framework and the Compound Word (CP) representation.  The core difference lies in the addition of new tokens in XMusic, highlighted by dotted boxes.  These new tokens represent 'Tag' (for emotion and genre), 'Instrument' (for specifying instruments), and enhanced 'Rhythm' (with 'Density' and 'Strength' tokens for finer-grained rhythm control). The CP representation, by comparison, is a more basic model.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x4.png", "caption": "Figure 4: Data statistics of our XMIDI dataset.", "description": "Figure 4 presents a comprehensive statistical overview of the XMIDI dataset, visualizing the distribution of emotion and genre labels, as well as the lengths of the musical pieces. The emotion distribution shows that some emotions are more frequent than others, while the genre distribution is relatively balanced. The music length distribution indicates that most pieces last between one and five minutes.", "section": "IV. Experiments"}]