[{"figure_path": "https://arxiv.org/html/2501.08809/x1.png", "caption": "Figure 1: The architectural overview of our XMusic framework. It contains two essential components: XProjector and XComposer. XProjector parses various input prompts into specific symbolic music elements. These elements then serve as control signals, guiding the music generation process within the Generator of XComposer. Additionally, XComposer includes a Selector that evaluates and identifies high-quality generated music. The Generator is trained on our large-scale dataset, XMIDI, which includes precise emotion and genre labels.", "description": "The XMusic framework consists of two main components: XProjector and XComposer.  XProjector acts as a multi-modal parser, taking various input prompts (images, videos, text, tags, humming) and translating them into symbolic musical elements such as emotion, genre, rhythm, and notes. These elements then serve as control signals for the XComposer. The XComposer contains a Generator, which produces music based on these control signals, and a Selector, which uses multi-task learning (quality assessment, emotion recognition, genre recognition) to choose high-quality outputs from the Generator. The Generator is trained using the large-scale XMIDI dataset, which is annotated with emotion and genre labels.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x2.png", "caption": "Figure 2: Illustration of the proposed XMusic, which supports flexible (a) X-Prompts to guide the generation of high-quality symbolic music. The XProjector analyzes these prompts, mapping them to symbolic music elements within the (b) Projection Space. Subsequently, the (c) Generator of XComposer transforms these symbolic music elements into token sequences based on our enhanced representation. It employs a Transformer Decoder as the generative model to predict successive events iteratively, thereby creating complete musical compositions. Finally, the (d) Selector of XComposer utilizes a Transformer Encoder to encode the complete token sequences and employs a multi-task learning scheme to evaluate the quality of the generated music.", "description": "XMusic is a framework for generating high-quality symbolic music from various inputs (images, videos, texts, tags, humming).  The figure details the process:  (a) Multimodal prompts are fed into XProjector, which extracts relevant features. (b) These features are mapped to symbolic music elements within a projection space. (c) The XComposer's Generator, a Transformer Decoder, uses these elements to create a sequence of tokens representing the music. (d)  Finally, the XComposer's Selector (a Transformer Encoder and a multi-task learning system) evaluates the generated music's quality, emotion, and genre to select the best output.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x3.png", "caption": "Figure 3: Comparison between our representation and Compound Word (CP) \u00a0[10] representation. The dotted boxes represent our new tokens in comparison with those of the CP representation.", "description": "Figure 3 illustrates the differences between the proposed symbolic music representation and the Compound Word (CP) representation from prior work [10].  The figure highlights the additions made to the CP representation.  These additions include new family tokens, such as the 'Tag' token encompassing 'Emotion' and 'Genre' sub-tokens, and the 'Instrument' token with a 'Program' sub-token.  Additionally, the 'Rhythm' family token has been enhanced with 'Density' and 'Strength' sub-tokens within the bar and beat level events. These modifications allow for more fine-grained control over various musical aspects during the generation process, providing greater flexibility and expressiveness.  The dotted boxes visually emphasize the newly added tokens and their sub-tokens in the proposed representation, showing the enhanced richness and structure compared to the original CP representation.", "section": "III. METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08809/x4.png", "caption": "Figure 4: Data statistics of our XMIDI dataset.", "description": "Figure 4 presents a visual summary of the XMIDI dataset's statistical properties. It comprises three subfigures: (a) shows the distribution of the 11 emotion classes, highlighting the prevalence of certain emotions like 'exciting' and 'warm' over others like 'fear' and 'magnificent'; (b) illustrates the distribution of the six genre classes, revealing a relatively balanced representation of genres like 'rock', 'pop', 'classical', etc; and (c) displays the distribution of music lengths categorized into different time intervals (0-60s, 60-120s, etc.), indicating that the majority of the songs fall within the 2-5 minute range.  This figure provides an overview of the dataset's composition in terms of emotional content, musical style, and song duration.", "section": "IV. Experiments"}]