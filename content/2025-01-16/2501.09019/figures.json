[{"figure_path": "https://arxiv.org/html/2501.09019/x1.png", "caption": "Figure 1: Illustration of FIFO-Diffusion\u00a0(Kim et\u00a0al. 2024) (top) and our Ouroboros-Diffusion (bottom) for tuning-free long video generation.", "description": "This figure illustrates the processes of FIFO-Diffusion and Ouroboros-Diffusion for generating long videos without the need for model fine-tuning.  The top panel shows FIFO-Diffusion, where a queue of video frames with progressively increasing noise levels is maintained.  Clean frames are continuously output from the queue's head while noisy frames are added to the tail.  The bottom panel depicts Ouroboros-Diffusion, which enhances structural and content consistency through novel techniques like coherent tail latent sampling, subject-aware cross-frame attention (SACFA), and self-recurrent guidance.  These additions aim to resolve the temporal inconsistencies often found in FIFO-Diffusion's output.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09019/x2.png", "caption": "Figure 2: \nAn overview of our Ouroboros-Diffusion. The whole framework (a) contains three key components: coherent tail latent sampling in queue manager , (b) Subject-Aware Cross-frame Attention (SACFA), and (c) self-recurrent guidance.\nThe coherent tail latent sampling in queue manager derives the enqueued frame latents at the queue tail to improve structural consistency.\nThe Subject-Aware Cross-frame Attention (SACFA) aligns subjects across frames within short segments for better visual coherence.\nThe self-recurrent guidance leverages information from all historical cleaner frames to guide the denoising of noisier frames, fostering rich and contextual global information interaction.", "description": "Ouroboros-Diffusion is composed of three key components working together to improve long video generation: 1) Coherent Tail Latent Sampling:  Instead of using random noise, this component intelligently samples the next frame's latent representation from the previous frame's low-frequency component, ensuring smoother transitions and structural consistency. 2) Subject-Aware Cross-Frame Attention (SACFA): This module aligns subjects across consecutive frames by using subject tokens from multiple frames as a context, enhancing visual coherence. 3) Self-Recurrent Guidance: Using information from all previously generated clean frames, this mechanism provides global information to guide the denoising of the noisier frames in the queue, leading to more consistent and contextualized results. The figure visually depicts how these components interact within the Ouroboros-Diffusion framework.", "section": "4 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.09019/x3.png", "caption": "Figure 3: The detailed illustration of coherent tail latent sampling in the queue manager.", "description": "This figure details the process of coherent tail latent sampling within the Ouroboros-Diffusion queue manager.  It shows how, instead of using purely random Gaussian noise, the model generates the new latent vector for the tail of the queue by combining the low-frequency components of the second-to-last frame latent (obtained via a Fast Fourier Transform filter) with the high-frequency components of random Gaussian noise. This approach aims to maintain structural consistency between consecutive frames while still introducing necessary dynamic variation.", "section": "4.2 Coherent Tail Latent Sampling"}, {"figure_path": "https://arxiv.org/html/2501.09019/x4.png", "caption": "Figure 4: Visual examples of single-scene long video generation by different approaches. The text prompt is \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d", "description": "This figure displays visual results of single-scene long video generation using five different methods: StreamingT2V, StreamingT2V-VideoTetris, FIFO-Diffusion, FreeNoise, and Ouroboros-Diffusion.  Each method was tasked with generating a video based on the prompt: \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d  The figure allows for a direct visual comparison of the different models' ability to maintain visual consistency, motion smoothness, and overall video quality when generating long-form videos.  The generated videos are shown as a sequence of frames for each method.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09019/x5.png", "caption": "Figure 5: Visual examples of multi-scene long video generation by different approaches. The multi-scene prompts are: 1). an astronaut is riding a horse in space; 2). an astronaut is riding a dragon in space; 3). an astronaut is riding a motorcycle in space.", "description": "Figure 5 presents a comparison of multi-scene long video generation results from four different methods: Ouroboros-Diffusion, FreeNoise, and FIFO-Diffusion.  Each method was given three sequential prompts describing an astronaut riding different vehicles in space (horse, dragon, motorcycle). The figure visually demonstrates the differences in subject consistency, background consistency, and overall video coherence produced by each approach. The goal is to showcase how Ouroboros-Diffusion excels in maintaining consistent visual elements and smooth transitions between the scenes.", "section": "5 Experiments"}]