[{"figure_path": "https://arxiv.org/html/2501.09019/x1.png", "caption": "Figure 1: Illustration of FIFO-Diffusion\u00a0(Kim et\u00a0al. 2024) (top) and our Ouroboros-Diffusion (bottom) for tuning-free long video generation.", "description": "This figure provides a visual comparison of two video generation methods: FIFO-Diffusion and Ouroboros-Diffusion.  FIFO-Diffusion, shown at the top, uses a queue to generate video frames sequentially, adding noise at the end and removing clean frames from the beginning. However, this method can struggle with maintaining long-range consistency. Ouroboros-Diffusion, shown at the bottom, improves upon FIFO-Diffusion by introducing new mechanisms to improve the structural and content consistency of the generated videos across frames. It incorporates techniques to ensure smooth visual transitions and to align subjects across frames.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09019/x2.png", "caption": "Figure 2: \nAn overview of our Ouroboros-Diffusion. The whole framework (a) contains three key components: coherent tail latent sampling in queue manager , (b) Subject-Aware Cross-frame Attention (SACFA), and (c) self-recurrent guidance.\nThe coherent tail latent sampling in queue manager derives the enqueued frame latents at the queue tail to improve structural consistency.\nThe Subject-Aware Cross-frame Attention (SACFA) aligns subjects across frames within short segments for better visual coherence.\nThe self-recurrent guidance leverages information from all historical cleaner frames to guide the denoising of noisier frames, fostering rich and contextual global information interaction.", "description": "Ouroboros-Diffusion is composed of three key parts working together to improve video generation: 1) Coherent Tail Latent Sampling:  This part of the system intelligently generates new frames at the end of a queue of video frames, ensuring smooth transitions and maintaining the overall structure of the video.  Instead of using random noise, it leverages information from existing frames to create a new frame that is structurally consistent.\n2) Subject-Aware Cross-Frame Attention (SACFA): This mechanism enhances visual coherence by focusing on the main subject of the video across multiple frames. It aligns the subject across short segments within the video, resulting in a more consistent visual representation of the subject across time.\n3) Self-Recurrent Guidance:  This mechanism uses information from already processed (clean) frames to guide the generation of noisier frames later in the process, leading to a more consistent video. It essentially uses the past frames to improve the quality of future frames, creating a more rich and contextual video.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2501.09019/x3.png", "caption": "Figure 3: The detailed illustration of coherent tail latent sampling in the queue manager.", "description": "Figure 3 illustrates the process of coherent tail latent sampling within the queue manager of the Ouroboros-Diffusion model.  Instead of simply adding random Gaussian noise to the tail of the queue (as in FIFO-Diffusion), Ouroboros-Diffusion leverages the low-frequency components of the second-to-last frame latent. This low-frequency information preserves the overall structure and layout of the video, ensuring a smoother transition between frames.  High-frequency components from random Gaussian noise are then added to introduce dynamics and variation, maintaining a balance between structural consistency and visual dynamism.  The figure visually depicts the process, showing the flow of information from the second-to-last frame to the creation of the new tail latent.", "section": "4.2 Coherent Tail Latent Sampling"}, {"figure_path": "https://arxiv.org/html/2501.09019/x4.png", "caption": "Figure 4: Visual examples of single-scene long video generation by different approaches. The text prompt is \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d", "description": "Figure 4 presents a comparison of single-scene long video generation results from five different methods: StreamingT2V, StreamingT2V-VideoTetris, FIFO-Diffusion, FreeNoise, and Ouroboros-Diffusion.  All methods were given the same text prompt: \"A cat wearing sunglasses and working as a lifeguard at a pool.\" The figure visually demonstrates the differences in output video quality, specifically in terms of subject consistency, background consistency, motion smoothness, temporal coherence, and overall aesthetic quality.  Each row displays frames from different points in the generated videos, allowing for a direct visual comparison across methods. This comparison highlights the strengths and weaknesses of each approach in generating long, coherent videos from a single text prompt.", "section": "5. Experimental Results"}, {"figure_path": "https://arxiv.org/html/2501.09019/x5.png", "caption": "Figure 5: Visual examples of multi-scene long video generation by different approaches. The multi-scene prompts are: 1). an astronaut is riding a horse in space; 2). an astronaut is riding a dragon in space; 3). an astronaut is riding a motorcycle in space.", "description": "Figure 5 presents a comparison of multi-scene long video generation results from four different methods: Ouroboros-Diffusion, FreeNoise, and FIFO-Diffusion.  All models were tasked with generating videos based on three sequential prompts: an astronaut riding a horse in space, then an astronaut riding a dragon in space, and finally an astronaut riding a motorcycle in space. The figure showcases the generated video frames from each method to visually demonstrate their performance in handling multiple scene transitions and maintaining content consistency throughout a long video.", "section": "5 Experiments"}]