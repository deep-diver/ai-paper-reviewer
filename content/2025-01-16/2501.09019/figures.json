[{"figure_path": "https://arxiv.org/html/2501.09019/x1.png", "caption": "Figure 1: Illustration of FIFO-Diffusion\u00a0(Kim et\u00a0al. 2024) (top) and our Ouroboros-Diffusion (bottom) for tuning-free long video generation.", "description": "This figure illustrates the difference between FIFO-Diffusion (Kim et al., 2024) and the proposed Ouroboros-Diffusion for generating long videos without fine-tuning.  The top panel shows FIFO-Diffusion, which uses a queue to manage video frames.  Noisy frames enter the queue at one end, are processed, and clean frames exit at the other. However, the independently generated noise at the queue tail may lead to inconsistencies. The bottom panel displays Ouroboros-Diffusion, which addresses these inconsistencies by using a novel latent sampling technique, subject-aware cross-frame attention, and self-recurrent guidance to improve structural and content consistency.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.09019/x2.png", "caption": "Figure 2: \nAn overview of our Ouroboros-Diffusion. The whole framework (a) contains three key components: coherent tail latent sampling in queue manager , (b) Subject-Aware Cross-frame Attention (SACFA), and (c) self-recurrent guidance.\nThe coherent tail latent sampling in queue manager derives the enqueued frame latents at the queue tail to improve structural consistency.\nThe Subject-Aware Cross-frame Attention (SACFA) aligns subjects across frames within short segments for better visual coherence.\nThe self-recurrent guidance leverages information from all historical cleaner frames to guide the denoising of noisier frames, fostering rich and contextual global information interaction.", "description": "Ouroboros-Diffusion is composed of three key components working together to generate consistent long videos.  (a) Coherent Tail Latent Sampling:  Improves the structural consistency of the generated video by creating the next frame's noisy latent using a low-frequency component from the previous frame and random high-frequency noise. This ensures a smooth transition between frames and maintains the overall structure.\n(b) Subject-Aware Cross-Frame Attention (SACFA): Enhances the subject consistency by focusing attention on subject regions across multiple frames, ensuring that the subject's appearance remains coherent throughout the video. This improves visual coherence and reduces subject inconsistencies.\n(c) Self-Recurrent Guidance: Improves the temporal consistency by using previously generated clean frames to guide the denoising of the noisier frames at the end of the queue.  This leverages rich global information from the earlier frames, leading to better subject consistency and improved temporal coherence. The system works by seamlessly integrating information across time to produce consistent and visually pleasing long videos.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2501.09019/x3.png", "caption": "Figure 3: The detailed illustration of coherent tail latent sampling in the queue manager.", "description": "Figure 3 illustrates the process of coherent tail latent sampling within the Ouroboros-Diffusion queue manager.  Instead of simply adding random Gaussian noise to the end of the queue (as in FIFO-Diffusion), Ouroboros-Diffusion leverages the low-frequency components of the second-to-last frame in the queue. This low-frequency information, representing the overall structure and layout, is combined with high-frequency random noise to create a new tail latent. This method ensures a smoother and more consistent transition between frames, enhancing the visual coherence of the generated video.", "section": "4.2 Coherent Tail Latent Sampling"}, {"figure_path": "https://arxiv.org/html/2501.09019/x4.png", "caption": "Figure 4: Visual examples of single-scene long video generation by different approaches. The text prompt is \u201cA cat wearing sunglasses and working as a lifeguard at a pool.\u201d", "description": "This figure shows visual comparisons of long video generation results (128 frames each) from different approaches using the same prompt: \"A cat wearing sunglasses and working as a lifeguard at a pool.\".  It visually demonstrates the differences in subject consistency, background consistency, motion smoothness, temporal consistency, and overall aesthetic quality achieved by each method.  Each approach's output is presented as a sequence of frames across a time series, highlighting the advantages and drawbacks of each model.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.09019/x5.png", "caption": "Figure 5: Visual examples of multi-scene long video generation by different approaches. The multi-scene prompts are: 1). an astronaut is riding a horse in space; 2). an astronaut is riding a dragon in space; 3). an astronaut is riding a motorcycle in space.", "description": "Figure 5 presents a comparison of multi-scene long video generation results from different approaches.  Three prompts were used, all featuring an astronaut as the main subject but riding different vehicles in space: a horse, a dragon, and a motorcycle. The figure visually demonstrates how well each method handles changes in the scene and maintains consistency across multiple scenes within a single, long video.", "section": "5. Experiments"}]