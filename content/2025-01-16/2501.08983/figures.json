[{"figure_path": "https://arxiv.org/html/2501.08983/x1.png", "caption": "Figure 1: Overview of CityDreamer4D. 4D city generation is divided into static and dynamic scene generation, conditioned on \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L and \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, produced by Unbounded Layout Generator and Traffic Scenario Generator, respectively. City Background Generator uses \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L to create background images \ud835\udc08^Gsubscript^\ud835\udc08\ud835\udc3a\\mathbf{\\hat{I}}_{G}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT for stuff like roads, vegetation, and the sky, while Building Instance Generator renders the buildings {\ud835\udc08^Bi}subscript^\ud835\udc08subscript\ud835\udc35\ud835\udc56\\{\\mathbf{\\hat{I}}_{B_{i}}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } within the city. Using \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, Vehicle Instance Generator generates vehicles {\ud835\udc08^Vit}superscriptsubscript^\ud835\udc08subscript\ud835\udc49\ud835\udc56\ud835\udc61\\{\\mathbf{\\hat{I}}_{V_{i}}^{t}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } at time step t\ud835\udc61titalic_t. Finally, Compositor combines the rendered background, buildings, and vehicles into a unified and coherent image \ud835\udc08^Ctsuperscriptsubscript^\ud835\udc08\ud835\udc36\ud835\udc61\\mathbf{\\hat{I}}_{C}^{t}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. \u201cGen.\u201d, \u201cMod.\u201c, \u201cCond.\u201d, \u201cBG.\u201d, \u201cBLDG.\u201d, and \u201cVEH.\u201d denote \u201cGeneration\u201d, \u201cModulation\u201d, \u201cCondition\u201d, \u201cBackground\u201d, \u201cBuilding\u201d, and \u201cVehicle\u201d, respectively.", "description": "CityDreamer4D is a framework for generating unbounded 4D cities by separating the generation process into static and dynamic components.  Static elements (city layout) are created using an Unbounded Layout Generator, while dynamic elements (traffic) are handled by a Traffic Scenario Generator. These are then fed into three instance generators: City Background, Building Instance, and Vehicle Instance.  Each generator uses a specialized neural field to render its respective elements (background, buildings, vehicles). The final image is assembled by the Compositor. The model uses a highly compact BEV scene representation for efficiency and allows for instance-level editing.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08983/x2.png", "caption": "Figure 2: Overview of the OSM and GoogleEarth Datasets. (a) Examples of the 2D and 3D annotations in the GoogleEarth dataset, which can be automatically generated using the OSM dataset. (b) The automatic annotation pipeline can be readily adapted for worldwide cities. (c) The dataset statistics highlight the diverse perspectives in the GoogleEarth dataset.", "description": "Figure 2 showcases the OSM and Google Earth datasets used in the CityDreamer4D model.  Panel (a) provides examples of 2D and 3D annotations within the Google Earth dataset; these annotations are automatically generated using the OSM dataset's information.  Panel (b) demonstrates the adaptability of the automatic annotation pipeline to various cities globally.  Finally, panel (c) presents statistical details of the Google Earth dataset, highlighting the variety of viewpoints and perspectives captured.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x3.png", "caption": "Figure 3: Overview of the CityTopia Dataset. (a) The virtual city generation pipeline. \u201cPro.Inst.\u201d, \u201cSur.Spl\u201d, and \u201c3D Inst. Anno.\u201d denote \u201cPrototype Instantiation\u201d, \u201cSurface Sampling\u201d, and \u201c3D Instance Annotation\u201d, respectively. (b) Examples of 2D and 3D annotations in the CityTopia dataset are shown from both daytime and nighttime street-view and aerial-view perspectives, automatically generated during virtual city generation. (c) The dataset statistics highlight the diverse perspectives in both street and aerial views.", "description": "Figure 3 provides a comprehensive overview of the CityTopia dataset, a key contribution of the paper.  Part (a) details the virtual city generation pipeline, which involves three main steps: prototype instantiation (creating a basic city structure), surface sampling (adding details to the city model by sampling from a diverse set of assets), and 3D instance annotation (labeling individual buildings and vehicles). Part (b) showcases examples of both 2D and 3D annotations from this dataset, including street-view and aerial-view images captured during both day and night. These diverse perspectives enhance the richness and complexity of the dataset. Lastly, part (c) presents statistical summaries of the dataset, highlighting the variety of viewpoints and altitudes captured, further illustrating the comprehensive nature of the CityTopia dataset.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x4.png", "caption": "Figure 4: Qualitative Comparison on Google Earth. For SceneDreamer\u00a0[7] and CityDreamer4D, vehicles are generated using models trained on CityTopia due to the lack of semantic annotations for vehicles in Google Earth. For DimensionX\u00a0[107], the initial frame is provided by CityDreamer4D. The visual results of InfiniCity\u00a0[26], provided by the authors, have been zoomed in for better viewing. \u201cPers.Nature\u201d stands for \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure displays a qualitative comparison of various 3D city generation models, trained on the Google Earth dataset.  Because the Google Earth dataset lacks semantic annotations for vehicles, SceneDreamer and CityDreamer4D used models trained on the CityTopia dataset to generate vehicles.  The initial frame for DimensionX was provided by CityDreamer4D.  The InfiniCity results were zoomed in to improve visibility.", "section": "5.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.08983/x5.png", "caption": "Figure 5: Qualitative Comparison on CityTopia. The initial frame for DimensionX and the input frames for DreamScene4D are chosen from the dataset. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "Figure 5 presents a qualitative comparison of various 4D scene generation methods on the CityTopia dataset.  It shows a visual comparison of the results generated by different methods, including SGAM, PersistentNature, SceneDreamer, DreamScene4D, DimensionX, and the proposed CityDreamer4D. The images showcase the differences in terms of overall scene quality, realism, and multi-view consistency.  Note that for methods such as DimensionX and DreamScene4D, input frames or initial frames were chosen from the CityTopia dataset.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x6.png", "caption": "Figure 6: User Study on 4D City Generation. All scores are in the range of 5, with 5 indicating the best. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents the results of a user study evaluating the performance of different methods in 4D city generation.  Users rated each method's performance across three aspects: perceptual quality, 4D realism, and view consistency, each on a scale of 1 to 5, with 5 being the best. The results show CityDreamer4D significantly outperforms other methods, demonstrating its superiority in generating high-quality, realistic, and consistent 4D city scenes.", "section": "5.3 Main Results"}, {"figure_path": "https://arxiv.org/html/2501.08983/x7.png", "caption": "Figure 7: Qualitative Comparison of City Layout Generators. The height map values are normalized to a range of [0,1]01[0,1][ 0 , 1 ] by dividing each value by the maximum value within the map.", "description": "Figure 7 presents a qualitative comparison of three different city layout generators: the proposed method (Ours), InfinityGAN, and IPSM.  The generated height maps for each method are visualized, with values normalized to a range between 0 and 1 for easier comparison.  This normalization is achieved by dividing all height map values by the maximum height value found in each respective map.  The figure allows a visual assessment of the quality and characteristics of city layouts produced by different algorithms, showcasing variations in detail, smoothness, and overall structure.", "section": "3.1 Unbounded Layout Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x8.png", "caption": "Figure 8: Qualitative Comparison of Building Instance Generator (BIG) Variants. (a) and (b) illustrate the effects of removing BIG and instance labels, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different configurations of the Building Instance Generator (BIG) within the CityDreamer4D model.  Panels (a) and (b) show the impact of removing BIG entirely and removing instance labels, respectively, highlighting their importance for realistic building generation. Panels (c) through (f) explore the effect of different scene parameterization techniques. These techniques vary the encoder (global vs. local) and positional encoding method (hash grid vs. sinusoidal). Each panel displays sample generated building images, illustrating the visual differences resulting from the various configurations.", "section": "3.4 Building Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x9.png", "caption": "Figure 9: Qualitative Comparison of Vehicle Instance Generator (VIG) Variants. (a) and (b) illustrate the effects of removing VIG and canonicalization, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different versions of the Vehicle Instance Generator (VIG) to demonstrate the impact of key components on the quality of generated vehicles within the 4D city scenes.  Specifically, (a) and (b) show the consequences of removing VIG entirely and of removing the canonicalization process (a step in converting vehicle coordinates to a standard coordinate system) respectively. The remaining panels (c) through (f) illustrate the effects of using different combinations of global and local encoders (components that process scene information) and different scene parameterization techniques (mathematical methods for representing the scene's properties).  The results showcase how these choices influence the visual quality and realism of the generated vehicles.", "section": "3.5 Vehical Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x10.png", "caption": "Figure 10: Localized Editing on the Generated Cities. (a) and (c) show vehicle editing results, while (b) and (d) present building editing results.", "description": "This figure showcases the localized editing capabilities of the CityDreamer4D model.  Panels (a) and (c) demonstrate the ability to modify vehicle attributes within a generated city scene, such as changing vehicle types or their positions. Panels (b) and (d) illustrate similar editing capabilities applied to building attributes, such as adjusting building height or swapping building styles. This highlights the model's capacity for fine-grained control and manipulation of individual elements within the generated city environments.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x11.png", "caption": "Figure 11: Text-driven City Stylization with ControlNet. The multi-view consistency is preserved in stylized Minecraft and Cyberpunk cities.", "description": "This figure demonstrates the application of ControlNet to stylize generated city scenes.  ControlNet, a technique that allows for guiding image generation based on additional input, is used here to steer the style of the generated city towards either a Minecraft-like aesthetic or a Cyberpunk aesthetic. The key takeaway is that even with significant stylistic changes, the multi-view consistency of the generated 3D model is maintained, ensuring that the model remains internally coherent from multiple viewpoints.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x12.png", "caption": "Figure 12: COLMAP Reconstruction of 600-frame Orbital Videos. The red ring shows the camera positions, and the clear point clouds demonstrate CityDreamer4D\u2019s consistent rendering. Note that \u201dRecon.\u201d stands for \u201dReconstruction.\u201d", "description": "This figure visualizes the results of 3D reconstruction performed using COLMAP on a series of 600 orbital videos generated by CityDreamer4D.  The red ring highlights the estimated camera positions throughout the video sequence. The clarity of the resulting point cloud demonstrates the high consistency and accuracy of the 3D scene generation achieved by the CityDreamer4D model.  This provides strong evidence supporting the model's ability to produce temporally consistent and geometrically accurate 4D city sequences.", "section": "5.6 Discussions"}, {"figure_path": "https://arxiv.org/html/2501.08983/x13.png", "caption": "Figure 13: Directional Light Relighting Effect. (a) and (b) show the lighting intensity. (c) illustrates the relighting effect. Note that \u201cS.M.\u201d denotes \u201cShadow Mapping\u201d.", "description": "Figure 13 demonstrates the relighting effects applied in CityDreamer4D.  It showcases the results of two lighting components: Lambertian shading (a), which provides uniform illumination, and shadow mapping (b), which adds realistic shadows and occlusions. The combined effect (c) shows a more visually appealing and realistic scene with the camera positioned on the left. This highlights the capabilities of CityDreamer4D's relighting process to enhance the realism of the generated scenes.", "section": "3 METHOD"}]