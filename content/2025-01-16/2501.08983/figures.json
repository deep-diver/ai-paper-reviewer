[{"figure_path": "https://arxiv.org/html/2501.08983/x1.png", "caption": "Figure 1: Overview of CityDreamer4D. 4D city generation is divided into static and dynamic scene generation, conditioned on \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L and \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, produced by Unbounded Layout Generator and Traffic Scenario Generator, respectively. City Background Generator uses \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L to create background images \ud835\udc08^Gsubscript^\ud835\udc08\ud835\udc3a\\mathbf{\\hat{I}}_{G}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT for stuff like roads, vegetation, and the sky, while Building Instance Generator renders the buildings {\ud835\udc08^Bi}subscript^\ud835\udc08subscript\ud835\udc35\ud835\udc56\\{\\mathbf{\\hat{I}}_{B_{i}}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } within the city. Using \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, Vehicle Instance Generator generates vehicles {\ud835\udc08^Vit}superscriptsubscript^\ud835\udc08subscript\ud835\udc49\ud835\udc56\ud835\udc61\\{\\mathbf{\\hat{I}}_{V_{i}}^{t}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } at time step t\ud835\udc61titalic_t. Finally, Compositor combines the rendered background, buildings, and vehicles into a unified and coherent image \ud835\udc08^Ctsuperscriptsubscript^\ud835\udc08\ud835\udc36\ud835\udc61\\mathbf{\\hat{I}}_{C}^{t}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. \u201cGen.\u201d, \u201cMod.\u201c, \u201cCond.\u201d, \u201cBG.\u201d, \u201cBLDG.\u201d, and \u201cVEH.\u201d denote \u201cGeneration\u201d, \u201cModulation\u201d, \u201cCondition\u201d, \u201cBackground\u201d, \u201cBuilding\u201d, and \u201cVehicle\u201d, respectively.", "description": "CityDreamer4D is a framework for generating unbounded 4D cities by separating static and dynamic elements.  Static elements (city layout) are generated using the Unbounded Layout Generator, while dynamic elements (traffic) are created using the Traffic Scenario Generator.  The City Background Generator renders background elements like sky, roads, and vegetation.  Building Instance Generators produce building images, and Vehicle Instance Generators create vehicle images. These are then combined by the Compositor to form a final 4D city image.  The caption also lists abbreviations used in the figure.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08983/x2.png", "caption": "Figure 2: Overview of the OSM and GoogleEarth Datasets. (a) Examples of the 2D and 3D annotations in the GoogleEarth dataset, which can be automatically generated using the OSM dataset. (b) The automatic annotation pipeline can be readily adapted for worldwide cities. (c) The dataset statistics highlight the diverse perspectives in the GoogleEarth dataset.", "description": "Figure 2 provides a detailed look into the OSM and Google Earth datasets used in the CityDreamer4D project.  Part (a) showcases examples of 2D and 3D annotations within the Google Earth dataset, demonstrating how these annotations can be automatically derived from the OSM data. Part (b) illustrates the adaptability of the automated annotation pipeline for global city application. Finally, part (c) presents statistical analyses of the Google Earth dataset, highlighting the diversity of viewpoints (drone altitudes and angles) it offers.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x3.png", "caption": "Figure 3: Overview of the CityTopia Dataset. (a) The virtual city generation pipeline. \u201cPro.Inst.\u201d, \u201cSur.Spl\u201d, and \u201c3D Inst. Anno.\u201d denote \u201cPrototype Instantiation\u201d, \u201cSurface Sampling\u201d, and \u201c3D Instance Annotation\u201d, respectively. (b) Examples of 2D and 3D annotations in the CityTopia dataset are shown from both daytime and nighttime street-view and aerial-view perspectives, automatically generated during virtual city generation. (c) The dataset statistics highlight the diverse perspectives in both street and aerial views.", "description": "Figure 3 provides a comprehensive overview of the CityTopia dataset, a key contribution of the paper.  Part (a) details the virtual city generation pipeline, outlining the stages of Prototype Instantiation, Surface Sampling, and 3D Instance Annotation. Part (b) showcases examples of the high-quality 2D and 3D annotations within the dataset, highlighting the diversity of viewpoints (daytime/nighttime, street-view/aerial-view) and the automated generation process. Finally, part (c) presents statistical analysis of the dataset, emphasizing the variation in viewpoints obtained through elevation and altitude parameters, showcasing the rich diversity of perspectives included in CityTopia.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x4.png", "caption": "Figure 4: Qualitative Comparison on Google Earth. For SceneDreamer\u00a0[7] and CityDreamer4D, vehicles are generated using models trained on CityTopia due to the lack of semantic annotations for vehicles in Google Earth. For DimensionX\u00a0[107], the initial frame is provided by CityDreamer4D. The visual results of InfiniCity\u00a0[26], provided by the authors, have been zoomed in for better viewing. \u201cPers.Nature\u201d stands for \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure provides a qualitative comparison of different 3D and 4D city generation models on the Google Earth dataset.  The models compared are SceneDreamer, CityDreamer4D, DimensionX, InfiniCity, and PersistentNature. Because the Google Earth dataset lacks semantic annotations for vehicles, CityDreamer4D and SceneDreamer use models trained on the CityTopia dataset for vehicle generation.  DimensionX uses an initial frame provided by CityDreamer4D. The InfiniCity results, provided by the paper's authors, are zoomed in for better visualization. The comparison demonstrates CityDreamer4D's superior ability to generate realistic and detailed 4D cities.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x5.png", "caption": "Figure 5: Qualitative Comparison on CityTopia. The initial frame for DimensionX and the input frames for DreamScene4D are chosen from the dataset. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "Figure 5 presents a qualitative comparison of CityDreamer4D's 4D city generation capabilities against several state-of-the-art methods on the CityTopia dataset.  The comparison includes SGAM, PersistentNature, SceneDreamer, DreamScene4D, and DimensionX.  For methods requiring initial frames or video inputs, these were selected from the CityTopia dataset. The figure visually showcases the generated city sequences from each method, allowing for a direct comparison of their visual realism, scene consistency, and overall quality in generating 4D urban scenes.", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x6.png", "caption": "Figure 6: User Study on 4D City Generation. All scores are in the range of 5, with 5 indicating the best. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents the results of a user study comparing the performance of different methods for generating 4D city models, including CityDreamer4D, SceneDreamer, PersistentNature, InfiniCity, DimensionX, and SGAM.  Users rated each method on a scale of 1 to 5 across three key dimensions: perceptual quality, 4D realism, and view consistency.  Higher scores indicate better performance. The figure clearly shows that CityDreamer4D outperforms all other methods across all three evaluation criteria.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x7.png", "caption": "Figure 7: Qualitative Comparison of City Layout Generators. The height map values are normalized to a range of [0,1]01[0,1][ 0 , 1 ] by dividing each value by the maximum value within the map.", "description": "Figure 7 presents a qualitative comparison of three different city layout generators:  the proposed method, InfinityGAN, and IPSM. The figure visually compares the generated height maps produced by each method.  Height map values are normalized to a range of 0 to 1 for consistent comparison across methods.  This allows for easier visual assessment of the similarities and differences in generated city layouts by the three approaches, including texture and shape.", "section": "3.1 Unbounded Layout Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x8.png", "caption": "Figure 8: Qualitative Comparison of Building Instance Generator (BIG) Variants. (a) and (b) illustrate the effects of removing BIG and instance labels, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different variations of the Building Instance Generator (BIG) within the CityDreamer4D model.  The top row shows the impact of removing the BIG module entirely (a) and the impact of removing instance labels while retaining the BIG module (b). The significant drop in image quality highlights the importance of both the BIG module and the use of instance labels. The bottom row (c-f) explores the effects of different scene parameterizations within the BIG. Specifically, it compares different encoder types (global vs. local) and different positional encoding methods (hash grids vs. sinusoidal). These experiments demonstrate how the chosen parameterization strategy impacts the realism and fidelity of the generated building images.", "section": "3.4 Building Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x9.png", "caption": "Figure 9: Qualitative Comparison of Vehicle Instance Generator (VIG) Variants. (a) and (b) illustrate the effects of removing VIG and canonicalization, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different configurations of the Vehicle Instance Generator (VIG) within the CityDreamer4D model.  (a) and (b) show the impact of removing VIG entirely and removing the canonicalization process, respectively. The remaining subfigures (c) through (f) explore the effects of different scene parameterization strategies.  These variations involve using different encoders (global vs. local) combined with different positional encoding techniques (hash grids vs. sinusoidal functions).  The goal is to demonstrate the impact of each component on the quality of generated vehicle instances within the simulated city scenes.", "section": "3.5 Vehical Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x10.png", "caption": "Figure 10: Localized Editing on the Generated Cities. (a) and (c) show vehicle editing results, while (b) and (d) present building editing results.", "description": "This figure showcases the localized editing capabilities of the CityDreamer4D model.  Panels (a) and (c) demonstrate modifications to vehicles within the generated city scenes, illustrating changes such as vehicle type and placement. Panels (b) and (d) exhibit similar localized editing applied to buildings, showing adjustments to building height and style. These examples highlight the model's ability to make precise, localized alterations without affecting the overall scene coherence.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x11.png", "caption": "Figure 11: Text-driven City Stylization with ControlNet. The multi-view consistency is preserved in stylized Minecraft and Cyberpunk cities.", "description": "This figure showcases the results of applying ControlNet to stylize generated cityscapes. ControlNet allows for directing the style of the generated image while preserving multi-view consistency; even when changing the style significantly, the structural integrity of the scene remains consistent across different viewpoints. The example shown demonstrates successful stylization of generated cities to resemble the visual styles of Minecraft and Cyberpunk.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x12.png", "caption": "Figure 12: COLMAP Reconstruction of 600-frame Orbital Videos. The red ring shows the camera positions, and the clear point clouds demonstrate CityDreamer4D\u2019s consistent rendering. Note that \u201dRecon.\u201d stands for \u201dReconstruction.\u201d", "description": "This figure displays the results of a 3D reconstruction performed using COLMAP on a sequence of 600 orbital videos generated by CityDreamer4D.  The videos were captured from a circular trajectory around a scene, and the red ring in the image highlights the estimated camera positions during the video recording.  The clarity and density of the resulting point cloud illustrate the high quality and consistency of CityDreamer4D's rendering across the entire sequence of frames.", "section": "5.6 Discussions"}, {"figure_path": "https://arxiv.org/html/2501.08983/x13.png", "caption": "Figure 13: Directional Light Relighting Effect. (a) and (b) show the lighting intensity. (c) illustrates the relighting effect. Note that \u201cS.M.\u201d denotes \u201cShadow Mapping\u201d.", "description": "This figure demonstrates the relighting effect achieved in CityDreamer4D. It showcases three images: (a) shows the lighting intensity based on Lambertian shading, which provides uniform lighting across all directions; (b) shows the lighting intensity based on shadow mapping, which accounts for light visibility and occlusion; (c) combines both Lambertian shading and shadow mapping to produce a realistic relighting effect, with the camera positioned on the left side of the scene.", "section": "3 METHOD"}]