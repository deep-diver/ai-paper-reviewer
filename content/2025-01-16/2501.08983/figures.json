[{"figure_path": "https://arxiv.org/html/2501.08983/x1.png", "caption": "Figure 1: Overview of CityDreamer4D. 4D city generation is divided into static and dynamic scene generation, conditioned on \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L and \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, produced by Unbounded Layout Generator and Traffic Scenario Generator, respectively. City Background Generator uses \ud835\udc0b\ud835\udc0b\\mathbf{L}bold_L to create background images \ud835\udc08^Gsubscript^\ud835\udc08\ud835\udc3a\\mathbf{\\hat{I}}_{G}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT for stuff like roads, vegetation, and the sky, while Building Instance Generator renders the buildings {\ud835\udc08^Bi}subscript^\ud835\udc08subscript\ud835\udc35\ud835\udc56\\{\\mathbf{\\hat{I}}_{B_{i}}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT } within the city. Using \ud835\udc13tsubscript\ud835\udc13\ud835\udc61\\mathbf{T}_{t}bold_T start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, Vehicle Instance Generator generates vehicles {\ud835\udc08^Vit}superscriptsubscript^\ud835\udc08subscript\ud835\udc49\ud835\udc56\ud835\udc61\\{\\mathbf{\\hat{I}}_{V_{i}}^{t}\\}{ over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } at time step t\ud835\udc61titalic_t. Finally, Compositor combines the rendered background, buildings, and vehicles into a unified and coherent image \ud835\udc08^Ctsuperscriptsubscript^\ud835\udc08\ud835\udc36\ud835\udc61\\mathbf{\\hat{I}}_{C}^{t}over^ start_ARG bold_I end_ARG start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. \u201cGen.\u201d, \u201cMod.\u201c, \u201cCond.\u201d, \u201cBG.\u201d, \u201cBLDG.\u201d, and \u201cVEH.\u201d denote \u201cGeneration\u201d, \u201cModulation\u201d, \u201cCondition\u201d, \u201cBackground\u201d, \u201cBuilding\u201d, and \u201cVehicle\u201d, respectively.", "description": "CityDreamer4D is a 4D city generation framework that separates dynamic objects (like vehicles) from static scenes (like buildings and roads).  The process begins with the Unbounded Layout Generator and Traffic Scenario Generator, which create static city layouts and dynamic traffic scenarios, respectively.  These are input to three separate modules: the City Background Generator (for generating background images like sky and roads), the Building Instance Generator (for generating building images), and the Vehicle Instance Generator (for generating vehicle images at each time step). Finally, the Compositor merges these generated elements into a single unified 4D city image.  The figure illustrates this compositional process in detail.", "section": "3 METHOD"}, {"figure_path": "https://arxiv.org/html/2501.08983/x2.png", "caption": "Figure 2: Overview of the OSM and GoogleEarth Datasets. (a) Examples of the 2D and 3D annotations in the GoogleEarth dataset, which can be automatically generated using the OSM dataset. (b) The automatic annotation pipeline can be readily adapted for worldwide cities. (c) The dataset statistics highlight the diverse perspectives in the GoogleEarth dataset.", "description": "Figure 2 provides a detailed look at the OSM and Google Earth datasets used in the CityDreamer4D model.  Panel (a) showcases examples of 2D and 3D annotations within the Google Earth dataset, demonstrating how these annotations can be automatically derived from the OSM dataset.  Panel (b) further emphasizes the scalability of this automatic annotation process, showing its adaptability to various cities around the world. Finally, panel (c) presents statistical analyses of the Google Earth dataset, highlighting the diversity in viewpoints (elevation angles and altitudes) captured within this dataset, which is crucial for the training and evaluation of the CityDreamer4D model.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x3.png", "caption": "Figure 3: Overview of the CityTopia Dataset. (a) The virtual city generation pipeline. \u201cPro.Inst.\u201d, \u201cSur.Spl\u201d, and \u201c3D Inst. Anno.\u201d denote \u201cPrototype Instantiation\u201d, \u201cSurface Sampling\u201d, and \u201c3D Instance Annotation\u201d, respectively. (b) Examples of 2D and 3D annotations in the CityTopia dataset are shown from both daytime and nighttime street-view and aerial-view perspectives, automatically generated during virtual city generation. (c) The dataset statistics highlight the diverse perspectives in both street and aerial views.", "description": "Figure 3 provides a comprehensive overview of the CityTopia dataset, a crucial component of the CityDreamer4D model.  Panel (a) details the dataset's creation pipeline, starting with prototype instantiation ('Pro.Inst.') of city elements, followed by surface sampling ('Sur.Spl.') to obtain detailed geometry, and culminating in the generation of dense 3D instance annotations ('3D Inst. Anno.'). Panel (b) showcases examples of these 2D and 3D annotations, illustrating the variety of perspectives (daytime/nighttime, street-view/aerial-view) and the automated annotation process. Panel (c) presents statistical summaries of the dataset, including the distribution of camera viewpoints (elevation angle and altitude), emphasizing the diverse perspectives captured within CityTopia.", "section": "4 DATASETS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x4.png", "caption": "Figure 4: Qualitative Comparison on Google Earth. For SceneDreamer\u00a0[7] and CityDreamer4D, vehicles are generated using models trained on CityTopia due to the lack of semantic annotations for vehicles in Google Earth. For DimensionX\u00a0[107], the initial frame is provided by CityDreamer4D. The visual results of InfiniCity\u00a0[26], provided by the authors, have been zoomed in for better viewing. \u201cPers.Nature\u201d stands for \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure provides a qualitative comparison of 3D city generation results on the Google Earth dataset using different methods: SGAM, PersistentNature, SceneDreamer, InfiniCity, DimensionX, and CityDreamer4D.  Because the Google Earth dataset lacks semantic annotations for vehicles, the vehicles in the SceneDreamer and CityDreamer4D results were generated using models trained on the CityTopia dataset, which does contain such annotations. For DimensionX, the initial frame of its output was supplied by CityDreamer4D. The image from InfiniCity has been zoomed in to enhance readability.", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x5.png", "caption": "Figure 5: Qualitative Comparison on CityTopia. The initial frame for DimensionX and the input frames for DreamScene4D are chosen from the dataset. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure provides a qualitative comparison of CityDreamer4D's 4D city generation capabilities against several state-of-the-art methods on the CityTopia dataset.  It visually showcases the generated results of each method, highlighting differences in realism, consistency, and overall quality.  The initial frame for DimensionX and the input frames for DreamScene4D were taken directly from the CityTopia dataset to ensure a fair comparison.  The method \"Pers.Nature\" refers to the work of PersistentNature [105].", "section": "5 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.08983/x6.png", "caption": "Figure 6: User Study on 4D City Generation. All scores are in the range of 5, with 5 indicating the best. \u201cPers.Nature\u201d refers to \u201cPersistentNature\u201d\u00a0[105].", "description": "This figure presents the results of a user study evaluating the quality of 4D city generation across different methods.  Participants rated each method on three aspects: perceptual quality, 4D realism, and view consistency.  Scores ranged from 1 to 5, with 5 representing the best possible score. The figure clearly shows CityDreamer4D outperforming all other methods across all three evaluation criteria.  The abbreviation \"Pers.Nature\" refers to the \"PersistentNature\" method described in reference [105].", "section": "5 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08983/x7.png", "caption": "Figure 7: Qualitative Comparison of City Layout Generators. The height map values are normalized to a range of [0,1]01[0,1][ 0 , 1 ] by dividing each value by the maximum value within the map.", "description": "Figure 7 presents a qualitative comparison of different methods for generating city layouts.  It specifically focuses on the height maps produced by each method.  To facilitate comparison, all height map values have been normalized to a range of 0 to 1, ensuring that differences in the overall height scales across the methods do not obscure variations in the detail of the height maps. The figure visually demonstrates the relative performance of each layout generation method in terms of the realism and detail of the terrain and building heights.", "section": "3.1 Unbounded Layout Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x8.png", "caption": "Figure 8: Qualitative Comparison of Building Instance Generator (BIG) Variants. (a) and (b) illustrate the effects of removing BIG and instance labels, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different variations of the Building Instance Generator (BIG) used in CityDreamer4D.  The first row shows the impact of removing BIG altogether (a) and the effect of removing instance labels while keeping BIG (b).  The results clearly demonstrate the importance of both components for high-quality building generation. The second row shows the effects of different scene parameterization strategies within BIG. Variations include using a global encoder with a hash grid (c) or sinusoidal positional encoding (d), and a local encoder with a hash grid (e) or sinusoidal positional encoding (f).  By comparing the generated building images, this visualization helps to quantify the influence of architectural choices within BIG on the final building renderings.", "section": "3.4 Building Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x9.png", "caption": "Figure 9: Qualitative Comparison of Vehicle Instance Generator (VIG) Variants. (a) and (b) illustrate the effects of removing VIG and canonicalization, respectively. (c)\u2013(f) present the results of various scene parameterizations. Note that \u201cEnc.\u201d is an abbreviation for \u201cEncoder\u201d.", "description": "This figure compares different versions of the Vehicle Instance Generator (VIG) used in the CityDreamer4D model.  Specifically, it shows the impact of removing the VIG entirely (a), removing the canonicalization step (b), and using different combinations of global/local encoders and scene parameterizations (hash grids or sinusoidal positional encoding) (c-f). The results highlight the importance of the VIG and canonicalization for generating high-quality vehicle instances in the 4D city scenes.  'Enc.' is an abbreviation for 'Encoder'.", "section": "3.5 Vehicle Instance Generator"}, {"figure_path": "https://arxiv.org/html/2501.08983/x10.png", "caption": "Figure 10: Localized Editing on the Generated Cities. (a) and (c) show vehicle editing results, while (b) and (d) present building editing results.", "description": "This figure showcases the localized editing capabilities of CityDreamer4D.  Panels (a) and (c) demonstrate modifications made to vehicles within the generated city scenes, highlighting changes in vehicle types and placement. Panels (b) and (d) illustrate similar localized edits applied to buildings, demonstrating adjustments to building heights and styles. These examples illustrate the model's ability to perform fine-grained edits on individual instances within a large, complex city environment without affecting other elements.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x11.png", "caption": "Figure 11: Text-driven City Stylization with ControlNet. The multi-view consistency is preserved in stylized Minecraft and Cyberpunk cities.", "description": "This figure demonstrates the application of ControlNet for text-driven city stylization.  Using ControlNet, the model successfully transforms generated city scenes into distinct styles, such as Minecraft and Cyberpunk, while maintaining multi-view consistency.  This showcases the ability of the CityDreamer4D model to preserve the structural integrity of the city despite significant stylistic changes, ensuring that different viewpoints of the same stylized city remain coherent and consistent.", "section": "5.5 Applications"}, {"figure_path": "https://arxiv.org/html/2501.08983/x12.png", "caption": "Figure 12: COLMAP Reconstruction of 600-frame Orbital Videos. The red ring shows the camera positions, and the clear point clouds demonstrate CityDreamer4D\u2019s consistent rendering. Note that \u201dRecon.\u201d stands for \u201dReconstruction.\u201d", "description": "This figure showcases the results of 3D reconstruction performed using COLMAP on a set of 600 orbital video frames generated by CityDreamer4D.  The red ring highlights the camera positions throughout the video sequence, and the resulting point cloud demonstrates the high quality and consistency of the 3D scene generated by the model. The density and clarity of the point cloud effectively illustrate CityDreamer4D's capability to produce temporally consistent 3D outputs suitable for applications like urban simulation and virtual reality.", "section": "5.6 Discussions"}, {"figure_path": "https://arxiv.org/html/2501.08983/x13.png", "caption": "Figure 13: Directional Light Relighting Effect. (a) and (b) show the lighting intensity. (c) illustrates the relighting effect. Note that \u201cS.M.\u201d denotes \u201cShadow Mapping\u201d.", "description": "This figure demonstrates the relighting effect achieved by separating Lambertian shading and shadow mapping. (a) shows the lighting intensity calculated using only Lambertian shading, resulting in uniform lighting across all directions. (b) shows the lighting intensity from shadow mapping, which accounts for light visibility and occlusion from objects. (c) combines both (a) and (b) to produce the final relighting effect, with a light source positioned on the left side of the scene, demonstrating more realistic lighting and shadows.", "section": "3 METHOD"}]