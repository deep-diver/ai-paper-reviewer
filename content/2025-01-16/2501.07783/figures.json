[{"figure_path": "https://arxiv.org/html/2501.07783/x1.png", "caption": "Figure 1: Different multi-resolution designs in visual perception and multimodal understanding.\n(a)(e) Plain network without multi-scale features.\n(b)(c)(f) Inefficient image pyramid networks using equivalently large models for all scales, either with shared weights or with separate weights and interactions.\n(d) Parameter-direct image pyramid network which processes high-resolution images with large models, leading to high computational cost.\n(g) Multi-resolution approaches on multimodal tasks based on grid partition.\n(h) Our efficient and effective parameter-inverted image pyramid network (PIIP), which pairs models of increasing parameter sizes inversely with images of decreasing resolution. It achieves better performance with much lower computational cost.", "description": "Figure 1 illustrates various multi-resolution approaches used in visual perception and multimodal understanding tasks.  (a) and (e) show basic network designs lacking multi-scale feature processing.  (b), (c), and (f) depict traditional image pyramid methods that use the same large model for all image resolutions, resulting in high computational costs.  This is true even if the model weights are shared or separate weights are used with interactions.  (d) demonstrates a parameter-direct method where large models process high-resolution images, again leading to high computation. (g) presents grid-based partitioning approaches for multimodal tasks. In contrast, (h) introduces the proposed Parameter-Inverted Image Pyramid Network (PIIP), which uses smaller models for higher-resolution images and larger models for lower-resolution images, achieving better performance with significantly reduced computation.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.07783/x2.png", "caption": "Figure 2: Overall architecture of PIIP. We use multi-resolution branches to process images of different resolutions, where larger images are handled by smaller models. Each branch leverages pretrained ViTs or CNNs. Interaction units build connections between adjacent branches. Branch merging is inserted after all the blocks or within certain intermediate blocks to combine the features of all branches.", "description": "The figure illustrates the architecture of the Parameter-Inverted Image Pyramid Network (PIIP).  PIIP uses multiple branches to process images at different resolutions, a key innovation being that higher-resolution images are processed by smaller models to optimize computational efficiency.  Each branch employs a pre-trained Vision Transformer (ViT) or Convolutional Neural Network (CNN).  Intermediate interaction units connect adjacent branches to integrate multi-scale feature information. Finally, a branch merging mechanism combines the features from all branches, either at the end or at intermediate stages, to produce the final output.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x3.png", "caption": "Figure 3: Illustration of PIIP-LLaVA for multimodal understanding.\nWe use one projector after each branch to align the visual features with the language embedding space of the LLM, and combine the features to obtain the visual features.", "description": "The figure illustrates the architecture of PIIP-LLaVA, a multimodal large language model designed for efficient high-resolution image understanding.  It uses a parameter-inverted image pyramid network (PIIP), where multiple branches process images at different resolutions. Smaller models process higher-resolution images, and larger models handle lower-resolution inputs, balancing computational cost and performance.  Each branch is a pre-trained Vision Transformer (ViT). After each branch, a projector aligns visual features to the language embedding space of the underlying large language model (LLM).  Finally, the aligned features from all branches are combined to produce a comprehensive set of visual features for the LLM.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.07783/x4.png", "caption": "Figure 4: Detailed structure of the interaction unit. It consists of two deformable attentions with fully-connect layers and feed-forward networks.", "description": "The interaction unit in Figure 4 is a crucial component of the PIIP network, facilitating communication between branches processing different image resolutions.  It's designed to fuse features from adjacent branches effectively.  The unit employs two deformable attention mechanisms. Each mechanism consists of a fully-connected layer for feature projection, followed by a deformable attention module to capture cross-scale dependencies. A feed-forward network further refines the combined features after each attention module. This two-step attention process enhances feature fusion from different scales and semantic levels, integrating complementary information to improve visual representation.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.07783/x5.png", "caption": "Figure 5: Detailed design of branch merging in different tasks. For detection, segmentation and multimodal understanding, output features from all branches are fused together with projection and upsampling, and fed into the subsequent FPN or LLM. For classification, we employ the original classification heads to compute logits, and average them as the final prediction.", "description": "Figure 5 illustrates how feature maps from different branches are combined in various tasks. For object detection, instance segmentation, and multimodal understanding, features undergo projection, upsampling, and fusion before being input into either a Feature Pyramid Network (FPN) or a Large Language Model (LLM).  In contrast, for image classification, the original classification heads process the features from each branch separately; their final outputs (logits) are then averaged to obtain a single prediction.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.07783/extracted/6128466/figures/interaction_types/inter_type_v4.png", "caption": "(a) Object detection", "description": "This figure shows qualitative results of object detection on several images.  The results demonstrate the model's ability to accurately detect and localize objects, including small objects that are sometimes difficult to identify. Each image displays bounding boxes around detected objects and corresponding confidence scores.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.07783/x6.png", "caption": "(b) Instance segmentation", "description": "This figure shows qualitative results for instance segmentation using the proposed Parameter-Inverted Image Pyramid Networks (PIIP).  The images depict various scenes with multiple objects, each of which is segmented and labelled with its corresponding class name and confidence score. This visualization demonstrates the accuracy and effectiveness of the PIIP model in detecting and segmenting objects across different scales and complexities.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.07783/x7.png", "caption": "Figure 6: Performance of different PIIP variants by adjusting input resolutions on object detection and instance segmentation.", "description": "Figure 6 presents a comparative analysis of various PIIP network configurations on object detection and instance segmentation tasks.  The models are evaluated using different input resolutions while keeping track of computational cost (measured in GFLOPS). The graphs illustrate how each PIIP variant's performance (AP box and AP mask scores) varies across various GFLOPS ranges. This visualization helps understand the performance-efficiency trade-offs offered by different PIIP designs and input resolutions, showing the impact of computational resources on the accuracy of both object detection and instance segmentation.", "section": "IV. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.07783/x8.png", "caption": "TABLE V: Experiments of initializing with different pre-trained weights on COCO val2017 with PIIP-SBL 1568/1120/672.", "description": "Table V presents the results of experiments conducted to assess the impact of using various pre-trained weights on the performance of the PIIP-SBL model.  The PIIP-SBL model, with a resolution of 1568/1120/672, was initialized using different pre-trained ViT weights. The table compares the resulting performance in terms of Average Precision (AP) for both bounding boxes (APb) and masks (APm) on the COCO val2017 dataset. The pre-trained weights considered include those from AugReg, DeiT III, MAE, Uni-Perceiver, DINOv2, and BEiTv2, illustrating the effect of different pre-training strategies on the final model's object detection capabilities.", "section": "IV. Experiments"}]