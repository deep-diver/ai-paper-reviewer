{"references": [{"fullname_first_author": "H. Touvron", "paper_title": "Training data-efficient image transformers & distillation through attention", "publication_date": "2021-MM-DD", "reason": "This paper introduces a method for training data-efficient image transformers, a key component in the proposed PIIP architecture."}, {"fullname_first_author": "A. Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-MM-DD", "reason": "This paper introduces CLIP, a foundational model used in the PIIP architecture for multimodal understanding."}, {"fullname_first_author": "T.-Y. Lin", "paper_title": "Feature pyramid networks for object detection", "publication_date": "2017-MM-DD", "reason": "This paper introduces Feature Pyramid Networks (FPN), a crucial technique that PIIP improves upon for efficient multi-scale feature extraction."}, {"fullname_first_author": "Z. Chen", "paper_title": "Vision transformer adapter for dense predictions", "publication_date": "2023-MM-DD", "reason": "This paper introduces the ViT-Adapter, a relevant multi-branch architecture that PIIP builds upon and improves for efficiency."}, {"fullname_first_author": "Z. Chen", "paper_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2024-MM-DD", "reason": "This paper introduces InternViT, a large-scale vision foundation model that PIIP leverages and improves upon for superior performance."}]}