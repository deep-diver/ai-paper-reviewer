{"importance": "This paper is crucial for researchers in information retrieval and multi-modal learning.  **It introduces MMDocIR**, a novel benchmark dataset addressing the limitations of existing benchmarks for multi-modal document retrieval, specifically focusing on long documents.  **Its comprehensive evaluation** and findings on visual vs. text-based retrieval methods provide valuable insights for developing more robust and effective retrieval systems. This work **opens new avenues for research** in multi-modal understanding and retrieval, particularly in handling complex, visually rich documents. The availability of MMDocIR as an open-source dataset (https://huggingface.co/MMDocIR) further enhances its impact and potential for future studies.", "summary": "MMDocIR benchmark dataset tackles multi-modal long document retrieval, revealing visual methods outperform text-based ones.", "takeaways": ["MMDocIR benchmark dataset is introduced, addressing limitations of current benchmarks for multi-modal long document retrieval.", "Experiments show that visual-based retrieval methods significantly outperform text-based methods in long document retrieval tasks.", "MMDocIR dataset is publicly available to support further research in multi-modal document retrieval and large language models' capabilities in handling long document understanding and retrieval."], "tldr": "Current multi-modal document retrieval benchmarks fall short in handling long documents and assessing retrieval at different granularities (page-level vs layout-level).  This limits development of robust systems. The paper addresses these gaps by presenting MMDocIR, a new benchmark dataset with detailed annotations for both page and layout-level retrieval in various domains and document types. \nMMDocIR contains a rich dataset with 313 long documents, 1685 meticulously annotated questions (page and layout-level labels), and additional bootstrapped labels.  Rigorous experiments using various retrieval baselines (visual and text-based) demonstrate that visual-based retrievers significantly outperform text-based counterparts.  Further analysis highlights the benefits of using Vision Language Models and token-level embedding for improved performance, but also points out that purely image-based approaches outperform hybrid methods.", "affiliation": "Noah's Ark Lab, Huawei", "categories": {"main_category": "Multimodal Learning", "sub_category": "Cross-Modal Retrieval"}, "podcast_path": "2501.08828/podcast.wav"}