{"importance": "This paper is crucial for researchers in information retrieval and multi-modal learning.  **It introduces a novel benchmark, MMDocIR, addressing the critical need for robust evaluation in multi-modal document retrieval.** This benchmark allows researchers to effectively evaluate their systems and facilitates the development of more advanced techniques.  **The findings on visual retrieval's superiority and the effectiveness of integrating visual elements open new avenues for future research**, potentially leading to significant improvements in document search and understanding.", "summary": "MMDocIR benchmark revolutionizes multi-modal document retrieval with a dual-task approach, enabling precise page and layout-level evaluations, significantly advancing the field.", "takeaways": ["MMDocIR provides a new benchmark for multi-modal document retrieval with page and layout level annotations.", "Visual-driven retrieval methods outperform text-based methods significantly.", "The study reveals the benefits of using vision-language models over OCR for text extraction in multi-modal retrieval."], "tldr": "Current multi-modal document retrieval benchmarks lack a comprehensive evaluation framework, especially for long documents. Existing benchmarks often have low-quality questions, incomplete documents, or only offer page-level retrieval, failing to capture the richness of multi-modal information. This limitation hinders accurate evaluation and progress in the field.\nThis paper introduces MMDocIR, a novel benchmark that overcomes these limitations. **MMDocIR features a dual-task framework for page and layout-level retrieval,** using high-quality, expertly annotated questions and comprehensive document sets.  **The results demonstrate the significant advantage of visual retrieval methods** compared to text-only methods, highlighting the importance of leveraging visual information for effective retrieval.  **MMDocIR provides a robust and comprehensive benchmark** that will accelerate research and development in multi-modal document retrieval.", "affiliation": "Noah's Ark Lab, Huawei", "categories": {"main_category": "Multimodal Learning", "sub_category": "Cross-Modal Retrieval"}, "podcast_path": "2501.08828/podcast.wav"}