[{"figure_path": "https://arxiv.org/html/2501.08994/x1.png", "caption": "Figure 1: The examples generated by RepVideo. RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.", "description": "Figure 1 showcases example videos generated using the RepVideo model.  These examples highlight the model's ability to create diverse video content with improved temporal coherence (smooth transitions between frames) and high level of detail (fine-grained spatial features).  This demonstrates the effectiveness of RepVideo in addressing the challenges of producing high-quality videos with both strong spatial and temporal aspects.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x2.png", "caption": "Figure 2: The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.", "description": "The figure illustrates the typical architecture of modern transformer-based video diffusion models.  These models comprise three main parts: a 3D Variational Autoencoder (VAE) for compressing video input into a lower-dimensional latent space, a text encoder to process the textual input and map it to a numerical representation, and a transformer network to capture spatial and temporal relationships within the latent video representation and generate the final video output based on both the text and video information. The VAE handles the video input, reducing its dimensionality. The text encoder processes the text prompt providing semantic understanding.  The transformer network combines these inputs to generate the target video.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x3.png", "caption": "Figure 3: The visualization of the attention distribution of each frame\u2019s token across the entire token sequence. The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.", "description": "This figure visualizes how attention mechanisms in a transformer network distribute attention across different frames within a video sequence.  Each subplot represents a different frame within the video.  The x-axis corresponds to tokens in the complete sequence (representing video frames and potentially textual context), and the y-axis represents the attention weight.  Multiple lines within each subplot show the attention distribution for different layers of the transformer.  The key observation is that shallower layers exhibit more spread-out attention across all tokens, while deeper layers show stronger attention to tokens within the same frame, indicating that deeper layers become increasingly frame-specific in their attention.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x4.png", "caption": "Figure 4: The visualization of attention maps across transformer layers. Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model\u2019s ability to establish coherent spatial semantics within individual frames.", "description": "Figure 4 visualizes attention maps across different layers of a transformer network used in video generation.  Each layer focuses on unique parts of the image, showing how the model processes spatial details. However, the lack of consistent attention across layers leads to disjointed visual representations, reducing the overall coherence and semantic understanding within each frame of the generated video.  This highlights the problem of fragmented features and lack of coordination between layers that the RepVideo method is designed to solve.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x5.png", "caption": "Figure 5: The average similarity between adjacent frame features across diffusion layers and denoising steps. The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.", "description": "This figure shows the average similarity between features of adjacent frames in a video generation model, considering both different layers of the model and different steps of the denoising process.  The x-axis represents the layer number, and the y-axis represents the average similarity.  Separate lines are shown for different stages of the denoising process. As the layer depth increases (moving along the x-axis), the similarity between adjacent frame features decreases. This means that deeper layers in the model generate features that are less similar from one frame to the next, suggesting a potential loss of temporal coherence. Additionally, as the denoising process progresses (moving from one line to the next, representing different steps), the overall similarity between adjacent frames also declines. This indicates a progressive loss of temporal consistency throughout the denoising process.  Therefore, the figure highlights how the deeper layers of the model increasingly differentiate between features of consecutive frames, possibly contributing to reduced temporal coherence in the generated video.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x6.png", "caption": "Figure 6: The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module. The aggregated features demonstrate more comprehensive semantic information and clearer structural details.", "description": "This figure compares feature maps from a standard transformer layer against those produced by the Feature Cache Module (FCM).  The FCM aggregates features from multiple adjacent transformer layers.  The comparison highlights how the aggregated features, generated by the FCM, offer more complete semantic information and more clearly defined structural details than the original, single-layer feature maps. This improvement is a key aspect of the RepVideo model, demonstrating enhanced feature representation.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x7.png", "caption": "Figure 7: The comparison of adjacent frame similarity between original and aggregated features. The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.", "description": "Figure 7 presents a comparative analysis of the similarity between adjacent frames' features.  It compares the similarity scores obtained from the original transformer layers' features to those obtained from the features aggregated within the Feature Cache Module. The results demonstrate that the aggregated features, which combine information from multiple transformer layers, exhibit significantly higher similarity between adjacent frames than the features extracted from individual transformer layers. This higher similarity signifies an improvement in temporal coherence within the generated videos, illustrating the effectiveness of the Feature Cache Module in creating temporally consistent video sequences.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x8.png", "caption": "Figure 8: The architecture of the enhanced cross-layer\nrepresentation framework.", "description": "The figure illustrates the RepVideo architecture, a novel framework designed to enhance the cross-layer representations in video diffusion models.  It shows how features from multiple adjacent transformer layers are aggregated (using a feature cache module) to create a more stable and comprehensive representation. This aggregated representation is then integrated with the original transformer inputs via a gating mechanism. This process helps maintain consistent feature details across frames and improve both spatial details and temporal coherence in generated videos.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x9.png", "caption": "Figure 9: The qualitative comparison between our method and the baseline CogVideoX-2B\u00a0[31]. The first row shows results from the baseline CogVideoX-2B\u00a0[31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.", "description": "Figure 9 presents a qualitative comparison of video generation results between the baseline model, CogVideoX-2B, and the proposed RepVideo model.  For four different text prompts, the top row displays videos generated by CogVideoX-2B, while the bottom row shows the corresponding videos generated by RepVideo. The visual comparison highlights RepVideo's superior performance in terms of video quality and temporal coherence, producing smoother and more visually consistent results.  Specific examples show improvements in fine details and reduced artifacts.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x10.png", "caption": "Figure 10: The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo. The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B\u00a0[31].", "description": "This figure provides a layer-wise comparison of feature maps generated by CogVideoX-2B and RepVideo for two example video prompts.  The visualization highlights that RepVideo consistently captures richer semantic information and preserves coherent spatial details across various layers of the model, unlike CogVideoX-2B which shows degradation in spatial details at deeper layers. This difference demonstrates the effectiveness of RepVideo's enhanced cross-layer representation in improving video generation quality.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x11.png", "caption": "Figure 11: The comparison of attention maps between CogVideoX-2B and RepVideo. The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B\u00a0[31].", "description": "Figure 11 visualizes attention maps from CogVideoX-2B and RepVideo, highlighting RepVideo's superior ability to maintain consistent semantic relationships between objects across layers.  CogVideoX-2B shows fluctuating and inconsistent attention patterns, implying weaker semantic coherence in its generated videos. In contrast, RepVideo demonstrates more stable attention throughout the layers, indicating stronger and more consistent semantic alignment between objects in the videos.", "section": "V. DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x12.png", "caption": "Figure 12: The cosine similarity between consecutive frames across layers.", "description": "This figure visualizes the temporal coherence of video generation across different layers of a transformer network.  It plots the cosine similarity between consecutive frames for various layers of the network.  Higher cosine similarity indicates stronger temporal consistency (smoother transitions between frames), while lower similarity suggests weaker temporal coherence (more abrupt changes between frames). The x-axis represents the frame number, and the y-axis represents the cosine similarity.  Separate lines are used for different layers of the model, allowing for comparison of temporal coherence across layers. The graph reveals how the similarity evolves across frames and layers, providing insights into the model's ability to generate temporally consistent videos.", "section": "IV. EXPERIMENTS"}]