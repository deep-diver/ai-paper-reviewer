[{"figure_path": "https://arxiv.org/html/2501.08994/x1.png", "caption": "Figure 1: The examples generated by RepVideo. RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.", "description": "Figure 1 showcases example video outputs generated by the RepVideo model.  These examples highlight the model's capability to generate diverse video content with improved temporal coherence (smooth and realistic transitions between frames) and fine-grained spatial details (clear and sharp visuals).  The videos depict a variety of scenes, demonstrating RepVideo's ability to handle different levels of complexity and visual styles.", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x2.png", "caption": "Figure 2: The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.", "description": "This figure illustrates the typical architecture of modern transformer-based video diffusion models.  It breaks down the process into three main components: a 3D Variational Autoencoder (VAE) which compresses the input video into a lower-dimensional latent representation; a text encoder that processes the textual input prompt (e.g., a description of the desired video) and translates it into a numerical representation suitable for the model; and a transformer network that takes both the latent video representation and the text embedding as input to generate the final video. The 3D VAE handles the temporal aspects of video processing. The transformer network, with its attention mechanisms, is key to capturing the complex spatial and temporal relationships within the video for a coherent output.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x3.png", "caption": "Figure 3: The visualization of the attention distribution of each frame\u2019s token across the entire token sequence. The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.", "description": "This figure visualizes how attention mechanisms in a transformer-based video generation model distribute attention across different frames within a video sequence.  The visualizations show attention maps for tokens representing each frame at various layers of the transformer.  Key observations are that shallower layers distribute attention more evenly across frames, indicating a broader contextual awareness. Conversely, deeper layers exhibit significantly more focused attention on tokens from the current frame, highlighting the increasing specificity of the model's representation as it processes through subsequent layers. This shift toward intra-frame focus in deeper layers suggests a transition from global contextual understanding to fine-grained feature analysis within individual frames.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x4.png", "caption": "Figure 4: The visualization of attention maps across transformer layers. Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model\u2019s ability to establish coherent spatial semantics within individual frames.", "description": "This figure visualizes attention maps across different layers of a transformer network used in video generation. Each layer focuses on different spatial regions, showcasing the model's ability to capture diverse features.  However, the lack of coordination between layers leads to fragmented representations, hindering the model's ability to create consistent spatial relationships and semantic coherence within individual frames. This inconsistency between layers negatively impacts the overall quality and coherence of generated videos, as it impacts the model's ability to produce internally consistent and semantically meaningful frames.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x5.png", "caption": "Figure 5: The average similarity between adjacent frame features across diffusion layers and denoising steps. The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.", "description": "This figure visualizes the average similarity between features of adjacent frames in a video generation model.  Two key trends are observed. First, at any given step in the denoising process, similarity decreases as you go deeper through the model's layers. This shows that deeper layers differentiate features more strongly between frames. Second, across the denoising process (as the steps progress), the similarity between adjacent frames gradually decreases.  This decrease suggests that although the model refines video features over the denoising process, it also increases variability between frames, potentially introducing artifacts such as abrupt motion changes.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x6.png", "caption": "Figure 6: The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module. The aggregated features demonstrate more comprehensive semantic information and clearer structural details.", "description": "This figure compares feature maps from a standard transformer layer with those produced by the Feature Cache Module in the RepVideo model. The Feature Cache Module aggregates features from multiple adjacent transformer layers, resulting in aggregated features that show more comprehensive semantic information and clearer structural details compared to the original feature maps. This demonstrates the effectiveness of the module in improving the quality of video generation by providing richer, more stable representations.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x7.png", "caption": "Figure 7: The comparison of adjacent frame similarity between original and aggregated features. The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.", "description": "Figure 7 presents a comparative analysis of the similarity between adjacent frames' features.  Two sets of features are compared: the original features directly from the transformer layers of a video generation model, and the aggregated features produced by the Feature Cache Module (a component of the RepVideo framework introduced in the paper). The graph shows that the aggregated features exhibit substantially higher similarity between consecutive frames. This improved similarity is a key indicator of enhanced temporal coherence in the generated videos, signifying smoother and more natural transitions between frames.  The enhanced coherence is achieved because the Feature Cache Module combines features from multiple transformer layers, creating a more stable and consistent representation.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x8.png", "caption": "Figure 8: The architecture of the enhanced cross-layer\nrepresentation framework.", "description": "The figure illustrates the RepVideo architecture, an enhanced cross-layer representation framework designed to improve video generation.  It shows how features from multiple adjacent transformer layers are aggregated using a feature cache module.  This aggregated information is then combined with the original transformer inputs via a gating mechanism, creating enriched feature representations that are fed into subsequent transformer layers.  This process aims to enhance both the temporal consistency and the spatial details of the generated videos by stabilizing feature representation across the layers and reducing inconsistencies between adjacent frames.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x9.png", "caption": "Figure 9: The qualitative comparison between our method and the baseline CogVideoX-2B\u00a0[31]. The first row shows results from the baseline CogVideoX-2B\u00a0[31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.", "description": "This figure presents a qualitative comparison of video generation results between the proposed RepVideo model and the baseline CogVideoX-2B model.  The comparison uses four different video prompts, each visualized across two rows. The top row displays the output from CogVideoX-2B, while the bottom row shows the improved results generated by RepVideo. The visualization clearly showcases that RepVideo produces videos with superior visual quality and enhanced temporal and spatial coherence, demonstrating a significant improvement in video generation capabilities. Each set of videos, generated from the same prompt, allows for direct comparison of the model's performance in terms of color accuracy, scene consistency, and movement fluidity. The improved quality in RepVideo's output is evident across all prompts.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x10.png", "caption": "Figure 10: The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo. The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B\u00a0[31].", "description": "Figure 10 presents a layer-by-layer comparison of feature maps generated by RepVideo and CogVideoX-2B.  The visualization reveals that RepVideo consistently produces feature maps with richer semantic information and more coherent spatial details throughout the different layers of the transformer network.  In contrast, CogVideoX-2B shows a decline in the quality of feature maps, indicating less coherent spatial representations and less semantic information in deeper layers. Two example images are shown to illustrate this difference.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x11.png", "caption": "Figure 11: The comparison of attention maps between CogVideoX-2B and RepVideo. The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B\u00a0[31].", "description": "Figure 11 visualizes a comparison of attention maps generated by two different video generation models: CogVideoX-2B and RepVideo.  The images show attention map visualizations across multiple layers for a single frame generated under the same prompt.  The main point is to highlight that RepVideo, unlike CogVideoX-2B, maintains a more consistent and coherent semantic relationship across the various layers of its network. This consistency is reflected in the visual attention patterns, indicating better alignment and understanding of the scene elements.  Inconsistency in CogVideoX-2B\u2019s attention maps across layers suggests the model struggles to maintain a unified understanding of the subject and context across different processing stages, potentially leading to less coherent and lower quality video generation.", "section": "V. DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x12.png", "caption": "Figure 12: The cosine similarity between consecutive frames across layers.", "description": "This figure visualizes the temporal consistency of video generation across different layers of a transformer network.  It plots the cosine similarity between consecutive frames as a function of layer depth within the network.  Higher cosine similarity indicates stronger temporal coherence, meaning smoother transitions between frames.  The figure shows that, as the layer depth increases, the cosine similarity tends to decrease, suggesting that deeper layers introduce greater variability and potentially disrupt temporal coherence in the video generation process.  Different lines within the plot represent the similarity at different steps in the denoising process of the diffusion model, further illustrating how temporal consistency evolves across layers and during the denoising process.", "section": "V. DISCUSSION"}]