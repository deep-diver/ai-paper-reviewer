[{"figure_path": "https://arxiv.org/html/2501.08994/x1.png", "caption": "Figure 1: The examples generated by RepVideo. RepVideo effectively generates diverse videos with enhanced temporal coherence and fine-grained spatial details.", "description": "This figure showcases example videos generated using the RepVideo model.  Each video demonstrates the model's capacity to generate diverse video content with high visual fidelity.  Specifically, the examples highlight RepVideo's ability to produce videos exhibiting enhanced temporal coherence (smooth and natural transitions between frames) and fine-grained spatial details (sharp, clear visuals with accurate depictions of textures and objects).", "section": "I. INTRODUCTION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x2.png", "caption": "Figure 2: The architecture of recent transformer-based video diffusion models. These methods typically consist of three core components: a 3D VAE, the text encoder, and a transformer network.", "description": "This figure illustrates the typical architecture of modern transformer-based video diffusion models.  These models consist of three main parts working together: 1) A 3D Variational Autoencoder (VAE) which compresses the input video into a lower-dimensional latent representation, reducing the computational burden of processing high-resolution videos. 2) A text encoder processes the input text prompt, transforming the textual description into a numerical representation that can be understood by the model.  3) A transformer network then takes both the compressed video (latent representation) and the textual embedding, which it uses to predict the sequence of latent representations that correspond to the target video, effectively generating a video from text. The transformer uses its attention mechanism to learn spatial and temporal relationships within the data.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x3.png", "caption": "Figure 3: The visualization of the attention distribution of each frame\u2019s token across the entire token sequence. The results highlight significant variations in attention distributions across layers, with deeper layers focusing more on tokens from the same frame and exhibiting weaker attention to tokens from other frames.", "description": "This figure visualizes how attention mechanisms in a transformer network distribute attention across different frames within a video sequence.  The visualization shows that attention patterns vary significantly across different layers of the transformer. In shallower layers, attention is more broadly distributed across all frames. As the network goes deeper, the attention becomes increasingly concentrated on tokens belonging to the same frame, indicating a stronger focus on individual frame details in later processing stages. This is shown through the distribution of attention weights across tokens from different frames.  The shift in attention distribution with depth suggests a trade-off between global context (in shallower layers) and local detail (in deeper layers) during video generation.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x4.png", "caption": "Figure 4: The visualization of attention maps across transformer layers. Each layer attends to distinct regions, capturing diverse spatial features. However, the lack of coordination across layers results in fragmented feature representations, weakening the model\u2019s ability to establish coherent spatial semantics within individual frames.", "description": "Figure 4 visualizes the attention maps generated by different layers of a transformer network within a video diffusion model.  Each layer's attention map shows where the model focuses its attention when processing information for a single frame.  While each layer focuses on different aspects of the image (capturing diverse spatial features), the lack of coordination between layers causes the overall representation to be fragmented and less coherent. This fragmentation weakens the model's ability to accurately represent the spatial relationships within the frame and produce a semantically consistent image.  In essence, while each layer independently learns aspects of the scene, the holistic understanding of the scene's spatial structure is diminished due to this lack of communication between layers.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x5.png", "caption": "Figure 5: The average similarity between adjacent frame features across diffusion layers and denoising steps. The similarity decreases as layer depth increases for a given denoising step, indicating greater differentiation in deeper layers. Additionally, similarity between adjacent frames declines as the denoising process progresses.", "description": "This figure shows the average similarity between consecutive frames' features as the denoising process unfolds across different layers of a diffusion model.  The x-axis represents the layer number (depth within the network), and the y-axis represents the similarity score, indicating how alike the features of adjacent frames are. Separate lines are shown for different denoising steps, representing different stages of the video generation process. The key observation is a downward trend along both axes: similarity decreases as you go deeper into the network (more layers), and similarity also decreases as the denoising progresses (more noise removed). This visualizes how the model's representation of the video becomes increasingly differentiated between frames as both layer depth and the diffusion process progress, potentially leading to issues with temporal coherence.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x6.png", "caption": "Figure 6: The comparison of the original feature maps from a standard transformer layer with those obtained after aggregation in the Feature Cache Module. The aggregated features demonstrate more comprehensive semantic information and clearer structural details.", "description": "This figure compares feature maps from a standard transformer layer against those produced by the Feature Cache Module (FCM). The FCM aggregates features from multiple layers, resulting in enhanced semantic information and structural clarity in the aggregated feature maps compared to the original features from a single layer.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x7.png", "caption": "Figure 7: The comparison of adjacent frame similarity between original and aggregated features. The aggregated features from the Feature Cache Module exhibit higher similarity between adjacent frames compared to the original transformer layers, indicating improved temporal coherence.", "description": "Figure 7 displays a comparison of the similarity between adjacent frames' features using two different methods: the original features from the standard transformer layers and the aggregated features from the Feature Cache Module (FCM).  The graph shows that the aggregated features from the FCM consistently demonstrate higher similarity scores between successive frames. This clearly indicates that the FCM effectively improves the temporal coherence of generated videos by producing features with stronger consistency between adjacent frames.", "section": "III. METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2501.08994/x8.png", "caption": "Figure 8: The architecture of the enhanced cross-layer\nrepresentation framework.", "description": "The figure illustrates the RepVideo architecture, an enhanced cross-layer representation framework designed to improve video generation.  The core of RepVideo is the Feature Cache Module. This module aggregates feature maps from multiple adjacent transformer layers within the video diffusion model.  The aggregated features, representing a stable and enriched semantic representation, are then combined with the original transformer inputs using a gating mechanism. This gating mechanism allows for a dynamic balance between the enhanced features and layer-specific information. The RepVideo framework aims to increase both the spatial coherence and temporal consistency in generated video outputs by leveraging these stable and enriched representations.", "section": "III. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.08994/x9.png", "caption": "Figure 9: The qualitative comparison between our method and the baseline CogVideoX-2B\u00a0[31]. The first row shows results from the baseline CogVideoX-2B\u00a0[31], while the second row presents the results generated by RepVideo, demonstrating significant improvements in quality and coherence.", "description": "This figure presents a qualitative comparison of video generation results between the proposed RepVideo method and the baseline CogVideoX-2B.  The top row displays samples generated by CogVideoX-2B, while the bottom row shows corresponding results from RepVideo. The comparison highlights RepVideo's significant improvements in terms of video quality and temporal coherence.  Each column represents a different video generation task based on a textual prompt. RepVideo produces visually more appealing and temporally more consistent videos, demonstrating the effectiveness of the proposed method.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x10.png", "caption": "Figure 10: The Layer-wise comparison of feature maps between CogVideoX-2B and RepVideo. The comparison shows that RepVideo consistently captures richer semantic information and maintains more coherent spatial details across layers compared to CogVideoX-2B\u00a0[31].", "description": "This figure visualizes a layer-wise comparison of feature maps generated by RepVideo and CogVideoX-2B for two different video generation examples. It demonstrates that RepVideo consistently produces feature maps that are richer in semantic information and exhibit greater spatial coherence across different layers when compared to CogVideoX-2B.  The enhanced clarity and detail in RepVideo's feature maps suggest its superior ability to capture and maintain fine-grained spatial information throughout the video generation process, contributing to higher-quality and more coherent video outputs.", "section": "IV. EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2501.08994/x11.png", "caption": "Figure 11: The comparison of attention maps between CogVideoX-2B and RepVideo. The comparison demonstrates that RepVideo could maintain more consistent semantic relationship compared to CogVideoX-2B\u00a0[31].", "description": "Figure 11 visualizes a comparison of attention maps generated by two different video generation models: CogVideoX-2B and RepVideo.  The attention maps highlight which parts of the input video frames are most influential in generating the output.  The key finding is that RepVideo demonstrates more consistent and stable attention patterns across different layers of its network. This suggests a stronger, more coherent understanding of the scene's semantic content. In contrast, CogVideoX-2B shows more variability and less consistent attention, potentially leading to a less coherent output video.  The figure provides visual evidence supporting the claim that RepVideo's approach enhances the semantic consistency of the generated videos, demonstrating its ability to maintain a stable interpretation of the scene throughout the generation process.", "section": "V. DISCUSSION"}, {"figure_path": "https://arxiv.org/html/2501.08994/x12.png", "caption": "Figure 12: The cosine similarity between consecutive frames across layers.", "description": "This figure visualizes the temporal consistency of video generation across different layers of a transformer network.  It plots the cosine similarity between consecutive frames for various layers of the model. Higher cosine similarity indicates stronger temporal coherence, meaning smoother transitions between frames. The x-axis represents the frame number, and the y-axis shows the cosine similarity.  The plot allows for analysis of how well the model maintains temporal consistency as the network processes the video, and whether this consistency changes across the different layers.  This is a key metric to assess the quality and coherence of the generated video.", "section": "V. DISCUSSION"}]