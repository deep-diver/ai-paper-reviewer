[{"figure_path": "https://arxiv.org/html/2503.21758/x1.png", "caption": "Figure 1: High-quality samples from our Lumina-Image 2.0, showcasing its capabilities in ultra-realistic, text generation, artistic versatility, bilingual mastery, logical reasoning, and unified multi-image generation.", "description": "Figure 1 showcases the high-quality image generation capabilities of Lumina-Image 2.0 across various tasks.  The samples demonstrate its ability to produce ultra-realistic images, generate images from text prompts with artistic versatility, handle bilingual prompts with accuracy, perform logical reasoning tasks within image generation, and generate multiple images in a unified sequence.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21758/x2.png", "caption": "Figure 2: Overview of Lumina-Image 2.0, which consists of Unified Captioner and Unified Next-DiT. The Unified Captioner re-captions web-crawled and synthetic images to construct hierarchical text-image pairs, which are then used to optimize Unified Next-DiT with our efficient training strategy.", "description": "Lumina-Image 2.0 is composed of two main components: the Unified Captioner and the Unified Next-DiT.  The Unified Captioner takes web-crawled and synthetic images as input and generates multi-level, high-quality captions for each image. These captions are then used to create a hierarchical dataset of image-text pairs. This dataset is used to train the Unified Next-DiT model, which is a unified text-to-image generation model. The efficient training strategy helps ensure that the model can generate high-quality images. ", "section": "4 Lumina-Image 2.0"}, {"figure_path": "https://arxiv.org/html/2503.21758/x3.png", "caption": "Figure 3: We compare the Diffusion Transformer architectures between our Unified Next-DiT, and PixArt\u00a0[15], Lumina-Next\u00a0[17], Stable Diffusion 3\u00a0[36], OmniGen\u00a0[37] and FLUX\u00a0[9].", "description": "Figure 3 provides a visual comparison of different Diffusion Transformer architectures.  It highlights the key architectural differences between the authors' proposed Unified Next-DiT model and several other prominent models, including PixArt [15], Lumina-Next [17], Stable Diffusion 3 [36], OmniGen [37], and FLUX [9]. The comparison focuses on the core components and arrangement of the transformer blocks to illustrate the design choices and potential advantages of each architecture.  The figure aids in understanding the differences and the design choices behind the Unified Next-DiT architecture.", "section": "3 Revisiting Lumina-Next"}, {"figure_path": "https://arxiv.org/html/2503.21758/x4.png", "caption": "Figure 4: The training loss curve with respect to captions with different lengths. The \u201cAvg. Length\u201d represents the average character number.", "description": "This figure shows the training loss curves for three different caption lengths: short captions from Florence, relatively short captions from UniCap, and longer, more detailed captions from UniCap.  The x-axis represents the number of training steps, and the y-axis represents the training loss.  The graph visually demonstrates that the model converges faster and achieves a lower loss when trained with longer, more detailed captions compared to shorter captions.  The 'Avg. Length' specifies the average number of characters in each caption type.", "section": "4.3 Unified Captioner"}, {"figure_path": "https://arxiv.org/html/2503.21758/x5.png", "caption": "Figure 5: Illustration of reformulating the Image-Text Attention as an FFN generated by a hyper-network, with its weights and hidden dimensions dynamically determined by the input text token.", "description": "Figure 5 illustrates how the image-text attention mechanism in a transformer network can be reinterpreted as a feed-forward network (FFN). This FFN is generated by a hyper-network, meaning its weights and the dimensionality of its hidden layer are dynamically adjusted according to the input text tokens. The diagram visually represents this transformation by showing how the input text token influences the hyper-network's output, which in turn determines the FFN's structure and parameters. This reformulation helps to explain how the model's capacity adapts to different text lengths, implying that longer descriptions allow for a richer representation and better integration of textual and visual information.", "section": "4.2 Unified Next-DiT"}, {"figure_path": "https://arxiv.org/html/2503.21758/x8.png", "caption": "Table 5: Comparison of ELO scores evaluated in text-to-image arena from Artificial Analysis 1 (as of February 23, 2025).", "description": "This table presents a comparison of ELO scores from the Artificial Analysis 1 text-to-image arena as of February 23, 2025.  ELO scores are a measure of relative performance, reflecting the results of competition between different models in the arena. The table allows for a direct comparison of the performance of various models, showing their relative strengths and weaknesses in generating images from text prompts.", "section": "5.2 Quantitative Performance"}, {"figure_path": "https://arxiv.org/html/2503.21758/x9.png", "caption": "Table 6: Comparison of ELO scores evaluated in text-to-image arena from Rapidata 2 (as of February 23, 2025).", "description": "This table compares the ELO scores of different text-to-image models in the Rapidata 2 arena as of February 23, 2025.  The ELO scores reflect the relative performance of each model based on head-to-head comparisons judged by human evaluators.  Higher scores indicate better performance.", "section": "5.2 Quantitative Performance"}, {"figure_path": "https://arxiv.org/html/2503.21758/x10.png", "caption": "Figure 6: Visualization results of multilingual text-to-image generation by our Lumina-Image 2.0, covering five languages: Chinese, Japanese, English, Russian, and German.", "description": "This figure displays the results of Lumina-Image 2.0's multilingual text-to-image generation capabilities.  It shows examples of images generated from the same prompts translated into five different languages: Chinese, Japanese, English, Russian, and German. This demonstrates the model's ability to understand and generate images accurately across multiple languages, showcasing its robustness and multilingual capabilities.", "section": "5.3 Qualitative Performance"}, {"figure_path": "https://arxiv.org/html/2503.21758/x11.png", "caption": "Figure 7: Comparison with ShareGPT4V [46] and Florence [62] in complex scenes and dense text for caption generation. The blue underline correspond to areas with more detailed and accurate descriptions, while red underline and red strikethrough represent the incorrect and insufficient descriptions respectively.", "description": "Figure 7 compares the captioning abilities of three models: UniCap (the model introduced in this paper), ShareGPT4V, and Florence.  The figure shows example captions generated by each model for a complex image.  Blue underlines highlight areas where UniCap provides significantly more detailed and accurate descriptions than the other models. Conversely, red underlines and strikethroughs show where ShareGPT4V and Florence provide inaccurate or incomplete information compared to UniCap. This illustrates UniCap's superiority in handling complex scenes and generating comprehensive and precise descriptions.", "section": "4.3 Unified Captioner"}, {"figure_path": "https://arxiv.org/html/2503.21758/x12.png", "caption": "Figure 8: Comparison with ShareGPT4V [46] and Florence [62] in visual understanding and spatial relationships. The blue underline correspond to areas with more detailed and accurate descriptions, while red underline and red strikethrough represent the incorrect and insufficient descriptions respectively.", "description": "Figure 8 presents a comparison of captioning results generated by UniCap, ShareGPT4V [46], and Florence [62] for images containing complex visual relationships.  UniCap's superior performance is highlighted by comparing the captions side-by-side; UniCap provides more comprehensive and accurate descriptions of the visual details and spatial relationships within the images. Conversely, ShareGPT4V and Florence show deficiencies in accurately capturing the visual understanding and precise spatial aspects. Blue underlines indicate where UniCap provided superior descriptions; red underlines and strikethroughs indicate where the other models made mistakes or omissions.", "section": "5.3 Qualitative Performance"}, {"figure_path": "https://arxiv.org/html/2503.21758/x13.png", "caption": "Figure 9: High-quality image generation examples from Lumina-Image 2.0, showcasing its precise prompt-following ability and its capability to generate highly aesthetic and realistic images across different resolutions.", "description": "Figure 9 presents a collection of images generated by Lumina-Image 2.0.  These examples highlight the model's ability to accurately interpret and fulfill detailed user prompts, producing highly realistic and visually appealing results. The images showcase a variety of styles and subjects, demonstrating the model's versatility and capacity to generate high-quality output across different resolutions. The figure serves as visual evidence of the model's capabilities in precise prompt-following, aesthetic generation, and resolution scalability.", "section": "5.3 Qualitative Performance"}, {"figure_path": "https://arxiv.org/html/2503.21758/x14.png", "caption": "Figure 10: Loss curves for the three training stages, showing a steady performance increase in the DPG\u00a0[30] and GenEval\u00a0[31] benchmark.", "description": "This figure presents the training loss curves for Lumina-Image 2.0 across three training stages: low-resolution, high-resolution, and high-quality tuning.  Each stage is characterized by a distinct dataset resolution and size, with progressively higher quality data used in later stages. The curves show a general downward trend, indicating that the model is learning effectively throughout the training process. Notably, there is a clear improvement in performance (lower loss) as the training progresses to the later stages, which demonstrates the effectiveness of the multi-stage training strategy on both DPG and GenEval benchmarks.", "section": "4.4 Efficient Training"}]