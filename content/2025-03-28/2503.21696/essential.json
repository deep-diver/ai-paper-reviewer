{"importance": "This paper offers a valuable dataset and model for **embodied AI research**. It tackles the challenges of interactive reasoning, paving the way for more intelligent agents in complex environments. It opens doors for future work in long-horizon tasks and real-world applications.", "summary": "Embodied-Reasoner: Integrates visual search, reasoning, and action for interactive tasks, outperforming existing models in embodied environments.", "takeaways": ["Introduces a new framework for extending deep-thinking to embodied scenarios.", "Presents a data engine that synthesizes diverse embodied reasoning trajectories.", "Demonstrates superior performance over state-of-the-art models in complex, long-horizon tasks."], "tldr": "Deep thinking models excel in math and coding but struggle with embodied tasks needing constant environment interaction. **This paper introduces Embodied-Reasoner**, which brings 01-style reasoning to embodied search. Unlike logic-based math reasoning, embodied tasks need spatial understanding, temporal reasoning, and reflection using interaction history. It includes 9.3k Observation-Thought-Action trajectories with 64k images and 90k thinking processes for analysis and planning.\n\nThe model trains in three stages: imitation learning, self-exploration through rejection sampling, and self-correction using reflection tuning. **Embodied-Reasoner outperforms visual reasoning models like OpenAI 01, 03-mini, and Claude-3.7 by +9%, 24%, and +13% respectively**. It reduces repeated searches and logical errors, excelling in complex tasks. Real-world tests prove its superiority with fewer repeated searches and logical inconsistencies.", "affiliation": "Zhejiang University", "categories": {"main_category": "Multimodal Learning", "sub_category": "Embodied AI"}, "podcast_path": "2503.21696/podcast.wav"}