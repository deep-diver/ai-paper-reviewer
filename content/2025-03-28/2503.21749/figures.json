[{"figure_path": "https://arxiv.org/html/2503.21749/x2.png", "caption": "Figure 1: Given the prompts for visual text generation, our proposed LeX-FLUX and LeX-Lumina can generate text images with multiple words, aesthetic complex layout, and good text attributes controllability.", "description": "This figure showcases examples of visual text generation produced by the LeX-FLUX and LeX-Lumina models.  Each image demonstrates the model's ability to generate multiple words within a complex layout, exhibiting high aesthetic quality and precise control over text attributes (such as font, color, and position). The prompts used to generate these images are displayed alongside each resulting image, highlighting the models' capacity to interpret diverse and complex instructions accurately.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.21749/x3.png", "caption": "Figure 2: Illustration of LeX-Art Framework.", "description": "The LeX-Art framework is composed of three key components: dataset construction, model fine-tuning, and benchmark evaluation.  The dataset is created using DeepSeek-R1 to enhance prompts and filter images, resulting in LeX-10K.  LeX-Enhancer, a prompt enrichment model, and two text-to-image models (LeX-FLUX and LeX-Lumina) are trained using this dataset. The LeX-Bench benchmark evaluates the fidelity, aesthetics, and alignment of visual text generation, using metrics like Pairwise Normalized Edit Distance (PNED).", "section": "3. LeX-Art"}, {"figure_path": "https://arxiv.org/html/2503.21749/x4.png", "caption": "Figure 3: The framework of data construction pipeline. The red words in the R1 enhanced prompt are not rendered in the generated image, and it is fixed after the knowledge-augmented recaption by gpt-4o.", "description": "This figure illustrates the data construction pipeline used in LeX-Art for generating high-quality text-image datasets. It starts with seed prompts that are enhanced using DeepSeek-R1, resulting in prompts that include detailed descriptions of visual attributes.  These enriched prompts are then used to generate images using a text-to-image model. The generated images and captions are then subjected to a multi-stage filtering process to ensure high quality and alignment. Finally, GPT-40 is utilized for knowledge-augmented recaptioning, where the captions are refined to precisely match the generated images, handling cases where certain words in the enhanced prompt were not accurately rendered.  The example showcases how an initial prompt is enhanced, an image is generated, and then the caption is refined to accurately reflect the final image.", "section": "3. LeX-Art"}, {"figure_path": "https://arxiv.org/html/2503.21749/x5.png", "caption": "Figure 4: Images generated by FLUX.1 [dev]\u00a0[25] based on different prompts. The origin caption from the first raw to the bottom raw: (1) A poster with the words Good Music remixed and unreleased on it, with text on it: \u201cUNRELEASED\u201d, \u201cREMIXED\u201d, \u201cGOOD.MUSIC\u201d, \u201cKANYEWEST\u201d, \u201cSPERIOD\u201d. (2) A movie poster, with text on it: \u201cAFACE\u201d, \u201cWITHOUT\u201d, \u201cEYES\u201d, \u201cDOL\u201d, \u201cJUL\u201d. (3) A menu of a fast food restaurant that contains \u201cSandwich Combo\u201d, \u201cGrilled Chicken\u201d, \u201cLettuce\u201d, \u201cTomato\u201d, \u201cMayo\u201d, \u201cFries&Drink\u201d, and \u201cPepsi\u201d.", "description": "This figure showcases examples of text image generation using the FLUX.1 [dev] model.  Three different prompts were used, resulting in three distinct images. The first image is a poster advertising 'Good Music', with the words 'UNRELEASED', 'REMIXED', 'GOOD.MUSIC', 'KANYEWEST', and 'SPERIOD' incorporated into the design.  The second image resembles a movie poster, featuring the words 'AFACE', 'WITHOUT', 'EYES', 'DOL', and 'JUL'.  The third image is a stylized fast-food menu, advertising a 'Sandwich Combo' and listing ingredients like 'Grilled Chicken', 'Lettuce', 'Tomato', 'Mayo', 'Fries & Drink', and 'Pepsi'.  These diverse outputs illustrate the model's ability to generate images with varying styles and information.", "section": "3. LeX-Art"}, {"figure_path": "https://arxiv.org/html/2503.21749/x6.png", "caption": "Figure 5: Image quality score and image aesthetics score distribution of AnyText dataset\u00a0[43] and LeX-10K. We randomly sampled 10K data entries from AnyWord-3M. Using Q-Align\u00a0[50], we calculated the quality scores and aesthetic scores for these 10K data entries along with the images in LeX-10K, and visualized the distributions of these two types of scores. We observed that LeX-10K generally has higher quality scores and aesthetic scores overall.", "description": "This figure compares the image quality and aesthetics scores between the AnyText dataset and the LeX-10K dataset.  10,000 samples were randomly selected from the AnyWord-3M dataset for comparison. The Q-Align metric was used to assess both image quality and aesthetics. The results show that LeX-10K images generally have significantly higher scores in both categories.", "section": "3.1 LeX-10K dataset"}, {"figure_path": "https://arxiv.org/html/2503.21749/extracted/6315872/figures/win_tie_lose.png", "caption": "Figure 6: Overview of LeX-Bench. Prompts in LeX-Bench are split into three levels: 630 Easy-Level (2\u20134 words), 480 Medium-Level (5\u20139 words), and 200 Hard-Level (10\u201314 words). Prompts of the easy level also contain text attributes: color, font, position.", "description": "LeX-Bench is a benchmark dataset designed to evaluate the performance of text-to-image generation models. It consists of 1310 prompts categorized into three difficulty levels: Easy (2-4 words), Medium (5-9 words), and Hard (10-14 words). The Easy level prompts further incorporate text attribute constraints including color, font, and position, providing a more comprehensive evaluation of the models' capabilities.", "section": "3.4. LeX-Bench"}, {"figure_path": "https://arxiv.org/html/2503.21749/x7.png", "caption": "Figure 7: Human preference result on text accuracy, text recall rate and aesthetics for LeX-Lumina. For ease\nof illustration, we visualize the proportion of votes where LeX-Lumina wins, loses and ties with Lumina-Image 2.0.", "description": "This figure presents the results of a human preference evaluation comparing the performance of LeX-Lumina and Lumina-Image 2.0 across three aspects: text accuracy, text recall rate, and aesthetics.  The visualization uses a bar chart to show the percentage of votes where each model (LeX-Lumina and Lumina-Image 2.0)  'wins', 'loses', or 'ties' in each category. This provides a clear visual representation of the relative strengths and weaknesses of each model in terms of visual text generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.21749/x8.png", "caption": "Figure 8: Qualitative comparison between LeX-Lumina, LeX-FLUX and glyph-conditioned models. We compare our models with AnyText\u00a0[43] and TextDiffuser\u00a0[5] for five different prompts. We observe that our models generally achieve high fidelity, better text attribute controllability and higher aesthetics.", "description": "Figure 8 presents a qualitative comparison of text-to-image generation results from LeX-Lumina, LeX-Flux, AnyText [43], and TextDiffuser [5] across five distinct prompts.  The comparison highlights LeX-Lumina and LeX-Flux's superior performance in terms of text fidelity, precise control over text attributes (color, font, etc.), and overall aesthetic appeal compared to the glyph-conditioned models AnyText and TextDiffuser.", "section": "3. LeX-Art"}, {"figure_path": "https://arxiv.org/html/2503.21749/x9.png", "caption": "Figure 9: Qualitative comparison between Lumina-Image 2.0\u00a0[35] and LeX-Lumina. The first column shows Lumina-Image-2.0 without LeX-Enhancer using Simple Caption; the second column shows the trained LeX-Lumina without LeX-Enhancer using Simple Caption; the third column shows Lumina-Image-2.0 with LeX-Enhancer enabled; and the fourth column shows LeX-Lumina with LeX-Enhancer enabled. We observe that (1) LeX-Lumina exhibits a better text rendering capability in terms of text fidelity and aesthetics; (2) LeX-Enhancer exhibits a strong capability for enhancing simple prompts.", "description": "This figure presents a qualitative comparison of image generation results from two models: Lumina-Image 2.0 and LeX-Lumina.  Each model is evaluated under two conditions: without and with LeX-Enhancer (a prompt enhancement module), and using simple captions. The comparison showcases LeX-Lumina's superior text rendering capabilities, demonstrating higher text fidelity and better aesthetics compared to Lumina-Image 2.0.  Additionally, it highlights the effectiveness of LeX-Enhancer in improving results for both models, suggesting that this prompt enhancement method significantly boosts the quality of image generation from simple captions.", "section": "3. LeX-Art"}, {"figure_path": "https://arxiv.org/html/2503.21749/x10.png", "caption": "Figure 10: Qualitative comparison between FLUX.1 [dev]\u00a0[25] and LeX-FLUX. The first column shows FLUX.1 [dev] without LeX-Enhancer using Simple Caption; the second column shows the trained LeX-FLUX without LeX-Enhancer using Simple Caption; the third column shows FLUX.1 [dev] with LeX-Enhancer enabled; and the fourth column shows LeX-FLUX with LeX-Enhancer enabled. We observe that (1) LeX-FLUX exhibits a better text rendering capability in terms of text fidelity and text attributes controllability; (2) LeX-Enhancer exhibits a strong capability for enhancing simple prompts.", "description": "This figure presents a qualitative comparison of text rendering results between FLUX.1 [dev] and LeX-FLUX, with and without the LeX-Enhancer.  The comparison is based on five different prompts. Each row shows the results for a single prompt, while each column shows different model configurations:  (1) FLUX.1 [dev] with a simple caption and no LeX-Enhancer, (2) LeX-FLUX (trained with the LeX-10K dataset) with a simple caption and no LeX-Enhancer, (3) FLUX.1 [dev] with a simple caption and LeX-Enhancer enabled, and (4) LeX-FLUX with a simple caption and LeX-Enhancer enabled. The results visually demonstrate that LeX-FLUX exhibits improved text fidelity and controllability over text attributes (color, font, position) compared to FLUX.1 [dev], and that the LeX-Enhancer significantly improves the quality of results generated from simple prompts.", "section": "3. LeX-Art"}, {"figure_path": "https://arxiv.org/html/2503.21749/x11.png", "caption": "Figure 11: Comparison of data samples from AnyWord-3M\u00a0[43], MAION-10M\u00a0[5] and LeX-10K.", "description": "This figure presents a comparison of image samples from three different datasets: AnyWord-3M, MARIO-10M, and LeX-10K. Each dataset is represented by a grid of 16 images. The figure allows for a visual comparison of the image quality, aesthetic appeal, diversity of text styles and layouts across the datasets. It highlights the improvements in image quality and text rendering achieved by LeX-10K compared to existing web-crawled datasets.", "section": "3.1. LeX-10K dataset"}, {"figure_path": "https://arxiv.org/html/2503.21749/extracted/6315872/figures/pned.png", "caption": "Figure 12: Showcase of text rendering results from LeX-Lumina (first two rows) and LeX-FLUX (last two rows) on text-to-image tasks. The examples demonstrate the models\u2019 ability to generate clear, well-aligned, and aesthetically pleasing text within images.", "description": "This figure showcases the text rendering capabilities of two models, LeX-Lumina and LeX-FLUX, for text-to-image generation tasks.  The top two rows display results from LeX-Lumina, while the bottom two rows show results from LeX-FLUX.  Each image demonstrates the models' ability to generate text that is clear, accurately positioned, and aesthetically integrated with the overall image.  The examples highlight various text styles, font choices, layouts, and background designs, illustrating the versatility of both models in creating visually appealing text within diverse image contexts.", "section": "3. LeX-Art"}]