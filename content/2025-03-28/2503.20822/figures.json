[{"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/simulation_main_vis-v3.png", "caption": "Figure 1: Our synthetic-data-enhanced video generation model is capable of producing videos depicting human dancing (rows 1), scenes featuring large camera orbiting around the object (row 2), and animals against solid-color backgrounds for matting (row 3).", "description": "Figure 1 showcases the capabilities of a novel video generation model enhanced with synthetic data.  The figure presents three rows of video examples, each demonstrating a different aspect of the model's capabilities. Row 1 displays videos of humans dancing, highlighting the model's ability to generate realistic human motion. Row 2 shows scenes with a large camera orbiting around an object, demonstrating the model's capacity to handle complex camera movements while maintaining 3D consistency. Row 3 features examples of animals against solid-color backgrounds, showcasing the model's performance on the challenging task of video matting, preparing the generated videos for seamless integration with other footage or backgrounds.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/appearance-gap.png", "caption": "Figure 2: Visualization of the pipeline to augment video generation model with synthetic video data. We first plan the synthetic videos and generation descriptive tags for each elements (e.g. object, character, motion, etc). Then we combine the element descriptions to form the caption for synthetic videos. During training, we mix the synthetic videos with real-world video data to improve physics fidelity in challenging video generation tasks.", "description": "This figure illustrates the process of integrating synthetic video data into a video generation model to enhance the model's understanding of physics.  The pipeline begins by planning synthetic videos and assigning descriptive tags to their components (objects, characters, motions, etc.). These descriptions are then combined to create captions for the synthetic videos.  Finally, the synthetic videos and their captions are integrated with real-world video data during model training. This process is designed to improve physical realism in the model's output, particularly for complex video generation tasks.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/shadow_visualization.png", "caption": "Figure 3: Visualizations of synthetic videos highlighting both good- and poor-quality 3D assets (a) and rendering (b).", "description": "This figure visualizes examples of synthetic videos generated using different qualities of 3D assets and rendering techniques.  Subfigure (a) compares videos created with high-quality 3D assets against those with low-quality assets, showcasing the visual impact of asset quality on the realism of the final video. Subfigure (b) demonstrates the effect of rendering quality on the synthetic videos, showing differences between high-quality and low-quality renderings, and their impact on the overall visual fidelity. These visual comparisons highlight the importance of both high-quality 3D assets and rendering techniques to bridge the appearance gap between synthetic and real-world videos, essential for effectively training video generation models using synthetic data.", "section": "2. Synthetic Video Generation using Computer Graphics Techniques"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/figures/wireframe_v2.jpg", "caption": "Figure 4: Visualizations of the videos generated by our improved model,\ntrained using synthetic data.\nRows\u00a01,2 highlight wide-angle camera motion;\nrows\u00a03 display layer decomposition;\nand rows\u00a04,5,6 demonstrate large human motion.", "description": "Figure 4 presents video results generated by a video generation model enhanced with synthetic training data.  The figure is organized into six rows, each showcasing different video generation capabilities. Rows 1 and 2 demonstrate the model's ability to handle wide-angle camera motion, showing smooth transitions and consistent object representation despite large camera movements.  Row 3 illustrates the model's successful layer decomposition, cleanly separating foreground elements (objects and subjects) from the background, even when presented with complex scenes. Rows 4, 5, and 6 focus on the generation of large human motions, showcasing the model's ability to generate realistic and physically consistent human movements without artifacts or distortions even during extreme motion.", "section": "4. Results"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/diversebg_v2.jpg", "caption": "Figure 5: Visualization of video frames with large human motion generated by our model.\nThe shadow of human body follows the human motion.", "description": "This figure displays several frames from videos showcasing large human motions generated by the proposed model.  The key takeaway is that the model accurately generates realistic shadows that dynamically move and change shape in response to the human body's movements. This demonstrates an improvement in the physical fidelity of the model's output, a crucial aspect of realistic video generation.", "section": "4.2 Results"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/bad_asset_affect_layer.png", "caption": "Figure 6: 3D scene setup in Blender and Unreal Engine. The wireframes and corresponding rendering outputs.", "description": "Figure 6 shows the 3D scene setups in Blender and Unreal Engine, two popular computer graphics software packages.  The left column displays wireframe representations of the 3D scenes, illustrating the object, camera placement, and lighting configurations.  The right column presents the resulting rendered images that are produced based on the specified setup in the left column. This visualization helps to illustrate how the parameters used in the scene setup (as detailed in the paper) impact the final rendered output, emphasizing the configurability and control offered by these CGI pipelines.", "section": "2. Synthetic Video Generation using Computer Graphics Techniques"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/bad_asset_affect_spin.png", "caption": "Figure 7: Examples of our synthetic video data. We render the synthetic videos with diverse background to alleviate the potential biases in synthetic videos.", "description": "Figure 7 showcases examples of synthetic video data generated using diverse backgrounds.  The diversity in backgrounds aims to mitigate potential biases that might arise from using only a limited set of backgrounds during the training process, which can result in the model overfitting to specific visual characteristics of the synthetic data and not generalizing well to real-world videos.", "section": "2. Synthetic Video Generation using Computer Graphics Techniques"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/overtrained.png", "caption": "Figure 8: Example outputs from video generation models trained on synthetic datasets with low-quality assets. The resulting objects frequently exhibit cartoonish or animated characteristics, diverging from the intended original visual style.", "description": "This figure showcases the negative impact of using low-quality synthetic data for training video generation models.  The images demonstrate that models trained on these datasets produce videos where the generated objects have an unrealistic, cartoonish, or animated appearance. This differs significantly from the intended, more photorealistic visual style.", "section": "3.1. Curating Synthetic Pixels"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/caption-v2.png", "caption": "Figure 9: Visualization of generated outputs from video generation models trained with synthetic videos of low quality assets in large camera motion task. The objects in these generated videos more likely to appear static or animated.", "description": "This figure visualizes the results of video generation models trained using synthetic data with low-quality assets.  The models were tasked with generating videos featuring large camera motions.  The generated videos of objects show a higher likelihood of appearing static or exhibiting unnatural, animated movements compared to videos generated with high-quality assets, highlighting the importance of high-quality synthetic data in training for physically accurate video generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/sim_drop1.png", "caption": "Figure 10: Visualization of over training video generation models trained with synthetic videos. Visual patterns such as color tone are more likely to appear in generated videos.", "description": "This figure demonstrates the negative impact of overtraining a video generation model using synthetic data.  When trained for excessive iterations, the model starts to incorporate artifacts from the training data, such as specific color palettes or visual styles, which are not reflective of real-world videos. The generated videos become less realistic due to overfitting. This highlights the importance of carefully balancing training with real and synthetic data to avoid overemphasizing the artificial features of the synthetic datasets.  The figure likely visually shows a series of videos generated after various training epochs, showcasing a progressive shift towards artificial visual patterns.", "section": "3. Training with Synthetic Videos"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/visualizations/sim_drop2.png", "caption": "Figure 11: A comparison of generating captions for synthetic videos\nusing existing methods (Generic Caption) and our method (Fine-Grained Caption).\nWe also show a comparison of captions with special tags and without special tags.", "description": "This figure compares different captioning methods for synthetic videos.  The existing methods generate generic captions, while the proposed method generates fine-grained captions that provide more detailed descriptions of the video content, including specific actions and visual elements.  The figure also demonstrates the impact of adding \"special tags\" to the captions, which help the model distinguish between synthetic and real videos, improving the transfer of physical fidelity from synthetic to real video generation.", "section": "3.2. Crafting Captions for Synthetic Videos"}, {"figure_path": "https://arxiv.org/html/2503.20822/extracted/6306596/figures/layer_change_background_v4.jpg", "caption": "Figure 12: A comparison showcasing the effect of SimDrop. Row 1 is the result without SimDrop and Row 2 is the video with the method. The color tone in row two is significantly more better and without color pattern from the synthetic data.", "description": "This figure compares video generation results with and without the SimDrop method.  The top row (Row 1) shows videos generated without SimDrop, exhibiting noticeable color inconsistencies and artifacts stemming from the synthetic training data. The bottom row (Row 2) displays videos generated using SimDrop.  SimDrop effectively mitigates these artifacts, resulting in videos with more natural and consistent color tones, demonstrating improved visual fidelity.", "section": "3.3. Training with Synthetic Videos"}]