[{"figure_path": "https://arxiv.org/html/2503.21729/x1.png", "caption": "Figure 1: Unlike LRMs, ReaRAG iteratively constructs knowledge-guided reasoning chains for factual answers.", "description": "This figure illustrates the key difference between traditional Large Reasoning Models (LRMs) and the proposed ReaRAG model. LRMs generate answers based solely on their internal knowledge, which can limit factual accuracy, particularly for complex, multi-hop questions.  ReaRAG, on the other hand, employs an iterative retrieval-augmented generation (RAG) approach.  It starts with an LRM generating a reasoning chain, but it then uses a RAG engine to retrieve external knowledge based on queries generated within that reasoning chain. This process continues iteratively until a final answer is produced. The diagram visually represents this iterative process, showing how ReaRAG constructs its answer by incorporating external information, thus enhancing factual accuracy compared to the single-step approach of LRMs.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.21729/x2.png", "caption": "Figure 2: Overview of our approach to develop a factuality-enhanced reasoning model ReaRAG. To equip ReaRAG with knowledge-guided reasoning ability, we propose an automated data construction approach (Algorithm\u00a01). Next, we fine-tune ReaRAG on the constructed dataset to conduct reasoning iteratively, following the Thought-Action-Observation Paradigm to solve complex queries. Pseudocode for the inference stage is provided in Algorithm\u00a02.", "description": "Figure 2 illustrates the ReaRAG model, a factuality-enhanced reasoning model.  The figure details the model's development process, beginning with an automated data construction approach (Algorithm 1) to create a dataset with knowledge-guided reasoning chains. This dataset is then used to fine-tune the ReaRAG model.  The core of ReaRAG iteratively solves complex queries by leveraging the Thought-Action-Observation paradigm, where the model generates a thought, performs an action (either searching for external knowledge or concluding with a final answer), observes the outcome, and repeats this iterative reasoning process until a final answer is reached. Algorithm 2 provides pseudocode for the inference stage.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.21729/x3.png", "caption": "Figure 3: Comparison of chain length between ReaRAG and Search-o1 across multi-hop QA tasks. We measure the reasoning steps needed for both models to achieve a full ACCLsubscriptACC\ud835\udc3f\\text{ACC}_{L}ACC start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT score. Search-o1 consistently requires more steps than ReaRAG, highlighting the tendency of RL-based models to overthink in multi-hop QA tasks.", "description": "This figure compares the number of reasoning steps taken by ReaRAG and Search-o1 to achieve a perfect ACCL score (accuracy using LLM as judge) across various multi-hop question answering tasks.  The x-axis represents the chain length (number of reasoning steps), and the y-axis shows the number of instances at each chain length. The bars are grouped for each model (ReaRAG and Search-o1) and dataset. The average chain length is shown for each model. The key finding is that Search-o1 consistently needs more reasoning steps than ReaRAG, demonstrating the overthinking tendency often observed in reinforcement learning (RL)-based models on complex reasoning tasks.", "section": "4 Experiments"}]