{"importance": "This paper is important for researchers as it introduces **ReaRAG, a framework enhancing factuality in LRMs for question answering**. The framework's knowledge-guided reasoning addresses overthinking in RL-based models and improves integration of reasoning and RAG. The result will open new directions for integrating reasoning into RAG.", "summary": "ReaRAG enhances factuality in large reasoning models (LRMs) by integrating knowledge-guided reasoning with iterative retrieval augmented generation.", "takeaways": ["ReaRAG enhances LRMs' factuality through knowledge-guided reasoning chains.", "ReaRAG combines strong reasoning with RAG, leveraging deliberation before acting.", "The method achieves substantial performance improvements on multi-hop benchmarks."], "tldr": "Large Reasoning Models (LRMs) demonstrate reasoning abilities but rely on parametric knowledge, limiting factual accuracy. RL-based LRMs with retrieval suffer from overthinking and lack robustness. To address this, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations is needed. This model is expected to solve issues that other prompt based solutions have such as unreliable token generation.\n\nTo this end, **ReaRAG** leverages an LRM to generate thinking and select actions from a predefined action space (Search/Finish). Search queries are executed against a RAG engine, with results guiding later steps until a Finish action is chosen. A data framework with reasoning chain length upper bound is created to improve retrieval robustness. The experiment demonstrated substantial performance improvement.", "affiliation": "Tsinghua University", "categories": {"main_category": "Natural Language Processing", "sub_category": "Question Answering"}, "podcast_path": "2503.21729/podcast.wav"}