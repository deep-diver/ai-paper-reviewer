{"references": [{"fullname_first_author": "A. Vaswani", "paper_title": "Attention Is All You Need", "publication_date": "2017-12-00", "reason": "This paper introduced the Transformer architecture, which is fundamental to the model architecture in this work."}, {"fullname_first_author": "J. Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-00-00", "reason": "This paper lays the groundwork for diffusion models, which are used in the proposed research."}, {"fullname_first_author": "H. Chang", "paper_title": "Maskgit: Masked generative image transformer", "publication_date": "2022-00-00", "reason": "This paper introduces masked generative image transformer, and it uses the same loss as UniDisc except there is no reweighting term."}, {"fullname_first_author": "S.Y. Gadre", "paper_title": "Datacomp: In search of the next generation of multimodal datasets", "publication_date": "2024-00-00", "reason": "This paper introduces DataComp datasets, which is a key dataset used for training the UniDisc models."}, {"fullname_first_author": "C. Team", "paper_title": "Chameleon: Mixed-modal early-fusion foundation models", "publication_date": "2024-00-00", "reason": "This paper introduces the Chameleon project, which provides a decoder-only transformer with causal attention and rotary positional embeddings and is used as a baseline in the proposed research."}]}