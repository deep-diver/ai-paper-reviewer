[{"figure_path": "https://arxiv.org/html/2503.18102/extracted/6300559/figures/AgentRxiv.png", "caption": "Figure 1: Collaborative Autonomous Research via AgentRxiv. Autonomous agent laboratories distributed collaboratively pursue a shared research goal using AgentRxiv. Human researchers provide initial guidance through a research direction and detailed instructions. Agents autonomously perform research and upload research papers to the centralized AgentRxiv preprint server, enabling laboratories to access each other\u2019s discoveries, accelerating scientific progress.", "description": "The figure illustrates the collaborative research process facilitated by AgentRxiv. Multiple autonomous agent laboratories work together towards a common research goal. Human researchers provide initial direction and instructions. Agents conduct research independently, and upon completion, upload their findings (research papers) to the shared AgentRxiv preprint server.  This centralized server allows agents to access and build upon each other's work, accelerating overall scientific progress. The figure visually depicts this process, showcasing the interaction between human researchers, individual agent laboratories, and the AgentRxiv server.", "section": "3. AgentRxiv: Towards Collaborative Autonomous Research"}, {"figure_path": "https://arxiv.org/html/2503.18102/extracted/6300559/figures/Untitled-3.png", "caption": "Figure 2: Agent Laboratory Workflow. (Top) This image shows Agent Laboratory\u2019s three phases: Literature Review, Experimentation, and Report Writing. Human researchers collaborate with AI agents (e.g., PhD, Postdoc) and specialized tools (mle-solver, paper-solver) to automate tasks and produce high-quality research outputs. (Bottom) This", "description": "The figure illustrates the Agent Laboratory workflow, which comprises three main phases: Literature Review, Experimentation, and Report Writing.  The top panel provides a high-level overview of the workflow, showing the interaction between human researchers and AI agents (PhD, Postdoc, etc.), along with specialized tools like 'mle-solver' and 'paper-solver'. These tools automate various tasks, such as literature review, experimental design, code generation, and report writing, to produce high-quality research outputs. The bottom panel offers a more detailed, zoomed-in perspective on the virtual, text-based environment where AI agents carry out these tasks, highlighting the iterative and multi-faceted nature of the process.", "section": "2.1. Agent Laboratory"}, {"figure_path": "https://arxiv.org/html/2503.18102/extracted/6300559/figures/arxival.png", "caption": "Figure 3: AgentRxiv Framework for Autonomous Research Collaboration. Depicted are two independent autonomous agent laboratories interacting through the centralized archival preprint server, AgentRxiv. (Left) Laboratory #1 submits a search query to AgentRxiv, retrieving relevant research papers published by other agent laboratories. (Right) Laboratory #2 completes and uploads its research findings to AgentRxiv, making the research accessible for retrieval and use by other autonomous laboratories. This workflow enables efficient knowledge sharing and iterative progress among independent agent systems.", "description": "AgentRxiv is a centralized preprint server that allows multiple independent autonomous agent laboratories to share their research findings.  The figure shows two laboratories. Laboratory #1 searches AgentRxiv for relevant papers published by other labs, and Laboratory #2 uploads its completed research to the server. This shared resource enables iterative progress and efficient knowledge sharing among the laboratories.", "section": "3. AgentRxiv: Towards Collaborative Autonomous Research"}, {"figure_path": "https://arxiv.org/html/2503.18102/extracted/6300559/figures/SoA_main.png", "caption": "Figure 4: Designing Novel Reasoning Techniques on MATH-500. Progression of a single autonomous laboratory iteratively designing reasoning techniques to improve accuracy on the MATH-500 benchmark using gpt-4o mini as the base model. Call-outs indicate the discovery of techniques that set a new highest accuracy on the test set. Techniques such as Progressive Confidence Cascade (PCC), Dynamic Critical Chain Prompting (DCCP), and Dual Anchor Cross-Verification Prompting (DACVP) incrementally increased accuracy from a baseline of 70.2% (gpt-4o mini zero-shot) up to 78.2% (+11.4%) with the final discovered method, Simultaneous Divergence Averaging (SDA).", "description": "This figure shows the iterative process of a single autonomous laboratory improving its accuracy on the MATH-500 benchmark.  Starting with a baseline accuracy of 70.2% using the gpt-40 mini model, the laboratory progressively discovers and implements new reasoning techniques. Each new technique builds upon previous ones, resulting in incremental improvements in accuracy.  Specific techniques highlighted include Progressive Confidence Cascade (PCC), Dynamic Critical Chain Prompting (DCCP), and Dual Anchor Cross-Verification Prompting (DACVP).  The final technique, Simultaneous Divergence Averaging (SDA), achieves the highest accuracy of 78.2%, representing an 11.4% relative improvement over the baseline.", "section": "3.1 Discovering reasoning techniques"}, {"figure_path": "https://arxiv.org/html/2503.18102/extracted/6300559/figures/generality.png", "caption": "Figure 5: Properties of autonomous discovery. A. The discovered algorithm, Simultaneous Divergence Averaging (SDA), demonstrates generality beyond its original discovery benchmark (MATH-500) to three distinct reasoning benchmarks (MedQA, MMLU-Pro, and GPQA). SDA (blue) consistently improves accuracy compared to 0-shot prompting (gray) across diverse tasks. B. Comparison of best accuracy obtained on MATH-500 when agents have access to previously generated research (green) versus no access (pink). Agents referencing prior research consistently achieve higher performance, indicating the value of cumulative knowledge integration. C. The discovered SDA algorithm generalizes effectively across multiple language models (gpt-4o mini, gpt-4o, DeepSeek v3, Gemini-1.5-Pro, Gemini-2.0-Flash) and across several reasoning benchmarks. SDA (blue) demonstrates higher average accuracy compared to 0-shot prompting (gray).", "description": "Figure 5 demonstrates the generalizability and effectiveness of the SDA algorithm. Panel A shows SDA's consistent accuracy improvement over 0-shot prompting across three diverse reasoning benchmarks (MedQA, MMLU-Pro, GPQA). Panel B highlights the impact of access to prior research, with agents using AgentRxiv showing consistently higher accuracy on MATH-500 than those without access.  Panel C illustrates SDA's generalization across multiple language models and benchmarks, again outperforming 0-shot prompting.", "section": "3.1 Discovering reasoning techniques"}, {"figure_path": "https://arxiv.org/html/2503.18102/extracted/6300559/figures/Untitled-61.png", "caption": "Figure 6: Designing Novel Reasoning Techniques on MATH-500 in Parallel. Progression of three autonomous laboratories concurrently (red, blue, and green) compared with non collaborative autonomous laboratories (gray) performing iterative research to improve accuracy on the MATH-500 benchmark, each using gpt-4o mini as the base model. Call-outs indicate the discovery of reasoning techniques that achieve a new highest accuracy on the test set. Laboratories independently develop techniques such as Residual Feedback Prompting (RFP), Adaptive Dynamic Multi-Layer Prompting (ADMPT), and Adaptive Token-Level Gradient Reweighting, collectively raising accuracy from 70.2% (gpt-4o mini zero-shot baseline) to 79.8% (+9.6%). Parallel experimentation, combined with immediate result sharing via AgentRxiv, accelerates the pace of research progress and achieves higher final accuracy compared to sequential experimentation.", "description": "This figure displays the results of an experiment where three autonomous research laboratories simultaneously worked on improving accuracy on the MATH-500 benchmark using the gpt-40 mini language model.  Each laboratory independently developed new reasoning techniques (as shown by the call-outs).  The chart illustrates how the accuracy increased over time for each laboratory individually (red, blue, and green lines) and in comparison to a single, non-collaborative laboratory (gray line).  The collaboration enabled by AgentRxiv significantly improved the final accuracy (from 70.2% to 79.8%), highlighting the benefits of collaborative, parallel research.", "section": "3.2. Collaborative execution of parallel agent labs"}]