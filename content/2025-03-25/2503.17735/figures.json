[{"figure_path": "https://arxiv.org/html/2503.17735/extracted/6301426/imgs/overview.jpg", "caption": "Figure 1: \nAn overview of resource-efficient dual-mask training framework.\nWe propose a discrete frame generation network to model the discreteness between animated sticker frames.\nFurthermore, the dual masks, i.e.formulae-sequence\ud835\udc56\ud835\udc52i.e.italic_i . italic_e ., condition mask and loss mask, are designed to improve the availability and expand the diversity of limited data.\nThe difficulty-adaptive curriculum learning is applied to facilitate convergence.", "description": "This figure illustrates the resource-efficient dual-mask training framework (RDTF) for animated sticker generation.  It highlights three key components: 1) A discrete frame generation network, designed to handle the unique characteristics of animated stickers (low frame rates and discrete frames).  2) Dual masks (condition and loss masks) which improve data utilization by selectively focusing on certain parts of the data during training, increasing the effective data size and diversity.  3) A difficulty-adaptive curriculum learning strategy, which gradually increases the difficulty of training data to enhance model convergence and performance. This framework allows for training smaller, more efficient video generation models, suitable for resource-constrained environments.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2503.17735/extracted/6301426/imgs/plt_bar_framesnum.jpg", "caption": "Figure 2: Frame distribution in collected sticker dataset, which follows the long-tail distribution, i.e.formulae-sequence\ud835\udc56\ud835\udc52i.e.italic_i . italic_e ., more short frames and fewer long frames.", "description": "This figure shows the distribution of frame counts across the collected animated sticker dataset.  The distribution is skewed towards shorter videos; there are significantly more stickers with a small number of frames than stickers with a large number of frames. This long-tail distribution is a key characteristic of the dataset used and influences the methods for training and data utilization described in the paper.", "section": "Dual-mask based Data Utilization Strategy"}, {"figure_path": "https://arxiv.org/html/2503.17735/extracted/6301426/imgs/cluster.jpg", "caption": "Figure 3: Frame extraction algorithm based on feature clustering. During training, data are clustered into k\ud835\udc58kitalic_k clusters randomly to increase the information density.", "description": "This figure illustrates a data augmentation technique used to improve the performance of the animated sticker generation model.  The method involves clustering the frames of animated stickers into k groups based on their visual features. This clustering step aims to increase the information density of the dataset by highlighting common patterns within each cluster. By randomly selecting clusters during the model training process, the model is exposed to a wider range of visual patterns, thus potentially enhancing its learning and generalization capabilities. The original caption only briefly describes the clustering step; this expanded description provides a more comprehensive understanding of its purpose and effect within the training process.", "section": "Dual-mask based Data Utilization Strategy"}, {"figure_path": "https://arxiv.org/html/2503.17735/extracted/6301426/imgs/CLproblem.jpg", "caption": "Figure 4: The masked frame length and task type during training are independent of each other, which is difficult to determine a route so as to obtain entropy increase samples stably.", "description": "This figure illustrates the challenge in designing a curriculum learning strategy for animated sticker generation.  The training process involves two independent factors: the length of the masked frames and the type of task being performed (interpolation, prediction, or generation).  These factors are not directly correlated, making it difficult to create a curriculum that consistently increases the complexity of the training samples over time to facilitate stable model convergence. A suitable curriculum should smoothly increase the information entropy of the training data, gradually moving from simpler to more complex tasks.  However, the independence of frame length and task type makes it difficult to create a curriculum that consistently increases entropy.", "section": "Difficulty-adaptive Curriculum Learning"}, {"figure_path": "https://arxiv.org/html/2503.17735/extracted/6301426/imgs/visual.jpg", "caption": "Figure 5: \nVisual comparison for animated sticker generation on I&T\u2192\u2192\\rightarrow\u2192V task between Customize-A-Video, I2V-Adapter, SimDA and RDTF.\nCompared with other methods, the motion obtained by RDTF method is smoother and clearer than others.\nClick here to see dynamic results.", "description": "This figure displays a visual comparison of animated sticker generation results using four different methods: Customize-A-Video, I2V-Adapter, SimDA, and RDTF (the authors' proposed method). Each method is applied to the same input (image and text), and the resulting animated stickers are shown. The comparison highlights the smoother and clearer motion achieved by the RDTF method compared to the other three.  The caption encourages viewers to click to see dynamic results, suggesting the animation quality is a key aspect of the comparison.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17735/extracted/6301426/imgs/comp_other_task.jpg", "caption": "Figure 6: \nVisual comparison for interpolation and prediction tasks between SimDA and RDTF.\nGray boxes indicate visual guidance.", "description": "Figure 6 presents a visual comparison of the interpolation and prediction capabilities of two video generation models: SimDA and the proposed RDTF (Resource-efficient Dual-mask Training Framework).  The figure showcases example results for both tasks, highlighting the differences in generated video quality and detail. Grey boxes in the figure indicate where visual guidance was provided as input to the model. This comparison demonstrates RDTF's superior performance in generating smoother and more visually appealing results compared to SimDA.", "section": "Experiments"}]