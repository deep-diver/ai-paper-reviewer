{"references": [{"fullname_first_author": "Edward J. Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-04-25", "reason": "This paper introduced LoRA, a parameter-efficient fine-tuning method widely used in the field, which is relevant because this paper uses AdaLoRA for knowledge editing."}, {"fullname_first_author": "Kevin Meng", "paper_title": "Locating and editing factual associations in GPT", "publication_date": "2022-11-28", "reason": "This paper is seminal work in knowledge editing, providing a foundation for the field which this paper is based on and which this paper extends to ADS-Edit."}, {"fullname_first_author": "Kevin Meng", "paper_title": "Mass-editing memory in a transformer", "publication_date": "2023-05-01", "reason": "This paper builds upon the prior 'Locating and editing factual associations in GPT' paper by Meng et al, focusing on efficient memory editing in transformers which improves upon other knowledge editing approaches."}, {"fullname_first_author": "Ningyu Zhang", "paper_title": "A comprehensive study of knowledge editing for large language models", "publication_date": "2024-01-01", "reason": "This paper provides a comprehensive study of knowledge editing techniques for large language models, which is relevant in this paper's approach to knowledge editing using LMMs."}, {"fullname_first_author": "Tom Hartvigsen", "paper_title": "Aging with GRACE: lifelong model editing with discrete key-value adaptors", "publication_date": "2023-12-10", "reason": "This paper presents GRACE, a method for lifelong model editing, which is relevant because this paper evaluates GRACE along with other knowledge editing methods."}]}