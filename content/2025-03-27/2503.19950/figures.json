[{"figure_path": "https://arxiv.org/html/2503.19950/x1.png", "caption": "Figure 1: The observed log-distribution pattern is evident not only in the magnitude of attention scores but also in the positions of attention spikes. These spikes become sparser as the model attends to tokens further from the most recent position, indicating that the model not only focuses on nearby tokens. This phenomenon, illustrated here with Llama3-8B-Instruct\u00a0(Dubey et\u00a0al., 2024) on the GSM8K dataset\u00a0(Cobbe et\u00a0al., 2021), is consistent across different tasks and models, as further detailed in Section\u00a02.", "description": "This figure displays a graph showing the distribution of attention scores across different token positions. The x-axis represents the token position, and the y-axis represents the attention score. The graph shows that the attention scores follow a log-distribution pattern, with higher scores concentrated near the most recent token position and gradually decreasing as the distance from the most recent token increases. The figure illustrates this phenomenon using the Llama3-8B-Instruct model and the GSM8K dataset. The observation is consistent across different models and tasks, and it forms the basis of the LogQuant algorithm for efficiently compressing KV cache in LLMs. The log-distribution means the model's attention is more focused on recent tokens.", "section": "2 METHODOLOGY"}, {"figure_path": "https://arxiv.org/html/2503.19950/x2.png", "caption": "Figure 2: The maximum attention score of each token position across four consecutive decoding steps, marking the high attention positions for illustrating the unpredictable nature of attention scores. This analysis was conducted using Llama3-8B-Instruct\u00a0(Dubey et\u00a0al., 2024) on the GSM8K\u00a0(Cobbe et\u00a0al., 2021) and OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018) datasets.", "description": "Figure 2 visualizes the unpredictable nature of attention weights in LLMs over time. It displays the maximum attention score for each token position across four consecutive decoding steps, using the Llama3-8B-Instruct model on both GSM8K and OpenBookQA datasets. The unpredictable fluctuations highlight the challenges in accurately predicting important tokens for efficient memory management, especially when considering compression techniques.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.19950/x3.png", "caption": "Figure 3: Attention distribution across different token positions, represented as boxplots based on 25% quantiles across all attention heads. The median and overall distribution of attention scores for sink tokens\u00a0(Xiao et\u00a0al., 2023) (tokens 0 and 1) are greater than the sum of the most recent 128 tokens. The attention scores are derived from experiments using Llama3-8B-Instruct\u00a0(Dubey et\u00a0al., 2024) and the GSM8K\u00a0(Cobbe et\u00a0al., 2021) dataset.", "description": "Figure 3 illustrates the distribution of attention weights across different token positions within the context window of a large language model.  Boxplots summarize the attention scores across all attention heads, showing the median and interquartile range (25th and 75th percentiles) for each token position.  The figure highlights that the attention scores for the first two tokens (referred to as 'sink tokens'), which are typically the most recently processed tokens, exhibit a higher median and overall distribution of scores compared to the combined scores of the subsequent 128 tokens. This observation supports the notion that the most recent tokens carry more weight in the attention mechanism.  The data presented in this graph is derived from experiments conducted using the Llama3-8B-Instruct model on the GSM8K dataset.", "section": "2.1 PRELIMINARY STUDY OF KV CACHE AND ATTENTION SCORES"}, {"figure_path": "https://arxiv.org/html/2503.19950/x4.png", "caption": "Figure 4: The attention coverage without the first two sink tokens for different selection methods\u00a0(Liu et\u00a0al., 2024c; Xiao et\u00a0al., 2023; Zhang et\u00a0al., 2024) and different models\u00a0(Dubey et\u00a0al., 2024; Yang et\u00a0al., 2024; Abdin et\u00a0al., 2024), tested on a subset of the GSM8K\u00a0(Cobbe et\u00a0al., 2021) dataset. Details of LogQuant will be introduced in Section 2.5.", "description": "Figure 4 compares the effectiveness of different token selection methods for compressing the key-value (KV) cache in large language models (LLMs).  It shows the attention coverage achieved by four different methods: LogQuant, Kivi, Streaming, and H2O.  The comparison is made across various LLMs (Llama3-8B-Instruct, Qwen-2-7B-Instruct, Phi-3-mini-128k-Instruct) and uses a subset of the GSM8K dataset. The x-axis represents the length of the reserved portion of the KV cache, while the y-axis shows the average attention scores captured by each selection method. The figure demonstrates that LogQuant achieves better attention coverage than the other methods, indicating its superior ability to select and retain important tokens while reducing memory usage.  The first two sink tokens (tokens with consistently high attention scores) are excluded from the analysis to focus on the relative performance of the selection methods.", "section": "2.1 Preliminary Study of KV Cache and Attention Scores"}, {"figure_path": "https://arxiv.org/html/2503.19950/x5.png", "caption": "Figure 5: Eviction and Quantization Loss on Attention Distribution", "description": "This figure compares the effects of two different KV Cache compression strategies: quantization and eviction.  It demonstrates that using quantization to reduce the numerical precision of tokens instead of removing them entirely (eviction) leads to significantly less distortion of the attention distribution.  The plot visualizes the L1 error between the original attention distribution and the distributions after compression using both methods.", "section": "2.3 Comparison of Quantization and Eviction Strategies"}, {"figure_path": "https://arxiv.org/html/2503.19950/x6.png", "caption": "Figure 6: LogQuant\u2019s KV cache compression workflow. The number of reserved original-precision tokens increases from 2\u2062W2\ud835\udc4a2W2 italic_W to 3\u2062W3\ud835\udc4a3W3 italic_W. We then apply a log-sparse strategy to filter the first 2\u2062W2\ud835\udc4a2W2 italic_W tokens, quantize half of these tokens, and compress the reserved token length back to 2\u2062W2\ud835\udc4a2W2 italic_W.", "description": "LogQuant's KV cache compression workflow is illustrated.  Initially, 3W tokens are kept at full precision. A log-sparse filtering strategy is then applied to the first 2W tokens, resulting in half of them being quantized. This process reduces the number of full-precision tokens, ultimately compressing the reserved token length back down to 2W. This cyclical process ensures efficient memory management.", "section": "2.5 LOGQUANT: ALGORITHM AND IMPLEMENTATION"}, {"figure_path": "https://arxiv.org/html/2503.19950/x7.png", "caption": "Figure 7: Accuracy(EM) with different compression ratio in GSM8K tasks for different models.", "description": "This figure displays the accuracy (exact match) results on the GSM8K dataset for various language models using different compression ratios.  It visualizes the performance trade-off between compression and accuracy for different models and compression strategies, allowing for a comparison of the effectiveness of LogQuant relative to other approaches.", "section": "3.2 ACCURACY AND EFFICIENCY ANALYSIS"}, {"figure_path": "https://arxiv.org/html/2503.19950/x8.png", "caption": "Figure 8: memory usage and throughput comparison between 2bit LogQuant and 16bit baseline under huggingface generation pipeline with llama3.1-8B and H100.", "description": "Figure 8 illustrates a comparison of memory usage and throughput between LogQuant with 2-bit quantization and a 16-bit baseline.  The experiment used the Hugging Face generation pipeline, the Llama 3.1-8B model, and an NVIDIA H100 GPU.  The graph shows how throughput and memory consumption change as the batch size increases for both methods. This helps to demonstrate the memory efficiency and performance gains achieved by LogQuant.", "section": "3.2.4 EFFICIENCY COMPARISON"}]