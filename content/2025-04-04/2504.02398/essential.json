{"importance": "This paper pioneers **scaling analysis for interleaved SLMs**, crucial for optimizing compute allocation. It challenges textless-SLM scaling laws, suggesting distinct scaling dynamics. The opensource models and data foster further research, accelerating progress in efficient and high-quality SLM development.", "summary": "Interleaved Speech-Text Language Models scale more efficiently than textless ones!", "takeaways": ["Interleaved SLMs scale more efficiently with compute than textless SLMs.", "Scaling dynamics of interleaved SLMs differ significantly from textless SLMs, requiring more compute for model size.", "High-quality TextLMs with synthetic data can optimize scaling capabilities."], "tldr": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question if interleaved SLMs scale more efficiently than textless SLMs. Despite the great results, the scaling properties of such training paradigms remains unclear.\n\nIn this paper the authors conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. The results indicate that the scaling-dynamics are significantly different than textless-SLMs, suggesting one should allocate notably more of the compute budget for increasing model size over training tokens. Results suggest, that the scaled up model achieves comparable performance with leading models on speech semantic metrics while using less compute and data than other approaches. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/.", "affiliation": "Hebrew University of Jerusalem", "categories": {"main_category": "Speech and Audio", "sub_category": "Speech Recognition"}, "podcast_path": "2504.02398/podcast.wav"}