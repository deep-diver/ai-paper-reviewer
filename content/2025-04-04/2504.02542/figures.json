[{"figure_path": "https://arxiv.org/html/2504.02542/x2.png", "caption": "Figure 1: In this work, we aim to develop a framework that not only generates videos driven by multiple signals without causing control conflicts in the facial region (first three rows) but also supports video generation driven by a single signal (last two rows).", "description": "This figure showcases the capabilities of the proposed ACTalker framework for talking head video generation. The top three rows demonstrate the ability to generate videos driven by multiple simultaneous signals (audio and facial motion) without control conflicts in the facial area.  The bottom two rows illustrate the framework's capacity to generate videos controlled by a single signal (either audio or facial motion).  This highlights the versatility and robustness of the approach in handling various control scenarios.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2504.02542/x3.png", "caption": "Figure 2: Illustration of our ACTalker framework. ACTalker takes multiple signals inputs (i.e., audio and visual facial motion) to drive the generation of talking head videos. In addition to the standard layers (e.g., spatial convolution, temporal convolution, spatial attention, and temporal attention) in the stable video diffusion model, we introduce a parallel-control mamba layer to harness the power of multiple signals control. Audio and facial motion signals are fed into this parallel-control mamba layer, along with their corresponding masks, which indicates the regions to focus on for manipulation.", "description": "This figure illustrates the architecture of ACTalker, a novel framework for generating talking head videos driven by multiple signals (audio and facial motion).  It shows how audio and motion signals are processed through separate parallel branches within the model, each with its own mask defining the regions of influence.  The core innovation, the parallel-control mamba layer, integrates these independently processed signals to allow for simultaneous control of the video generation without conflicts, and enhances the natural coordination of the generated video temporally and spatially. The figure also details the standard layers in a stable diffusion model (spatial convolution, temporal convolution, spatial and temporal attention), which are incorporated alongside the mamba layer.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2504.02542/x4.png", "caption": "Figure 3: Illustration of parallel-control mamba layer. There are two parallel branches in this layer, one for audio control and the other is for expression control. We utilize a gate in each branch to control the accessing of control signal during training. During inference, we can manually modify the statue of gates to enable single signal control or multiple signals control.", "description": "The Parallel-Control Mamba layer in ACTalker uses two parallel branches to process audio and expression signals separately for talking head video generation. Each branch contains a Mask-SSM unit that combines the signal with a mask to independently control specific facial regions.  A gate mechanism controls signal access to each branch during both training and inference. During training, gates randomly open or close, allowing for single or multiple signal control. During inference, gates can be manually set to enable flexible control using either single or multiple signals.", "section": "3.2. Parallel-control Mamba Layer"}, {"figure_path": "https://arxiv.org/html/2504.02542/x5.png", "caption": "Figure 4: The illustrating of the Mask-SSM in audio branch of parallel-control mamba layer. The visual branch is the same but replace with the motion embedding and motion mask", "description": "This figure details the Mask-SSM (Masked Selective State Space Model) architecture within the audio branch of the parallel-control mamba layer.  The Mask-SSM takes audio embeddings and a corresponding audio mask as input. The mask identifies specific regions of the feature map to be processed.  A 'Mask Drop' operation removes irrelevant features outside these specified regions, focusing processing on relevant areas. The remaining features are concatenated with the audio embeddings and passed through an SSM for spatio-temporal feature manipulation. The visual branch follows the same structure but uses motion embeddings and a motion mask instead. This design allows for parallel processing of audio and visual signals while preventing control conflicts by spatially separating their influences. ", "section": "3.2. Parallel-control Mamba Layer"}, {"figure_path": "https://arxiv.org/html/2504.02542/x6.png", "caption": "Figure 5: Comparison of different methods for audio-driven talking head generation. Our method can produce more natural and accurate lip-synced videos. Due to the page limitation, the results of SadTalker\u00a0[63] and Hallo\u00a0[56] are reported in Supplementary Material", "description": "Figure 5 presents a comparison of various methods for generating audio-driven talking head videos.  The figure showcases the results of several approaches, highlighting the visual quality and accuracy of lip synchronization.  The authors' method (Ours) is shown alongside results from Memo, Echominic, EDTalker, and VExpress.  A visual inspection reveals that the authors' approach produces more natural-looking and accurate lip-synced videos compared to the other methods. Due to space constraints in the paper, results for the SadTalker and Hallo methods are provided in supplementary material.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02542/x7.png", "caption": "Figure 6: Comparison of different methods on VFHQ. Self reenactment (first row) and cross reenactment (last row).", "description": "Figure 6 presents a qualitative comparison of different talking head generation methods on the VFHQ dataset. The top row showcases self-reenactment results, where the input and output videos feature the same person.  The bottom row displays cross-reenactment, using a different person's video as input for generating the output. This comparison highlights the differences in the ability of each method to generate realistic and high-fidelity talking head videos, particularly in terms of accurate facial expressions and movements.", "section": "4.2. Quantitative and Qualitative Analysis"}, {"figure_path": "https://arxiv.org/html/2504.02542/x8.png", "caption": "Figure 7: Visualization of multiple signals control. Our generated video accurately replicates the lip movements driven by the audio source and captures the head motion\u2014particularly the eye movements and pose\u2014as guided by the motion source. Once we remove the masks in both Mask-SSMs and generate the video using multiple driving signals, the motion source can also affect the mouth movement (\u201cOurs w/o MD\u201d), causing a control conflict.", "description": "This figure demonstrates the effectiveness of the proposed parallel-control mamba layer in handling multiple driving signals (audio and motion) for talking head generation. The left panel shows the results when both audio and motion signals are used with corresponding masks applied to each Mask-SSM. The generated video accurately synchronizes lip movements with audio and head pose with motion. The right panel shows the results when the masks are removed. In this case, the motion signal interferes with the audio signal, causing a conflict where motion influences lip movements. This illustrates how the masks prevent signal conflicts and enable precise, independent control of different facial regions.", "section": "3.2 Parallel-control Mamba Layer"}, {"figure_path": "https://arxiv.org/html/2504.02542/x9.png", "caption": "Figure 8: The visualization of ablation studies driven by audio. Our full method can produce more natural videos.", "description": "Figure 8 displays a comparison of talking head videos generated using different model variations, all driven by audio input.  The variations include a baseline model, a model without the mask-drop (MD) strategy, and a model without the identity embedding (ID). This allows for a visual assessment of the impact of each component on the overall naturalism and quality of the generated videos. The results show that the full model (with both MD and ID) produces significantly more natural-looking and higher-quality talking head videos compared to the ablation models.", "section": "4.3. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2504.02542/x10.png", "caption": "Figure 9: The type of masks we used in our framework.", "description": "This figure shows the three types of masks used in the ACTalker framework: face mask, audio mask, and motion mask.  The face mask broadly defines the facial region in the source image, while the audio mask is more specific, focusing on the mouth area to guide lip movements.  The motion mask is derived from the face mask to isolate the areas related to facial expressions.  These masks are used to help the model control specific areas of the face when generating videos with multiple driving signals, such as audio and facial motion.", "section": "3.3. Mask-SSM"}, {"figure_path": "https://arxiv.org/html/2504.02542/x11.png", "caption": "Figure 10: \nThe results generated by our method under facial motion control.", "description": "Figure 10 presents a comparison of talking head generation results using different methods, all driven by facial motion. The top row displays the reference or ground truth videos showing the target facial expressions and movements.  Each subsequent row shows the results generated using various methods. This illustrates the ability of the proposed approach to accurately capture nuanced facial expressions and movements, such as subtle eye movements and lip articulation, which other methods may fail to produce. The results highlight the superior performance of the proposed method in terms of both accuracy and realism.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02542/x12.png", "caption": "Figure 11: \nThe results generated by our method under audio control.", "description": "This figure displays a comparison of talking head generation results driven solely by audio input.  It showcases the results from the authors' proposed ACTalker model alongside several other state-of-the-art methods. Each row represents a different input audio clip, and the columns show the generated video from each method. This comparison visually demonstrates the capabilities of ACTalker in terms of lip-synchronization accuracy, facial expression naturalness, and overall video quality compared to existing approaches.", "section": "4. Experiments"}]