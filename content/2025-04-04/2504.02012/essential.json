{"importance": "This paper is vital for researchers because it introduces a scalable & coherent approach to neural network parameter generation. **It streamlines model adaptation and opens new avenues for research** in hyper-representation learning and architecture search by enabling a unified framework for diverse models.", "summary": "IGPG: An autoregressive framework that unifies parameter synthesis across tasks and architectures, enabling efficient, coherent neural network generation.", "takeaways": ["IGPG allows task-conditioned generation from language or dataset descriptors.", "The framework is architecture-agnostic, leveraging knowledge from multiple pretrained models.", "Autoregressive modeling ensures coherence across layers for faster convergence and enhanced transfer."], "tldr": "Existing methods for generating neural network parameters conditioned on tasks and architectures are limited by scalability, rigidity in handling varying network depths, and disjointed parameter generation. These limitations undermine inter-layer coherence. To address these issues, this paper introduces Instruction-Guided Parameter Generation(**IGPG**), an autoregressive framework. **IGPG unifies parameter synthesis across diverse tasks and architectures**. It leverages a VQ-VAE and an autoregressive model to generate network parameters conditioned on task instructions, dataset, and architecture details. By generating neural network weights' tokens autoregressively, **IGPG ensures inter-layer coherence and efficient adaptation** across models and datasets. \n\nOperating at the token level, **IGPG captures complex parameter distributions from a broad spectrum of pretrained models**. Experiments on vision datasets show that **IGPG consolidates diverse pretrained models into a single, flexible generative framework**. The synthesized parameters achieve competitive or superior performance, especially in scalability and efficiency with large architectures. These results underscore **IGPG's potential for pretrained weight retrieval, model selection, and rapid task-specific fine-tuning**.", "affiliation": "KAIST AI", "categories": {"main_category": "Machine Learning", "sub_category": "Transfer Learning"}, "podcast_path": "2504.02012/podcast.wav"}