[{"figure_path": "https://arxiv.org/html/2504.02826/x1.png", "caption": "Figure 1: Comparison of leading models on our Reasoning-Informed viSual Editing tasks.", "description": "This figure showcases the performance comparison of several leading models on visual editing tasks that require reasoning capabilities.  The models are evaluated across various aspects to assess their ability to accurately follow complex instructions, maintain appearance consistency, and produce visually plausible results.  The visualization helps to identify strengths and weaknesses of different models in handling different types of reasoning challenges within a visual editing context.", "section": "ABSTRACT"}, {"figure_path": "https://arxiv.org/html/2504.02826/x2.png", "caption": "Figure 2: Overview of RISEBench. We present illustrative example questions from each of the four problem categories, each demanding profound image understanding and reasoning capabilities.", "description": "RISEBench is a benchmark for evaluating reasoning-informed visual editing.  Figure 2 shows example questions from each of its four reasoning categories: Temporal, Causal, Spatial, and Logical. Each example requires a deep understanding of the image and the ability to reason about changes over time, cause-and-effect relationships, spatial arrangements, and logical inferences. This highlights the complexity of the tasks in RISEBench and the advanced reasoning skills needed to solve them.", "section": "2 RISEBENCH"}, {"figure_path": "https://arxiv.org/html/2504.02826/x3.png", "caption": "Figure 3: Evaluation metrics of RISEBench. RISEBench assesses the quality of generated images along three key dimensions: Instruction Following, Appearance Consistency, and Visual Plausibility. For each dimension, carefully crafted prompts are provided to the evaluator model (GPT-4o in this study), which analyzes various inputs and returns scores for each corresponding sub-dimension.", "description": "RISEBench evaluates generated images based on three key aspects: how well the image follows instructions, how consistent the image's appearance is with the original, and how visually realistic the image is.  For each aspect, GPT-40 assesses various image features and provides a score.", "section": "2.2 EVALUATION PIPELINE"}, {"figure_path": "https://arxiv.org/html/2504.02826/x4.png", "caption": "Figure 4: Comparison across models on three evaluation sub-dimensions. The native generation models, GPT-4o-Native and Gemini-2-Flash, demonstrate strong performance across all three evaluation dimensions. GPT-4o\u2217 performs well in instruction reasoning but struggles with appearance consistency. The remaining models fail to follow instructions, highlighting the gap in reasoning-informed visual editing.", "description": "This figure compares the performance of various large multimodal models on three key aspects of reasoning-informed visual editing: Instruction Reasoning (how well the model follows instructions), Appearance Consistency (how well the model maintains the original image's visual attributes during editing), and Visual Plausibility (how realistic and coherent the generated image is).  As shown, GPT-40-Native and Gemini-2-Flash, which use native image generation, significantly outperform the other models across all three dimensions.  GPT-40*, although proficient at understanding instructions, struggles to preserve the original image's appearance. The remaining models exhibit poor performance in following instructions, which underscores the challenge of effectively integrating reasoning into visual editing.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02826/x5.png", "caption": "Figure 5: Examples of models\u2019 outputs on RISEBench. The complete outputs of the five evaluated models are provided in Appx.\u00a0B for comprehensive comparison.", "description": "This figure visualizes the outputs of five different large multimodal models on a selection of tasks from the RISEBench benchmark.  Each row represents a specific reasoning task (temporal, causal, spatial, or logical), showing the input image and the results generated by each of the five models.  The models' outputs demonstrate varying degrees of success in understanding and correctly performing the instructed image manipulations. The differences highlight the challenges models still face in general visual editing, especially when complex reasoning is involved. Appendix B in the paper provides a more complete set of results for all tasks.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02826/x6.png", "caption": "Figure 6: Correlation between human and model-based judgments. (a) illustrates the score distributions from human annotators and the model-based evaluator, while (b) presents the MAE between the two, with scores ranging from 0 to 4. The close alignment in score distributions and the low MAE values together indicate a strong correlation between human and model judgments.", "description": "Figure 6 displays the strong agreement between human and model evaluations of image generation quality.  Subfigure (a) shows overlapping score distributions for human judges and a GPT-40 model acting as an automated evaluator.  Subfigure (b) shows the Mean Absolute Error (MAE) between human and model scores across three dimensions (Instruction Reasoning, Appearance Consistency, Visual Plausibility) and the overall score, all with low MAE values. This indicates the model's assessments closely match human judgments.", "section": "3.1 EVALUATION RESULTS (LMM-AS-A-JUDGE)"}, {"figure_path": "https://arxiv.org/html/2504.02826/x7.png", "caption": "Figure 7: Example of cascade generation pipeline in GPT-4o\u2217.", "description": "This figure illustrates the multi-stage process used by the GPT-40* model for image generation.  It starts with analyzing the input image and instructions.  Then, it creates a textual description of the reasoning process, which is then used to generate code that overlays elements (such as lines or text) onto the original image.  This approach contrasts with the native generation models that directly modify pixel values of the image.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02826/x8.png", "caption": "Figure 8: Logical Reasoning Outputs \u2013 Part 1.", "description": "This figure displays examples of outputs from various large multimodal models (LMMs) on a subset of the logical reasoning tasks within the RISEBench benchmark.  Each row presents a single task, showing the input image and the generated outputs from five different models: EMU2, FLUX.1-Canny, GPT-40*, Gemini-2.0-Flash, and GPT-40-Native. The tasks include solving mazes, completing shape patterns, solving mathematical equations represented visually, and solving classic puzzles like the Klotski game. The figure allows for a visual comparison of the models' performance on a diverse range of logical reasoning problems.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02826/x9.png", "caption": "Figure 9: Logical Reasoning Outputs \u2013 Part 2.", "description": "Figure 9 shows the outputs of five different large multimodal models on a subset of logical reasoning tasks from the RISEBench benchmark.  Each task requires the model to solve a visual puzzle or logical challenge, such as completing a pattern, solving a maze, or arranging objects according to specific rules. The figure visually compares the performance of each model, highlighting their relative strengths and weaknesses in terms of their ability to perform logical reasoning on visual tasks.", "section": "RISEBench"}, {"figure_path": "https://arxiv.org/html/2504.02826/x10.png", "caption": "Figure 10: Spatial Reasoning Outputs \u2013 Part 1.", "description": "This figure displays example outputs from different large multi-modality models (LMMs) on spatial reasoning tasks within the RISEBench benchmark.  The tasks require the models to understand and manipulate spatial relationships between objects. Examples include generating a top-down view of a scene, assembling objects into a complete structure, or understanding the spatial relationships between objects to correctly arrange them in 3D space. The figure showcases the differences in the models' abilities to perform these tasks accurately and effectively, highlighting both strengths and limitations of each model.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02826/x11.png", "caption": "Figure 11: Spatial Reasoning Outputs \u2013 Part 2.", "description": "This figure shows the outputs of five different large multi-modality models (LMMs) on a series of spatial reasoning tasks.  Each task requires the model to manipulate or reason about the spatial relationships between objects, such as assembling parts to form a complete object (e.g., a table or clock), creating a specific shape from smaller parts (e.g., a tangram), or completing a visual puzzle. The models' responses demonstrate varying levels of success in understanding and correctly performing these tasks.  The image highlights differences in the models' abilities to understand spatial relationships, spatial transformations and geometric structures.", "section": "Spatial Reasoning"}, {"figure_path": "https://arxiv.org/html/2504.02826/x12.png", "caption": "Figure 12: Temporal Reasoning Outputs \u2013 Part 1.", "description": "This figure displays example outputs from five different models (EMU2, FLUX1-Canny, GPT-40*, Gemini-2.0-Flash, GPT-40-Native) when tasked with temporal reasoning tasks.  Each row presents a different prompt, showing the original image and the resulting image generated by each model. The prompts involve predicting how an object's appearance will change over time (e.g., a banana ripening, a puppy growing, a person aging). The figure helps to visualize how well different models understand temporal change and generate consistent, realistic outputs.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02826/x13.png", "caption": "Figure 13: Temporal Reasoning Outputs \u2013 Part 2.", "description": "This figure displays example outputs from five different visual editing models (EMU2, FLUX 1.0-Canny, GPT-40*, Gemini 2.0-Flash, GPT-40-Native) on temporal reasoning tasks.  Each row presents a task, showing the input image and the outputs generated by each model.  The tasks involve predicting the visual changes of an image over time, such as fruit rotting, a person's hair growing, or a building weathering. The results highlight the varying capabilities of these models in terms of accuracy, realism, and consistency.", "section": "3 EXPERIMENTS"}, {"figure_path": "https://arxiv.org/html/2504.02826/x14.png", "caption": "Figure 14: Causal Reasoning Outputs \u2013 Part 1.", "description": "This figure showcases example outputs from five different large multi-modality models (LMMs) for causal reasoning tasks within the RISEBench benchmark.  Each row presents a single causal reasoning challenge, beginning with the input image and prompt.  The subsequent columns display the outputs generated by each of the five models: EMU2, FLUX.1-Canny, GPT-40*, Gemini-2.0-Flash, and GPT-40-Native. The figure illustrates the variation in model performance in addressing causal reasoning scenarios related to visual editing.  The challenges involve understanding and depicting the visual consequences of actions or events on an object, such as the effect of biting an apple, roasting a steak, or the impact of a ball hitting an obstacle.", "section": "3 Experiments"}, {"figure_path": "https://arxiv.org/html/2504.02826/x9.png", "caption": "Figure 15: Causal Reasoning Outputs \u2013 Part 2.", "description": "This figure shows examples of model outputs for causal reasoning tasks in the RISEBench benchmark.  Each row presents a causal reasoning task, beginning with the input image and the instruction, followed by outputs from five different large multi-modal models (LMMs): EMU2, FLUX.1-Canny, GPT-40*, Gemini-2.0-Flash, and GPT-40-Native. The tasks require the models to understand and generate images representing the result of a causal event or action applied to the input image. The variety of tasks and model outputs demonstrate the challenges in accurate causal reasoning in visual editing.", "section": "2 RISEBENCH"}]