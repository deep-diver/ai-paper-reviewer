{"importance": "This paper introduces a novel **LLM-driven approach** for time series model selection, offering a **computationally efficient alternative** to traditional methods, which can potentially reshape research in automated forecasting and meta-learning.", "summary": "LLMs streamline time series model selection, outperforming traditional methods with reduced overhead.", "takeaways": ["LLMs can effectively perform zero-shot model selection for time series forecasting.", "LLM-based selection significantly reduces computational costs compared to traditional meta-learning.", "Prompt design, incorporating meta-features, impacts LLM performance in model selection."], "tldr": "Time series forecasting is vital across many real-world applications. Selecting the right forecasting model is complex, often needing deep expertise and significant computational resources. Traditional methods involve manual effort in model design, feature engineering, and hyperparameter tuning. Meta-learning approaches try to automate this but depend on pre-built performance matrices which are expensive to create and maintain. It can be difficult to deliver consistently high-quality predictions due to the inherent diversity in time series data. \n\nTo solve these issues, this paper explores using **Large Language Models (LLMs)** for model selection, removing the need for explicit performance matrices. The study uses Llama, GPT, and Gemini, demonstrating superior performance over traditional meta-learning and heuristic baselines, while greatly cutting computational overhead. The findings show LLMs have the potential to efficiently select models for time series forecasting. The authors also investigate various prompt designs and their effects on different LLMs.", "affiliation": "Virginia Tech", "categories": {"main_category": "Machine Learning", "sub_category": "Meta Learning"}, "podcast_path": "2504.02119/podcast.wav"}