{"references": [{"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-07-16", "reason": "This paper introduced CLIP, a foundational vision-language model that this work aims to interpret and steer."}, {"fullname_first_author": "Trenton Bricken", "paper_title": "Towards monosemanticity: Decomposing language models with dictionary learning", "publication_date": "2023-01-01", "reason": "This paper is a core inspiration, focusing on achieving monosemanticity in language models using sparse autoencoders."}, {"fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "publication_date": "2023-01-01", "reason": "This paper introduces LLaVA, a multimodal language model that is steered with the help of the sparse autoencoders in the present work."}, {"fullname_first_author": "Leo Gao", "paper_title": "Scaling and evaluating sparse autoencoders", "publication_date": "2025-01-01", "reason": "This paper directly relates to the implementation details as it focuses on scaling and evaluating sparse autoencoders, which are a fundamental component of this work."}, {"fullname_first_author": "Bart Bussmann", "paper_title": "Batch-topk sparse autoencoders", "publication_date": "2024-01-01", "reason": "This paper proposes batch-topk sparse autoencoders which were directly compared with Matryoshka autoencoders in this work."}]}