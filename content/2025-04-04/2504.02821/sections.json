[{"heading_title": "SAE for VLMs", "details": {"summary": "SAEs for VLMs represent a compelling avenue for enhancing model interpretability and control. By applying SAEs to VLM activations, the entangled concepts within neurons are disentangled, resulting in **more monosemantic representations**. This approach allows for targeted interventions, enabling steering of model outputs towards specific concepts. The use of a Monosemanticity Score(MS) offers a quantitative way to validate improved concept separation, potentially revealing **hierarchical relationships**. Future work involves exploring applications beyond vision, potentially **benefiting other modalities** within VLMs by extending the framework to other representation types."}}, {"heading_title": "Monosemanticity", "details": {"summary": "**Monosemanticity** in neural networks, especially in vision-language models (VLMs), refers to the degree to which a single neuron represents a single, clear concept. A highly monosemantic neuron will consistently activate for similar inputs, indicating a focused understanding of a specific feature. The lack of it suggests **polysemanticity**, where neurons respond to diverse, unrelated concepts, hindering interpretability. Improving monosemanticity enhances a model's **transparency and control**, allowing for more targeted interventions and steering of outputs. Techniques like Sparse Autoencoders (SAEs) are crucial for disentangling representations and promoting monosemanticity, leading to more understandable and steerable AI systems by separating entangled concepts."}}, {"heading_title": "Steering MLLMs", "details": {"summary": "**Steering Multimodal Language Models (MLLMs)** is a promising area that allows influencing the behavior of these models in a more controlled and interpretable manner. The paper explores how Sparse Autoencoders (SAEs) can be leveraged to achieve this goal. By attaching SAEs to the vision encoder of an MLLM, it becomes possible to intervene on specific neuron activations and steer the model's output towards desired concepts. This is particularly interesting because it offers a way to **modify the model's behavior without directly altering its parameters**, which is highly useful. The approach involves first training an SAE on the vision encoder's activations to discover disentangled concepts. Then, during inference, the activation of a chosen SAE neuron is manipulated, which effectively biases the visual information fed into the LLM component of the MLLM. The results demonstrate that this technique can successfully steer the MLLM's output towards the concept represented by the intervened neuron, even if that concept is not explicitly present in the input image.  This opens up exciting possibilities for **fine-grained control over MLLMs** and could have significant implications for various applications where precise and interpretable model behavior is crucial."}}, {"heading_title": "MS evaluation", "details": {"summary": "Evaluating monosemanticity through the proposed MS metric allows for a quantitative assessment of how well individual neurons within SAEs disentangle concepts. **The MS metric facilitates the comparison of neuron specialization, both within SAEs and against original VLM neurons.** High MS values indicate neurons are consistently activated by similar images, suggesting focused concept representation. The metric facilitates analysis of the impact of factors like expansion factor and sparsity on monosemanticity. **By evaluating across different SAE configurations, the effectiveness of various architectural choices in promoting concept disentanglement can be quantified.** It aids in assessing hierarchical structure learning, showing an alignment with expert-defined taxonomies."}}, {"heading_title": "iNat hierarchy", "details": {"summary": "The 'iNat hierarchy,' seemingly referring to the iNaturalist taxonomy, plays a crucial role in the study. It acts as an **expert-defined structure** against which learned representations are evaluated. The alignment between concepts discovered by Matryoshka SAEs and the iNaturalist taxonomy suggests the SAEs effectively capture **real-world, hierarchical relationships**. The ability of the model to mirror this pre-existing organization indicates a level of interpretability and semantic grounding beyond simple feature extraction. Further more, the comparison between learned representations and the 'iNat hierarchy' provides a means of **quantifying the granularity** of the discovered concepts. Analyzing the mapping of neurons to different levels reveals how the model organizes information. The results indicate that the most specialized neurons are found in the lower levels which allows interpretable results."}}]