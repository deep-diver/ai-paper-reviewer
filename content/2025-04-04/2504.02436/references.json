{"references": [{"fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "publication_date": "2022-01-01", "reason": "This work introduces Latent Diffusion Models (LDMs), which significantly reduces computational costs and enables high-resolution image synthesis, making it fundamental to many subsequent works in the field."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This paper introduces CLIP (Contrastive Language-Image Pre-training), a model trained on a massive dataset of image-text pairs which enables zero-shot image classification and serves as a key component in many multimodal systems."}, {"fullname_first_author": "Colin Raffel", "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "publication_date": "2019-01-01", "reason": "This work presents the T5 model, a text-to-text transformer that achieves state-of-the-art results on a variety of NLP tasks after being pre-trained on a large corpus, and is used as the text encoder in this project."}, {"fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "publication_date": "2023-01-01", "reason": "This study introduces Diffusion Transformers (DiT), demonstrating how transformer architectures can be effectively scaled to achieve high-quality image generation within diffusion models, influencing advancements in controllable generation."}, {"fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "publication_date": "2020-01-01", "reason": "This foundational paper introduces denoising diffusion probabilistic models (DDPMs), which provide a mathematical framework for generative modeling that iteratively refines noisy data into structured outputs, thus influencing subsequent research in generative modeling."}]}