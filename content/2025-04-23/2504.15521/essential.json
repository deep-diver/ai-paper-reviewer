{"importance": "This paper is a wake-up call, highlighting the limitations of relying solely on translated benchmarks. By advocating for **culturally and linguistically authentic evaluations**, this work can reshape multilingual research, driving improvements that are relevant across diverse global communities. This work opens many avenues for creating high quality datasets by prioritizing human alignment and localized evaluation.", "summary": "Multilingual benchmarks are biased, under-representative and not well-aligned with human judgment; this paper proposes guidelines for effective evaluation.", "takeaways": ["Current multilingual benchmarks are heavily biased towards English and high-resource languages.", "Simply translating benchmarks is insufficient; localized benchmarks better reflect human judgments.", "The research community needs global collaboration to create culturally and linguistically authentic benchmarks."], "tldr": "As multilingual Large Language Models(LLMs) grow, robust evaluation is key for equitable advancement. This paper examines 2,000+ multilingual benchmarks from 148 countries (2021-2024), assessing past, present, and future practices. It reveals that English is over-represented, benchmarks favor original language content, mostly from high-resource countries, and benchmark performance shows disparities with human judgments (especially in non-STEM tasks).\n\nTo address the issues, the paper highlights limitations in multilingual evaluation, proposing principles for effective benchmarking and critical research directions. It advocates for global collaboration to develop human-aligned benchmarks and also **emphasizes creating culturally and linguistically tailored benchmarks instead of relying on translations**. This comprehensive analysis aims to guide future directions for a more equitable, representative, and effective evaluation of language technologies.", "affiliation": "Alibaba International Digital Commerce", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2504.15521/podcast.wav"}