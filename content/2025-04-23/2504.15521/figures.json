[{"figure_path": "https://arxiv.org/html/2504.15521/extracted/6378882/figures/past_present_future-1.png", "caption": "Figure 1: The overview of this work. We examine over 2,000 multilingual (non-English) benchmarks, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking.", "description": "This figure provides a visual overview of the paper's research on multilingual benchmarks. It illustrates the three phases of the study: past (2021-2024), present, and future.  The past phase shows the number of benchmarks, their correlation with human evaluations, and their approximate costs. The present phase displays the current status of multilingual evaluation by highlighting the distribution of tasks, the correlation of benchmark performance with human evaluations in various language groups, and insights into current evaluation practices. The future phase outlines the key research directions proposed by the authors, focusing on improvements to existing methodologies and the development of more equitable and comprehensive multilingual benchmarks.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.15521/x1.png", "caption": "Figure 2: \nDistribution of the top 50 languages in our multilingual benchmark collection. Although English is deliberately excluded from the collection, it still appears as the most frequent language in the collection. This distribution illustrates the current imbalance in multilingual evaluation benchmarks.", "description": "Figure 2 shows the frequency distribution of the top 50 languages used in the 2024 multilingual benchmark dataset.  Despite the deliberate exclusion of English-language benchmarks during data collection, English remains the most frequently represented language.  This visualization highlights a significant imbalance in the current landscape of multilingual benchmark creation, with high-resource languages heavily overrepresented and low-resource languages underrepresented.", "section": "PAST: What Benchmarks Do We Have?"}, {"figure_path": "https://arxiv.org/html/2504.15521/extracted/6378882/figures/translation.png", "caption": "Figure 3: Distribution of translation methods used in benchmark creation.", "description": "This figure shows a pie chart that illustrates the distribution of translation methods employed in the creation of multilingual benchmarks.  The methods are categorized as: 'Not Translated' (original language content), 'Human Translator' (professional human translation), 'Google Translate' (machine translation using Google Translate), and other machine translation tools such as GPT series models, DeepL, NLLB series, IndicTrans, Opus-MT, and Microsoft Translator. The chart visually represents the proportion of benchmarks created using each method.", "section": "4. PAST: What Benchmarks Do We Have?"}, {"figure_path": "https://arxiv.org/html/2504.15521/extracted/6378882/figures/domains1.png", "caption": "Figure 5: Distribution of domains across multilingual benchmarks in our collection.", "description": "This figure shows the distribution of domains across the 2024 multilingual benchmarks collected in this study.  It reveals the source domains of the datasets used for benchmark creation. The most frequent domains are news (17.0%), social media (13.3%), and Wikipedia-derived content (9.6%), showing a prevalence of publicly accessible data.  Less frequent are specialized and higher-value domains like healthcare, law, and entertainment. This highlights a bias towards easily accessible data rather than specialized or high-value content.", "section": "PAST: What Benchmarks Do We Have?"}, {"figure_path": "https://arxiv.org/html/2504.15521/x2.png", "caption": "Figure 7: Distribution of user instruction categories across six languages. We discard the \u201cGreetings\" category, as it is not a task-oriented instruction.", "description": "This figure shows the distribution of user requests across six different languages (English, Chinese, French, German, Spanish, and Russian) within the Chatbot Arena platform.  The data is categorized into several task-oriented instruction types, such as Writing, Translation, Math, and Programming, offering insights into how users across various linguistic backgrounds utilize large language models. Notably, the \"Greetings\" category has been excluded from the analysis because it is not considered a task-oriented instruction.", "section": "5.1. What Are the Multilingual Users Interested in?"}, {"figure_path": "https://arxiv.org/html/2504.15521/x3.png", "caption": "Figure 8: A conceptual framework illustrating the essential characteristics of effective multilingual benchmarks and future research directions for advancing multilingual evaluation.", "description": "Figure 8 presents a conceptual framework outlining the key characteristics of effective multilingual benchmarks and suggests future research directions to improve multilingual evaluation. The framework is divided into two main parts: Core Characteristics and Future Directions.  Core Characteristics lists essential qualities for effective benchmarks, including accuracy, absence of contamination (where training data overlaps with benchmark data), sufficient challenge, practical relevance, linguistic diversity, and cultural authenticity.  Future Directions highlights five key areas for future research: improving natural language generation capabilities, broadening coverage to include low-resource languages, developing localized benchmarks, using LLMs as evaluators, and creating efficient benchmarking methods. The figure visually links these future directions to the previously mentioned core characteristics, showing how each research area contributes to improving the overall quality and effectiveness of multilingual benchmarks.", "section": "6. FUTURE: What We Need and What We Should Do Next?"}]