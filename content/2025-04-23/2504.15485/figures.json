[{"figure_path": "https://arxiv.org/html/2504.15485/x1.png", "caption": "Figure 1: CAPTURe example with an output from GPT4-o.\nWhile people can easily infer the missing number of cups and correctly reason over occluded patterns, models generally struggle to reason over these occluded scenes.", "description": "This figure shows an example from the CAPTURE task, where the goal is to count the number of cups arranged in a grid pattern, some of which are hidden behind a black box.  Humans can easily infer the number of hidden cups by extrapolating the visible pattern, demonstrating strong spatial reasoning skills. In contrast, the example shows that the GPT-4-0 model fails to correctly count the cups due to its difficulty in reasoning about occluded objects.  This highlights a key limitation of current vision-language models in handling real-world scenarios where objects are frequently partially or fully occluded.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2504.15485/x2.png", "caption": "Figure 2: Example images with GPT-4o responses to CAPTURerealreal{}^{\\text{real}}start_FLOATSUPERSCRIPT real end_FLOATSUPERSCRIPT and CAPTUResyntheticsynthetic{}^{\\text{synthetic}}start_FLOATSUPERSCRIPT synthetic end_FLOATSUPERSCRIPT occluded splits.", "description": "This figure shows example images from the CAPTURE dataset, specifically the real and synthetic subsets with occluded objects.  For each image, the human-provided ground truth count of the objects is shown, along with GPT-40's response. This demonstrates how well (or poorly) the model is able to extrapolate the pattern of objects from the visible portion to estimate the total number of objects, which are partially hidden behind a black box.", "section": "2. CAPTURE"}, {"figure_path": "https://arxiv.org/html/2504.15485/x3.png", "caption": "Figure 3: # of objects in CAPTURerealreal{}^{\\text{real}}start_FLOATSUPERSCRIPT real end_FLOATSUPERSCRIPT images.", "description": "This histogram shows the distribution of the number of objects present in the images of the CAPTUREreal dataset.  The x-axis represents ranges of object counts, and the y-axis shows the frequency of images falling within each count range. This visualization helps understand the scale and variability of object counts in the real-world images used for the CAPTURE benchmark.", "section": "2.2 Dataset"}, {"figure_path": "https://arxiv.org/html/2504.15485/x4.png", "caption": "Figure 4: # of occluded objects in CAPTUResyntheticsynthetic{}^{\\text{synthetic}}start_FLOATSUPERSCRIPT synthetic end_FLOATSUPERSCRIPT images.", "description": "This histogram shows the distribution of the number of occluded objects across all images in the CAPTURESynthetic dataset.  The x-axis represents the number of occluded objects, and the y-axis shows the frequency (number of images) with that count of occluded objects.  This visualization helps to understand the range and typical number of occluded objects present in the dataset, which is useful for evaluating model performance on the CAPTURE task.", "section": "2. CAPTURE"}, {"figure_path": "https://arxiv.org/html/2504.15485/x5.png", "caption": "Figure 5: VLM vs. VLM + CountGD hybrid on questions from the CAPTURerealreal{}^{\\text{real}}start_FLOATSUPERSCRIPT real end_FLOATSUPERSCRIPT (occluded split) that are not in CountGD training set. Metric: sMAPE (lower is better).", "description": "This figure compares the performance of four vision-language models (VLMs) individually against their performance when combined with the CountGD object detection model.  The comparison is made on a subset of the CAPTUREreal dataset (the real-world images with occlusions) specifically excluding images used to train the CountGD model. The metric used to evaluate model performance is the Symmetric Mean Absolute Percentage Error (SMAPE), with lower scores indicating better performance. The goal is to illustrate how CountGD, while strong at counting visible objects, can supplement the VLMs to improve their overall amodal counting performance (counting objects that are partially or fully occluded).", "section": "4. Main Results on CAPTUREreal"}, {"figure_path": "https://arxiv.org/html/2504.15485/x6.png", "caption": "Figure 6: Effect of number of total objects in the image and number of occluded objects on sMAPE from CAPTUResyntheticsynthetic{}^{\\text{synthetic}}start_FLOATSUPERSCRIPT synthetic end_FLOATSUPERSCRIPT (occluded split). Metric: sMAPE (lower is better).", "description": "This figure displays the relationship between the total number of objects in an image, the number of occluded objects, and the Symmetric Mean Absolute Percentage Error (SMAPE) obtained from the CAPTUREsynthetic dataset (occluded split).  The x-axis in the left panel represents the total number of objects, while the right panel's x-axis shows the number of occluded objects. The y-axis in both panels represents the SMAPE score. Lower SMAPE values indicate better model performance.  The figure aims to show how model performance changes based on the number of total objects and the number of occluded objects in the images.  Each line represents a different Vision Language Model (VLM).", "section": "4.2. Effect of Data Factors on VLM Performance"}, {"figure_path": "https://arxiv.org/html/2504.15485/x7.png", "caption": "Figure 7: Effect of pattern type in CAPTUResyntheticsynthetic{}^{\\text{synthetic}}start_FLOATSUPERSCRIPT synthetic end_FLOATSUPERSCRIPT (occluded split) on sMAPE. Metric: sMAPE (lower is better).", "description": "This figure shows the impact of different object arrangement patterns on the model's performance in the CAPTUREsynthetic (occluded) dataset.  The x-axis represents the type of pattern (circle, rectangle, triangle), and the y-axis represents the Symmetric Mean Absolute Percentage Error (SMAPE). Lower SMAPE values indicate better model performance. The figure helps to understand how the model's spatial reasoning ability varies depending on the arrangement of objects.", "section": "4.2. Effect of Data Factors on VLM Performance"}, {"figure_path": "https://arxiv.org/html/2504.15485/x8.png", "caption": "Figure 8: Example image and text inputs for experiments with auxiliary information experiments (Sec.\u00a04.3). Blue eyes indicate objects for which the All Object Coordinate Oracle or Visible Object Coordinate Oracle extracts coordinates. Brighter part of image represents the area which Inpainting Pipeline fills in.\nExample prompts shown in italics.\nBlue eye overlays and faded parts of images are for demonstration purposes and are not passed with the image.", "description": "Figure 8 shows examples of how auxiliary information was provided to the models in the experiments described in Section 4.3.  The figure presents three variations on an image of occluded cans: one with just the original occluded image, one where object coordinates (visible and all) were provided as text to the model, and one where an image inpainting pipeline was used to fill in the occluded area. Each version shows the prompt used for that condition. The \"blue eyes\" overlay on some objects indicates which objects' coordinates were extracted and provided to the models (in the coordinate oracle conditions). The brighter area in the inpainting condition shows the area that was filled in by the inpainting model; this filled-in region is not part of the image that is provided to the model.", "section": "4.3. Analysis with Auxiliary Information"}, {"figure_path": "https://arxiv.org/html/2504.15485/x9.png", "caption": "Figure 9: Reliability curve of prompted confidence vs. sMAPE.", "description": "This figure shows the relationship between the model's confidence in its prediction and the accuracy of that prediction, measured by sMAPE (Symmetric Mean Absolute Percentage Error).  Higher confidence scores ideally correspond to lower sMAPE values (more accurate predictions). The graph helps analyze whether the model's confidence is well-calibrated; a well-calibrated model would show a strong negative correlation between confidence and error.  Separate curves are shown for different scenarios (real vs. synthetic data, and occluded vs. unoccluded images), highlighting how confidence calibration may vary under different conditions.", "section": "4.3. Analysis with Auxiliary Information"}, {"figure_path": "https://arxiv.org/html/2504.15485/x10.png", "caption": "Figure 10: Reliability curve of sampled confidence vs. sMAPE.", "description": "This figure shows the relationship between the model's confidence in its predictions and the Symmetric Mean Absolute Percentage Error (sMAPE) for the CAPTURE task.  The x-axis represents the model's sampled confidence (calculated by generating multiple predictions and determining the frequency of the most common answer). The y-axis represents the sMAPE, a measure of prediction accuracy, where lower values indicate better accuracy. Separate lines are plotted for real and synthetic data, both with and without occlusion, illustrating how the relationship between confidence and accuracy varies across different scenarios of the CAPTURE dataset. The graph helps assess if the model is well-calibrated (i.e., does its confidence accurately reflect its prediction error).", "section": "C.3 Uncertainty/Confidence"}, {"figure_path": "https://arxiv.org/html/2504.15485/x11.png", "caption": "Figure 11: Confusion matrix: predicted vs. ground truth counts for CAPTURerealreal{}^{\\text{real}}start_FLOATSUPERSCRIPT real end_FLOATSUPERSCRIPT\u2019s occluded split.", "description": "This figure presents confusion matrices visualizing the performance of four different vision-language models (GPT-40, InternVL2, Molmo, and Qwen2VL) on the CAPTUREreal (occluded split) task. Each matrix shows the counts of predictions versus ground truth object counts. The matrices reveal the models' tendencies for over- or under-prediction and their accuracy across various object counts.", "section": "4. Results and Analysis"}]