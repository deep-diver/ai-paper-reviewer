[{"figure_path": "https://arxiv.org/html/2503.10632/x1.png", "caption": "Figure 1: Model parameters vs.\u00a0Top-1 Test accuracy in ImegeNet-1K training of vanilla ViTs (Dosovitskiy et\u00a0al., 2020), Vision KAN (DeiT+KAN) by (Chen et\u00a0al., 2024), ViT+KAN and Kolmogorov-Arnold Transformer (KAT) by (Yang & Wang, 2025).", "description": "This figure compares the performance of different vision transformer models on the ImageNet-1K dataset. The x-axis represents the number of model parameters (in millions), while the y-axis shows the Top-1 test accuracy.  The plot includes results for vanilla Vision Transformers (ViTs) of varying sizes (Tiny, Small, Base), Vision KAN (DeiT+KAN) which replaces MLP layers in DeiT with KANs, ViT+KAN that replaces MLP layers in ViTs with KANs, and the Kolmogorov-Arnold Transformer (KAT) which uses a refined group-KAN strategy.  The figure illustrates the relationship between model complexity and accuracy, allowing for a comparison of different approaches to incorporating Kolmogorov-Arnold networks into vision transformers.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10632/x2.png", "caption": "Figure 2: (a) The standard softmax self-attention for ithsuperscript\ud835\udc56thi^{\\rm th}italic_i start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPT head in the jthsuperscript\ud835\udc57thj^{\\rm th}italic_j start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPT encoder block. (b) The Kolmogorov-Arnold Attention (KArAt) replaces the softmax with a learnable function \u03a6i,jsuperscript\u03a6\ud835\udc56\ud835\udc57\\Phi^{i,j}roman_\u03a6 start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT. (c) The ideal KArAt is with an operator matrix, \u03a6i,jsuperscript\u03a6\ud835\udc56\ud835\udc57\\Phi^{i,j}roman_\u03a6 start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT with N\u00d7N\ud835\udc41\ud835\udc41N\\times Nitalic_N \u00d7 italic_N learnable units that act on each row of \ud835\udc9ci,jsuperscript\ud835\udc9c\ud835\udc56\ud835\udc57{\\cal A}^{i,j}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT. (d) While \u03a6i,j\u2208\u211dN\u00d7Nsuperscript\u03a6\ud835\udc56\ud835\udc57superscript\u211d\ud835\udc41\ud835\udc41\\Phi^{i,j}\\in\\mathbb{R}^{N\\times N}roman_\u03a6 start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT \u2208 blackboard_R start_POSTSUPERSCRIPT italic_N \u00d7 italic_N end_POSTSUPERSCRIPT is impossible to implement due to computational constraints, our architecture uses an operator \u03a6^i,jsuperscript^\u03a6\ud835\udc56\ud835\udc57\\widehat{\\Phi}^{i,j}over^ start_ARG roman_\u03a6 end_ARG start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT with N\u00d7r\ud835\udc41\ud835\udc5fN\\times ritalic_N \u00d7 italic_r learnable units r\u226aNmuch-less-than\ud835\udc5f\ud835\udc41r\\ll Nitalic_r \u226a italic_N, followed by a linear projector with learnable weights W\u2208\u211dr\u00d7N\ud835\udc4asuperscript\u211d\ud835\udc5f\ud835\udc41W\\in\\mathbb{R}^{r\\times N}italic_W \u2208 blackboard_R start_POSTSUPERSCRIPT italic_r \u00d7 italic_N end_POSTSUPERSCRIPT.", "description": "Figure 2 illustrates the architecture of the proposed Kolmogorov-Arnold Attention (KArAt). (a) shows the standard softmax self-attention mechanism where the attention matrix is computed using the scaled dot-product of query, key, and value matrices. (b) replaces the softmax function with a learnable function, which is the core idea of KArAt. (c) depicts the ideal KArAt, where a large N x N learnable operator matrix acts directly on the attention matrix. However, due to computational limitations, (d) presents the practical implementation of KArAt using a smaller N x r learnable operator followed by a linear projection. This reduced architecture makes training feasible while still capturing the essence of learnable attention.", "section": "3. How Can We Design Learnable Attention?"}, {"figure_path": "https://arxiv.org/html/2503.10632/x3.png", "caption": "(a) Attention matrix \ud835\udc9ci,jsuperscript\ud835\udc9c\ud835\udc56\ud835\udc57{\\mathcal{A}^{i,j}}caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT\nbefore softmax activation.", "description": "Figure 3(a) displays the spectral analysis of attention matrices in the jth encoder block's ith head before applying the softmax function. The analysis uses all three heads in the last encoder block of a ViT-tiny model, processing five randomly selected images from the CIFAR-10 validation set. The plot shows 15 singular vectors (each with 197 dimensions), with singular values arranged in descending order.", "section": "5.2. Analysis of Performance"}, {"figure_path": "https://arxiv.org/html/2503.10632/x4.png", "caption": "(b) Attention matrix \u03c3\u2062(\ud835\udc9ci,j)\ud835\udf0esuperscript\ud835\udc9c\ud835\udc56\ud835\udc57\\sigma(\\mathcal{A}^{i,j})italic_\u03c3 ( caligraphic_A start_POSTSUPERSCRIPT italic_i , italic_j end_POSTSUPERSCRIPT ) after softmax activation.", "description": "The figure shows a heatmap visualization of the attention matrix after applying the softmax activation function. Each row of the matrix represents a query vector, and each column represents a key vector. The values in the matrix indicate the attention weights between each query-key pair. The softmax function normalizes these weights, so that they sum to 1 for each row. The visualization helps to understand the focus of the attention mechanism on different regions of the input image.", "section": "3. How Can We Design Learnable Attention?"}, {"figure_path": "https://arxiv.org/html/2503.10632/x5.png", "caption": "Figure 3: Spectral analysis of the attention matrices before and after softmax shows that they are low-rank. For this experiment, we use all 3 heads in the last encoder block of ViT tiny on 5 randomly sampled images from the CIFAR-10 validation set. We plot all 15 singular vectors (each of 197 dimensions) where the singular values are arranged in non-increasing order.", "description": "This figure presents a spectral analysis of attention matrices from a Vision Transformer (ViT-tiny) model, comparing the singular value decomposition (SVD) before and after applying the softmax activation function.  The analysis uses the last encoder block's three attention heads and five randomly chosen images from the CIFAR-10 validation set.  The plot displays the 15 singular vectors (one for each head across the five images), each having 197 dimensions, with singular values arranged in descending order. The results visually demonstrate that the attention matrices exhibit a low-rank structure, both before and after the softmax activation.", "section": "Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/TrainingPlots.png", "caption": "Figure 4: Different configurations to update \u03a6^^\u03a6\\widehat{\\Phi}over^ start_ARG roman_\u03a6 end_ARG:(a) Blockwise configuration, where \u03a6i,1\u2260\u03a6i,2\u2260\u22ef\u2260\u03a6i,Lsuperscript\u03a6\ud835\udc561superscript\u03a6\ud835\udc562\u22efsuperscript\u03a6\ud835\udc56\ud835\udc3f\\Phi^{i,1}\\neq\\Phi^{i,2}\\neq\\cdots\\neq\\Phi^{i,L}roman_\u03a6 start_POSTSUPERSCRIPT italic_i , 1 end_POSTSUPERSCRIPT \u2260 roman_\u03a6 start_POSTSUPERSCRIPT italic_i , 2 end_POSTSUPERSCRIPT \u2260 \u22ef \u2260 roman_\u03a6 start_POSTSUPERSCRIPT italic_i , italic_L end_POSTSUPERSCRIPT for all i=1,2,\u2026,h\ud835\udc5612\u2026\u210ei=1,2,...,hitalic_i = 1 , 2 , \u2026 , italic_h (b) universal configuration, where \u03a6i,1=\u03a6i,2=\u22ef=\u03a6i,L=\u03a6isuperscript\u03a6\ud835\udc561superscript\u03a6\ud835\udc562\u22efsuperscript\u03a6\ud835\udc56\ud835\udc3fsuperscript\u03a6\ud835\udc56\\Phi^{i,1}=\\Phi^{i,2}=\\cdots=\\Phi^{i,L}=\\Phi^{i}roman_\u03a6 start_POSTSUPERSCRIPT italic_i , 1 end_POSTSUPERSCRIPT = roman_\u03a6 start_POSTSUPERSCRIPT italic_i , 2 end_POSTSUPERSCRIPT = \u22ef = roman_\u03a6 start_POSTSUPERSCRIPT italic_i , italic_L end_POSTSUPERSCRIPT = roman_\u03a6 start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT for all i=1,2,\u2026,h.\ud835\udc5612\u2026\u210ei=1,2,...,h.italic_i = 1 , 2 , \u2026 , italic_h .", "description": "Figure 4 illustrates two different ways of updating the learnable activation function, \u03a6, within the multi-head self-attention mechanism of a Vision Transformer.  In the (a) Blockwise configuration, a separate \u03a6 is learned for each head (indexed by i) and each encoder block (indexed by j), allowing for greater flexibility and potential to capture diverse relationships within the data. Each head learns different patterns in different blocks. In the (b) Universal configuration, a single \u03a6 is learned and shared across all heads and blocks, making the model more efficient but potentially less capable of adapting to complex data patterns.  All heads share the same learned parameters in all blocks.", "section": "4. Our Architecture"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/WeightDist.png", "caption": "Figure 5: Training loss and test accuracy of vanilla ViTs and their Fourier KArAt versions on CIFAR-10, CIFAR-100 and ImageNet-1K.", "description": "This figure displays the training loss and testing accuracy curves for standard Vision Transformers (ViTs) and modified versions incorporating Fourier Kolmogorov-Arnold Attention (Fourier KArAt) across three benchmark datasets: CIFAR-10, CIFAR-100, and ImageNet-1K.  Separate curves are shown for each dataset and model variant (ViT-Tiny, ViT-Small, ViT-Base with and without Fourier KArAt).  The plots allow for a visual comparison of the training performance and generalization ability of the models, highlighting the differences in convergence speed and final accuracy.", "section": "5. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10632/x6.png", "caption": "Figure 6: Weight distribution of (top to bottom) ViT-Tiny, ViT-Tiny+Fourier KArAt, ViT-Base, ViT-Base+Fourier KArAt. The columns (left to right) represent weights at initialization, at epoch 50, at epoch 100, and their superposition.", "description": "This figure visualizes the weight distributions of four different models at various training stages.  The top two rows show the weight distributions for ViT-Tiny and ViT-Tiny enhanced with Fourier KArAt. The bottom two rows illustrate the weight distributions for ViT-Base and ViT-Base enhanced with Fourier KArAt. Each row displays four columns, representing the weight distribution at initialization, after 50 epochs, after 100 epochs, and a superposition of these three. This visual representation allows for a comparison of how weight distributions evolve during training in standard Vision Transformers (ViTs) and ViTs that incorporate learnable Fourier Kolmogorov-Arnold Attention (Fourier KArAt).", "section": "5. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/LossLandscapeAllV2.png", "caption": "Figure 7: Spectral decomposition of the attention matrix for ViT-Tiny on CIFAR-10 dataset with vanilla softmax attention and our learnable Fourier attention.", "description": "This figure displays the singular value decomposition of the attention matrices from ViT-Tiny model trained on CIFAR-10 dataset.  It compares the spectral properties of attention matrices generated using the standard softmax activation function and the proposed learnable Fourier attention mechanism. The plot shows the singular values (log scale) against their index, illustrating the relative importance of different components in each attention matrix. The relative importance of the singular values reflects the low rank nature of the attention mechanism.  Comparing the two shows the difference in the distributions of the singular values between the standard softmax and the learnable Fourier attention, indicating a difference in the complexity of their respective information encoding.", "section": "5.2 Analysis of Performance"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/DinoAttnVisV2.png", "caption": "Figure 8: Loss landscape for ViT-Tiny and ViT-Base (the smallest and the largest model) along the two largest principal component directions of the successive change of model parameters. The top row provides a 3D-visulaization of the loss surface including the local optima and saddle points. The bottom row shows the loss contours along with the trajectory of the optimizers.", "description": "This figure visualizes the loss landscapes of ViT-Tiny and ViT-Base models, comparing the vanilla softmax attention mechanism with the Fourier KArAt variant.  The top row presents 3D visualizations of the loss surfaces, highlighting local optima (minima) and saddle points. These points represent areas of the loss surface where the model's performance is either optimal or relatively unchanging with respect to small parameter changes. The bottom row displays 2D contour plots of the loss landscapes. The paths of the optimizers during training are overlaid on these contours, illustrating the model's journey through the loss landscape towards the minimum.  By comparing the loss landscapes of the vanilla and Fourier KArAt models, the visualization offers insights into their optimization behavior and generalization capabilities.", "section": "5.2. Analysis of Performance"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c10.png", "caption": "Figure 9: Visualizing attention maps for Vit-Tiny. The original images used for inference are on the left, and on the right, we show the attention score map and the image regions of the dominant head (Top row: Fourier KArAt, bottom row: Vanilla MHSA).", "description": "This figure visualizes attention maps for ViT-Tiny, comparing Fourier KArAt and Vanilla MHSA.  The left side shows the original images used for inference. The right side displays both the attention score map (highlighting which parts of the image the model focuses on) and the corresponding image regions of the dominant head (most influential part of the attention mechanism). The top row shows results from using Fourier KArAt, while the bottom row shows results using the standard Vanilla MHSA.", "section": "5. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_c100.png", "caption": "(a) Training time on CIFAR-10 dataset.", "description": "This figure displays a bar chart comparing the training times of various Vision Transformer models on the CIFAR-10 dataset.  The models compared include the vanilla ViT (without learnable attention), and several variants of the Fourier KArAt model.  The Fourier KArAt models are differentiated by their grid size (G) and update configuration (blockwise or universal), reflecting different settings for the learnable attention mechanism. The chart visually represents the computational cost associated with each model, allowing for easy comparison of training efficiency between the vanilla ViT and the different configurations of Fourier KArAt.", "section": "5. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/train_time_in1k.png", "caption": "(b) Training time on CIFAR-100 dataset.", "description": "This figure shows the training time in hours for different vision transformer models (ViT-Tiny, ViT-Small, ViT-Base) and their corresponding Fourier KArAt variants on the CIFAR-100 dataset.  The training was conducted for 100 epochs using the hyperparameters listed in Table 2 of the paper.  The figure compares the training times of vanilla models with their Fourier KArAt counterparts (G3U/G1U for universal configuration and G3B/G1B for blockwise configuration), showing the increase in training time introduced by the learnable attention mechanism.", "section": "5. Empirical Analysis"}, {"figure_path": "https://arxiv.org/html/2503.10632/extracted/6276418/Figures/throughput.png", "caption": "(c) Training time on Imagenet-1K dataset.", "description": "This figure shows the training time in hours for different Vision Transformer models (ViT-Tiny, ViT-Small, ViT-Base) and their corresponding Fourier KArAt versions on the ImageNet-1K dataset.  The training was conducted for 100 epochs using the hyperparameters specified in Table 2 of the paper. The figure allows for a comparison of the computational cost between vanilla Vision Transformers and their Fourier KArAt counterparts across different model sizes.", "section": "5. Empirical Analysis"}]