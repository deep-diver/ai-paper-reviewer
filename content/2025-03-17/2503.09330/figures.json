[{"figure_path": "https://arxiv.org/html/2503.09330/x1.png", "caption": "Figure 1: Comparing unlearning approaches. Previous works assume the forget set to be uniformly distributed. However, real-life unlearning requests do not comply with the uniform distribution assumption\u00a0[3]. If the forget set distribution is predominant in some groups (e.g., old males), it can lead to performance degradation in such dominant forget groups (i.e., the blue group in the figure).\nGroup-robust Unlearning prevents this from happening.", "description": "This figure compares different machine unlearning approaches.  Traditional methods assume that the data being unlearned (the 'forget set') is evenly distributed across all groups within the dataset.  However, in reality, unlearning requests may disproportionately come from certain demographics (e.g., older men). This figure illustrates how this non-uniform distribution can cause a significant drop in model accuracy for the over-represented group (shown as the blue group experiencing a drop in accuracy).  In contrast, 'Group-robust Unlearning,' the focus of this paper, aims to mitigate this performance degradation by considering the uneven distribution of the forget set.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.09330/x2.png", "caption": "Figure 2: \nUnlearning non-uniformly distributed data.\nWe test standard model retraining, and popular approximate unlearning methods (L1-sparse\u00a0[23], SalUn\u00a0[13], and SCRUB\u00a0[28]) in group-robust unlearning.\nThe more attractive males are unlearned from CelebA\u00a0[31], the lower the model accuracy on that group.", "description": "This figure illustrates the impact of non-uniformly distributed data on machine unlearning.  It demonstrates how existing approximate unlearning methods (L1-sparse, SalUn, and SCRUB) fail to maintain accuracy for a dominant group (attractive males) within the dataset when a disproportionate number of samples from that group are requested for removal ('forget set'). Specifically, as more attractive males are removed from the CelebA dataset, the model's accuracy in classifying the remaining attractive males progressively decreases.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.09330/x3.png", "caption": "Figure 3: reweight vs. group-DRO\u00a0[42]. Retrain + reweight achieves a better test and group accuracy alignment with the original model (higher is better). Thus, it better preserves original performance after unlearning.", "description": "This figure compares the performance of two methods for group-robust machine unlearning: RETRAIN + REWEIGHT and RETRAIN + GROUP-DRO.  The x-axis represents different datasets (CelebA, Waterbirds, and FairFace). The y-axis represents the gap between the performance of the model after unlearning and the original pre-trained model's performance.  The bars show that RETRAIN + REWEIGHT achieves a smaller gap, indicating that it better preserves both overall test accuracy and the accuracy within specific groups (the dominant group in the forget set) after the unlearning process.  This suggests that the sample reweighting strategy in RETRAIN + REWEIGHT is more effective in maintaining model performance and robustness than the GROUP-DRO optimization approach.", "section": "3.3 Frustratingly easy group-robust unlearning"}, {"figure_path": "https://arxiv.org/html/2503.09330/x4.png", "caption": "Figure 4: reweight for group-robust unlearning. As in Fig.\u00a02, we test different methods and reweight in group-robust unlearning on CelebA\u00a0[31]. Darker colors are used for methods without the reweighting, while lightened ones correspond to methods coupled with reweight.\nAs the unlearning ratio grows, methods GA degrade. Instead, adding reweight restores the original GA.", "description": "Figure 4 illustrates the impact of the proposed sample reweighting strategy (REWEIGHT) on group-robust machine unlearning.  The experiment uses the CelebA dataset [31], focusing on the accuracy of the 'dominant group' (GA) within the forget set, that is the group of data most heavily affected by the unlearning process. Different unlearning methods are tested, each evaluated both with and without REWEIGHT. The graph plots the dominant group accuracy (GA) against the unlearning ratio (the proportion of data from the dominant group that is removed during the unlearning process). The results show that as the unlearning ratio increases, the dominant group accuracy (GA) decreases for methods without REWEIGHT.  However, incorporating the REWEIGHT strategy effectively mitigates the performance drop, keeping the dominant group accuracy close to its original level even with a higher unlearning ratio. This demonstrates that REWEIGHT improves the robustness of machine unlearning when dealing with non-uniformly distributed forget sets.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09330/x5.png", "caption": "Figure 5: Group-robust unlearning across different unlearning ratios.\nWe compare L1-sparse\u00a0[23], SalUn\u00a0[13], and SCRUB\u00a0[28] against our approach while using the reweight strategy on all methods. MIU achieves overall the best Avg.\u00a0Gap when varying the unlearning ratio.", "description": "This figure compares the performance of several machine unlearning methods, including L1-sparse, SalUn, SCRUB, and the authors' proposed MIU method, across different unlearning ratios. The performance is measured using the average gap (Avg. Gap) metric, which quantifies the difference between the unlearned model and the ideal retrained model. The reweight strategy is applied to all methods to mitigate the impact of non-uniformly distributed forget sets.  The results show that MIU consistently achieves the best Avg. Gap across all unlearning ratios, indicating its superior robustness and effectiveness in handling imbalanced data.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09330/x6.png", "caption": "Figure 6: Sampling the forget set from multiple groups. We evaluate our method against L1-sparse\u00a0[23], SalUn\u00a0[13], and SCRUB\u00a0[28] when the forget set is sampled from multiple FairFace\u00a0[24] groups. MIU is more consistent across experiments, always achieving the best result.", "description": "This figure displays a comparison of different machine unlearning methods' performance when the forget set is sampled from multiple groups, focusing on the consistency and overall effectiveness of each method.  The experiment uses the FairFace dataset and compares MIU to L1-sparse, SalUn, and SCRUB.  The results show that MIU consistently achieves the best results across various experimental settings.  The x-axis represents the number of groups sampled from, and the y-axis shows the average performance gap of each method relative to the best possible outcome.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.09330/x7.png", "caption": "Figure 7: Unlearning non-uniformly sampled data. We test standard model retraining, and popular approximate unlearning methods (L1-sparse\u00a0[23], SalUn\u00a0[13], SCRUB\u00a0[28]) in group-robust unlearning. The more samples from a specified group are unlearned, the lower the model accuracy on that group. While the drop is more evident in CelebA\u00a0[31], methods also show significant performance degradation in Waterbirds\u00a0[42] and FairFace\u00a0[24] overall.", "description": "This figure displays a comparison of how different machine unlearning methods handle non-uniformly distributed data.  Three popular approximate unlearning methods (L1-sparse, SalUn, and SCRUB) are tested, along with standard model retraining. The x-axis shows the proportion of data removed from a specific group (attractive males in CelebA, waterbirds on land in Waterbirds, and 20-29 year-old Afro-Americans in FairFace). The y-axis represents the model's accuracy on that same group.  The results show that as more data is removed from a single group, the accuracy for that group significantly decreases across all methods tested. While CelebA demonstrates the most pronounced drop, Waterbirds and FairFace datasets also exhibit substantial accuracy degradation, highlighting the challenge of preserving fairness and accuracy when unlearning data non-uniformly.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.09330/x8.png", "caption": "Figure 8: Ablating parameter \u03bb\ud835\udf06\\lambdaitalic_\u03bb. MIU Avg.\u00a0Gap when varying parameter \u03bb\ud835\udf06\\lambdaitalic_\u03bb in CelebA\u00a0[31], Waterbirds\u00a0[42], and FairFace\u00a0[24]. While \u03bb=1\ud835\udf061\\lambda=1italic_\u03bb = 1 is optimal in CelebA\u00a0[31] and FairFace\u00a0[24], Waterbirds\u00a0[42] benefits from higher lambdas.", "description": "This figure shows the impact of the hyperparameter lambda (\u03bb) on the performance of the MIU model across three different datasets: CelebA, Waterbirds, and FairFace.  The y-axis represents the average gap (Avg. Gap) which is a metric measuring how close the MIU model's performance comes to the ideal performance (RETRAIN + REWEIGHT). The x-axis shows the different values of \u03bb that were tested. The results indicate that for CelebA and FairFace, \u03bb=1 yields the best performance, whereas for Waterbirds, higher values of \u03bb may be beneficial. This suggests that the optimal value of \u03bb may be dataset-dependent.", "section": "4.5. Ablations"}]