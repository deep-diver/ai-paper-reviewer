[{"figure_path": "https://arxiv.org/html/2503.10696/x2.png", "caption": "Figure 1: Generated samples from NAR. Results are shown for 512\u00d7512512512512\\times 512512 \u00d7 512 text-guided image generation (1st row), 256\u00d7256256256256\\times 256256 \u00d7 256 class-conditional image generation (2nd row) and 128\u00d7128128128128\\times 128128 \u00d7 128 class-conditional video generation (3rd row).", "description": "Figure 1 presents image samples generated using the Neighboring Autoregressive Modeling (NAR) method.  The top row showcases examples of 512x512 pixel text-guided image generation, where the model creates images based on textual descriptions. The middle row displays 256x256 pixel images generated under class-conditional conditions, meaning the model produces images belonging to a specific predefined category.  The bottom row shows frames from 128x128 pixel videos generated using class-conditional video generation. This figure demonstrates the model's ability to generate various visual content across different resolutions and generation paradigms.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.10696/x3.png", "caption": "Figure 2: \nGeneration quality and efficiency comparisons between various visual generation methods. Data is collected from ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256 dataset over models with parameters around 300M.", "description": "Figure 2 presents a comparison of the generation quality and efficiency of different visual generation models.  The models all have approximately 300 million parameters.  The comparison is based on results from the ImageNet 256x256 dataset. The figure shows that NAR-M achieves a balance between high image quality and high generation speed, outperforming other models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10696/x4.png", "caption": "Figure 3: \nComparisons of different autoregressive visual generation paradigm. The proposed NAR paradigm formulates the generation process as an outpainting procedure, progressively expanding the boundary of the decoded token region. This approach effectively preserves locality, as all tokens near the starting point are consistently decoded before the current token.", "description": "This figure compares different autoregressive visual generation methods, highlighting their generation processes.  It illustrates how traditional methods (AR, MAR, PAR, VAR) generate images in raster or random order, often lacking spatial coherence and efficiency. In contrast, the proposed NAR (Neighboring Autoregressive) method progressively generates the image like an 'outpainting', starting from an initial token and expanding outwards.  This approach is shown to improve efficiency and maintain spatial consistency, as adjacent pixels are generated before more distant pixels.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10696/x5.png", "caption": "Figure 4: \nIllustration of the dimension-oriented decoding heads. The horizontal head and the vertical head are responsible for predicting the next token in the row and column dimensions, respectively. Here, L\ud835\udc3fLitalic_L is the number of Transformer blocks in the backbone network.", "description": "This figure illustrates the architecture of the dimension-oriented decoding heads in the Neighboring Autoregressive Modeling (NAR) framework.  Two heads process the input tokens in parallel. The horizontal head predicts the next token along the row dimension (horizontally), while the vertical head predicts the next token along the column dimension (vertically). Both heads utilize a backbone network consisting of L Transformer blocks, which process the input embeddings to generate the predictions. This parallel processing significantly speeds up inference by predicting multiple tokens simultaneously.", "section": "3.1 Neighboring Autoregressive Modeling"}, {"figure_path": "https://arxiv.org/html/2503.10696/x6.png", "caption": "Figure 5: Proximity-aware attention masks for the NAR paradigm. \u201cSn\ud835\udc5bnitalic_n\u201d denotes the n\ud835\udc5bnitalic_n-th generation step. Tokens generated within the same step are represented by the same color. To maintain the autoregressive property, a causal mask is applied between tokens across different generation steps (aligned with Figure\u00a03). Within each step, bidirectional attention is employed among the tokens to enhance consistency during parallel generation.", "description": "This figure illustrates the attention masks used in the Neighboring Autoregressive Modeling (NAR) framework.  It highlights how the model handles spatial relationships between image tokens during parallel generation.  The different colors represent tokens generated simultaneously in the same step, demonstrating the parallel aspect of NAR.  The causal masking ensures the autoregressive property of the model, preventing future tokens from influencing the generation of past tokens.  Bidirectional attention within each step fosters consistency by allowing tokens within the same generation step to interact with each other, regardless of their position relative to the starting point.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.10696/x7.png", "caption": "Figure 6: Efficiency comparisons between vanilla AR, VAR and the proposed NAR paradigm for visual generation. With a batch size larger than 64, NAR achieves a lower FID with lower latency, lower memory usage and significantly higher throughput.", "description": "Figure 6 presents a comparison of the efficiency of three different autoregressive image generation methods: vanilla AR, VAR, and the proposed NAR.  The comparison is shown across three metrics: latency (the time it takes to generate an image), GPU memory usage, and throughput (images generated per second). For batch sizes exceeding 64, NAR demonstrates superior performance, achieving a lower FID (Fr\u00e9chet Inception Distance, a measure of image quality), reduced latency, lower memory consumption, and substantially increased throughput.  This indicates that NAR offers a more efficient and effective approach to autoregressive image generation, especially when dealing with larger batch sizes.", "section": "4.3 Deployment Efficiency"}, {"figure_path": "https://arxiv.org/html/2503.10696/x8.png", "caption": "Figure A: \nVideo generation samples on UCF-101 dataset. Each row shows sampled frames from a 16-frame, 128\u00d7128128128128\\times 128128 \u00d7 128 resolution sequence generated by NAR-XL across various action categories.", "description": "This figure visualizes video generation results from the NAR-XL model on the UCF-101 dataset. Each row presents a sequence of 16 frames from a short video, each frame sized at 128x128 pixels.  The videos represent diverse action categories, showcasing the model's ability to generate coherent and visually consistent videos across different actions.", "section": "A. More Visualizations"}, {"figure_path": "https://arxiv.org/html/2503.10696/x9.png", "caption": "Figure B: \nClass-conditional image generation samples produced by NAR-XXL on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256.", "description": "This figure shows several example images generated by the NAR-XXL model, demonstrating its ability to generate images conditioned on a class label.  Each set of images shows different variations of the same object class generated from the same class label. The classes shown include siamese cat, coral reef, volcano, lesser panda, valley, great white shark, daisy, and geyser. This showcases the model's capacity to produce visually diverse outputs belonging to the same class.", "section": "4.2.1 Class-conditional Image Generation"}, {"figure_path": "https://arxiv.org/html/2503.10696/x10.png", "caption": "Figure C: \nClass-conditional image generation samples produced by NAR-XXL on ImageNet 256\u00d7256256256256\\times 256256 \u00d7 256.", "description": "This figure showcases several examples of images generated by the NAR-XXL model (Neighboring Autoregressive Model, extra-large size).  Each example demonstrates the model's ability to generate high-quality, class-conditional images of 256x256 pixels.  The images are grouped by class (e.g., cheeseburger, ice cream, schooner), showing the variety and detail within each class that the model produces.", "section": "4.2.1. Class-conditional Image Generation"}, {"figure_path": "https://arxiv.org/html/2503.10696/x11.png", "caption": "Figure D: \n256\u00d7256256256256\\times 256256 \u00d7 256 text-guided image generation samples produced by LlamaGen-XL-Stage1 with next-token prediction paradigm and NAR-XL-Stage1 with next-neighbor prediction paradigm.", "description": "This figure displays a comparison of image generation results between two models: LlamaGen-XL-Stage1 and NAR-XL-Stage1.  Both models generated 256x256 pixel images based on text prompts, but they employed different generation methods. LlamaGen-XL-Stage1 utilizes the standard 'next-token' prediction approach, sequentially predicting each pixel one by one. In contrast, NAR-XL-Stage1 employs a more efficient 'next-neighbor' prediction approach, which allows parallel processing and reduces the number of steps needed for generation. The figure visually showcases the differences in image quality and generation efficiency between these two methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10696/x12.png", "caption": "Figure E: \n512\u00d7512512512512\\times 512512 \u00d7 512 text-guided image generation samples produced by LlamaGen-XL-Stage2 with next-token prediction paradigm and NAR-XL-Stage2 with next-neighbor prediction paradigm. The text prompts are sampled from Parti prompts.", "description": "Figure E shows a comparison of image generation results between LlamaGen-XL-Stage2 (a model using the traditional next-token prediction approach) and NAR-XL-Stage2 (a model utilizing the novel next-neighbor prediction paradigm proposed in the paper).  Both models were tasked with generating 512x512 pixel images based on text prompts sourced from the Parti dataset. The figure visually demonstrates the differences in image quality and the efficiency of the two methods.", "section": "4. Experiments"}]