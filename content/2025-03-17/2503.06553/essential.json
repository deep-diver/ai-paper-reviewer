{"importance": "This paper introduces a new benchmark, ProJudgeBench, for evaluating MLLM's process judges in scientific problem-solving, and introduces a method for instruction-tuning with the new dataset. This will assist researchers in future advancements to improve MLLM for scientific use.", "summary": "ProJudge: MLLM judges' benchmark for sci-reasoning & instruction-tuning data to boost performance!", "takeaways": ["ProJudgeBench: A new multi-modal, multi-discipline benchmark for evaluating MLLM-based process judges.", "ProJudge-173k: A large-scale instruction-tuning dataset and a Dynamic Dual-Phase fine-tuning strategy to enhance the capabilities of open-source models.", "Experiments reveal key challenges and limitations in current models, providing valuable insights into multi-modal reasoning and process evaluation."], "tldr": "Multi-modal Large Language Models (MLLMs) often make errors when solving science problems. Evaluating their reasoning is key. However, **human evaluation is hard, so MLLMs as judges are popular but unreliable**. Current benchmarks are limited in scope/analysis, using synthetic data. This highlights the need for a better evaluation method.\n\nThis paper introduces **ProJudgeBench**, a comprehensive benchmark for MLLM process judges. It includes 2,400 test cases with 50,118 step-level labels across four sciences, annotated by experts for correctness and error type. The paper also introduces **ProJudge-173k**, a dataset and Dynamic Dual-Phase fine-tuning to enhance open-source models' evaluation abilities. Evaluation reveals significant performance gaps.", "affiliation": "WHU", "categories": {"main_category": "Multimodal Learning", "sub_category": "Multimodal Reasoning"}, "podcast_path": "2503.06553/podcast.wav"}