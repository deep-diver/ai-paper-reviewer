{"references": [{"fullname_first_author": "D. Silver", "paper_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm", "publication_date": "2017-12-00", "reason": "This paper is foundational for the RL approach used in DeepSeek-R1, demonstrating the potential of reinforcement learning for complex reasoning tasks."}, {"fullname_first_author": "A. Kumar", "paper_title": "Training language models to self-correct via reinforcement learning", "publication_date": "2024-09-00", "reason": "This paper directly addresses the use of reinforcement learning to improve reasoning in LLMs, which is central to DeepSeek-R1's methodology."}, {"fullname_first_author": "L. Gao", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2022-10-00", "reason": "This paper discusses the limitations and potential risks of reward models in RL, offering valuable insights for the design of the DeepSeek-R1 reward system."}, {"fullname_first_author": "D. Hendrycks", "paper_title": "Measuring massive multitask language understanding", "publication_date": "2020-09-00", "reason": "The MMLU benchmark, introduced in this paper, is a key evaluation metric for DeepSeek-R1, enabling a comparison with other LLMs."}, {"fullname_first_author": "Z. Shao", "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models", "publication_date": "2024-02-00", "reason": "This paper shares a similar focus on improving mathematical reasoning capabilities in LLMs, using reinforcement learning methods."}]}