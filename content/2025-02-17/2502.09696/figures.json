[{"figure_path": "https://arxiv.org/html/2502.09696/x1.png", "caption": "Figure 1: State of the art performance on public visual benchmarks. Frontier LMMs score highly on many popular benchmarks, leaving little headroom. By comparison, our ZeroBench proves impossible for current models, leaving maximum headroom.", "description": "The figure displays the state-of-the-art performance of Large Multimodal Models (LMMs) on various public visual benchmarks.  It showcases that while LMMs achieve high scores on many established benchmarks,  the performance gains are rapidly diminishing, indicating that the benchmarks are becoming too easy and not effectively measuring true visual understanding capabilities.  In contrast, the newly proposed ZeroBench benchmark demonstrates a significant challenge, as current LMMs score 0%, indicating there is ample room for improvement and a need for more challenging visual reasoning evaluations.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09696/x2.png", "caption": "Figure 2: Rapid progress was made on visual benchmarks last year. Compiled from (OpenCompass Contributors, 2023).", "description": "The figure shows a line graph illustrating the rapid improvement in the state-of-the-art performance on several visual benchmarks throughout 2024.  Each line represents a different benchmark, showing the increase in the highest-achieved score (SOTA) over time. The graph highlights the rapid reduction in benchmark headroom (the gap between perfect performance and the SOTA score), indicating that many existing benchmarks are becoming too easy for current large multimodal models.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.09696/x3.png", "caption": "Figure 3: Example ZeroBench questions and answers\u2020\u2020. Our benchmark contains 100 of these challenging questions.", "description": "Figure 3 showcases three example questions from the ZeroBench benchmark, each accompanied by its corresponding answer and a brief explanation.  The questions require multi-step reasoning and visual interpretation to solve, highlighting the challenging nature of the benchmark. ZeroBench consists of 100 similar manually-curated questions designed to be impossible for current Large Multimodal Models (LMMs). These examples represent the diverse range of reasoning abilities and visual interpretation skills needed to answer ZeroBench questions.", "section": "3. ZeroBench"}, {"figure_path": "https://arxiv.org/html/2502.09696/x4.png", "caption": "Figure 4: Question length distribution.", "description": "This histogram shows the distribution of question lengths in the ZeroBench dataset. The x-axis represents the length of a question in terms of the number of characters, and the y-axis shows the frequency of questions with that length.  The distribution is heavily skewed towards shorter questions, with a long tail indicating that a small number of questions contain many more characters.", "section": "3. ZeroBench"}]