[{"Alex": "Hey everyone, and welcome to the podcast! Today, we're diving into something super fascinating: why AI translations can sometimes sound\u2026 well, a bit off. Think robotic, unnatural, like they're trying way too hard to be perfect. We're going to unpack a recent paper that gets to the heart of this problem, revealing some surprising truths about how we train our AI translation models. Get ready for a wild ride into the world of 'translationese' and how we can fix it! I'm Alex, and I\u2019m thrilled to be your guide.", "Jamie": "Wow, Alex, that sounds intriguing! I've definitely noticed that awkwardness in AI translations before. So, what exactly is this 'translationese' and why should we care?"}, {"Alex": "Great question, Jamie! Translationese is basically that unnatural, overly literal style that can creep into translations. It's when a translation sticks too closely to the original language's structure and phrasing, resulting in something that just doesn't sound right in the target language. And we care because it affects how effectively we communicate. If a translation is clunky and awkward, it's harder to understand and engage with.", "Jamie": "Hmm, that makes sense. So, is this paper saying that Large Language Models, or LLMs, are prone to this problem, even though they're trained on so much natural language?"}, {"Alex": "Exactly! That's one of the key findings. LLMs have achieved impressive results in translation, but the paper argues that they still exhibit translationese errors. It's kind of ironic, right? They're trained on vast amounts of natural utterances, yet they can still produce these unnatural translations.", "Jamie": "That *is* surprising! Where did the LLMs go wrong, and could we find what went wrong in these LLMs through the paper's content?"}, {"Alex": "Well, the paper digs into the 'supervised fine-tuning' stage. That's where these models learn to perform specific tasks, like translation. The researchers found that biases can be introduced during this stage, leading the LLMs to overemphasize literal semantic mapping at the expense of fluent language generation. It tries to almost over map things, it's crazy!", "Jamie": "Umm, so it's not necessarily a problem with the models themselves, but with how we're teaching them to translate? That's wild."}, {"Alex": "Precisely! The paper shows that LLMs actually possess the potential for generating natural translations, thanks to their pre-training on native utterances. It\u2019s the supervised fine-tuning process that can sometimes throw them off course, pushing them towards overly literal translations.", "Jamie": "Okay, I'm starting to get a clearer picture. So, how did the researchers actually demonstrate this? What kind of experiments did they run?"}, {"Alex": "Ah, the fun part! The researchers conducted a systematic evaluation involving both English-Chinese and German-English translations. They collected documents from various writing domains and used translation-specialized and general LLMs to generate translations. Then, they had expert translators meticulously analyze these translations and identify specific spans exhibiting translationese errors.", "Jamie": "Expert translators, seriously? That seems intense! I guess you need a real human touch to catch those subtle nuances, right?"}, {"Alex": "Absolutely. To quantify the translationese, they had multiple expert translators on it. The expert translators identify the types of translationese through the Span Ratio or TSR, averaging across the translator to have a quantitative measure of translationese prevalence.", "Jamie": "Interesting. And what did they discover when they examined the translations more closely?"}, {"Alex": "The results were pretty revealing. They found that all LLMs exhibited significant translationese errors in both language pairs. Even advanced models like GPT-4 showed a considerable proportion of translations with substantial translationese patterns!", "Jamie": "Whoa, GPT-4 too? I thought those models were supposed to be super smart and close to human-level translation quality. I bet the team was scratching their head about that."}, {"Alex": "I know! It was definitely a head-scratcher, highlighting that it may have limitations. Interestingly, the researchers also discovered that when LLMs were asked to refine their own translations, they produced more natural outputs with markedly lower translationese. It shows that LLMs know more that they're presenting us!", "Jamie": "So, the LLMs kind of knew what they were doing wrong all along? That's like a student knowing the answer but writing something completely different on the test! Do they have other ways for translationese like they did with the translation prompt?"}, {"Alex": "Exactly! It suggests that LLMs possess their own prior knowledge and the potential for generating natural translations, but they get biased during the supervised training. That excessive emphasis on literal semantic mapping takes precedence over generating fluent language.", "Jamie": "That's a really insightful way to frame it. The LLMs have the knowledge but are somehow forced to prioritize something else. Did the paper just stop at the point of highlighting the root of the problem, or does it have more to add?"}, {"Alex": "Oh, it gets even better! The paper doesn't just identify the problem, it also proposes solutions. They validated the LLMs' potential for generating natural translations by showing a positive correlation between their predicted perplexities and human evaluation: higher perplexities often correlate with increased translationese.", "Jamie": "Wait, perplexities? Can you break that down for those of us who aren't AI experts?"}, {"Alex": "Sure! Perplexity is basically a measure of how surprised a language model is by a given sentence. A higher perplexity means the model found the sentence less predictable or natural. So, in this case, the researchers found that translations with higher perplexities were more likely to exhibit translationese.", "Jamie": "Got it. So, it's like an automatic way to detect translationese? Pretty neat!"}, {"Alex": "Precisely! It gives you an automatic metric for detecting translationese. To further verify the biases that come from supervised fine-tuning, expert translators analyze translationese in training instances from SFT datasets. It showed that LLMs might bias towards generating unnatural translations during SFT because the training instances exhibit translationese patterns.", "Jamie": "Umm, if the data they train with has translationese, that is just reinforcing it, isn't it? So what do they do to address this issue?"}, {"Alex": "Exactly! They proposed two mitigation strategies. First, they leveraged LLMs' natural potential to refine golden training references, reducing translationese patterns. Second, they used pre-trained LLMs to filter unnatural translations from supervised fine-tuning data.", "Jamie": "Ah, makes sense! It's like cleaning up the training data and giving the models a clearer signal of what good translation looks like."}, {"Alex": "Spot on! Empirical evaluations with Llama-3.1-8B and Qwen-2.5-7B showed refining training instances improved translation naturalness significantly, confirmed by automatic and human evaluations.", "Jamie": "Nice, it sounds like they've got some solid evidence that their approaches work. Is this something that can be applied to other languages?"}, {"Alex": "Yes, the paper extended the hypothesis to more languages like English to German, Russian, Czech, and Icelandic, and the results consistently show how polished training data reduces translationese bias.", "Jamie": "Okay, so it's not just limited to the language pairs they initially looked at. Is there something about one or two of these languages that's important to this research?"}, {"Alex": "The results showed that polishing the training data consistently and significantly reduces translationese bias across all four languages, yielding more natural translations.", "Jamie": "That\u2019s awesome. It\u2019s great to see that the techniques they developed are broadly applicable. This is a great paper."}, {"Alex": "It is! You know, this research really highlights the importance of being mindful of data quality and training methodologies when developing AI translation systems. It's not enough to just throw massive amounts of data at these models; we need to be smarter about how we're training them.", "Jamie": "Definitely, and it's also a good reminder that AI isn't magic. These models are only as good as the data they're trained on, and human expertise is still crucial for identifying and correcting these kinds of subtle issues."}, {"Alex": "Couldn't agree more! It's a really important step towards making translations better. The researchers have actually released their data and code, so others can build on their work and explore these issues further.", "Jamie": "That\u2019s fantastic news! Hopefully, this will inspire more research and development in this area."}, {"Alex": "Absolutely! To summarise, this research unveiled how translationese, that long-standing issue in machine translation, persists even in state-of-the-art LLMs because of biases introduced during supervised training. The good news is it showed how polishing up data is a reliable approach to resolve it. Thanks for joining me today, Jamie, and thanks to all our listeners for tuning in! Join us next time to explore the world of LLMs.", "Jamie": "Thanks Alex for the wonderful tour. I am really looking forward to the next episode!"}]