{"references": [{"fullname_first_author": "OpenAI", "paper_title": "GPT-4 technical report", "publication_date": "2024-03-15", "reason": "This is a key paper that documents the technical details of GPT-4, a highly influential large language model frequently used in translation tasks."}, {"fullname_first_author": "Xu", "paper_title": "A paradigm shift in machine translation: Boosting translation performance of large language models.", "publication_date": "2024-05-07", "reason": "This paper investigates how to boost machine translation performance of LLMs, therefore being highly relevant to this work."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-01-01", "reason": "This is a seminal paper that introduces the Transformer architecture, which is the foundation for many modern NMT systems and LLMs."}, {"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper presents a methodology for training language models to follow instructions using human feedback, a central aspect in supervised fine-tuning."}, {"fullname_first_author": "Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-01", "reason": "This paper describes the Qwen models, which are used in the experiments of the main paper."}]}