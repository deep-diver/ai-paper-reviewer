<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta name=generator content="Hugo 0.136.5"><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>AI Paper Reviews by AI</title>
<meta name=title content="AI Paper Reviews by AI"><meta name=description content="Explore AI papers with thorough reviews generated by AI"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/><link rel=alternate type=application/rss+xml href=/ai-paper-reviewer/index.xml title="AI Paper Reviews by AI"><link rel=alternate type=application/json href=/ai-paper-reviewer/index.json title="AI Paper Reviews by AI"><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.67eb0befb1fb57e6528684f3a2ff6e0606a82100504705b66488d54a9f3fda9c2605c9bf3b70163208427288637a7b4d1a168384c1115e417458eae6b44b329c.css integrity="sha512-Z+sL77H7V+ZShoTzov9uBgaoIQBQRwW2ZIjVSp8/2pwmBcm/O3AWMghCcohjentNGhaDhMERXkF0WOrmtEsynA=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/"><meta property="og:site_name" content="AI Paper Reviews by AI"><meta property="og:title" content="AI Paper Reviews by AI"><meta property="og:description" content="Explore AI papers with thorough reviews generated by AI"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="AI Paper Reviews by AI"><meta name=twitter:description content="Explore AI papers with thorough reviews generated by AI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","@id":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/","name":"AI Paper Reviews by AI","description":"Explore AI papers with thorough reviews generated by AI","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/","publisher":{"@type":"Person","name":"AI Paper Reviews by AI"}}</script><meta name=author content="AI Paper Reviews by AI"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">AI Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Paper Reviews</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Paper Reviews</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article class="max-w-full prose dark:prose-invert"><div class=relative><div class="absolute inset-x-0 bottom-0 h-1/2 bg-gray-100"></div><div class="mx-auto max-w-7xl p-0"><div class="relative sm:overflow-hidden"><div class="fixed inset-x-0 top-0" style=z-index:-10><img class="w-full h-[1000px] object-cover m-0 nozoom" src=/ai-paper-reviewer/img/cover.png role=presentation><div class="absolute inset-0 h-[1000px] bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="opacity-60 absolute inset-0 h-[1000px] bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div class="relative px-1 py-1 flex flex-col items-center justify-center text-center"><img class="mb-2 rounded-full h-36 w-36" width=144 height=144 alt="AI Paper Reviews by AI" src=/ai-paper-reviewer/img/avatar_hu1619077346392698443.png><h1 class="mb-2 text-4xl font-extrabold text-neutral-800 dark:text-neutral-200">AI Paper Reviews by AI</h1><h2 class="mt-0 mb-0 text-xl text-neutral-800 dark:text-neutral-300">Discover AI research through comprehensive reviews with advanced AI models<br>(powered by Gemini 1.5 & Upstage&rsquo;s Document Parse)</h2><div class="mt-3 mb-10 text-2xl"><div class="flex flex-wrap"><a class="px-1 hover:text-primary-400 text-primary-800 dark:text-primary-200" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></a><a class="px-1 hover:text-primary-400 text-primary-800 dark:text-primary-200" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></a></div></div><section class="prose dark:prose-invert"></section></div></div></div></div></article><section><h2 class="mt-8 text-2xl font-extrabold mb-10">Recent</h2><section class="w-full grid gap-4 sm:grid-cols-2 md:grid-cols-3"><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>41 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=1 style=font-size:18px><tr><td>Dataset</td><td>Image</td><td>Instance</td><td>Type</td><td>Task</td></tr><tr><td>UW-III [339]</td><td>100</td><td>/</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>InftyCDB-1 [340]</td><td>467</td><td>21000</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>Marmo [341]t</td><td>594</td><td>9500</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>ICDAR-2017 POD [342]</td><td>3900</td><td>5400</td><td>Only displayed Formula</td><td>MED</td></tr><tr><td>TFD-ICDAR 2019 [343]</td><td>851</td><td>38000</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>ICDAR-2021 IBEM [344]</td><td>8900</td><td>166000</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>FormulaNet [345]</td><td>46,672</td><td>1000,00</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>ArxivFormula [91]</td><td>700000</td><td>813.3</td><td>Inline and displayed Formula</td><td>MED</td></tr><tr><td>Pix2tex [346]</td><td colspan=2>189117</td><td>Printed</td><td>MER</td></tr><tr><td>CROHME [347]</td><td colspan=2>12178</td><td>Handwritten</td><td>MER</td></tr><tr><td>HME100K [348]</td><td colspan=2>99109</td><td>Handwritten</td><td>MER</td></tr><tr><td>UniMERNet [96]</td><td colspan=2>1,061,791</td><td>Printed and Handwritten</td><td>MER</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_1/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_1/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>168 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=3 style=font-size:16px><tr><td>Dataset</td><td>Instance</td><td>Type</td><td>Language</td><td>Task</td><td>Feature</td></tr><tr><td>ICDAR2013 [349]</td><td>150</td><td>Government Documents</td><td>English</td><td>TD & TSR</td><td>Covers complex structures and cross-page tables</td></tr><tr><td>ICDAR2017 POD [342]</td><td>1548</td><td>Scientific papers</td><td>English</td><td>TD</td><td>Includes shape and formula detec- tion</td></tr><tr><td>ICDAR2019 [350]</td><td>2439</td><td>Multiple Types</td><td>English</td><td>TD & TSR</td><td>Includes historical and modern ta- bles</td></tr><tr><td>TABLE2LATEX-450K [124]</td><td>140000</td><td>Scientific papers</td><td>English</td><td>TSR</td><td></td></tr><tr><td>RVL-CDIP (subset) [351]</td><td>518</td><td>Receipts</td><td>English</td><td>TD</td><td>Derived from RVL-CDIP</td></tr><tr><td>IIIT-AR-13K [352]</td><td>17,000 (not only tables)</td><td>Annual Reports</td><td>Multi-langugae</td><td>TD</td><td>Does not only contain tables</td></tr><tr><td>CamCap [353]</td><td>85</td><td>Table images</td><td>English</td><td>TD & TSR</td><td>Used for evaluating table detection in camera-captured images</td></tr><tr><td>UNLV Table [354]</td><td>2889</td><td>Journals, Newspapers, Business Letters</td><td>English</td><td>TD</td><td></td></tr><tr><td>UW-3 Table [355]</td><td>1,600 (around 120 tables)</td><td>Books, Magazines</td><td>English</td><td>TD</td><td>Manually labeled bounding boxes</td></tr><tr><td>Marmot [356]</td><td>2000</td><td>Conference Papers</td><td>English and Chinese</td><td>TD</td><td>Includes diversified table types; still expanding</td></tr><tr><td>TableBank [357]</td><td>417234</td><td>Multiple Types</td><td>English</td><td>TD & TSR</td><td>Automatically created by weakly su- pervised methods</td></tr><tr><td>DeepFigures [287]</td><td>5,500,000 (tables and figures)</td><td>Scientific papers</td><td>English</td><td>TD</td><td>Supports figure extraction</td></tr><tr><td>PubTabNet [125]</td><td>568000</td><td>Scientific papers</td><td>English</td><td>TSR</td><td>Structure and content recognition of tables</td></tr><tr><td>PubTables-1M [358]</td><td>1000000</td><td>Scientific papers</td><td>English</td><td>TSR [122]</td><td>Evaluates the oversegmentation is- sue</td></tr><tr><td>SciTSR [359]</td><td>15000</td><td>Scientific papers</td><td>English</td><td>TSR</td><td></td></tr><tr><td>FinTable [359]</td><td>112887</td><td>Scientific and Financial Tables</td><td>English</td><td>TD & TSR</td><td>Automatic Annotation methods</td></tr><tr><td>SynthTabNet [360]</td><td>600000</td><td>Multiple Types</td><td>English</td><td>TD & TSR</td><td>Synthetic tables</td></tr><tr><td>Wired Table in the Wild [121]</td><td>14582 (pages)</td><td>Photos, Files, and Web Pages</td><td>English</td><td>TSR</td><td>Deformed and occluded images</td></tr><tr><td>WikiTableSet [361]</td><td>50000000</td><td>Wikipedia</td><td>English, Japanese, French</td><td>TSR</td><td></td></tr><tr><td>STDW [362]</td><td>7000</td><td>Multiple Types</td><td>English</td><td>TD</td><td></td></tr><tr><td>TableGraph-350K [363]</td><td>358,767</td><td>Academic Table</td><td>English</td><td>TSR</td><td>including TableGraph-24K</td></tr><tr><td>TabRecSet [364]</td><td>38100</td><td>Multiple Types</td><td>English and Chinese</td><td>TSR</td><td></td></tr><tr><td>DECO [365]</td><td>1165</td><td>Multiple Types</td><td>English</td><td>TD</td><td>Enron document electronic table files</td></tr><tr><td>iFLYTAB [366]</td><td>17291</td><td>Multiple Types</td><td>Chinese and English</td><td>TD & TSR</td><td>Online and offline tables from vari- ous scenarios</td></tr><tr><td>FinTab [367]</td><td>1,600</td><td>Financial Table</td><td>Chinese</td><td>TSR</td><td></td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_25_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_25_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>144 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=1 style=font-size:14px><tr><td>Dataset</td><td>Year</td><td>Instance</td><td>Class</td><td>Task</td><td>Feature</td></tr><tr><td>DeepChart [368]</td><td>2015</td><td>5000</td><td>5</td><td>Chart Classification</td><td></td></tr><tr><td>VIEW [369]</td><td>2012</td><td>300</td><td>3</td><td>Chart Classification</td><td>-</td></tr><tr><td>ReVision [288]</td><td>2011</td><td>2601</td><td>10</td><td>Chart Classification</td><td>Based on ChartSense dataset</td></tr><tr><td>CHART 2019 [370] - PMC</td><td>2019</td><td>4242</td><td>multi-class</td><td>Chart Classification</td><td>Real charts from scientific publications</td></tr><tr><td>CHART 2019 - Syn- thetic [371]</td><td>2019</td><td>202,550</td><td>multi-class</td><td>Chart Classification</td><td>Synthetic charts</td></tr><tr><td>DocFigure [370]</td><td>2019</td><td>33000</td><td>28</td><td>Chart Classification</td><td>Includes various figure images</td></tr><tr><td>UB-PMC 2019 [370]</td><td>2019</td><td>4242</td><td>7</td><td>Chart Classification</td><td>Competition dataset</td></tr><tr><td>UB-PMC 2020 [372]</td><td>2020</td><td>2123</td><td>4</td><td>Chart Data Extraction</td><td>Real charts from PubMedCentra</td></tr><tr><td>UM-PMC 2021 [373]</td><td>2021</td><td>22924</td><td>15</td><td>Chart Classification</td><td>Competition dataset</td></tr><tr><td>UB-PMC 2022 [136]</td><td>2022</td><td>33186</td><td>15</td><td>Chart Classification</td><td>Competition dataset</td></tr><tr><td>Synth 2020 [373]</td><td>2020</td><td>9600</td><td>4</td><td>Chart Data Extraction</td><td>Synthetic charts</td></tr><tr><td>LINEEX430k [300]</td><td>2023</td><td>430,000</td><td>Line charts</td><td>Chart Data Extraction</td><td>Focused on line charts</td></tr><tr><td>ICPR 2022 [136]</td><td>2022</td><td>26,596</td><td>15</td><td>Chart Classification</td><td>Charts with embedded text</td></tr><tr><td>ExcelChart400K</td><td>[3742021</td><td>400,0000</td><td>Pie and bar charts</td><td>Chart Data Extraction</td><td>Extracted from Excel charts with JSON annotations</td></tr><tr><td>CHARTER [375]</td><td>2021</td><td>32334</td><td>4</td><td>Chart Data Extraction</td><td>Sourced from document pages, web pages, PubMed, FigureQA, etc.</td></tr><tr><td>StructChart dataset [376]</td><td>2023</td><td>16466</td><td>Organization and structure charts</td><td>Chart Structure Extraction</td><td>-</td></tr><tr><td>OneChart [377]</td><td>2023</td><td>10000000</td><td>5</td><td>Chart Information Extraction, QA, and Inference</td><td>Synthesized using Matplotlib</td></tr><tr><td>Chart-to-Text [378]</td><td>2023</td><td>8305</td><td>6</td><td>Chart Information Extraction</td><td>Contains chart samples and corresponding data</td></tr><tr><td>ChartLlama [379]</td><td>2023</td><td>1500</td><td>10</td><td>7 comprehensive chart tasks in- cluding chart information extrac- tion</td><td>GPT-4 generates charts and instruction data</td></tr><tr><td>ChartX [299]</td><td>2024</td><td>48000</td><td>18</td><td>7 comprehensive chart tasks in- cluding chart information extrac- tion</td><td>Automatically generated by GPT-4 and manually checked</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_26_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_26_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>85 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=6 style=font-size:14px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>IoU</td><td>Area of Overlap IoU = Area of Union TP</td><td>Measures the overlap between predicted and ground truth boxes.</td></tr><tr><td>ReCall</td><td>ReCall = TP + FN N</td><td>Measures how many true positive samples are correctly predicted by the model.</td></tr><tr><td>mAP</td><td>1 mAP = APi N i=1 M 1</td><td>Average precision across all classes, assessing overall model performance.</td></tr><tr><td>mAP@IoU[a:b]</td><td>mAP@IoU[a:b] = mAPI⌀U j M j=1</td><td>Computes over a range of IoU thresholds [a, b], calculating at specified intervals and averaged.</td></tr><tr><td>F1-score</td><td>Precision X Recall F1-score = 2 x Precision + Recall</td><td>Balances precision and recall and useful in imbalanced class scenarios.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>114 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=1 style=font-size:14px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>CER</td><td>S+ D + I CER N</td><td>Measures the character-level discrepancy between recognized and ground truth text, suitable for OCR tasks requiring high precision.</td></tr><tr><td>Edit Distance</td><td>D(i-1,j)+1 D(i,j) = min [ D(i,j -1)+1 D(i - 1, j -1) + Cost(s1 [i], s2[j]) N</td><td>Measures the minimum edit distance needed to convert recognized text into ground truth text.</td></tr><tr><td>BLEU</td><td>BLEU = BP x exp M Wn log Pn ) n=1</td><td>Measures the minimum edit distance needed to convert recognized text into ground truth text.</td></tr><tr><td>METEOR</td><td>METEOR = Fmean x (1 - Penalty)</td><td>Accounts for both precision and recall, and supports stem and synonym matching.</td></tr><tr><td>ROUGE-N</td><td>� ngramE Reference min(Countmatch (ngram), Countcandidate (ngram)) ROUGE-N = ngramEReference Countreference (ngram)</td><td>An improved version of BLEU that focuses on recall rather than precision.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_1/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_1/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>97 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=4 style=font-size:16px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>ExpRate</td><td>Number of exact matches ExpRate Total number of samples</td><td>Measures the proportion of samples that are com- pletely correct, suitable for scenarios requiring high accuracy.</td></tr><tr><td>MSE</td><td>m n 1 MSE ��(I(i,j) - K(i,j))2 mn i=1 j=1</td><td>Measures the average squared difference between corresponding pixels in two images.</td></tr><tr><td>SSIM</td><td>(2�x�� + C1)(2�xy + C2) SSIM(x, y) = (사로 + 사립 + C1)(⌀2 + ⌀2 + C2)</td><td>Measures the structural similarity of images, taking into account brightness, contrast, and structural in- formation.</td></tr><tr><td>CDM</td><td>2x TP CDM = 2 x TP + FP + FN</td><td>Converts LaTeX mathematical expression into im- age and matches it with the corresponding image structure.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_28_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_28_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>157 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=2 style=font-size:14px><tr><td>Metric</td><td>Definition</td><td>Description</td></tr><tr><td>Purity</td><td>k 1 Purity = max ICinL⌀l N j i=1 k 1</td><td>Measures the level of noise contained in the detected results.</td></tr><tr><td>Completeness</td><td>Completeness max |Lj n Cil N 2 j=1</td><td>Measure the proportion of table areas detected within the tables.</td></tr><tr><td>CAR</td><td>�i=1 1(predicted adjacency(Ci) = true adjacency(Ci)) CAR n</td><td>Evaluates boundary detection and relative positioning of table cells, reflecting the structural relationships of the table.</td></tr><tr><td>TEDS</td><td>TED(T1 , T2) TEDS(T1,T2) = 1 - max(size(T1), size(T2)) AcolEd (i)H</td><td>Measures similarity based on tree edit distance, focusing on table structure, including tags and content.</td></tr><tr><td>Aall</td><td>K⌀il ArowSt (i) n ArowEd (i) n AcolSt(i) n Aall = N</td><td>A cell's prediction is considered correct if and only if all four of its logical positions are accurately predicted.</td></tr><tr><td>F_beta</td><td>(1 +0.52) . H . Aall F�=0.5 = 0.52 . H + Aall</td><td>Combines spatial positioning and logical accuracy, balancing evaluation better than F1-score. layout and spatial location</td></tr><tr><td>WAF</td><td>� =1 IoUi · F�= 1 @IoUi W AF = �1=1 IoU⌀</td><td>Evaluates adjacency relation prediction based on intersection over union (IoU).</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_29_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_29_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>211 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">1 min</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><br><table id=2 style=font-size:14px><tr><td>Tools</td><td>Developer</td><td>Time</td><td>Introduction</td></tr><tr><td>GROBID</td><td>Patrice Lopez</td><td>2011</td><td>A machine learning library that focuses on extracting and restructuring original documents, converting them into structured formats such as XML/TEI encoding.</td></tr><tr><td>PyMuPDF</td><td>Jorj X. McKie</td><td>2011</td><td>A Python library for extracting, analyzing, converting, and processing data from PDFs and other documents, supporting tables, figures, and other types of content.</td></tr><tr><td>doc2text</td><td>Joe Sutherland</td><td>2016.9</td><td>Specializes in extracting low-quality documents; only ensures compatibility in Linux.</td></tr><tr><td>pdfplumber</td><td>Jeremy Singer- Vine</td><td>2019.1</td><td>Tools for extraction and parsing of characters, images, lines, tables, and other elements from digital PDF documents.</td></tr><tr><td>Parsr</td><td>axa-group</td><td>2019.8</td><td>A tool for cleaning, parsing, and extracting content from various document types, with outputs including JSON, Markdown, CSV/pandasDF, and txt formats.</td></tr><tr><td>PP-StructureV2</td><td>Baidu</td><td>2021.8</td><td>Intelligent document analysis system, supports layout analysis of Chinese and English documents, table recognition, and semantic recognition.</td></tr><tr><td>DocxChain</td><td>Alibaba</td><td>2023.9</td><td>A system for non-structured or semi-structured document conversion into various information and formats, including complex document applications based on computational capabilities.</td></tr><tr><td>pdf2htmlEX</td><td>Lu Wang</td><td>2023.12</td><td>A project to convert PDF documents into HTML format.</td></tr><tr><td>MinerU</td><td>OpenDataLab</td><td>2024.4</td><td>A system for extracting content from PDF and converting it into markdown or JSON formats.</td></tr><tr><td>PDF-Extract-Kit</td><td>OpenDataLab</td><td>2024.7</td><td>A system based on MinerU to extract various content from PDF, including layout analysis, OCR, table recognition, and formula recognition tasks.</td></tr><tr><td>OmniParser</td><td>Adithya s Kolavi</td><td>2024.6</td><td>A platform for extracting and parsing any unstructured data, transforming it into structured, actionable data optimized for GenAI applications.</td></tr><tr><td>LLM_aided_ocr</td><td>Jeff Emanuel</td><td>2024.8</td><td>Uses Tesseract for document OCR, followed by LLM-based error correction, with final output in markdown or similar formats.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a><a href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_38_0/ class=min-w-full><div class="min-h-full border border-neutral-200 dark:border-neutral-700 border-2 rounded overflow-hidden shadow-2xl relative"><div class="px-6 py-4"><div class="font-bold text-xl text-neutral-800 decoration-primary-500 hover:underline hover:underline-offset-2 dark:text-neutral" href=/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_38_0/></div><div class="text-sm text-neutral-500 dark:text-neutral-400"><div class="flex flex-row flex-wrap items-center"><time datetime=0001-01-01T00:00:00+00:00>1 January 0001</time><span class="px-2 text-primary-500">&#183;</span><span>481 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">3 mins</span></div><div class="flex flex-row flex-wrap items-center"></div></div><div class="py-1 prose dark:prose-invert"><table id=0 style=font-size:18px><tr><td>[94]</td><td>Jianshu Zhang, Jun Du, and Lirong Dai. Multi-scale attention with dense encoder for hand- written mathematical expression recognition. In 2018 24th international conference on pattern recognition (ICPR), pages 2245-2250. IEEE, 2018.</td></tr><tr><td>[95]</td><td>Zhe Li, Lianwen Jin, Songxuan Lai, and Yecheng Zhu. Improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. In 2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 175-180. IEEE, 2020.</td></tr><tr><td>[96]</td><td>Bin Wang, Zhuangcheng Gu, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. Unimernet: A universal network for real-world mathematical expression recognition. arXiv preprint arXiv:2404.15254, 2024.</td></tr><tr><td>[97]</td><td>Wei Zhang, Zhiqiang Bai, and Yuesheng Zhu. An improved approach based on cnn-rnns for mathematical expression recognition. In Proceedings of the 2019 4th international conference on multimedia systems and signal processing, pages 57-61, 2019.</td></tr><tr><td>[98]</td><td>Wenqi Zhao, Liangcai Gao, Zuoyu Yan, Shuai Peng, Lin Du, and Ziyin Zhang. Handwritten mathematical expression recognition with bidirectionally trained transformer. In Document analysis and recognition-ICDAR 2021: 16th international conference, Lausanne, Switzerland, September 5-10, 2021, proceedings, part II 16, pages 570-584. Springer, 2021.</td></tr><tr><td>[99]</td><td>Wenqi Zhao and Liangcai Gao. Comer: Modeling coverage for transformer-based handwritten mathematical expression recognition. In European conference on computer vision, pages 392-408. Springer, 2022.</td></tr><tr><td>[100]</td><td>Bohan Li, Ye Yuan, Dingkang Liang, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, and Xiang Bai. When counting meets hmer: counting-aware network for handwritten mathematical expression recognition. In European conference on computer vision, pages 197-214. Springer, 2022.</td></tr><tr><td>[101]</td><td>Jianhua Zhu, Liangcai Gao, and Wenqi Zhao. Ical: Implicit character-aided learning for enhanced handwritten mathematical expression recognition. In International Conference on Document Analysis and Recognition, pages 21-37. Springer, 2024.</td></tr><tr><td>[102]</td><td>Chungkwong Chan. Stroke extraction for offline handwritten mathematical expression recog- nition. IEEE Access, 8:61565-61575, 2020.</td></tr><tr><td>[103]</td><td>Jiaming Wang, Jun Du, Jianshu Zhang, and Zi-Rui Wang. Multi-modal attention network for handwritten mathematical expression recognition. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1181-1186. IEEE, 2019.</td></tr><tr><td>[104]</td><td>Leipeng Hao, Liangcai Gao, Xiaohan Yi, and Zhi Tang. A table detection method for pdf documents based on convolutional neural networks. In 2016 12th IAPR Workshop on Document Analysis Systems (DAS), pages 287-292. IEEE, 2016.</td></tr><tr><td>[105]</td><td>Azka Gilani, Shah Rukh Qasim, Imran Malik, and Faisal Shafait. Table detection using deep learning. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 771-776. IEEE, 2017.</td></tr><tr><td>[106]</td><td>Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 1162-1167. IEEE, 2017.</td></tr><tr><td>[107]</td><td>Shoaib Ahmed Siddiqui, Muhammad Imran Malik, Stefan Agne, Andreas Dengel, and Sheraz Ahmed. Decnt: Deep deformable cnn for table detection. IEEE access, 6:74151-74161, 2018.</td></tr><tr><td>[108]</td><td>Yilun Huang, Qinqin Yan, Yibo Li, Yifan Chen, Xiong Wang, Liangcai Gao, and Zhi Tang. A yolo-based table detection method. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 813-818. IEEE, 2019.</td></tr><tr><td>[109]</td><td>Bin Xiao, Murat Simsek, Burak Kantarci, and Ala Abu Alkheir. Table detection for visually rich document images. Knowledge-Based Systems, 282:111080, 2023.</td></tr></table></div></div><div class="px-6 pt-4 pb-2"></div></div></a></section><div class="mt-10 flex justify-center"><a href=/ai-paper-reviewer/paper-reviews/><button class="bg-transparent hover:text-primary-500 prose dark:prose-invert font-semibold hover:text-white py-2 px-4 border border-primary-500 hover:border-transparent rounded">
Show More</button></a></div></section><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2024
AI Paper Reviews by AI</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>