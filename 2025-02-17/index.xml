<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-02-17s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/</link><description>Recent content in 2025-02-17s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Fri, 14 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/index.xml" rel="self" type="application/rss+xml"/><item><title>AdaPTS: Adapting Univariate Foundation Models to Probabilistic Multivariate Time Series Forecasting</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.10235/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.10235/</guid><description>AdaPTS effectively adapts pre-trained univariate time series models to probabilistic multivariate forecasting, improving accuracy and uncertainty quantification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.10235/cover.png"/></item><item><title>Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.10248/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.10248/</guid><description>Step-Video-T2V: A 30B parameter text-to-video model generating high-quality videos up to 204 frames, pushing the boundaries of video foundation models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.10248/cover.png"/></item><item><title>V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.09980/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.09980/</guid><description>V2V-LLM leverages multi-modal LLMs for safer cooperative autonomous driving by fusing perception data from multiple vehicles, answering driving-related questions, and improving trajectory planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.09980/cover.png"/></item><item><title>ZeroBench: An Impossible Visual Benchmark for Contemporary Large Multimodal Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.09696/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.09696/</guid><description>ZeroBench: a new visual reasoning benchmark, proves impossible for current large multimodal models, pushing the boundaries of AI visual understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.09696/cover.png"/></item><item><title>Cluster and Predict Latents Patches for Improved Masked Image Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.08769/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.08769/</guid><description>CAPI: a novel masked image modeling framework boosts self-supervised visual representation learning by predicting latent clusterings, achieving state-of-the-art ImageNet accuracy and mIoU.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.08769/cover.png"/></item><item><title>MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.07856/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.07856/</guid><description>MRS: a novel, training-free sampler, drastically speeds up controllable image generation using Mean Reverting Diffusion, achieving 10-20x speedup across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.07856/cover.png"/></item><item><title>We Can't Understand AI Using our Existing Vocabulary</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.07586/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.07586/</guid><description>To understand AI, we need new words! This paper argues that developing neologisms—new words for human &amp;amp; machine concepts—is key to bridging the communication gap and achieving better AI control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-17/2502.07586/cover.png"/></item></channel></rss>