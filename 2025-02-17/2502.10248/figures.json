[{"figure_path": "https://arxiv.org/html/2502.10248/extracted/6204622/figure/model_architecture.png", "caption": "Figure 1: Architecture overview of Step-Video-T2V. Videos are represented by a high-compression Video-VAE, achieving 16x16 spatial and 8x temporal compression ratios. User prompts are encoded using two bilingual pre-trained text encoders to handle both English and Chinese. A DiT with 3D full attention is trained using Flow Matching and is employed to denoise input noise into latent frames, with text embeddings and timesteps serving as conditioning factors. To further enhance the visual quality of the generated videos, a video-based DPO approach is applied, which effectively reduces artifacts and ensures smoother, more realistic video outputs.", "description": "Step-Video-T2V's architecture uses a high-compression Video-VAE to reduce computational costs while preserving video quality.  This VAE achieves 16x16 spatial and 8x temporal compression. User prompts in English or Chinese are processed by two bilingual text encoders. A Diffusion Transformer (DiT) with 3D full attention, trained using Flow Matching, denoises the encoded prompts to generate latent video frames.  Video-based Direct Preference Optimization (Video-DPO) refines the results, reducing artifacts and improving realism and smoothness.", "section": "4 Model"}, {"figure_path": "https://arxiv.org/html/2502.10248/x1.png", "caption": "Figure 2: Architecture overview of Video-VAE.", "description": "This figure provides a detailed illustration of the Video-VAE architecture, a crucial component of the Step-Video-T2V model. It showcases the encoder and decoder pathways, highlighting the dual-path architecture employed for efficient compression and reconstruction of video data.  Specific components such as convolutional modules, Res3DModules, and downsampling/upsampling operations are clearly depicted, allowing for a comprehensive understanding of the Video-VAE's mechanisms for compressing videos into latent space representations and reconstructing them back into their original format.", "section": "4.1 Video-VAE"}, {"figure_path": "https://arxiv.org/html/2502.10248/extracted/6204622/figure/dit-arch.png", "caption": "Figure 3: The model architecture of our bilingual text encoder and DiT with 3D Attention.", "description": "This figure details the architecture of the Step-Video-T2V model's text encoder and the Diffusion Transformer (DiT) with 3D attention.  The text encoder is bilingual, processing both English and Chinese prompts using Hunyuan-CLIP and Step-LLM.  The DiT incorporates a cross-attention layer to integrate text embeddings with visual features, utilizes 3D full attention for efficient spatial and temporal modeling, and employs adaptive layer normalization (AdaLN) with optimized computation and ROPE-3D positional encoding.", "section": "4 Model"}, {"figure_path": "https://arxiv.org/html/2502.10248/x2.png", "caption": "Figure 4: Overall pipeline of incorporating human feedback.", "description": "This figure illustrates the process of integrating human feedback into the Step-Video-T2V model to improve the visual quality of generated videos.  It shows the pipeline, beginning with a pool of prompts that are used to generate videos. These videos are then rated by human annotators using a reward model, providing feedback on which videos are preferred and which are not. This feedback is then used to fine-tune the Step-Video-T2V model, resulting in higher quality video outputs. The reward model is trained using labeled data consisting of both positive and negative example videos for various prompts.  The final output is a refined Step-Video-T2V model that generates improved videos based on human preferences.", "section": "4.4 Video-DPO"}, {"figure_path": "https://arxiv.org/html/2502.10248/x3.png", "caption": "Figure 5: We generate different samples with same prompt (\"A ballet dancer practicing in the dance studio\" in this case), and annotate these samples as non-preferred (a) or preferred (b).", "description": "This figure shows two examples of video generation results from the Step-Video-T2V model using the same prompt: \"A ballet dancer practicing in the dance studio\".  Image (a) represents a non-preferred generation, highlighting common issues like artifacts or inconsistencies in the dancer's movements and pose. Image (b) shows a preferred generation, demonstrating improved realism, fluidity of motion, and overall visual quality.", "section": "4.4 Video-DPO"}, {"figure_path": "https://arxiv.org/html/2502.10248/x10.png", "caption": "Figure 6: Generated samples with Step-Video-T2V Turbo with 10 NFE.", "description": "This figure showcases sample video frames generated using the Step-Video-T2V Turbo model.  The key point highlighted is that these high-quality videos were generated with only 10 noise-to-image diffusion steps (NFE). This demonstrates the effectiveness of the model's distillation process for improving computational efficiency during inference while maintaining visual quality.", "section": "5 Distillation for Step-Video-T2V Turbo"}, {"figure_path": "https://arxiv.org/html/2502.10248/x11.png", "caption": "Figure 7: The workflow of Step-Video-T2V training system.", "description": "This figure illustrates the Step-Video-T2V training system's workflow, which involves offline and online stages. The offline stage uses a training emulator to determine the optimal resource allocation and training parallelism strategy.  This plan is then used in the online stage where the training job is deployed across two clusters: training clusters (for the video DiT) and inference clusters (for VAE and Text Encoder). StepRPC, a high-performance RPC framework, ensures efficient communication between these clusters.  StepTelemetry provides multi-dimensional system monitoring and analysis capabilities, which are crucial for identifying potential bottlenecks and failures. The workflow is designed to improve training efficiency and robustness.", "section": "6 System"}, {"figure_path": "https://arxiv.org/html/2502.10248/x12.png", "caption": "Figure 8: Load balancing with hybrid granularity.", "description": "This figure illustrates the Step-Video-T2V system's hybrid approach to load balancing during training.  The system addresses computational imbalances caused by training on videos and images with varying resolutions. A two-stage process is used: 1) Coarse-grained balancing: Batch sizes are adjusted for different resolutions to achieve rough FLOP (floating point operations) parity across batches. 2) Fine-grained balancing: Image padding is dynamically added to batches to compensate for any residual FLOP differences, ensuring optimal GPU utilization.", "section": "6.2.4 DP Load Balance"}, {"figure_path": "https://arxiv.org/html/2502.10248/extracted/6204622/figure/v2_training_loss.png", "caption": "Figure 9: The pipeline of Step-Video-T2V data process.", "description": "This figure details the comprehensive data processing pipeline used to prepare the Step-Video-T2V dataset.  The pipeline consists of several crucial stages:  First, raw video data is segmented into individual clips using scene detection and video splitting techniques. Then, the quality of each clip is assessed using a series of metrics, including aesthetic score, NSFW detection, watermark detection, saturation, blur, black borders, and motion analysis.  Video content is further analyzed for the presence of subtitles, and video-text alignment is checked using CLIP similarity scores. Finally, video clips are organized into clusters based on concepts and filtered to ensure data quality and balance. This rigorous procedure ensures a high-quality dataset for training Step-Video-T2V. ", "section": "7 Data"}, {"figure_path": "https://arxiv.org/html/2502.10248/x13.png", "caption": "Figure 10: Training curve of different training stages, where sisubscript\ud835\udc60\ud835\udc56s_{i}italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT denotes the it\u2062hsuperscript\ud835\udc56\ud835\udc61\u210ei^{th}italic_i start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT dataset used in the corresponding stage.", "description": "The figure shows the training loss curves for different stages of the Step-Video-T2V model training process.  Each curve represents a specific stage and uses a different dataset (denoted as s<sub>i</sub>). The x-axis shows the number of training iterations, while the y-axis represents the training loss.  The plot helps visualize how the model's performance improves across various stages and datasets, indicating the effectiveness of the training strategy.", "section": "8 Training Strategy"}, {"figure_path": "https://arxiv.org/html/2502.10248/x14.png", "caption": "Figure 11: Hierarchical data filtering for pre-training and post-training.", "description": "This figure illustrates the multi-stage data filtering process employed for both the pre-training and post-training phases of the Step-Video-T2V model.  Each stage uses various filters to remove low-quality or unsuitable data. Filters include evaluating aesthetic appeal, blurriness, motion quality, saturation, the presence of watermarks and subtitles, video resolution, and ensuring a balance of concepts.  The final post-training dataset is also refined via manual review. The diagram visually demonstrates how the dataset size decreases at each filtering stage, highlighting the impact of each filter on data quality.", "section": "7 Data"}, {"figure_path": "https://arxiv.org/html/2502.10248/x20.png", "caption": "Figure 12: Four frames sampled from the video generated based on the prompt \"In the video, a Chinese girl is dressed in an exquisite traditional outfit, smiling with a confident and graceful expression. She holds a piece of paper with the words \"we will open source\" clearly written on it. The background features an ancient and elegant setting, complementing the girl\u2019s demeanor. The entire scene is clear and has a realistic style.\".", "description": "The figure displays four frames from a video generated by the Step-Video-T2V model.  The video depicts a Chinese girl in a traditional outfit, smiling confidently. She holds a sign reading \"we will open source.\" The background is an ancient, elegant setting, enhancing the scene's overall realistic style and coherence with the prompt. The image showcases the model's ability to generate videos with clear visuals, detailed elements, and accurate text rendering.", "section": "9.5 Generating Text Content in Videos"}]