[{"figure_path": "https://arxiv.org/html/2502.03628/x3.png", "caption": "Figure 1: Analysis of token logits ranking patterns across 500 randomly selected images from MSCOCO dataset. Higher ranking indicates higher generation probability. Left: Average token ranking from the last five layers, showing temporal progression across early, mid, and late generation stages. Right: Layer-wise evolution of token rankings averaged across all time steps, demonstrating early-excitation phenomenon.", "description": "This figure analyzes token ranking patterns during the image caption generation process of Large Vision-Language Models (LVLMs).  It uses data from 500 randomly selected images from the MS COCO dataset. The left panel shows the average token ranking across the last five layers of the LVLMs, illustrating how token rankings change over time (early, mid, and late generation stages).  The right panel displays the average token ranking across all time steps for each layer, highlighting the 'early excitation' phenomenon where semantically meaningful tokens reach peak activation earlier in the network layers than the final layer.", "section": "Inspecting Token Dynamics in LVLMs"}, {"figure_path": "https://arxiv.org/html/2502.03628/x4.png", "caption": "Figure 2: Token ranking heatmaps for a representative image, demonstrating the evolution of token rankings across model layers (vertical axis) and generation steps (horizontal axis). Darker colors indicate higher ranking. The visualization reveals both gradual visual information loss and early excitation phenomena.", "description": "This figure visualizes the changes in token rankings over time and across different layers of a Large Vision-Language Model (LVLM) during the text generation process.  The heatmaps show the ranking of tokens (words) based on their probability of being generated at each step.  Specifically, it tracks three types of tokens: \n1) **Hidden Genuine Tokens:** Words that are visually present in the image but missing from the generated caption.\n2) **Decoded Genuine Tokens:** Words correctly included in the caption that are visually grounded in the image.\n3) **Hallucinated Tokens:** Words present in the generated caption that lack visual grounding in the image.\nThe heatmaps reveal two key phenomena: \n1) **Gradual Visual Information Loss:**  As generation proceeds, genuine tokens (both decoded and hidden) decrease in ranking, while hallucinated tokens rise in ranking.\n2) **Early Excitation:** Semantically meaningful tokens reach their peak activation earlier in the network's layers than the final layer, suggesting a shift towards syntactic elements in the final layer's decisions.  Darker colors represent higher probability (i.e., higher ranking) for the tokens.", "section": "Inspecting Token Dynamics in LVLMs"}, {"figure_path": "https://arxiv.org/html/2502.03628/x5.png", "caption": "Figure 3: Architectural overview of VISTA. VISTA introduces two complementary mechanisms: VSV extracts and reinforces visual grounding information (Vssubscript\ud835\udc49\ud835\udc60V_{s}italic_V start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT) at inference, and SLA leverages early-layer semantic information to guide token generation. Note: While three separate forward passes are shown for illustration purpose, they can be avoided in implementation.", "description": "VISTA, a training-free method for reducing hallucination in large vision-language models (LVLMs), is presented in this figure. It consists of two modules: Visual Steering Vector (VSV) and Self-Logits Augmentation (SLA). VSV leverages visual information from the image to counteract the loss of visual information during text generation. It does so by injecting a visual steering vector into the residual stream at each layer. Meanwhile, SLA uses the early excitation pattern of semantically meaningful tokens to guide decoding by incorporating early-layer logits into the final layer's logits.  The figure shows the flow of information through both modules. Note that while three separate forward passes are illustrated, they are not necessary in the actual implementation.", "section": "2. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.03628/x6.png", "caption": "Figure 4: Performance comparison on MMHal-Bench across different question categories: attributes (ATTR), adversarial objects (ADV), comparisons (COMP), counting (COUNT), spatial relations (SPAT), environmental inference (ENV), holistic descriptions (HOL), and others (OTHER). Scores are computed using GPT-4 evaluation protocol.", "description": "Figure 4 presents a detailed performance comparison of different Large Vision Language Models (LVLMs) on the MMHal-Bench benchmark.  MMHal-Bench is a comprehensive evaluation suite focusing on eight distinct question categories designed to assess various aspects of visual-language reasoning and holistic understanding.  The categories are: Attributes (ATTR), Adversarial Objects (ADV), Comparisons (COMP), Counting (COUNT), Spatial Relations (SPAT), Environmental Inference (ENV), Holistic Descriptions (HOL), and Others (OTHER).  The figure visually represents the performance of each model within each of these categories, showing their relative strengths and weaknesses in different aspects of visual-language understanding.  Performance scores are computed using GPT-4, a large language model serving as an evaluation oracle.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x7.png", "caption": "Figure 5: Cross-stage token ranking comparison between greedy and VISTA on LLAVA-1.5. VISTA effectively promotes the ranking of genuine tokens while depressing hallucination tokens.", "description": "This figure displays a comparison of token rankings across different stages of text generation for both a standard greedy decoding method and the proposed VISTA method, using the LLAVA-1.5 model.  The x-axis represents the generation stages (Early, Mid, Late), and the y-axis shows the average token rank.  Different colored lines represent different token types: Hidden Genuine Tokens (tokens present in the image but not generated), Decoded Genuine Tokens (tokens present in the image and generated), and Hallucinated Tokens (tokens not present in the image but generated). The graph visually demonstrates that VISTA effectively improves the ranking of genuine tokens (both hidden and decoded) while simultaneously reducing the ranking of hallucinated tokens.", "section": "2.2. Token Ranking Analysis"}, {"figure_path": "https://arxiv.org/html/2502.03628/x8.png", "caption": "Figure 6: Ablation matrices for VSV strength (\u03bb\ud835\udf06\\lambdaitalic_\u03bb) and SLA mixing ratio (\u03b3\ud835\udefe\\gammaitalic_\u03b3) on Shikra. Brighter color signifies the better performance. Red boxes highlight the parameter combinations we used. F1 score is included to demonstrate the overall generation quality.", "description": "This figure visualizes the ablation study results on the Shikra model, analyzing the effects of varying VSV (Visual Steering Vector) strength (lambda) and SLA (Self-Logits Augmentation) mixing ratio (gamma) on the model's performance. Each cell in the matrix represents a combination of lambda and gamma values. The color intensity indicates performance, with brighter colors signifying better results.  Three matrices are shown: one for CHAIRs, CHAIR1, and F1 score. The red boxes highlight the parameter combinations used in the main experiments of the paper. The F1 score matrix is included to assess the overall generation quality, supplementing the hallucination metrics (CHAIRs and CHAIR1).", "section": "3.5. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2502.03628/x9.png", "caption": "Figure 7: Cross-stage token ranking on Shikra.", "description": "This figure shows the average token ranking across different generation stages (early, mid, late) for three token categories in the Shikra model: hidden genuine tokens, decoded genuine tokens, and hallucinated tokens.  The graph visually represents how the ranking of each token category changes over time during the text generation process.  It demonstrates trends related to concepts like gradual visual information loss and the early excitation of semantically meaningful tokens.", "section": "Token Ranking Analysis"}, {"figure_path": "https://arxiv.org/html/2502.03628/x10.png", "caption": "Figure 8: Layer-wise token rankings on Shikra.", "description": "This figure shows the average token ranking across different layers of the Shikra model.  It displays how the ranking of hidden genuine tokens (tokens present in the image but not in the generated caption), decoded genuine tokens (tokens present in both the image and the caption), and hallucinated tokens (tokens present in the caption but not in the image) changes as the model processes information through its layers.  The x-axis represents the layer number, and the y-axis represents the average token rank.", "section": "Token Ranking Analysis"}, {"figure_path": "https://arxiv.org/html/2502.03628/x11.png", "caption": "Figure 9: Cross-stage token ranking comparison between greedy and VISTA (greedy-based) on Shikra. VISTA effectively promotes the ranking of genuine tokens while depressing hallucination tokens.", "description": "This figure displays a comparison of average token rankings across different generation stages (early, mid, late) for three categories of tokens: hidden genuine tokens (correct tokens not generated), decoded genuine tokens (correct tokens generated), and hallucinated tokens (incorrect tokens generated).  The comparison is made between a baseline greedy decoding method and the proposed VISTA method (using greedy decoding). The results show that VISTA significantly improves the rankings of genuine tokens, particularly hidden genuine tokens that are not generated but still relevant to the image, while suppressing the rankings of hallucinated tokens.  This demonstrates VISTA's ability to enhance the generation of accurate tokens based on visual information while mitigating the generation of incorrect ones.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x12.png", "caption": "Figure 10: Performance comparison on MMHal-Bench using beam search.", "description": "This figure presents a comparison of performance across four large vision-language models (LLaVA-1.5, MiniGPT-4, Shikra, and InstructBLIP) on the MMHal-Bench benchmark, utilizing beam search as the decoding strategy. The MMHal-Bench benchmark assesses eight different aspects of model capabilities: object attributes (ATTR), adversarial objects (ADV), comparisons (COMP), counting (COUNT), spatial relations (SPAT), environmental inferences (ENV), holistic descriptions (HOL), and other miscellaneous tasks (OTHER).  The radar charts visually compare the performance of each model against the baseline (vanilla) performance, with VISTA showing improvements across various aspects.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x13.png", "caption": "Figure 11: Performance comparison on MMHal-Bench using nucleus sampling.", "description": "This figure presents a comparison of model performance on the MMHal-Bench benchmark, specifically focusing on the nucleus sampling decoding method.  It illustrates the relative strengths and weaknesses of different large vision-language models (LVLMs) in handling various aspects of visual and linguistic understanding, as measured by MMHal-Bench's eight distinct question categories.  The visualization likely uses radar charts or a similar technique to compare models across multiple dimensions of performance, highlighting which models excel in certain areas and where they struggle.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x14.png", "caption": "Figure 12: Ablation matrices for VSV strength (\u03bb\ud835\udf06\\lambdaitalic_\u03bb) and SLA mixing ratio (\u03b3\ud835\udefe\\gammaitalic_\u03b3) on MiniGPT-4. Brighter color signifies the better performance, and red boxes highlight the parameter combinations used in Table\u00a01. F1 score is included to indicate the overall generation quality.", "description": "This figure presents ablation study results on the MiniGPT-4 model, specifically focusing on the impact of two key parameters: Visual Steering Vector (VSV) strength (\u03bb) and Self-Logits Augmentation (SLA) mixing ratio (\u03b3).  The ablation matrices visualize how changes in these parameters affect three metrics: CHAIRs (sentence-level caption hallucination), CHAIR\u2081 (instance-level caption hallucination), and F1 (overall generation quality). Brighter colors in the matrices indicate better performance across all three metrics.  Red boxes highlight the specific parameter combinations (\u03bb and \u03b3 values) that yielded the best results as reported in Table 1 of the paper. This helps readers understand the optimal balance between the two parameters for achieving the best performance in reducing hallucination without negatively impacting overall generation quality.", "section": "3.5 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2502.03628/x15.png", "caption": "Figure 13: Ablation matrices for VSV strength (\u03bb\ud835\udf06\\lambdaitalic_\u03bb) and SLA mixing ratio (\u03b3\ud835\udefe\\gammaitalic_\u03b3) on LLAVA-1.5. Brighter color signifies the better performance, and red boxes highlight the parameter combinations used in Table\u00a01. F1 score is included to indicate the overall generation quality.", "description": "This ablation study analyzes the effect of varying the visual steering vector (VSV) strength (\u03bb) and self-logits augmentation (SLA) mixing ratio (\u03b3) on the LLAVA-1.5 model.  The figure shows three matrices: one for CHAIRs, one for CHAIR\u2081, and one for F1 score.  Each matrix displays the performance of the model across different combinations of \u03bb and \u03b3.  Brighter colors indicate better performance. The red boxes highlight the specific parameter settings (\u03bb and \u03b3) used in the main experiments (Table 1) of the paper. The F1 score matrix provides a measure of the overall generation quality, balancing the effects of hallucination reduction and overall caption generation quality.  The study aims to find optimal values for \u03bb and \u03b3 that strike a balance between reducing hallucinations and maintaining good caption quality.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x16.png", "caption": "Figure 14: Ablation matrices for VSV strength (\u03bb\ud835\udf06\\lambdaitalic_\u03bb) and SLA mixing ratio (\u03b3\ud835\udefe\\gammaitalic_\u03b3) on InstructBLIP. Brighter color signifies the better performance, and red boxes highlight the parameter combinations used in Table\u00a01. F1 score is included to indicate the overall generation quality.", "description": "This figure presents ablation study results on the InstructBLIP model, showing the impact of varying the visual steering vector (VSV) strength (\u03bb) and self-logits augmentation (SLA) mixing ratio (\u03b3) on the model's performance.  The ablation matrices visualize the effects of these hyperparameters on three key metrics: CHAIRs (sentence-level hallucination), CHAIR\u2081 (instance-level hallucination), and F1 (overall generation quality). Brighter colors in the matrices indicate better performance. Red boxes highlight the specific parameter combinations (\u03bb and \u03b3) used in the main experiments of the paper (as detailed in Table 1). The F1 score provides a comprehensive measure of overall generation quality, balancing hallucination reduction with the generation quality.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x17.png", "caption": "Figure 15: Case study for LLAVA-1.5.", "description": "This figure presents a qualitative case study showcasing the effectiveness of VISTA on LLAVA-1.5.  It displays examples of image captioning outputs generated by the vanilla LLAVA-1.5 model and VISTA enhanced LLAVA-1.5. The examples illustrate how VISTA improves the accuracy and detail of the generated captions, reducing hallucinations (incorrect details in captions not present in images) and including missing genuine information (details present in images but not captured in vanilla captions).  Each example includes the image, the vanilla caption, and VISTA caption.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x18.png", "caption": "Figure 16: Case study for MiniGPT-4.", "description": "This figure showcases comparative examples of image captioning results between the vanilla MiniGPT-4 model and the VISTA-enhanced model.  Each pair of images presents the same input image, followed by a caption generated by the vanilla model and then a caption from VISTA. Differences between the two captions are highlighted to illustrate how VISTA reduces hallucination and improves the accuracy and completeness of the generated descriptions, focusing on object properties and relations.  Specifically, hallucinated elements in the vanilla captions are shown in red, while elements correctly added or corrected by VISTA are in blue.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.03628/x19.png", "caption": "Figure 17: Case study for Shikra.", "description": "This figure showcases a qualitative comparison between the vanilla Shikra model and the VISTA-enhanced Shikra model's performance on two image captioning examples.  The first example shows two girls sitting at a table enjoying donuts. The vanilla model's caption is generic and misses details such as the girls' glasses and the placement of a chair near the table. The VISTA model provides a more precise and detailed caption that includes the previously missed details.  The second example presents a picture of a man in a suit and tie, standing in a room with distinct features like paintings and books. Again, the vanilla model produces a less descriptive caption compared to the VISTA-enhanced model.  The VISTA caption provides more specific details about the man's appearance, the room's layout, and the additional items visible in the scene. This demonstrates VISTA's ability to enhance the Shikra model's image captioning abilities by including visually grounded details in the captions while reducing hallucinated or irrelevant details.", "section": "3. Experiments"}]