{"references": [{"fullname_first_author": "Bai, J.", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "publication_date": "2023-08-12", "reason": "This paper introduces a significant large vision-language model that serves as a key model for comparison in the experiments."}, {"fullname_first_author": "Chen, K.", "paper_title": "Shikra: Unleashing multimodal llm's referential dialogue magic", "publication_date": "2023-06-15", "reason": "This paper introduces another key model used in the experiments for evaluating VISTA's performance."}, {"fullname_first_author": "Zhu, D.", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "publication_date": "2023-04-10", "reason": "This model is among the models used for comparative evaluations in the study, highlighting its importance."}, {"fullname_first_author": "Dai, D.", "paper_title": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning", "publication_date": "2023-12-01", "reason": "This paper is a key reference that introduces a model used for comparison and evaluation in the research."}, {"fullname_first_author": "Liu, H.", "paper_title": "LLaVA-next: Improved reasoning, OCR, and world knowledge", "publication_date": "2024-01-30", "reason": "This paper details improvements made to a key model used in the experiments of the main paper."}]}