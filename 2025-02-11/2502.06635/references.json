{"references": [{"fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "publication_date": "2023-09-16", "reason": "This paper is frequently cited as a primary architectural influence on Steel-LLM, impacting the model's design and training process."}, {"fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "publication_date": "2023-02-23", "reason": "This paper is cited as a key source for the Steel-LLM architecture and provides comparative context regarding open-source LLMs."}, {"fullname_first_author": "Tianyu Zheng", "paper_title": "Opencodeinterpreter: Integrating code generation with execution and refinement", "publication_date": "2025-02-14", "reason": "This paper provides relevant context and comparison with regards to Steel-LLM in terms of training processes and resource utilization."}, {"fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "publication_date": "2022-05-14", "reason": "This paper's contribution is directly implemented in Steel-LLM, improving training efficiency by addressing memory constraints during self-attention."}, {"fullname_first_author": "Ge Zhang", "paper_title": "MAP-Neo: Highly capable and transparent bilingual large language model series", "publication_date": "2024-05-19", "reason": "This paper serves as a significant comparative example in the context of developing and deploying open-source LLMs with similar performance and resource requirements as Steel-LLM."}]}