[{"figure_path": "https://arxiv.org/html/2502.05415/x1.png", "caption": "Figure 1: 512 \u00d7\\times\u00d7 512 images generated by Show-o Turbo given various text prompts. From top to bottom, the images are generated by Show-o Turbo in 8, 4, and 2 sampling steps without reliance on classifier-free guidance\u00a0[20].", "description": "This figure showcases the image generation capabilities of Show-o Turbo, a multimodal model. It presents three sets of 512x512 pixel images, each generated from the same text prompt but with varying numbers of sampling steps (8, 4, and 2).  The key takeaway is that Show-o Turbo can produce high-quality images even with a significantly reduced number of sampling steps compared to other approaches.  The absence of classifier-free guidance highlights the model's inherent capabilities.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.05415/x2.png", "caption": "Figure 2: Illustration of the sampling trajectories of text and image tokens in Show-o.\nAs shown, they both display a denoising pattern.\nIn particular, the trajectory of text generation is yielded by Jacobi Decoding\u00a0[47].\nThe black line denotes the unified abstraction of the multimodal trajectory, and the red lines illustrate the objective of our Show-o Turbo\u2014to map an arbitrary point on the sampling trajectory to the endpoint.\nNote that we omit the trajectory segmentation strategy here for brevity.", "description": "Figure 2 illustrates the process of generating text and images in the Show-o model. Both processes follow a denoising pattern, where noise is gradually removed from the initial random tokens to arrive at the final output. Text generation uses Jacobi Decoding, an iterative method refining multiple text tokens simultaneously. The black line represents a unified multimodal trajectory encompassing both image and text generation.  Red lines show the goal of Show-o Turbo: to predict the final output from any point along this trajectory, thus accelerating the generation process. The figure simplifies the process by omitting trajectory segmentation details.", "section": "3. Preliminary: Show-o"}, {"figure_path": "https://arxiv.org/html/2502.05415/x3.png", "caption": "Figure 3: \nThe text sampling trajectory of Show-o Turbo in MMU cases. Show-o Turbo realizes acceleration by predicting multiple successive tokens in one iteration and correctly guessing the later tokens.", "description": "Show-o Turbo accelerates the text generation process in Multimodal Understanding (MMU) tasks by employing a parallel decoding strategy.  Instead of generating tokens one at a time, Show-o Turbo predicts multiple tokens simultaneously in each iteration. The figure illustrates this process, showing how the model quickly converges towards the final, correct sequence of tokens.  This parallel prediction significantly reduces the number of steps required for generation, leading to faster inference speeds.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.05415/x6.png", "caption": "Figure 4: Comparison between Show-o and Show-o Turbo on 512 resolution in T2I generation. The former crashes in two-step sampling, while the latter maintains good performance.", "description": "This figure compares the image generation results of Show-o and Show-o Turbo models at 512x512 resolution using different numbers of sampling steps (16, 8, 4, 2).  The images showcase various prompts and highlight the key difference: Show-o fails to generate coherent images when only two sampling steps are used, whereas Show-o Turbo produces good-quality images even with only two steps.", "section": "5. Experiments"}]