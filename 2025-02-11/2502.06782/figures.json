[{"figure_path": "https://arxiv.org/html/2502.06782/x1.png", "caption": "Figure 1: Lumina-Video demonstrates a strong ability to generate high-quality videos with rich details and remarkable temporal coherence, accurately following both simple and detailed text prompts.", "description": "This figure showcases four example videos generated by Lumina-Video, each corresponding to a different text prompt. The prompts range in complexity from simple descriptions (e.g., \"A massive explosion\") to more detailed and specific scenarios (e.g., \"A red-haired child in glasses, dressed in a brown shirt and holding a large shoulder bag, looks off-camera with curiosity or anticipation. The softly blurred background, likely a living room or study with a bookshelf or bulletin board, adds a homely feel to the scene.\"). The generated videos demonstrate Lumina-Video's capacity to produce high-quality, visually rich results, with details that match the provided textual prompts. Additionally, the videos exhibit remarkable temporal coherence, demonstrating Lumina-Video's strength in accurately capturing motion and ensuring smooth transitions throughout the video sequences.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06782/x2.png", "caption": "Figure 2: Architecture of Lumina-Video with Multi-scale Next-DiT and Motion Conditioning.", "description": "This figure illustrates the architecture of Lumina-Video, a novel framework for efficient and flexible video generation.  It highlights the use of a Multi-scale Next-DiT architecture, which incorporates multiple patch sizes to learn video structures at various scales, improving both efficiency and flexibility. The diagram also shows how motion scores, derived from optical flow, are incorporated as explicit conditions to directly control the dynamic degree of the generated videos. The process involves 3D encoding of the input video, text embedding, and motion embedding, which are then processed by the multi-scale Next-DiT blocks before unpatchifying and generating the final video output. The figure visually represents the multi-stage processing, highlighting the interplay between different scales and the motion conditioning in the video generation process.", "section": "3. Lumina-Video"}, {"figure_path": "https://arxiv.org/html/2502.06782/x3.png", "caption": "Figure 3: Loss curves for different patch sizes at different denoising timesteps. See Sec.\u00a0C.2 for the complete figure.", "description": "This figure displays the training loss curves for three different patch sizes used in the Lumina-Video model at various stages of the denoising process. The x-axis represents the denoising timestep, progressing from early stages (t close to 0) to later stages (t close to 1), while the y-axis shows the training loss. The three lines represent different patch sizes: (1,2,2), (2,2,2), and (2,4,4), each corresponding to a different level of detail in the representation. The curves illustrate how the loss changes for each patch size as the model progresses through the denoising process. This helps visualize the effectiveness of the multi-scale approach used in Lumina-Video, where different patch sizes are utilized at different stages to balance efficiency and quality.", "section": "3.1 Multi-scale Next-DiT"}, {"figure_path": "https://arxiv.org/html/2502.06782/x4.png", "caption": "Figure 4: Multi-scale Patchification allows Lumina-Video to perform flexible multi-stage denoising during inference, leading to a better tradeoff between quality and efficiency.", "description": "The figure illustrates how Lumina-Video's multi-scale patchification approach enables flexible multi-stage denoising during inference.  By using smaller patches at earlier stages, the model efficiently captures the overall video structure. Progressively, it switches to larger patches to refine details. This flexible strategy balances computational cost and quality, making inference adaptable to different resource constraints and improving the overall efficiency without sacrificing too much quality.", "section": "3.1 Multi-scale Next-DiT"}, {"figure_path": "https://arxiv.org/html/2502.06782/x5.png", "caption": "Figure 5: Comparison of generated videos using different patchification strategies.", "description": "This figure showcases the impact of different patchification strategies on the quality of generated videos using Lumina-Video.  It visually demonstrates how altering the patch size (the spatial and temporal resolution at which the video is processed) affects the final output. By comparing videos generated with different patch sizes, the figure provides a qualitative assessment of the trade-off between computational efficiency (larger patch sizes) and the level of detail and fidelity in the generated content (smaller patch sizes).", "section": "3.1. Multi-scale Next-DiT"}, {"figure_path": "https://arxiv.org/html/2502.06782/x6.png", "caption": "Figure 6: Comparison of generated videos using different positive and negative motion scores.", "description": "This figure displays a comparison of videos generated using different combinations of positive and negative motion scores. The motion score is a control mechanism in Lumina-Video that allows for adjusting the intensity of motion in generated videos. By varying the positive and negative motion scores, the model can produce videos with different levels of dynamism, ranging from very static to highly dynamic.  The figure visually demonstrates the effect of this control mechanism, illustrating how the choice of motion scores impacts the overall visual flow and movement within the generated video sequences.", "section": "5.2.2. \u039c\u039fTION SCORE"}, {"figure_path": "https://arxiv.org/html/2502.06782/x7.png", "caption": "Figure 7: Complete figure of loss curves for different patch sizes at different denoising timesteps.", "description": "This figure displays a comprehensive set of loss curves, illustrating the impact of various patch sizes on the quality of video denoising at different stages of the process.  Each curve represents the loss magnitude across multiple timesteps for a specific patch size. By comparing these curves, one can analyze the performance of various patch sizes during different phases of denoising. This visualization helps illustrate the benefits of employing a multi-scale approach by highlighting how different patch sizes are better suited for different stages in the process, with smaller patch sizes showing improved performance in later stages focused on finer detail and larger patch sizes performing better in earlier stages when capturing broad structures.", "section": "3.1 Multi-scale Next-DiT"}, {"figure_path": "https://arxiv.org/html/2502.06782/x8.png", "caption": "Figure 8: Illustration of Lumina-V2A Model based on Next-DiT", "description": "Lumina-V2A, a video-to-audio model, uses a Next-DiT architecture.  It processes video, text, and audio features using a co-attention mechanism to ensure semantic consistency and temporal synchronization.  The model's input includes a noisy audio latent representation from a pre-trained 2D VAE and visual and textual embeddings from CLIP.  Time embeddings and high-frame-rate visual features from Synchformer help align the audio with the visual and textual information. The model outputs a refined audio latent representation, then reconstructs it into a mel-spectrogram and finally an audio waveform using a pre-trained HiFi-GAN vocoder. The training data includes audio-visual pairs from VGGSound, selected for high temporal alignment.", "section": "D. Video-to-Audio"}]