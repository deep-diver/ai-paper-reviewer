<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-02-11s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/</link><description>Recent content in 2025-02-11s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 10 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/index.xml" rel="self" type="application/rss+xml"/><item><title>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06703/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06703/</guid><description>Smaller LLMs can outperform larger ones by strategically increasing computation during inference, defying conventional LLM scaling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06703/cover.png"/></item><item><title>CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06527/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06527/</guid><description>CustomVideoX: Zero-shot personalized video generation, exceeding existing methods in quality &amp;amp; consistency via 3D reference attention and dynamic adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06527/cover.png"/></item><item><title>Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06155/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06155/</guid><description>EFFICIENT-VDIT accelerates video generation by 7.8x using sparse attention and multi-step distillation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06155/cover.png"/></item><item><title>EVEv2: Improved Baselines for Encoder-Free Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06788/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06788/</guid><description>EVEv2.0: A novel encoder-free vision-language model outperforms existing approaches by using a divide-and-conquer architecture and a data-efficient training strategy, achieving strong vision-reasoning&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06788/cover.png"/></item><item><title>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06781/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06781/</guid><description>OREAL, a novel RL framework, achieves state-of-the-art mathematical reasoning in LLMs using only binary outcome rewards, demonstrating that a 7B model can match the performance of 32B models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06781/cover.png"/></item><item><title>Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06782/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06782/</guid><description>Lumina-Video: Efficient and flexible video generation using a multi-scale Next-DiT architecture with motion control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06782/cover.png"/></item><item><title>Matryoshka Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06786/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06786/</guid><description>Matryoshka Quantization (MatQuant) boosts low-precision model accuracy by up to 10% through a novel multi-scale training approach. It leverages the nested structure of integer data types, allowing a &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06786/cover.png"/></item><item><title>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06772/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06772/</guid><description>ReasonFlux boosts LLM mathematical reasoning by using hierarchical thought templates, outperforming top LLMs like OpenAI&amp;rsquo;s 01-preview and DeepSeek V3.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06772/cover.png"/></item><item><title>Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06635/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06635/</guid><description>Steel-LLM: A fully open-source, resource-efficient Chinese LLM trained with transparency, achieving competitive performance despite limited resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06635/cover.png"/></item><item><title>SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06394/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06394/</guid><description>SynthDetoxM generates high-quality multilingual parallel data for text detoxification using LLMs, outperforming existing datasets and models in few-shot settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06394/cover.png"/></item><item><title>Dual Caption Preference Optimization for Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06023/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06023/</guid><description>Dual Caption Preference Optimization (DCPO) significantly boosts diffusion model image quality by using paired captions to resolve data distribution conflicts and irrelevant prompt issues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06023/cover.png"/></item><item><title>LM2: Large Memory Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06049/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06049/</guid><description>LM2: Large Memory Models enhance Transformers by adding an auxiliary memory module, significantly improving multi-step reasoning and long-context information synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06049/cover.png"/></item><item><title>The Curse of Depth in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05795/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05795/</guid><description>Deep layers in LLMs underperform due to Pre-Layer Normalization; LayerNorm Scaling resolves this by controlling output variance, significantly improving training efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05795/cover.png"/></item><item><title>Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06060/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.06060/</guid><description>Language models learn effective social deduction strategies in a virtual game by using their goal to predict useful information as a dense reward signal, doubling win rates compared to standard RL.</description></item><item><title>APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05431/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05431/</guid><description>APE: a novel method significantly speeds up context-augmented generation (CAG). By using adaptive parallel encoding, APE achieves a 4.5x speedup and maintains high accuracy even with 128K length cont&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05431/cover.png"/></item><item><title>Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05415/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05415/</guid><description>Show-o Turbo dramatically speeds up multimodal understanding and generation by leveraging parallel decoding and consistency distillation, achieving significant performance gains with fewer sampling st&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.05415/cover.png"/></item><item><title>DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.04370/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.04370/</guid><description>DreamDPO: Revolutionizing text-to-3D generation by directly aligning outputs with human preferences via innovative preference optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.04370/cover.png"/></item><item><title>The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.03628/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.03628/</guid><description>VISTA steers LVLMs away from hallucinations by cleverly adjusting token rankings during inference, improving visual grounding and semantic coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-11/2502.03628/cover.png"/></item></channel></rss>