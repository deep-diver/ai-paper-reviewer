[{"figure_path": "https://arxiv.org/html/2501.18492/x1.png", "caption": "Figure 1: Demonstrations of LLaMA Guard 3 (left side) and our GuardReasoner (right side), mainly focusing on 3 aspects: (1) performance, (2) explainability, and (3) generalization. We sample this case from the WildGuardTest (Han et\u00a0al., 2024) dataset.", "description": "This figure compares the performance of LLaMA Guard 3 and GuardReasoner on a prompt from the WildGuardTest dataset.  The comparison highlights three key aspects: \n(1) **Performance:** The F1 scores show GuardReasoner's improved accuracy in identifying harmful prompts compared to LLaMA Guard 3.\n(2) **Explainability:** GuardReasoner provides detailed reasoning steps behind its classification, while LLaMA Guard 3 offers a less transparent output.\n(3) **Generalization:**  GuardReasoner demonstrates better generalization by handling prompts beyond predefined categories, whereas LLaMA Guard 3 relies on fixed, predefined harmful categories. The example shown illustrates how GuardReasoner successfully identifies a deceptive request, offering a step-by-step explanation to support its classification, in contrast to LLaMA Guard 3 which flags the prompt without providing such detail.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.18492/x2.png", "caption": "Figure 2: GuardReasoner consists of three modules: (1) Reasoning Data Synthesis, (2) Reasoning SFT, and (3) Hard Sample DPO. (1) First, GPT-4o is used to create reasoning data (GuardReasonerTrain) by inputting the user\u2019s prompt, the target model\u2019s response, and the ground truth. (2) Then, the base model is trained by R-SFT on this dataset to develop the reasoning model \u2133R-SFTsubscript\u2133R-SFT\\mathcal{M}_{\\text{R-SFT}}caligraphic_M start_POSTSUBSCRIPT R-SFT end_POSTSUBSCRIPT. (3) \u2133R-SFTsubscript\u2133R-SFT\\mathcal{M}_{\\text{R-SFT}}caligraphic_M start_POSTSUBSCRIPT R-SFT end_POSTSUBSCRIPT produces k\ud835\udc58kitalic_k outputs to identify the ambiguous samples with both correct and incorrect responses. Different reasoning models, which are trained on different subsets of the reasoning data, are used to improve the diversity of these samples, and an ensemble approach is applied. Lastly, HS-DPO is performed on these ambiguous samples, selecting correct outputs as positive data and incorrect ones as negative data, with a focus on hard samples by up-weighting those with more errors. In this way, we guide GuardReasoner to learn to reason.", "description": "GuardReasoner is composed of three stages: (1) Reasoning Data Synthesis uses GPT-4 to generate a dataset (GuardReasonerTrain) by providing it with user prompts, model responses, and ground truth labels.  The model then infers the reasoning steps needed to arrive at the ground truth. (2) Reasoning SFT (Supervised Fine-Tuning) trains a base model using the GuardReasonerTrain dataset to develop a reasoning model (\u2133R-SFT). (3) Hard Sample DPO (Direct Preference Optimization) identifies ambiguous samples from \u2133R-SFT's output by generating multiple outputs for the same input.  It uses an ensemble of reasoning models trained on subsets of the data to improve diversity and then uses HS-DPO, up-weighting harder samples to improve reasoning ability by focusing on the decision boundary.", "section": "3. GuardReasoner"}, {"figure_path": "https://arxiv.org/html/2501.18492/x3.png", "caption": "Figure 3: Performance: Baselinemixmix{}_{\\text{mix}}start_FLOATSUBSCRIPT mix end_FLOATSUBSCRIPT vs. GuardReasoner on one conventional case from the ToxicChat dataset (Lin et\u00a0al., 2023).", "description": "This figure showcases a comparison between Baselinemix and GuardReasoner's performance on a single example from the ToxicChat dataset.  It highlights how GuardReasoner, by incorporating a reasoning process, correctly identifies a harmful prompt where Baselinemix fails. This demonstrates GuardReasoner's improved accuracy in moderation tasks due to its enhanced reasoning capabilities.", "section": "3. GuardReasoner"}, {"figure_path": "https://arxiv.org/html/2501.18492/x4.png", "caption": "Figure 4: Performance: WildGuard vs. GuardReasoner against a scenario nesting attack from WildGuardTest (Han et\u00a0al., 2024). GuardReasoner successfully defends while WildGuard fails.", "description": "This figure showcases a comparison of WildGuard and GuardReasoner's performance against a 'scenario nesting attack,' a sophisticated evasion technique from the WildGuardTest benchmark dataset. The figure highlights a specific example where WildGuard incorrectly classifies a harmful prompt as safe, while GuardReasoner accurately identifies it as harmful. This demonstrates GuardReasoner's superior ability to detect complex and nested adversarial attacks, showcasing its improved safety and robustness compared to WildGuard.", "section": "4. Case Study"}, {"figure_path": "https://arxiv.org/html/2501.18492/x5.png", "caption": "Figure 5: Explainability: GuardReasoner offers transparent explanations for outcomes and helps labelers to fix the mislabelled label in the OpenAIModeration dataset (Markov et\u00a0al., 2023).", "description": "GuardReasoner not only provides moderation results but also gives detailed reasoning steps behind its decisions. This transparency helps users understand why a certain decision was made, which is crucial for building trust and improving the model's reliability.  The figure shows how GuardReasoner's explanations helped correct mislabeled data in the OpenAI Moderation dataset, showcasing its ability to enhance the quality and explainability of moderation decisions.", "section": "3.3. Inference with Reasoning"}, {"figure_path": "https://arxiv.org/html/2501.18492/x6.png", "caption": "Figure 6: Generalizability: LLaMA Guard 3 vs. GuardReasoner on one case in AegisSafetyTest (Ghosh et\u00a0al., 2024a). GuardReasoner provides open-ended non-fixed harmful categories.", "description": "This figure compares the performance of LLaMA Guard 3 and GuardReasoner on a specific example from the AegisSafetyTest dataset.  LLaMA Guard 3, a pre-existing model, relies on a predefined set of fixed harmful categories.  In contrast, GuardReasoner demonstrates superior generalizability by identifying harmful content without relying on these fixed categories, showcasing its ability to handle a broader range of potentially harmful scenarios.  The figure highlights GuardReasoner's open-ended and flexible approach to harmful content identification, suggesting greater adaptability to novel and evolving forms of misuse.", "section": "3. GuardReasoner"}, {"figure_path": "https://arxiv.org/html/2501.18492/x7.png", "caption": "Figure 7: Convergence of GuardReasoner. The first, second, and third row denotes 1B, 3B, and 8B models, respectively.", "description": "This figure displays the training loss curves for the GuardReasoner model across three different sizes (1B, 3B, and 8B parameters).  Each row shows the convergence of the model during training. The graphs show the loss for both reasoning supervised fine-tuning (R-SFT) and hard sample direct preference optimization (HS-DPO).  The smoothed curves help visualize the overall trend of the loss during training. Observing the convergence behavior across different model sizes allows for an understanding of the training efficiency and stability of the GuardReasoner model.", "section": "3. GuardReasoner"}, {"figure_path": "https://arxiv.org/html/2501.18492/x8.png", "caption": "Figure 8: One case of correcting the label from harmful to unharmful. It is from the AegisSafetyTest dataset (Ghosh et\u00a0al., 2024a).", "description": "This figure showcases a case where the GuardReasoner model successfully corrects a mislabeled example from the AegisSafetyTest dataset.  Initially, the example was incorrectly labeled as 'harmful'. The GuardReasoner model analyzes the user's prompt and the AI's response, providing a step-by-step reasoning process.  Through this reasoning, the model determines that the content is not harmful, thus correcting the label to 'unharmful'. This demonstrates the model's ability to not only perform classification tasks, but also provide explainable and potentially correctable results.", "section": "4.4. Case Study"}, {"figure_path": "https://arxiv.org/html/2501.18492/x9.png", "caption": "Figure 9: One case of correcting the label from unharmful to harmful. It is from the AegisSafetyTest dataset (Ghosh et\u00a0al., 2024a).", "description": "The figure showcases a case where the model initially classified a user's prompt as \"unharmful,\" but after review and correction, it was reclassified as \"harmful.\" The user's prompt is \"What does it feel like when you take LSD?\"  The GuardReasoner's initial analysis considered the request harmless, lacking explicit harmful content. However, upon further analysis, it identified that the question implicitly promotes or normalizes the use of a controlled substance (LSD), which could be considered harmful due to potential health risks and illegal implications. This highlights GuardReasoner's ability to learn from corrections and improve its classification accuracy. The example is from the AegisSafetyTest dataset.", "section": "4.4. Case Study"}, {"figure_path": "https://arxiv.org/html/2501.18492/x10.png", "caption": "Figure 10: One case of correcting the label from harmful to unharmful. It is from the BeaverTails dataset (Ji et\u00a0al., 2024b).", "description": "This figure showcases an example from the BeaverTails dataset where the original label was incorrectly marked as \"harmful.\"  The GuardReasoner model, through its reasoning process, successfully identifies the input as unharmful, highlighting its ability to correct mislabeled data and improve the accuracy of harmful content detection. The visualization likely displays the user's prompt, the AI's response, the GuardReasoner's step-by-step reasoning process leading to its conclusion, and the original and corrected labels.  This demonstrates the model's capacity for nuanced interpretation and its potential to enhance the reliability of existing safety datasets.", "section": "4.4. Case Study"}, {"figure_path": "https://arxiv.org/html/2501.18492/x11.png", "caption": "Figure 11: One case of correcting the label from unharmful to harmful. It is from the BeaverTails dataset (Ji et\u00a0al., 2024b).", "description": "This figure showcases a case where the initial label of a sample in the BeaverTails dataset was incorrectly classified as 'unharmful', but after review and correction, it was rightfully changed to 'harmful'.  The image likely displays the original user prompt, the AI's response, and the reasoning process behind the label correction.  The reasoning process shows step-by-step analysis highlighting why the response is actually harmful, even if it initially appeared benign.", "section": "4.4. Case Study"}, {"figure_path": "https://arxiv.org/html/2501.18492/x12.png", "caption": "Figure 12: The prompt for the reasoning data synthesis.", "description": "This figure displays the prompt used to instruct GPT-4 to generate the reasoning data for training the GuardReasoner model.  The prompt guides GPT-4 to perform three tasks:  (1) classify the user's request as harmful or unharmful; (2) classify the AI assistant's response as a refusal or compliance; and (3) classify the AI assistant's response as harmful or unharmful.  Importantly, the prompt emphasizes step-by-step reasoning, requiring GPT-4 to detail its thought process in the smallest possible units to ensure consistency between reasoning and conclusions.  The prompt also provides a specific format for GPT-4's response, ensuring uniformity and ease of data processing for training. ", "section": "3. GuardReasoner"}, {"figure_path": "https://arxiv.org/html/2501.18492/x13.png", "caption": "Figure 13: The demonstration for the training data of R-SFT.", "description": "This figure demonstrates the training data structure used in the Reasoning Supervised Fine-tuning (R-SFT) stage of the GuardReasoner model training.  It shows the input (user prompt and LLM response), the expected output (reasoning steps and classification labels for prompt, response and refusal), and the instruction given to the model. The instruction guides the model to perform step-by-step reasoning and provide a well-structured explanation for its classification decisions.  The goal of this training stage is to enable the model to learn to reason and make accurate moderation decisions by leveraging this structured reasoning data.", "section": "3.1 Reasoning Supervised Fine-tuning"}, {"figure_path": "https://arxiv.org/html/2501.18492/x14.png", "caption": "Figure 14: The demonstration for the training data of HS-DPO.", "description": "This figure demonstrates the training data used for Hard Sample Direct Preference Optimization (HS-DPO).  HS-DPO is a stage in the GuardReasoner model training that focuses on refining the model's ability to handle ambiguous samples (those with both correct and incorrect outputs). The figure showcases the structure of the data provided to the model during this training phase. This includes instructions to the model to perform three tasks (harmfulness detection on the prompt and response, and refusal detection), emphasizing step-by-step reasoning and consistency between reasoning steps and conclusions.  The provided input data includes a user's prompt and the AI's response, and the expected output consists of the model's reasoning steps and its classification of the prompt and response as either harmful or unharmful and refusal or compliance.", "section": "3. GuardReasoner"}, {"figure_path": "https://arxiv.org/html/2501.18492/x15.png", "caption": "Figure 15: The prompt for the inference of closed-source guard APIs, including GPT-4, GPT-4o, and Claude 3.5 Sonnet.", "description": "This figure displays the prompt used for evaluating closed-source guard APIs such as GPT-4, GPT-40, and Claude 3.5 Sonnet.  The prompt instructs the model to perform three tasks: 1) determine if the user's request is harmful, 2) determine if the AI assistant's response is a refusal or compliance, and 3) determine if the AI assistant's response is harmful. The model is instructed to provide a concise 'harmful' or 'unharmful' classification and must not use other descriptive words.", "section": "4. Experiments"}]