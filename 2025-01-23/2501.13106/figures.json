[{"figure_path": "https://arxiv.org/html/2501.13106/x1.png", "caption": "Figure 1: Performance Comparison of VideoLLaMA3 with the previous advanced image/video MLLM on various representative benchmarks. As shown in the figure, VideoLLaMA3 has achieved very competitive results on various benchmarks. Specifically, VideoLLaMA3 not only demonstrates strong video understanding capabilities\u00a0(VideoMME, PerceptionTest, MLVU) but also maintains excellent document comprehension abilities\u00a0(DocVQA) and multimodal mathematical reasoning skills\u00a0(MathVista).\nNote that LLaVA-OneVision is only used for evaluating image benchmarks, while LLaVA-Video is only used for evaluating video benchmarks.", "description": "This figure compares the performance of VideoLLaMA3 against other state-of-the-art image and video Multimodal Large Language Models (MLLMs) across a variety of benchmark datasets.  These benchmarks test different capabilities, including general video understanding (VideoMME, PerceptionTest, MLVU), document comprehension (DocVQA), and mathematical reasoning (MathVista).  The results show that VideoLLaMA3 achieves highly competitive performance across all categories.  Importantly, the figure notes that two specific models, LLaVA-OneVision and LLaVA-Video, were only used for evaluating image and video benchmarks, respectively, due to their specialized nature.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.13106/x2.png", "caption": "Figure 2: Training paradigm of VideoLLaMA3. The training of VideoLLaMA3 has four stages: (1) Vision Encoder Adaptation, (2) Vision-Language Alignment, (3) Multi-task Fine-tuning, and (4) Video-centric Fine-tuning.", "description": "This figure illustrates the four-stage training pipeline of the VideoLLaMA3 model. Stage 1, Vision Encoder Adaptation, focuses on aligning the vision encoder with the language model using various image data to handle different resolutions.  Stage 2, Vision-Language Alignment, jointly tunes the vision encoder, projector, and language model using large-scale image-text data. Stage 3, Multi-task Fine-tuning, incorporates data for downstream tasks and video-text data to establish a foundation for video understanding. Finally, Stage 4, Video-centric Fine-tuning, further improves the model's video understanding capabilities by including more video data.", "section": "2 Methodology"}, {"figure_path": "https://arxiv.org/html/2501.13106/x3.png", "caption": "Figure 3: The overall pipeline of our VideoLLaMA3. There are two key technical points: \u2776 Any-resolution Vision Tokenization\u00a0(AVT): AVT converts images or videos of any resolution into a set of 1-D token sequences, enabling compatibility with varying amounts of input images and videos of different resolutions, thereby supporting more flexible vision input; \u2777 Differential Frame Pruner\u00a0(DiffFP): Serving as a video compressor, DiffFP eliminates video content with minimal differences between adjacent frames. This approach enhances video processing efficiency, particularly for long-form videos.", "description": "VideoLLaMA3 processes image and video inputs using a two-stage pipeline. First, Any-Resolution Vision Tokenization (AVT) converts images/videos of any resolution into 1D token sequences, which allows for flexible input sizes.  Then, the Differential Frame Pruner (DiffFP) acts as a video compressor, removing redundant frames in videos to improve efficiency, particularly for long videos. This process ultimately feeds the information into a large language model for multimodal understanding.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2501.13106/x4.png", "caption": "Figure 4: The calculation flow of our DiffFP. We prune video tokens based on patch similarities in pixel space, removing patches with smaller distances to the previous frame.", "description": "The figure illustrates the process of Differential Frame Pruning (DiffFP), a video compression technique.  DiffFP analyzes video frames by comparing patches (small regions) between consecutive frames. The distance between these patches in pixel space is calculated (using 1-norm distance). Patches with small distances, representing high similarity and minimal changes between frames, are pruned (removed). This reduces the number of tokens used to represent the video, making video processing more efficient, especially for long videos with lots of redundant information. The output is a more compact and efficient representation of the video.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2501.13106/x5.png", "caption": "Figure 5: Data formats for different data types. \u2776 For image sequence, we use \"\\n\" to separate image tokens from different image; \u2777 For video sequence, we use \"Time: xxs\" to indicate timestamps of each frame, \",\" to separate different frames, and \"\\n\" to separate tokens from different videos; \u2778 For streaming video sequence, videos and texts are organized in an interleaved format.", "description": "Figure 5 illustrates how different data types (image sequences, video sequences, and streaming video sequences) are formatted for input into the VideoLLaMA3 model.  Different symbols are used to delineate various aspects of the data. For image sequences, a newline character ('\\n') separates tokens representing different images. Video sequences use 'Time: xxs' to mark timestamps for each frame, commas (',') to separate frames, and newlines ('\\n') to distinguish between videos. Lastly, streaming video sequences organize video and text tokens in an interleaved structure.", "section": "3.1 Data Format"}, {"figure_path": "https://arxiv.org/html/2501.13106/x6.png", "caption": "Figure 6: Case study of chart images understanding.", "description": "This figure showcases two examples of VideoLLaMA3's chart image understanding capabilities.  The first example demonstrates the model's ability to analyze stock trends and provide investment advice based on the visual information presented in a stock chart. The second example shows VideoLLaMA3 comparing the performance of different large language models (LLMs) based on a chart displaying their average performance and number of parameters.  This highlights the model's ability to extract and interpret quantitative information from chart visualizations.", "section": "4.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2501.13106/x7.png", "caption": "Figure 7: Case study of OCR and document images.", "description": "This figure showcases two examples demonstrating VideoLLaMA 3's capabilities in Optical Character Recognition (OCR) and understanding document images. The first example shows the model successfully extracting and interpreting text from a design image, offering suggestions for improvement. The second example demonstrates the model's ability to accurately perform OCR on a more complex, dense document image.", "section": "4.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2501.13106/x8.png", "caption": "Figure 8: Case study of multi-image understanding.", "description": "This figure showcases VideoLLaMA3's capacity for multi-image understanding through three distinct examples.  The first demonstrates the model's ability to differentiate between two bird species based on visual features. The second example highlights the model's comprehension of complex visual and textual information within a document, surpassing basic OCR capabilities by extracting answers from a document containing multiple images.  The final example illustrates VideoLLaMA3's understanding of narratives presented in a comic strip format, demonstrating the model's capacity for storytelling and contextual comprehension.", "section": "4.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2501.13106/x9.png", "caption": "Figure 9: Case study of images with general knowledge.", "description": "This figure showcases three examples where the VideoLLaMA3 model demonstrates its ability to understand images within the context of general knowledge.  The first example involves a basketball free throw, illustrating the model's grasp of sports imagery and terminology. The second shows the Mona Lisa, highlighting its understanding of art history and cultural significance. The last displays a space-themed video with puppies in astronaut suits, and shows how the model can describe the video.  Each example demonstrates VideoLLaMA3's comprehensive understanding beyond simple image recognition and its capacity to connect images to broader contextual information.", "section": "4.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2501.13106/x10.png", "caption": "Figure 10: Case study of video understanding.", "description": "Figure 10 presents several case studies showcasing VideoLLaMA3's video understanding capabilities.  Each case study demonstrates the model's ability to handle various tasks including identifying objects within a video and their positions, recognizing the last key to disappear on a keyboard, identifying unusual aspects (like bears eating sushi), detailing video contents, and understanding video competitions.", "section": "4.3 Case Study"}, {"figure_path": "https://arxiv.org/html/2501.13106/x11.png", "caption": "Figure 11: Case study of long video understanding, temporal grounding, and video-image joint understanding.", "description": "Figure 11 showcases VideoLLaMA 3's capabilities in handling complex video understanding tasks. It presents three examples demonstrating the model's proficiency in long video understanding, temporal grounding, and video-image joint understanding.  The first example involves describing a long video; the second shows temporal grounding by identifying specific timestamps within a video when an action occurs; and the third example demonstrates joint understanding by connecting a video clip with a separate image.", "section": "4.3 Case Study"}]