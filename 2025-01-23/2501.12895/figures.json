[{"figure_path": "https://arxiv.org/html/2501.12895/x1.png", "caption": "Figure 1: \nTraining-time preference optimization (e.g., RLHF and DPO) compared with test-time preference optimization (TPO), where the model aligns with human preferences during test-time with the model parameters fixed.", "description": "This figure compares and contrasts training-time and test-time preference optimization methods for large language models (LLMs). Training-time methods, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), involve updating the model's parameters during training to align its outputs with human preferences.  This requires iterative retraining and is computationally expensive. In contrast, test-time preference optimization (TPO), introduced in this paper, aligns the LLM's output with human preferences during inference without changing the model parameters. It achieves this by iteratively refining the LLM's response based on textual feedback from a reward model. The figure visually represents the key differences in the process and computational cost between these two approaches. ", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12895/x4.png", "caption": "Figure 2: Framework of test-time preference optimization (TPO), shown here on a real example from AlpacaEval 2.\nConcretely, the model samples responses and scores them with a reward model (Left), interprets reward model feedback of chosen response v3subscript\ud835\udc633v_{3}italic_v start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and rejected response v1subscript\ud835\udc631v_{1}italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT (Middle),\nand provides critiques, generates improvement suggestions (Right),\nand then updates new responses for the next iteration.\nIn analogy to traditional gradient-based methods, TPO performs gradient descent (loss calculation, gradient computation and variable optimization) in textual form to tailor model outputs based on numerical feedback from the reward model.", "description": "Figure 2 illustrates the Test-Time Preference Optimization (TPO) framework using an example from the AlpacaEval 2 benchmark.  The process begins (left) with the model generating multiple responses and receiving scores from a reward model.  The highest and lowest-scoring responses are selected for analysis (middle).  The system analyzes these responses to identify strengths and weaknesses, translating this into textual feedback (critiques and suggestions). This feedback then guides the model in generating improved responses for the next iteration (right), similar to gradient descent in traditional machine learning, but operating entirely within a text-based format.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2501.12895/x5.png", "caption": "Figure 3: Test-time training curve for the (unaligned) SFT model and (aligned) instruct models.\nThe colored lines represent the test-time training performance (reward model score) w.r.t. training steps (i.e., number of TPO iterations), while the dashed horizontal lines indicate scores for models without test-time training.", "description": "This figure displays the performance of different language models during test-time preference optimization (TPO).  It shows the reward model score (a measure of how well the model aligns with human preferences) over the number of TPO iterations. The colored lines show models undergoing TPO, demonstrating how their alignment improves with each iteration.  Dashed lines represent the initial scores of these models *before* TPO, providing a benchmark for comparison.  The figure includes both unaligned (SFT) and aligned (Instruct) models, allowing for a direct comparison of how well TPO works on different model types.", "section": "6.1 Test-time Training"}, {"figure_path": "https://arxiv.org/html/2501.12895/x6.png", "caption": "Figure 4: Inference stability of models with and without TPO.", "description": "Figure 4 illustrates the impact of test-time preference optimization (TPO) on the stability of large language model (LLM) inferences.  It compares the standard deviation of reward model scores across five generated outputs from LLMs with and without TPO applied. Lower standard deviation indicates higher stability and less susceptibility to producing unexpected or undesirable outputs.  The figure showcases that applying TPO enhances the stability of an unaligned LLM, surpassing the stability levels of aligned LLMs. This is achieved by concentrating the probability mass toward high-quality outputs, leading to more deterministic behavior.", "section": "6.2. Benchmark Performance"}, {"figure_path": "https://arxiv.org/html/2501.12895/x7.png", "caption": "Figure 5: Test-time training curves on the HH-RLHF dataset, evaluated under varying sampling widths (i.e., the number of responses sampled per iteration).", "description": "This figure shows the results of test-time training using the TPO method on the HH-RLHF dataset.  The x-axis represents the number of test-time training steps (iterations of TPO), and the y-axis represents the average reward score obtained from the reward model. Multiple lines are plotted, each representing a different number of responses sampled per TPO iteration (sampling width). This illustrates how the alignment with the reward model changes depending on the number of responses considered at each step.", "section": "6.1 Test-time Training"}, {"figure_path": "https://arxiv.org/html/2501.12895/x8.png", "caption": "Figure 6: Win-rates of TPO against Best-of-N sampling (BoN).", "description": "This figure compares the performance of Test-Time Preference Optimization (TPO) against the Best-of-N (BoN) sampling method on three benchmark datasets: AlpacaEval 2, Arena-Hard, and HH-RLHF.  The bar chart shows win rates for both TPO and BoN with different sample sizes (BoN-30 and BoN-60).  It demonstrates that even with a smaller number of total samples, the iterative refinement of TPO leads to a higher win rate compared to simply selecting the best response out of a larger number of candidates.", "section": "6.2 Benchmark Performance"}, {"figure_path": "https://arxiv.org/html/2501.12895/x9.png", "caption": "Figure 7: Test-time training curve of Llama-3.1-8B-Instruct (red line) on the HH-RLHF dataset.", "description": "This figure displays the performance of the Llama-3.1-8B-Instruct model on the HH-RLHF dataset during test-time training. The test-time training involves iteratively refining the model's output using the TPO method. The x-axis represents the number of TPO iterations, while the y-axis shows the reward model score, indicating the model's alignment with human preferences. The red line shows that the Llama-3.1-8B-Instruct model fails to improve its alignment with the reward model during the test-time training process. The flat trend of the red line indicates that the test-time optimization does not bring any improvements on this model.", "section": "6.1 Test-time Training"}, {"figure_path": "https://arxiv.org/html/2501.12895/x10.png", "caption": "Figure 8: Test-time training curves of the Llama-3.1-70B-SFT using FsfairX-LLaMA3-RM-v0.1.", "description": "This figure displays the performance of the Llama-3.1-70B-SFT model over multiple iterations of test-time preference optimization (TPO) using FsfairX-LLaMA3-RM-v0.1 as the reward model.  The x-axis represents the number of TPO steps, and the y-axis shows the reward model score. Separate plots are shown for different benchmark datasets: AlpacaEval 2, Arena-Hard, BeaverTails, XSTest, HH-RLHF, and MATH-500. The figure illustrates the progressive alignment of the model with the reward model's preferences during the test-time optimization process. Dashed lines indicate the reward scores before test-time training.", "section": "6.1. Test-time Training"}]