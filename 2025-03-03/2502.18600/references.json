{"references": [{"fullname_first_author": "Wei, Jason", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper is foundational, as it introduces the Chain-of-Thought (CoT) prompting strategy, which significantly improves the reasoning capabilities of LLMs and serves as a baseline for comparison in this paper."}, {"fullname_first_author": "Brown, Tom", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper demonstrates the capabilities of LLMs as few-shot learners, a fundamental concept used as a baseline for standard prompting in this paper's experiments."}, {"fullname_first_author": "Kojima, Takeshi", "paper_title": "Large language models are zero-shot reasoners", "publication_date": "2022-01-01", "reason": "This paper builds upon CoT by demonstrating that LLMs can be zero-shot reasoners, further solidifying the importance of reasoning capabilities in LLMs, and is used as a basis for CoT implementation."}, {"fullname_first_author": "Wang, Xuezhi", "paper_title": "Self-consistency improves chain of thought reasoning in language models", "publication_date": "2022-03-01", "reason": "This paper introduces self-consistency CoT, enhancing reasoning reliability, and it represents a key improvement in the CoT methodology."}, {"fullname_first_author": "Cobbe, Karl", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-01", "reason": "This paper introduces GSM8k, the benchmark dataset used for arithmetic reasoning, making it important for evaluating the proposed method's performance in mathematical tasks."}]}