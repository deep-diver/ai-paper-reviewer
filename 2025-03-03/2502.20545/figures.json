[{"figure_path": "https://arxiv.org/html/2502.20545/x1.png", "caption": "Figure 1: Demonstration of SoS Plain (left), SoS Simple (mid), and SoS Reasoning (right).", "description": "This figure compares three different prompting methods used to guide large language models (LLMs) in determining whether a given polynomial can be expressed as a sum of squares (SoS).  SoS Plain shows a simple question with no additional guidance. SoS Simple provides a structured classification based on simple criteria, while SoS Reasoning offers a detailed, step-by-step approach with progressively challenging criteria and expert-designed reasoning instructions. The figure highlights the increasing level of detail and guidance provided to the LLM, demonstrating how the structured prompts improve the accuracy and reasoning process.", "section": "2 SoS-1K Dataset"}, {"figure_path": "https://arxiv.org/html/2502.20545/x2.png", "caption": "Figure 2: Accuracy of different test sets using o1-mini.", "description": "This figure shows the accuracy of the OpenAI 01-mini language model on various subsets of the SoS-1K dataset, categorized by difficulty. It compares the model's performance under three different prompting conditions: SoS Plain (a simple question), SoS Simple (with simple instructions), and SoS Reasoning (with detailed, step-by-step instructions). The x-axis represents the test sets and the y-axis represents the accuracy. The figure illustrates how the quality of instructions significantly affects the model's ability to solve the SoS problem. Each test set has varying degrees of difficulty with some focusing on specific properties of SOS polynomials.", "section": "3 Evaluation of LLMs on SoS"}, {"figure_path": "https://arxiv.org/html/2502.20545/x3.png", "caption": "Figure 3: Number of correct samples with various response lengths.", "description": "This figure shows the relationship between the length of the LLMs' responses (in thousands of tokens) and the number of correct answers they provide for SoS problems.  It demonstrates how different models' performance varies depending on the length of their reasoning processes. For example, some models achieve peak accuracy at a shorter response length, while others require longer reasoning chains to get more correct answers.  This illustrates the effect of 'thinking steps' on accuracy, revealing different capabilities between different models.", "section": "Evaluation of LLMs on SoS"}]