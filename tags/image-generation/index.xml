<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Image Generation on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/image-generation/</link><description>Recent content in Image Generation on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 25 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/image-generation/index.xml" rel="self" type="application/rss+xml"/><item><title>Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19385/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19385/</guid><description>Inference-time scaling for flow models enhances alignment with user preferences via stochastic generation and budget allocation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19385/cover.png"/></item><item><title>CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18886/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18886/</guid><description>CFG-Zero*: A better Classifier-Free Guidance to improve the image quality and text alignment in Flow Matching models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18886/cover.png"/></item><item><title>Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18352/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18352/</guid><description>Diffusion-4K: Synthesizing ultra-high-resolution images with a new benchmark dataset and wavelet-based fine-tuning that makes 4K image creation more detailed and accessible!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18352/cover.png"/></item><item><title>Equivariant Image Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18948/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18948/</guid><description>Aligning image generation subtasks: Equivariant modeling boosts efficiency and generalization by leveraging natural visual signal invariance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18948/cover.png"/></item><item><title>Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18446/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18446/</guid><description>LSRNA: Super-resolution in latent space enhances image generation with diffusion models, achieving faster speeds and improved detail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.18446/cover.png"/></item><item><title>Training-free Diffusion Acceleration with Bottleneck Sampling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18940/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18940/</guid><description>Bottleneck Sampling: Accelerate diffusion models &lt;em>without&lt;/em> retraining by cleverly using low-resolution priors for efficient inference!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18940/cover.png"/></item><item><title>RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17735/</link><pubDate>Sat, 22 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17735/</guid><description>RDTF: Efficient animated sticker generation via dual-mask training, outperforming parameter-efficient tuning under constrained resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17735/cover.png"/></item><item><title>When Preferences Diverge: Aligning Diffusion Models with Minority-Aware Adaptive DPO</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16921/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16921/</guid><description>Adaptive Diffusion Models with Minority-Aware Adaptive DPO</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16921/cover.png"/></item><item><title>Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16430/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16430/</guid><description>TokenBridge bridges continuous and discrete tokens for autoregressive visual generation, achieving high-quality synthesis with simple autoregressive modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16430/cover.png"/></item><item><title>Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16057/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16057/</guid><description>Expert Race: A flexible routing strategy for scaling diffusion transformer with mixture of experts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16057/cover.png"/></item><item><title>Improving Autoregressive Image Generation through Coarse-to-Fine Token Prediction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16194/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16194/</guid><description>Coarse-to-Fine Token Prediction improves autoregressive image generation by assigning the same coarse label for similar tokens, balancing generation quality and computational efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16194/cover.png"/></item><item><title>InfiniteYou: Flexible Photo Recrafting While Preserving Your Identity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16418/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16418/</guid><description>InfU: A new framework for flexible photo re-creation while preserving identity using Diffusion Transformers(DiTs).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16418/cover.png"/></item><item><title>MagicMotion: Controllable Video Generation with Dense-to-Sparse Trajectory Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16421/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16421/</guid><description>MagicMotion: A controllable video generation framework enabling precise object motion control through dense-to-sparse trajectory guidance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16421/cover.png"/></item><item><title>Scale-wise Distillation of Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16397/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16397/</guid><description>SWD: Scale-wise distillation of diffusion models achieves faster image generation by upscaling resolution during denoising, outperforming counterparts with similar computation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16397/cover.png"/></item><item><title>Tokenize Image as a Set</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16425/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16425/</guid><description>TokenSet: Tokenizing images as unordered sets for dynamic capacity allocation and robust generation, breaking from fixed-position latent codes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16425/cover.png"/></item><item><title>Ultra-Resolution Adaptation with Ease</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16322/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16322/</guid><description>URA: Ultra-resolution adaptation made easy! Uses synthetic data &amp;amp; minor weight tuning for efficient, high-res text-to-image diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16322/cover.png"/></item><item><title>Efficient Personalization of Quantized Diffusion Model without Backpropagation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14868/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14868/</guid><description>Personalize diffusion models efficiently on devices without backpropagation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14868/cover.png"/></item><item><title>LEGION: Learning to Ground and Explain for Synthetic Image Detection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15264/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15264/</guid><description>LEGION: Grounding and explaining synthetic image detection and refinement via multimodal learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15264/cover.png"/></item><item><title>Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.14905/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.14905/</guid><description>FakeVLM: A multimodal model &amp;amp; artifact-annotated dataset for detecting synthetic images with interpretable explanations, setting a new benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.14905/cover.png"/></item><item><title>Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14492/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14492/</guid><description>Cosmos-Transfer1: An adaptable conditional world generation model using multimodal control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14492/cover.png"/></item><item><title>DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14487/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14487/</guid><description>DiffMoE: Dynamically selects tokens for scalable diffusion transformers, unlocking new efficiency levels in image generation.</description></item><item><title>BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13434/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13434/</guid><description>BlobCtrl: Precisely edit images at the element level with a unified, flexible framework, bridging the gap between generation and editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13434/cover.png"/></item><item><title>DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12885/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12885/</guid><description>DreamRenderer: Taming attribute control in large-scale text-to-image models with a plug-and-play, training-free approach for enhanced content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12885/cover.png"/></item><item><title>Edit Transfer: Learning Image Editing via Vision In-Context Relations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13327/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13327/</guid><description>Edit Transfer: Learns image edits from a single example and applies it to new images, surpassing text/reference-based methods!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13327/cover.png"/></item><item><title>Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13070/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13070/</guid><description>Rewards Are Enough!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13070/cover.png"/></item><item><title>Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12271/</link><pubDate>Sat, 15 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12271/</guid><description>Reflect-DiT: Scaling Text-to-Image Diffusion Transformers via In-Context Reflection!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12271/cover.png"/></item><item><title>Autoregressive Image Generation with Randomized Parallel Decoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10568/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10568/</guid><description>ARPG: Randomly generate high-quality images by parallel decoding, outperforming existing methods in efficiency, memory, and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10568/cover.png"/></item><item><title>CoSTA$st$: Cost-Sensitive Toolpath Agent for Multi-turn Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/</guid><description>COSTA*: A cost-effective agent that smartly navigates AI tools to edit images with high quality and low cost, balancing user preferences!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10613/cover.png"/></item><item><title>Distilling Diversity and Control in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10637/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10637/</guid><description>Distilling diffusion models?ðŸ’¡ This paper shows you how to retain base model diversity while keeping the distilled model&amp;rsquo;s speed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10637/cover.png"/></item><item><title>GoT: Unleashing Reasoning Capability of Multimodal Large Language Model for Visual Generation and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10639/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10639/</guid><description>GoT: Reasoning guides vivid image generation and editing!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10639/cover.png"/></item><item><title>Neighboring Autoregressive Modeling for Efficient Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10696/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10696/</guid><description>NAR: Neighboring Autoregressive Modeling for efficient visual generation by locality-preserved, parallel decoding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10696/cover.png"/></item><item><title>PerCoV2: Improved Ultra-Low Bit-Rate Perceptual Image Compression with Implicit Hierarchical Masked Image Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09368/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09368/</guid><description>PerCoV2: Open ultra-low bit-rate perceptual image compression using implicit hierarchical masked image modeling, built on Stable Diffusion 3 for bandwidth-constrained applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09368/cover.png"/></item><item><title>SANA-Sprint: One-Step Diffusion with Continuous-Time Consistency Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09641/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09641/</guid><description>SANA-Sprint: An efficient diffusion model for ultra-fast text-to-image generation with continuous-time consistency distillation, achieving state-of-the-art performance in speed and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09641/cover.png"/></item><item><title>Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09669/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09669/</guid><description>New &amp;lsquo;Silent Branding Attack&amp;rsquo; poisons text-to-image models, embedding brand logos without text prompts, raising ethical issues for image generation tools.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09669/cover.png"/></item><item><title>LightGen: Efficient Image Generation through Knowledge Distillation and Direct Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08619/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08619/</guid><description>LightGen: Efficient image generation via knowledge distillation and direct preference optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08619/cover.png"/></item><item><title>EasyControl: Adding Efficient and Flexible Control for Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07027/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07027/</guid><description>EasyControl: Efficient &amp;amp; flexible control for Diffusion Transformers, enabling sophisticated image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07027/cover.png"/></item><item><title>Effective and Efficient Masked Image Generation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07197/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07197/</guid><description>eMIGM: A unified, efficient masked image generation model achieving state-of-the-art performance with fewer resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07197/cover.png"/></item><item><title>PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07677/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07677/</guid><description>PLADIS: Sparsity boosts attention for diffusion models, enhancing text-to-image generation at inference time!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07677/cover.png"/></item><item><title>RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow Trajectories</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07699/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07699/</guid><description>RayFlow: Accelerating diffusion with instance-aware adaptive flow, boosting speed &amp;amp; quality!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07699/cover.png"/></item><item><title>Seedream 2.0: A Native Chinese-English Bilingual Image Generation Foundation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07703/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07703/</guid><description>Seedream 2.0: A native Chinese-English bilingual image generation model that understands cultural nuances and excels in text rendering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07703/cover.png"/></item><item><title>WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07265/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07265/</guid><description>WISE: Evaluates world knowledge in text-to-image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07265/cover.png"/></item><item><title>Learning Few-Step Diffusion Models by Trajectory Distribution Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06674/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06674/</guid><description>TDM: a new diffusion distillation paradigm unifying trajectory distillation and distribution matching, surpassing teachers in a data-free manner with state-of-the-art performance and low training cost&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06674/cover.png"/></item><item><title>ProReflow: Progressive Reflow with Decomposed Velocity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04824/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04824/</guid><description>ProReflow: Improves diffusion model efficiency via progressive training and direction-focused velocity alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04824/cover.png"/></item><item><title>Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02357/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02357/</guid><description>Q-Eval-100K: A new, large dataset for evaluating visual quality and text alignment in AI-generated content.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02357/cover.png"/></item><item><title>RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02537/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02537/</guid><description>RectifiedHR: Enables training-free high-resolution image generation via energy rectification, boosting both efficiency and effectiveness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02537/cover.png"/></item><item><title>Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01103/</link><pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01103/</guid><description>Likelihood-based generative models get a GAN-like boost via a new Direct Discriminative Optimization, ditching the joint training complexity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01103/cover.png"/></item><item><title>Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20172/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20172/</guid><description>DREAM ENGINE: Text-image interleaved control made easy, unifying text and visual cues for creative image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20172/cover.png"/></item><item><title>GCC: Generative Color Constancy via Diffusing a Color Checker</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17435/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17435/</guid><description>GCC: Color constancy through diffusion, inpainting a color checker for stable illumination estimation.</description></item><item><title>M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15167/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15167/</guid><description>M3-AGIQA: A multimodal AI solution that comprehensively assesses AI-generated image quality, achieving state-of-the-art performance by distilling online MLLM capabilities into a local model.</description></item><item><title>PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14397/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14397/</guid><description>PhotoDoodle: Mimicking artistic image editing with personalized decorative elements through learning from few-shot pairwise data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14397/cover.png"/></item><item><title>RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14377/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14377/</guid><description>RelaCtrl: Relevance-guided control boosts diffusion transformer efficiency, cutting parameters by intelligently allocating resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14377/cover.png"/></item><item><title>Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12146/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12146/</guid><description>Diffusion-Sharpening enhances diffusion model fine-tuning by optimizing sampling trajectories, achieving faster convergence and high inference efficiency without extra NFEs, leading to improved alignm&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12146/cover.png"/></item><item><title>Magic 1-For-1: Generating One Minute Video Clips within One Minute</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07701/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07701/</guid><description>Magic141 generates one-minute video clips in under a minute by cleverly factorizing the generation task and employing optimization techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07701/cover.png"/></item><item><title>MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07856/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07856/</guid><description>MRS: a novel, training-free sampler, drastically speeds up controllable image generation using Mean Reverting Diffusion, achieving 10-20x speedup across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07856/cover.png"/></item><item><title>VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07531/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07531/</guid><description>VidCRAFT3 enables high-quality image-to-video generation with precise control over camera movement, object motion, and lighting, pushing the boundaries of visual content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07531/cover.png"/></item><item><title>Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06145/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06145/</guid><description>Animate Anyone 2 creates high-fidelity character animations by incorporating environmental context, resulting in seamless character-environment integration and more realistic object interactions.</description></item><item><title>CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/</guid><description>CustomVideoX: Zero-shot personalized video generation, exceeding existing methods in quality &amp;amp; consistency via 3D reference attention and dynamic adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/cover.png"/></item><item><title>Dual Caption Preference Optimization for Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06023/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06023/</guid><description>Dual Caption Preference Optimization (DCPO) significantly boosts diffusion model image quality by using paired captions to resolve data distribution conflicts and irrelevant prompt issues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06023/cover.png"/></item><item><title>Goku: Flow Based Video Generative Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04896/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04896/</guid><description>Goku: a novel family of joint image-and-video generation models uses rectified flow Transformers, achieving industry-leading performance with a robust data pipeline and training infrastructure.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04896/cover.png"/></item><item><title>On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04363/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04363/</guid><description>On-device Sora makes high-quality, diffusion-based text-to-video generation possible on smartphones, overcoming computational and memory limitations through novel techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04363/cover.png"/></item><item><title>Improved Training Technique for Latent Consistency Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01441/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01441/</guid><description>Researchers significantly enhance latent consistency models&amp;rsquo; performance by introducing Cauchy loss, mitigating outlier effects, and employing novel training strategies, thus bridging the gap with dif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01441/cover.png"/></item><item><title>Inverse Bridge Matching Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01362/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01362/</guid><description>Boosting Diffusion Bridge Models: A new distillation technique accelerates inference speed by 4x to 100x, sometimes even improving image quality!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01362/cover.png"/></item><item><title>LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01105/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01105/</guid><description>LayerTracer innovatively synthesizes cognitive-aligned layered SVGs via diffusion transformers, bridging the gap between AI and professional design standards by learning from a novel dataset of sequen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01105/cover.png"/></item><item><title>OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01061/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01061/</guid><description>OmniHuman-1: Scaling up one-stage conditioned human animation through novel mixed-condition training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01061/cover.png"/></item><item><title>DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16764/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16764/</guid><description>DIFFSPLAT repurposes 2D image diffusion models to natively generate high-quality 3D Gaussian splats, overcoming limitations in existing 3D generation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16764/cover.png"/></item><item><title>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13926/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13926/</guid><description>Researchers significantly enhanced autoregressive image generation by integrating chain-of-thought reasoning strategies, achieving a remarkable +24% improvement on the GenEval benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13926/cover.png"/></item><item><title>GPS as a Control Signal for Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12390/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12390/</guid><description>GPS-guided image generation is here! This paper leverages GPS data to create highly realistic images reflecting specific locations, even reconstructing 3D models from 2D photos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12390/cover.png"/></item><item><title>TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/</guid><description>TokenVerse: Extract &amp;amp; combine visual concepts from multiple images for creative image generation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/cover.png"/></item><item><title>EMO2: End-Effector Guided Audio-Driven Avatar Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10687/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10687/</guid><description>EMO2 achieves realistic audio-driven avatar video generation by employing a two-stage framework: first generating hand poses directly from audio and then using a diffusion model to synthesize full-bod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10687/cover.png"/></item><item><title>Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10020/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10020/</guid><description>Textoon: Generating vivid 2D cartoon characters from text descriptions in under a minute, revolutionizing animation workflow.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10020/cover.png"/></item><item><title>X-Dyna: Expressive Dynamic Human Image Animation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10021/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10021/</guid><description>X-Dyna: a novel diffusion-based pipeline generates realistic human image animation using a zero-shot approach by integrating a Dynamics-Adapter for dynamic detail preservation, exceeding state-of-the-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10021/cover.png"/></item><item><title>AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09503/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09503/</guid><description>AnyStory: A unified framework enables high-fidelity personalized image generation for single and multiple subjects, addressing subject fidelity challenges in existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09503/cover.png"/></item><item><title>Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09732/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09732/</guid><description>Boosting diffusion model performance at inference time, this research introduces a novel framework that goes beyond simply increasing denoising steps. By cleverly searching for better noise candidates&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09732/cover.png"/></item><item><title>Learnings from Scaling Visual Tokenizers for Reconstruction and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09755/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09755/</guid><description>Scaling visual tokenizers dramatically improves image and video generation, achieving state-of-the-art results and outperforming existing methods with fewer computations by focusing on decoder scaling&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09755/cover.png"/></item><item><title>SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09756/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09756/</guid><description>SynthLight: A novel diffusion model relights portraits realistically by learning to re-render synthetic faces, generalizing remarkably well to real photographs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09756/cover.png"/></item><item><title>The GAN is dead; long live the GAN! A Modern GAN Baseline</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05441/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05441/</guid><description>R3GAN: A modernized GAN baseline achieves state-of-the-art results with a simple, stable loss function and modern architecture, debunking the myth that GANs are hard to train.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05441/cover.png"/></item><item><title>On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04377/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04377/</guid><description>This paper unveils critical thresholds for efficient visual autoregressive model computation, proving sub-quartic time is impossible beyond a certain input matrix norm while establishing efficient app&amp;hellip;</description></item><item><title>Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</guid><description>Through-The-Mask uses mask-based motion trajectories to generate realistic videos from images and text, overcoming limitations of existing methods in handling complex multi-object motion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/cover.png"/></item><item><title>MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02260/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02260/</guid><description>MagicFace achieves high-fidelity facial expression editing via AU control, preserving identity and background using a diffusion model and ID encoder, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02260/cover.png"/></item><item><title>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</guid><description>LightningDiT resolves the optimization dilemma in latent diffusion models by aligning latent space with pre-trained vision models, achieving state-of-the-art ImageNet 256x256 generation with over 21x &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/cover.png"/></item><item><title>Edicho: Consistent Image Editing in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</guid><description>Edicho: a novel training-free method for consistent image editing across diverse images, achieving precise consistency by leveraging explicit correspondence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/cover.png"/></item><item><title>VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/</guid><description>VisionReward, a novel reward model, surpasses existing methods by precisely capturing multi-dimensional human preferences for image and video generation, enabling more accurate and stable model optimi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/cover.png"/></item><item><title>Bringing Objects to Life: 4D generation from 3D objects</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</link><pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</guid><description>3to4D: Animate any 3D object with text prompts, preserving visual quality and achieving realistic motion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/cover.png"/></item><item><title>VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</guid><description>VideoMaker achieves high-fidelity zero-shot customized video generation by cleverly harnessing the inherent power of video diffusion models, eliminating the need for extra feature extraction and injec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/cover.png"/></item><item><title>Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</guid><description>Distilled Decoding (DD) drastically speeds up image generation from autoregressive models by using flow matching to enable one-step sampling, achieving significant speedups while maintaining acceptabl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/cover.png"/></item><item><title>CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</guid><description>CLEAR: Conv-Like Linearization boosts pre-trained Diffusion Transformers, achieving 6.3x faster 8K image generation with minimal quality loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/cover.png"/></item><item><title>Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</guid><description>Affordance-Aware Object Insertion uses a novel Mask-Aware Dual Diffusion model &amp;amp; SAM-FB dataset to realistically place objects in scenes, considering contextual relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/cover.png"/></item><item><title>Parallelized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</guid><description>Boosting autoregressive visual generation speed by 3.6-9.5x, this research introduces parallel processing while preserving model simplicity and generation quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/cover.png"/></item><item><title>UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</guid><description>UIP2P: Unsupervised instruction-based image editing achieves high-fidelity edits by enforcing Cycle Edit Consistency, eliminating the need for ground-truth data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/cover.png"/></item><item><title>FashionComposer: Compositional Fashion Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</guid><description>FashionComposer revolutionizes fashion image creation through flexible composition of garments, faces, and poses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/cover.png"/></item><item><title>ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</guid><description>ChatDiT enables zero-shot, multi-turn image generation using pretrained diffusion transformers and a novel multi-agent framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/cover.png"/></item><item><title>ColorFlow: Retrieval-Augmented Image Sequence Colorization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</guid><description>ColorFlow, a new AI model, accurately colorizes black-and-white image sequences while preserving character identity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/cover.png"/></item><item><title>BrushEdit: All-In-One Image Inpainting and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</guid><description>BrushEdit revolutionizes interactive image editing with instructions &amp;amp; inpainting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/cover.png"/></item><item><title>Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</guid><description>New attack fools breast ultrasound AI using subtle text prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/cover.png"/></item><item><title>Arbitrary-steps Image Super-resolution via Diffusion Inversion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/</guid><description>InvSR: a novel image super-resolution technique using diffusion inversion, enabling flexible sampling steps for efficient and high-fidelity results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/cover.png"/></item><item><title>DisPose: Disentangling Pose Guidance for Controllable Human Image Animation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/</guid><description>DisPose disentangles pose guidance for controllable human image animation, generating diverse animations while preserving appearance consistency using only sparse skeleton pose input, eliminating the &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/cover.png"/></item><item><title>EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/</guid><description>EasyRef uses multimodal LLMs to generate images from multiple references, overcoming limitations of prior methods by capturing consistent visual elements and offering improved zero-shot generalization&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/cover.png"/></item><item><title>FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</guid><description>Edit images precisely with AI, no masks needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/cover.png"/></item><item><title>FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</guid><description>FreeScale generates stunning 8K images and high-fidelity videos without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/cover.png"/></item><item><title>LoRACLR: Contrastive Adaptation for Customization of Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/</guid><description>LoRACLR merges multiple LoRA models for high-fidelity multi-concept image generation, using a contrastive objective to ensure concept distinctiveness and prevent interference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/cover.png"/></item><item><title>DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/</guid><description>DiffSensei: A new framework generates customized manga with dynamic multi-character control using multi-modal LLMs and diffusion models, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/cover.png"/></item><item><title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</guid><description>Introducing Evaluation Agent, a faster, more flexible human-like framework for evaluating visual generative AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/cover.png"/></item><item><title>FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/</guid><description>FireFlow makes editing images faster and better.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/cover.png"/></item><item><title>FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/</guid><description>FiVA dataset and its adaptation framework enable unprecedented fine-grained control over visual attributes in text-to-image generation, empowering users to craft highly customized images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/cover.png"/></item><item><title>STIV: Scalable Text and Image Conditioned Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/</guid><description>STIV: A novel, scalable method for text and image-conditioned video generation, systematically improving model architectures, training, and data curation for superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/cover.png"/></item><item><title>UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/</guid><description>UniReal: a universal framework for image generation and editing, unifying diverse tasks via learning real-world dynamics from video data, achieving highly realistic and versatile results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/cover.png"/></item><item><title>AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/</guid><description>AnyDressing: Customizable multi-garment virtual dressing via a novel latent diffusion model!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/cover.png"/></item><item><title>Hidden in the Noise: Two-Stage Robust Watermarking for Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/</guid><description>WIND: A novel, distortion-free image watermarking method leveraging diffusion models&amp;rsquo; initial noise for robust AI-generated content authentication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/cover.png"/></item><item><title>HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/</guid><description>HumanEdit: A new human-rewarded dataset revolutionizes instruction-based image editing by providing high-quality, diverse image pairs with detailed instructions, enabling precise model evaluation and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/cover.png"/></item><item><title>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/</guid><description>Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/cover.png"/></item><item><title>SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/</guid><description>SwiftEdit achieves lightning-fast, high-quality text-guided image editing in just 0.23 seconds via a novel one-step diffusion process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/cover.png"/></item><item><title>ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</guid><description>ZipAR accelerates autoregressive image generation by up to 91% through parallel decoding leveraging spatial locality in images, making high-resolution image generation significantly faster.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/cover.png"/></item><item><title>CleanDIFT: Diffusion Features without Noise</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/</guid><description>CleanDIFT revolutionizes diffusion feature extraction by leveraging clean images and a lightweight fine-tuning method, significantly boosting performance across various tasks without noise or timestep&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/cover.png"/></item><item><title>Imagine360: Immersive 360 Video Generation from Perspective Anchor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/</guid><description>Imagine360: Generating immersive 360Â° videos from perspective videos, improving quality and accessibility of 360Â° content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/cover.png"/></item><item><title>MV-Adapter: Multi-view Consistent Image Generation Made Easy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/</guid><description>MV-Adapter easily transforms existing image generators into multi-view consistent image generators, improving efficiency and adaptability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/cover.png"/></item><item><title>NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</guid><description>NVComposer: A novel generative NVS model boosts synthesis quality by implicitly inferring spatial relationships from multiple sparse, unposed images, eliminating reliance on external alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/cover.png"/></item><item><title>Scaling Image Tokenizers with Grouped Spherical Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/</guid><description>GSQ-GAN, a novel image tokenizer, achieves superior reconstruction quality with 16x downsampling using grouped spherical quantization, enabling efficient scaling for high-fidelity image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/cover.png"/></item><item><title>SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/</guid><description>SNOOPI supercharges one-step diffusion model distillation with enhanced guidance, achieving state-of-the-art performance by stabilizing training and enabling negative prompt control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/cover.png"/></item><item><title>Negative Token Merging: Image-based Adversarial Feature Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/</guid><description>NegToMe: Image-based adversarial guidance improves image generation diversity and reduces similarity to copyrighted content without training, simply by using images instead of negative text prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/cover.png"/></item><item><title>NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/</guid><description>NitroFusion achieves high-fidelity single-step image generation using a dynamic adversarial training approach with a specialized discriminator pool, dramatically improving speed and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/cover.png"/></item><item><title>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</guid><description>SWITTI: a novel scale-wise transformer achieves 7x faster text-to-image generation than state-of-the-art diffusion models, while maintaining competitive image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/cover.png"/></item><item><title>TinyFusion: Diffusion Transformers Learned Shallow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</guid><description>TinyFusion, a novel learnable depth pruning method, crafts efficient shallow diffusion transformers with superior post-fine-tuning performance, achieving a 2x speedup with less than 7% of the original&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/cover.png"/></item><item><title>Open-Sora Plan: Open-Source Large Video Generation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</guid><description>Open-Sora Plan introduces an open-source large video generation model capable of producing high-resolution videos with long durations, based on various user inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/cover.png"/></item><item><title>FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</guid><description>FAM Diffusion: Generate high-res images seamlessly from pre-trained diffusion models, solving structural and texture inconsistencies without retraining!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/cover.png"/></item><item><title>ROICtrl: Boosting Instance Control for Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</guid><description>ROICtrl boosts visual generation&amp;rsquo;s instance control by using regional instance control via ROI-Align and a new ROI-Unpool operation, resulting in precise regional control and high efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/cover.png"/></item><item><title>TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</guid><description>TryOffDiff generates realistic garment images from single photos, solving virtual try-on limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/cover.png"/></item><item><title>AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</guid><description>AnchorCrafter animates cyber-anchors selling products via human-object interacting video generation, achieving high visual fidelity and controllable interactions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/cover.png"/></item><item><title>ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</guid><description>ChatGen-Evo automates text-to-image generation from freestyle chatting, simplifying the process and significantly improving performance over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/cover.png"/></item><item><title>Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</guid><description>Collaborative Decoding (CoDe) dramatically boosts visual auto-regressive model efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/cover.png"/></item><item><title>DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</guid><description>DreamCache enables efficient, high-quality personalized image generation without finetuning by caching reference image features and using lightweight conditioning adapters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/cover.png"/></item><item><title>DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</guid><description>DreamMix enhances image inpainting by disentangling object attributes for precise editing, enabling both identity preservation and flexible text-driven modifications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/cover.png"/></item><item><title>Identity-Preserving Text-to-Video Generation by Frequency Decomposition</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</guid><description>ConsisID achieves high-quality, identity-preserving text-to-video generation using a tuning-free diffusion transformer model that leverages frequency decomposition for effective identity control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/cover.png"/></item><item><title>Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</guid><description>Omegance: One parameter precisely controls image detail in diffusion models, enabling flexible granularity adjustments without model changes or retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/cover.png"/></item><item><title>Controllable Human Image Generation with Personalized Multi-Garments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</guid><description>BootComp: generate realistic human images wearing multiple garments using a novel synthetic data pipeline &amp;amp; diffusion model, enabling diverse applications like virtual try-on.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/cover.png"/></item><item><title>Factorized Visual Tokenization and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</guid><description>FQGAN revitalizes image generation by introducing Factorized Quantization, enabling scalable and stable visual tokenization with state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/cover.png"/></item><item><title>One Diffusion to Generate Them All</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</guid><description>OneDiffusion: A single diffusion model masters image synthesis &amp;amp; understanding across diverse tasks, from text-to-image to depth estimation, pushing the boundaries of AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/cover.png"/></item><item><title>Pathways on the Image Manifold: Image Editing via Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</guid><description>Image editing is revolutionized by Frame2Frame, which uses video generation to produce seamless and accurate edits, preserving image fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/cover.png"/></item><item><title>Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</guid><description>New benchmark VCTÂ² reveals limitations of AI-generated image detectors; Visual AI Index (VAI) provides a robust evaluation framework.</description></item><item><title>Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</guid><description>Diptych Prompting: a novel zero-shot subject-driven image generator leveraging large-scale text-to-image models and inpainting for precise subject alignment and high-quality image synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/cover.png"/></item><item><title>Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</guid><description>Morph: a novel motion-free physics optimization framework drastically enhances human motion generation&amp;rsquo;s physical plausibility using synthetic data, achieving state-of-the-art quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/cover.png"/></item><item><title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</guid><description>OminiControl: A minimal, universal framework efficiently integrates image conditions into diffusion transformers, enabling diverse and precise control over image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/cover.png"/></item><item><title>Style-Friendly SNR Sampler for Style-Driven Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</guid><description>Style-friendly SNR sampler biases diffusion model training towards higher noise levels, enabling it to learn and generate images with higher style fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/cover.png"/></item><item><title>TEXGen: a Generative Diffusion Model for Mesh Textures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</guid><description>TEXGen: A groundbreaking generative diffusion model creates high-resolution 3D mesh textures directly from text and image prompts, exceeding prior methods in quality and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/cover.png"/></item><item><title>MyTimeMachine: Personalized Facial Age Transformation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</guid><description>MyTimeMachine personalizes facial age transformation using just 50 personal photos, outperforming existing methods by generating re-aged faces that closely match a person&amp;rsquo;s actual appearance at variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/cover.png"/></item><item><title>Stable Flow: Vital Layers for Training-Free Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</guid><description>Stable Flow achieves diverse, consistent image editing without training by strategically injecting source image features into specific &amp;lsquo;vital&amp;rsquo; layers of a diffusion transformer model. This training-f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/cover.png"/></item><item><title>VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</guid><description>VBench++: A new benchmark suite meticulously evaluates video generative models across 16 diverse dimensions, aligning with human perception for improved model development and fairer comparisons.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/cover.png"/></item><item><title>Stylecodes: Encoding Stylistic Information For Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</guid><description>StyleCodes enables easy style sharing for image generation by encoding styles as compact strings, enhancing control and collaboration while minimizing quality loss.</description></item><item><title>Continuous Speculative Decoding for Autoregressive Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</guid><description>Researchers have developed Continuous Speculative Decoding, boosting autoregressive image generation speed by up to 2.33x while maintaining image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/cover.png"/></item><item><title>FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</guid><description>FitDiT boosts virtual try-on realism by enhancing garment details via Diffusion Transformers, improving texture and size accuracy for high-fidelity virtual fashion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/cover.png"/></item><item><title>MagicQuill: An Intelligent Interactive Image Editing System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</guid><description>MagicQuill: an intelligent interactive image editing system enabling intuitive, precise image edits via brushstrokes and real-time intent prediction by a multimodal LLM.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/cover.png"/></item><item><title>Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</guid><description>Add-it: Training-free object insertion in images using pretrained diffusion models by cleverly balancing information from the scene, text prompt, and generated image, achieving state-of-the-art result&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/cover.png"/></item><item><title>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</guid><description>Edify Image: groundbreaking pixel-perfect photorealistic image generation using cascaded pixel-space diffusion models with a novel Laplacian diffusion process, enabling diverse applications including &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/cover.png"/></item><item><title>OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</guid><description>OmniEdit, a novel instruction-based image editing model, surpasses existing methods by leveraging specialist supervision and high-quality data, achieving superior performance across diverse editing ta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/cover.png"/></item><item><title>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</guid><description>StdGEN: Generate high-quality, semantically decomposed 3D characters from a single image in minutes, enabling flexible customization for various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/cover.png"/></item><item><title>SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</guid><description>SG-I2V: Zero-shot controllable image-to-video generation using a self-guided approach that leverages pre-trained models for precise object and camera motion control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/cover.png"/></item><item><title>SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</guid><description>SVDQuant boosts 4-bit diffusion models by absorbing outliers via low-rank components, achieving 3.5x memory reduction and 3x speedup on 12B parameter models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/cover.png"/></item><item><title>Training-free Regional Prompting for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</guid><description>Training-free Regional Prompting for FLUX boosts compositional text-to-image generation by cleverly manipulating attention mechanisms, achieving fine-grained control without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/cover.png"/></item><item><title>Constant Acceleration Flow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</guid><description>Constant Acceleration Flow (CAF) dramatically speeds up diffusion model generation by using a constant acceleration equation, outperforming state-of-the-art methods with improved accuracy and few-step&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/cover.png"/></item><item><title>Randomized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</guid><description>Randomized Autoregressive Modeling (RAR) sets a new state-of-the-art in image generation by cleverly introducing randomness during training to improve the model&amp;rsquo;s ability to learn from bidirectional c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/cover.png"/></item><item><title>In-Context LoRA for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</guid><description>In-Context LoRA empowers existing text-to-image models for high-fidelity multi-image generation by simply concatenating images and using minimal task-specific LoRA tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/cover.png"/></item><item><title>HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</guid><description>HelloMeme enhances text-to-image models by integrating spatial knitting attentions, enabling high-fidelity meme video generation while preserving model generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/cover.png"/></item></channel></rss>