<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Image Generation on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/image-generation/</link><description>Recent content in Image Generation on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Mon, 02 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/image-generation/index.xml" rel="self" type="application/rss+xml"/><item><title>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</guid><description>SWITTI: a novel scale-wise transformer achieves 7x faster text-to-image generation than state-of-the-art diffusion models, while maintaining competitive image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/cover.png"/></item><item><title>TinyFusion: Diffusion Transformers Learned Shallow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</guid><description>TinyFusion, a novel learnable depth pruning method, crafts efficient shallow diffusion transformers with superior post-fine-tuning performance, achieving a 2x speedup with less than 7% of the original&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/cover.png"/></item><item><title>Open-Sora Plan: Open-Source Large Video Generation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</guid><description>Open-Sora Plan introduces an open-source large video generation model capable of producing high-resolution videos with long durations, based on various user inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/cover.png"/></item><item><title>FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</guid><description>FAM Diffusion: Generate high-res images seamlessly from pre-trained diffusion models, solving structural and texture inconsistencies without retraining!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/cover.png"/></item><item><title>ROICtrl: Boosting Instance Control for Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</guid><description>ROICtrl boosts visual generation&amp;rsquo;s instance control by using regional instance control via ROI-Align and a new ROI-Unpool operation, resulting in precise regional control and high efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/cover.png"/></item><item><title>TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</guid><description>TryOffDiff generates realistic garment images from single photos, solving virtual try-on limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/cover.png"/></item><item><title>AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</guid><description>AnchorCrafter animates cyber-anchors selling products via human-object interacting video generation, achieving high visual fidelity and controllable interactions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/cover.png"/></item><item><title>ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</guid><description>ChatGen-Evo automates text-to-image generation from freestyle chatting, simplifying the process and significantly improving performance over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/cover.png"/></item><item><title>Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</guid><description>Collaborative Decoding (CoDe) dramatically boosts visual auto-regressive model efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/cover.png"/></item><item><title>DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</guid><description>DreamCache enables efficient, high-quality personalized image generation without finetuning by caching reference image features and using lightweight conditioning adapters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/cover.png"/></item><item><title>DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</guid><description>DreamMix enhances image inpainting by disentangling object attributes for precise editing, enabling both identity preservation and flexible text-driven modifications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/cover.png"/></item><item><title>Identity-Preserving Text-to-Video Generation by Frequency Decomposition</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</guid><description>ConsisID achieves high-quality, identity-preserving text-to-video generation using a tuning-free diffusion transformer model that leverages frequency decomposition for effective identity control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/cover.png"/></item><item><title>Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</guid><description>Omegance: One parameter precisely controls image detail in diffusion models, enabling flexible granularity adjustments without model changes or retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/cover.png"/></item><item><title>Controllable Human Image Generation with Personalized Multi-Garments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</guid><description>BootComp: generate realistic human images wearing multiple garments using a novel synthetic data pipeline &amp;amp; diffusion model, enabling diverse applications like virtual try-on.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/cover.png"/></item><item><title>Factorized Visual Tokenization and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</guid><description>FQGAN revitalizes image generation by introducing Factorized Quantization, enabling scalable and stable visual tokenization with state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/cover.png"/></item><item><title>One Diffusion to Generate Them All</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</guid><description>OneDiffusion: A single diffusion model masters image synthesis &amp;amp; understanding across diverse tasks, from text-to-image to depth estimation, pushing the boundaries of AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/cover.png"/></item><item><title>Pathways on the Image Manifold: Image Editing via Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</guid><description>Image editing is revolutionized by Frame2Frame, which uses video generation to produce seamless and accurate edits, preserving image fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/cover.png"/></item><item><title>Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</guid><description>New benchmark VCT² reveals limitations of AI-generated image detectors; Visual AI Index (VAI) provides a robust evaluation framework.</description></item><item><title>Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</guid><description>Diptych Prompting: a novel zero-shot subject-driven image generator leveraging large-scale text-to-image models and inpainting for precise subject alignment and high-quality image synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/cover.png"/></item><item><title>Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</guid><description>Morph: a novel motion-free physics optimization framework drastically enhances human motion generation&amp;rsquo;s physical plausibility using synthetic data, achieving state-of-the-art quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/cover.png"/></item><item><title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</guid><description>OminiControl: A minimal, universal framework efficiently integrates image conditions into diffusion transformers, enabling diverse and precise control over image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/cover.png"/></item><item><title>Style-Friendly SNR Sampler for Style-Driven Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</guid><description>Style-friendly SNR sampler biases diffusion model training towards higher noise levels, enabling it to learn and generate images with higher style fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/cover.png"/></item><item><title>TEXGen: a Generative Diffusion Model for Mesh Textures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</guid><description>TEXGen: A groundbreaking generative diffusion model creates high-resolution 3D mesh textures directly from text and image prompts, exceeding prior methods in quality and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/cover.png"/></item><item><title>MyTimeMachine: Personalized Facial Age Transformation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</guid><description>MyTimeMachine personalizes facial age transformation using just 50 personal photos, outperforming existing methods by generating re-aged faces that closely match a person&amp;rsquo;s actual appearance at variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/cover.png"/></item><item><title>Stable Flow: Vital Layers for Training-Free Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</guid><description>Stable Flow achieves diverse, consistent image editing without training by strategically injecting source image features into specific &amp;lsquo;vital&amp;rsquo; layers of a diffusion transformer model. This training-f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/cover.png"/></item><item><title>VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</guid><description>VBench++: A new benchmark suite meticulously evaluates video generative models across 16 diverse dimensions, aligning with human perception for improved model development and fairer comparisons.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/cover.png"/></item><item><title>Stylecodes: Encoding Stylistic Information For Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</guid><description>StyleCodes enables easy style sharing for image generation by encoding styles as compact strings, enhancing control and collaboration while minimizing quality loss.</description></item><item><title>Continuous Speculative Decoding for Autoregressive Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</guid><description>Researchers have developed Continuous Speculative Decoding, boosting autoregressive image generation speed by up to 2.33x while maintaining image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/cover.png"/></item><item><title>FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</guid><description>FitDiT boosts virtual try-on realism by enhancing garment details via Diffusion Transformers, improving texture and size accuracy for high-fidelity virtual fashion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/cover.png"/></item><item><title>MagicQuill: An Intelligent Interactive Image Editing System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</guid><description>MagicQuill: an intelligent interactive image editing system enabling intuitive, precise image edits via brushstrokes and real-time intent prediction by a multimodal LLM.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/cover.png"/></item><item><title>Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</guid><description>Add-it: Training-free object insertion in images using pretrained diffusion models by cleverly balancing information from the scene, text prompt, and generated image, achieving state-of-the-art result&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/cover.png"/></item><item><title>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</guid><description>Edify Image: groundbreaking pixel-perfect photorealistic image generation using cascaded pixel-space diffusion models with a novel Laplacian diffusion process, enabling diverse applications including &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/cover.png"/></item><item><title>OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</guid><description>OmniEdit, a novel instruction-based image editing model, surpasses existing methods by leveraging specialist supervision and high-quality data, achieving superior performance across diverse editing ta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/cover.png"/></item><item><title>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</guid><description>StdGEN: Generate high-quality, semantically decomposed 3D characters from a single image in minutes, enabling flexible customization for various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/cover.png"/></item><item><title>SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</guid><description>SG-I2V: Zero-shot controllable image-to-video generation using a self-guided approach that leverages pre-trained models for precise object and camera motion control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/cover.png"/></item><item><title>SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</guid><description>SVDQuant boosts 4-bit diffusion models by absorbing outliers via low-rank components, achieving 3.5x memory reduction and 3x speedup on 12B parameter models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/cover.png"/></item><item><title>Training-free Regional Prompting for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</guid><description>Training-free Regional Prompting for FLUX boosts compositional text-to-image generation by cleverly manipulating attention mechanisms, achieving fine-grained control without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/cover.png"/></item><item><title>Constant Acceleration Flow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</guid><description>Constant Acceleration Flow (CAF) dramatically speeds up diffusion model generation by using a constant acceleration equation, outperforming state-of-the-art methods with improved accuracy and few-step&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/cover.png"/></item><item><title>Randomized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</guid><description>Randomized Autoregressive Modeling (RAR) sets a new state-of-the-art in image generation by cleverly introducing randomness during training to improve the model&amp;rsquo;s ability to learn from bidirectional c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/cover.png"/></item><item><title>In-Context LoRA for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</guid><description>In-Context LoRA empowers existing text-to-image models for high-fidelity multi-image generation by simply concatenating images and using minimal task-specific LoRA tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/cover.png"/></item><item><title>HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</guid><description>HelloMeme enhances text-to-image models by integrating spatial knitting attentions, enabling high-fidelity meme video generation while preserving model generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/cover.png"/></item></channel></rss>