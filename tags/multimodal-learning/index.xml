<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Multimodal Learning on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/multimodal-learning/</link><description>Recent content in Multimodal Learning on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Mon, 09 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/multimodal-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06673/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06673/</guid><description>ILLUME: A unified multi-modal LLM efficiently integrates visual understanding &amp;amp; generation, achieving competitive performance with significantly less data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06673/cover.png"/></item><item><title>Chimera: Improving Generalist Model with Domain-Specific Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05983/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05983/</guid><description>Chimera boosts large multimodal models&amp;rsquo; performance on specialized tasks by cleverly integrating domain-specific expert models, achieving state-of-the-art results on multiple benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05983/cover.png"/></item><item><title>Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05271/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05271/</guid><description>InternVL 2.5, a new open-source multimodal LLM, surpasses 70% on the MMMU benchmark, rivaling top commercial models through model, data, and test-time scaling strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05271/cover.png"/></item><item><title>MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05237/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05237/</guid><description>MAmmoTH-VL: A novel approach to instruction tuning at scale creates a 12M dataset eliciting chain-of-thought reasoning, yielding state-of-the-art multimodal reasoning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05237/cover.png"/></item><item><title>Discriminative Fine-tuning of LVLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04378/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04378/</guid><description>VladVA: A novel training framework converts generative LVLMs into powerful discriminative models, achieving state-of-the-art performance on image-text retrieval and compositionality benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04378/cover.png"/></item><item><title>Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04432/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04432/</guid><description>Divot: A novel diffusion-powered video tokenizer enables unified video comprehension &amp;amp; generation with LLMs, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04432/cover.png"/></item><item><title>Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04424/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04424/</guid><description>Florence-VL enhances vision-language models by incorporating a generative vision encoder and a novel depth-breadth fusion architecture, achieving state-of-the-art results on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04424/cover.png"/></item><item><title>GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04440/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04440/</guid><description>GENMAC: Multi-agent collaboration revolutionizes compositional text-to-video generation, achieving state-of-the-art results by iteratively refining videos via specialized agents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04440/cover.png"/></item><item><title>VisionZip: Longer is Better but Not Necessary in Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04467/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04467/</guid><description>VisionZip boosts vision-language model efficiency by intelligently selecting key visual tokens, achieving near-state-of-the-art performance with drastically reduced computational costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04467/cover.png"/></item><item><title>Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03565/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03565/</guid><description>INST-IT boosts multimodal instance understanding by using explicit visual prompts for instruction tuning, achieving significant improvements on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03565/cover.png"/></item><item><title>PaliGemma 2: A Family of Versatile VLMs for Transfer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03555/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03555/</guid><description>PaliGemma 2: A family of versatile, open-weight VLMs achieving state-of-the-art results on various transfer tasks by scaling model size and resolution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03555/cover.png"/></item><item><title>Perception Tokens Enhance Visual Reasoning in Multimodal Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03548/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03548/</guid><description>Boosting visual reasoning in multimodal language models, AURORA leverages novel &amp;lsquo;Perception Tokens&amp;rsquo; for improved depth estimation and object counting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03548/cover.png"/></item><item><title>TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03069/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03069/</guid><description>TokenFlow: One image tokenizer, mastering both visual understanding &amp;amp; generation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03069/cover.png"/></item><item><title>AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/</guid><description>AV-Odyssey Bench reveals that current multimodal LLMs struggle with basic audio-visual understanding, prompting the development of a comprehensive benchmark for more effective evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02611/cover.png"/></item><item><title>Personalized Multimodal Large Language Models: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02142/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02142/</guid><description>This survey reveals the exciting advancements in personalized multimodal large language models (MLLMs), offering a novel taxonomy, highlighting key challenges and applications, ultimately pushing the &amp;hellip;</description></item><item><title>Collaborative Instance Navigation: Leveraging Agent Self-Dialogue to Minimize User Input</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01250/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01250/</guid><description>AIUTA minimizes user input in instance navigation by leveraging agent self-dialogue and dynamic interaction, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01250/cover.png"/></item><item><title>LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01292/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01292/</guid><description>LSceneLLM boosts large 3D scene understanding by adaptively focusing on task-relevant visual details using LLMs&amp;rsquo; visual preferences, surpassing existing methods on multiple benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01292/cover.png"/></item><item><title>OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01169/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01169/</guid><description>OmniFlow: a novel generative model masters any-to-any multi-modal generation, outperforming existing models and offering flexible control!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01169/cover.png"/></item><item><title>Towards Universal Soccer Video Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01820/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01820/</guid><description>Soccer video understanding gets a major boost with SoccerReplay-1988, the largest multi-modal dataset, and MatchVision, a new visual-language model achieving state-of-the-art performance on event clas&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01820/cover.png"/></item><item><title>VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01822/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01822/</guid><description>VLSI: Verbalized Layers-to-Interactions efficiently transfers knowledge from large to small VLMs using layer-wise natural language distillation, achieving significant performance gains without scaling&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01822/cover.png"/></item><item><title>X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01824/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01824/</guid><description>X-Prompt: a novel autoregressive vision-language model achieves universal in-context image generation by efficiently compressing contextual information and using a unified training framework for super&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01824/cover.png"/></item><item><title>Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00493/</link><pubDate>Sat, 30 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00493/</guid><description>Video-3D LLM masters 3D scene understanding by cleverly fusing video data with 3D positional encoding, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00493/cover.png"/></item><item><title>Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19460/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19460/</guid><description>Video-MaÂ²mba efficiently handles long videos by using State Space Models, achieving linear scaling in memory and time, and employing a novel Multi-Axis Gradient Checkpointing (MA-GC) for significant m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19460/cover.png"/></item><item><title>On Domain-Specific Post-Training for Multimodal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19930/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19930/</guid><description>AdaMLLM enhances multimodal LLMs for specific domains via a novel visual instruction synthesizer and a single-stage post-training pipeline, achieving superior performance compared to existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19930/cover.png"/></item><item><title>SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00174/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00174/</guid><description>SOLAMI: enabling immersive, natural interactions with 3D characters via a unified social vision-language-action model and a novel synthetic multimodal dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00174/cover.png"/></item><item><title>VLSBench: Unveiling Visual Leakage in Multimodal Safety</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19939/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19939/</guid><description>VLSBench exposes visual leakage in MLLM safety benchmarks, creating a new, leak-free benchmark to evaluate true multimodal safety.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19939/cover.png"/></item><item><title>MaskRIS: Semantic Distortion-aware Data Augmentation for Referring Image Segmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19067/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19067/</guid><description>MaskRIS revolutionizes referring image segmentation by using novel masking and contextual learning to enhance data augmentation, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19067/cover.png"/></item><item><title>VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19103/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19103/</guid><description>VARCO-VISION: A new open-source 14B parameter Korean-English vision-language model excels at bilingual image-text understanding and generation, expanding AI capabilities for low-resource languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19103/cover.png"/></item><item><title>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18203/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18203/</guid><description>Critic-V enhances VLM reasoning accuracy by incorporating a critic model that provides constructive feedback, significantly outperforming existing methods on several benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18203/cover.png"/></item><item><title>VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17991/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17991/</guid><description>VideoLLM&amp;rsquo;s interaction format is revolutionized by the novel Video-Text Duet, enabling real-time, time-sensitive video comprehension with significantly improved performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17991/cover.png"/></item><item><title>Free$^2$Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17041/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17041/</guid><description>FreeÂ²Guide: Gradient-free path integral control enhances text-to-video generation using powerful large vision-language models, improving alignment without gradient-based fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17041/cover.png"/></item><item><title>Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17686/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17686/</guid><description>FiCoCo: A unified paradigm accelerates Multimodal Large Language Model (MLLM) inference by up to 82.4% with minimal performance loss, surpassing state-of-the-art training-free methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17686/cover.png"/></item><item><title>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17465/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17465/</guid><description>ShowUI, a novel vision-language-action model, efficiently manages high-resolution GUI screenshots and diverse task needs via UI-guided token selection and interleaved streaming, achieving state-of-the&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17465/cover.png"/></item><item><title>SketchAgent: Language-Driven Sequential Sketch Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17673/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17673/</guid><description>SketchAgent uses a multimodal LLM to generate dynamic, sequential sketches from textual prompts, enabling collaborative drawing and chat-based editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17673/cover.png"/></item><item><title>SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16173/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16173/</guid><description>SALOVA, a novel video-LLM framework, enhances long-form video comprehension through targeted retrieval. It introduces SceneWalk, a high-quality dataset of densely-captioned long videos, and integrates&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16173/cover.png"/></item><item><title>UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16781/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16781/</guid><description>UniPose: A unified multimodal framework for human pose comprehension, generation, and editing, enabling seamless transitions across various modalities and showcasing zero-shot generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16781/cover.png"/></item><item><title>Knowledge Transfer Across Modalities with Natural Language Supervision</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15611/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15611/</guid><description>Teach AI new visual concepts using only their textual descriptions!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15611/cover.png"/></item><item><title>Large Multi-modal Models Can Interpret Features in Large Multi-modal Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14982/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14982/</guid><description>Large multimodal models&amp;rsquo; inner workings are demystified using a novel framework that identifies, interprets, and even steers their internal features, opening the door to safer, more reliable AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14982/cover.png"/></item><item><title>MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15296/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15296/</guid><description>This survey paper offers a comprehensive overview of Multimodal Large Language Model (MLLM) evaluation, systematically categorizing benchmarks and methods, and identifying gaps for future research, th&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15296/cover.png"/></item><item><title>VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14794/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14794/</guid><description>VideoEspresso: A new dataset and Hybrid LVLMs framework boost fine-grained video reasoning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14794/cover.png"/></item><item><title>GMAI-VL &amp; GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/</guid><description>GMAI-VL-5.5M &amp;amp; GMAI-VL: A new multimodal medical dataset and vision-language model achieve state-of-the-art results in various medical tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14522/cover.png"/></item><item><title>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14432/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14432/</guid><description>Insight-V: A multi-agent system enhances multi-modal LLMs&amp;rsquo; visual reasoning by generating high-quality long-chain reasoning data and employing a two-stage training pipeline, achieving significant perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14432/cover.png"/></item><item><title>VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13281/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13281/</guid><description>VideoAutoArena automates large multimodal model (LMM) evaluation using simulated users, offering a cost-effective and scalable solution compared to traditional human annotation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13281/cover.png"/></item><item><title>Awaker2.5-VL: Stably Scaling MLLMs with Parameter-Efficient Mixture of Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10669/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10669/</guid><description>Awaker2.5-VL: A novel Mixture-of-Experts architecture stably scales MLLMs, solving multi-task conflict with parameter efficiency and achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10669/cover.png"/></item><item><title>BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10640/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10640/</guid><description>BlueLM-V-3B: Algorithm and system co-design enables efficient, real-time multimodal language model deployment on mobile devices.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10640/cover.png"/></item><item><title>Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10442/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10442/</guid><description>Boosting multimodal reasoning in LLMs, researchers developed Mixed Preference Optimization (MPO) and a large-scale dataset (MMPR), significantly improving reasoning accuracy and achieving performance &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10442/cover.png"/></item><item><title>LLaVA-o1: Let Vision Language Models Reason Step-by-Step</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10440/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10440/</guid><description>LLaVA-01: A novel visual language model achieves superior reasoning performance through structured, multi-stage processing and efficient inference-time scaling, surpassing even larger, closed-source m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10440/cover.png"/></item><item><title>SmoothCache: A Universal Inference Acceleration Technique for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10510/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10510/</guid><description>SmoothCache: A universal technique boosts Diffusion Transformer inference speed by 8-71% across modalities, without sacrificing quality!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10510/cover.png"/></item><item><title>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07975/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07975/</guid><description>JanusFlow harmonizes autoregression and rectified flow for unified multimodal understanding and generation, achieving state-of-the-art results on standard benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07975/cover.png"/></item><item><title>LLM2CLIP: Powerful Language Model Unlock Richer Visual Representation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04997/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04997/</guid><description>LLM2CLIP boosts CLIP&amp;rsquo;s performance by cleverly integrating LLMs, enabling it to understand longer, more complex image captions and achieving state-of-the-art results across various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04997/cover.png"/></item><item><title>VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04923/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04923/</guid><description>VideoGLaMM: a new large multimodal model achieves precise pixel-level visual grounding in videos by seamlessly integrating a dual vision encoder, a spatio-temporal decoder, and a large language model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04923/cover.png"/></item><item><title>Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03823/</link><pubDate>Wed, 06 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03823/</guid><description>MM-Detect: a novel framework detects contamination in multimodal LLMs, enhancing benchmark reliability by identifying training set leakage and improving performance evaluations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03823/cover.png"/></item><item><title>Inference Optimal VLMs Need Only One Visual Token but Larger Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03312/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03312/</guid><description>Inference-optimal Vision Language Models (VLMs) need only one visual token but larger models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03312/cover.png"/></item><item><title>TIP-I2V: A Million-Scale Real Text and Image Prompt Dataset for Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04709/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04709/</guid><description>TIP-I2V: A million-scale dataset provides 1.7 million real user text &amp;amp; image prompts for image-to-video generation, boosting model development and safety.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04709/cover.png"/></item><item><title>AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/</guid><description>ANDROIDLAB, a novel framework, systematically benchmarks Android autonomous agents, improving LLM and LMM success rates on 138 tasks via a unified environment and open-source dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/cover.png"/></item><item><title>OS-ATLAS: A Foundation Action Model for Generalist GUI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/</guid><description>OS-Atlas: A new open-source toolkit and model dramatically improves GUI agent performance by providing a massive dataset and innovative training methods, enabling superior generalization to unseen int&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/cover.png"/></item><item><title>BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21969/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21969/</guid><description>BenchX: A unified benchmark framework reveals surprising MedVLP performance, challenging existing conclusions and advancing research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21969/cover.png"/></item><item><title>Survey of User Interface Design and Interaction Techniques in Generative AI Applications</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22370/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22370/</guid><description>This study provides a comprehensive taxonomy of user interface design and interaction techniques in generative AI, offering valuable insights for developers and researchers aiming to enhance user expe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22370/cover.png"/></item></channel></rss>