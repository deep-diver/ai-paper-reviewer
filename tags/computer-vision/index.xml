<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer Vision on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/computer-vision/</link><description>Recent content in Computer Vision on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 27 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/computer-vision/index.xml" rel="self" type="application/rss+xml"/><item><title>Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20378/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20378/</guid><description>EDGS: Achieves faster, high-quality dynamic scene rendering by sparse time-variant attribute modeling and intelligent static area filtering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20378/cover.png"/></item><item><title>Mobius: Text to Seamless Looping Video Generation via Latent Shift</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20307/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20307/</guid><description>Mobius generates seamless looping videos from text using latent shift, repurposing pre-trained models without training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20307/cover.png"/></item><item><title>Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20172/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20172/</guid><description>DREAM ENGINE: Text-image interleaved control made easy, unifying text and visual cues for creative image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20172/cover.png"/></item><item><title>Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.19459/</link><pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.19459/</guid><description>ArtGS: Achieves state-of-the-art, efficient interactable replicas of complex articulated objects via Gaussian Splatting.</description></item><item><title>DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17157/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17157/</guid><description>DICEPTION: A generalist diffusion model for visual perceptual tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17157/cover.png"/></item><item><title>GCC: Generative Color Constancy via Diffusing a Color Checker</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17435/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17435/</guid><description>GCC: Color constancy through diffusion, inpainting a color checker for stable illumination estimation.</description></item><item><title>VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17258/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17258/</guid><description>VideoGrain: Fine-grained video editing via space-time attention!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17258/cover.png"/></item><item><title>X-Dancer: Expressive Music to Human Dance Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17414/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17414/</guid><description>X-Dancer: Expressive dance video generation from music and a single image!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.17414/cover.png"/></item><item><title>M3-AGIQA: Multimodal, Multi-Round, Multi-Aspect AI-Generated Image Quality Assessment</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.15167/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.15167/</guid><description>M3-AGIQA: A multimodal AI solution that comprehensively assesses AI-generated image quality, achieving state-of-the-art performance by distilling online MLLM capabilities into a local model.</description></item><item><title>CrossOver: 3D Scene Cross-Modal Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15011/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15011/</guid><description>CrossOver: Flexible scene-level cross-modal alignment via modality-agnostic embeddings, unlocking robust 3D scene understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15011/cover.png"/></item><item><title>Dynamic Concepts Personalization from Single Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14844/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14844/</guid><description>Personalizing video models for dynamic concepts is now achievable with Set-and-Sequence: enabling high-fidelity generation, editing, and composition!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14844/cover.png"/></item><item><title>KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14949/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14949/</guid><description>KITAB-Bench: A new multi-domain Arabic OCR benchmark to bridge the performance gap with English OCR technologies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14949/cover.png"/></item><item><title>PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14397/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14397/</guid><description>PhotoDoodle: Mimicking artistic image editing with personalized decorative elements through learning from few-shot pairwise data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14397/cover.png"/></item><item><title>RelaCtrl: Relevance-Guided Efficient Control for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14377/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14377/</guid><description>RelaCtrl: Relevance-guided control boosts diffusion transformer efficiency, cutting parameters by intelligently allocating resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14377/cover.png"/></item><item><title>Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and Human-Like Reasoning Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13759/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13759/</guid><description>New geolocation dataset &amp;amp; reasoning framework enhance accuracy and interpretability by leveraging human gameplay data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13759/cover.png"/></item><item><title>JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13407/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13407/</guid><description>JL1-CD: New all-inclusive dataset &amp;amp; multi-teacher knowledge distillation framework for robust remote sensing change detection, achieving state-of-the-art results!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13407/cover.png"/></item><item><title>Diffusion-Sharpening: Fine-tuning Diffusion Models with Denoising Trajectory Sharpening</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12146/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12146/</guid><description>Diffusion-Sharpening enhances diffusion model fine-tuning by optimizing sampling trajectories, achieving faster convergence and high inference efficiency without extra NFEs, leading to improved alignm&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12146/cover.png"/></item><item><title>Intuitive physics understanding emerges from self-supervised pretraining on natural videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/</guid><description>AI models learn intuitive physics from self-supervised video pretraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/cover.png"/></item><item><title>MagicArticulate: Make Your 3D Models Articulation-Ready</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/</guid><description>MagicArticulate automates 3D model animation preparation by generating skeletons and skinning weights, overcoming prior manual methods&amp;rsquo; limitations, and introducing Articulation-XL, a large-scale benc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12135/cover.png"/></item><item><title>MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11663/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11663/</guid><description>MaskGWM: Improves driving world models by using video mask reconstruction for better generalization.</description></item><item><title>Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10248/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10248/</guid><description>Step-Video-T2V: A 30B parameter text-to-video model generating high-quality videos up to 204 frames, pushing the boundaries of video foundation models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10248/cover.png"/></item><item><title>Cluster and Predict Latents Patches for Improved Masked Image Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08769/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08769/</guid><description>CAPI: a novel masked image modeling framework boosts self-supervised visual representation learning by predicting latent clusterings, achieving state-of-the-art ImageNet accuracy and mIoU.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08769/cover.png"/></item><item><title>Enhance-A-Video: Better Generated Video for Free</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07508/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07508/</guid><description>Enhance-A-Video boosts video generation quality without retraining, by enhancing cross-frame correlations in diffusion transformers, resulting in improved coherence and visual fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07508/cover.png"/></item><item><title>Magic 1-For-1: Generating One Minute Video Clips within One Minute</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07701/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07701/</guid><description>Magic141 generates one-minute video clips in under a minute by cleverly factorizing the generation task and employing optimization techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07701/cover.png"/></item><item><title>MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07856/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07856/</guid><description>MRS: a novel, training-free sampler, drastically speeds up controllable image generation using Mean Reverting Diffusion, achieving 10-20x speedup across various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07856/cover.png"/></item><item><title>Next Block Prediction: Video Generation via Semi-Autoregressive Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07737/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07737/</guid><description>Next-Block Prediction (NBP) revolutionizes video generation by using a semi-autoregressive model that predicts blocks of video content simultaneously, resulting in significantly faster inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07737/cover.png"/></item><item><title>VidCRAFT3: Camera, Object, and Lighting Control for Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07531/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07531/</guid><description>VidCRAFT3 enables high-quality image-to-video generation with precise control over camera movement, object motion, and lighting, pushing the boundaries of visual content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07531/cover.png"/></item><item><title>Animate Anyone 2: High-Fidelity Character Image Animation with Environment Affordance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06145/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06145/</guid><description>Animate Anyone 2 creates high-fidelity character animations by incorporating environmental context, resulting in seamless character-environment integration and more realistic object interactions.</description></item><item><title>CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/</guid><description>CustomVideoX: Zero-shot personalized video generation, exceeding existing methods in quality &amp;amp; consistency via 3D reference attention and dynamic adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/cover.png"/></item><item><title>Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06155/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06155/</guid><description>EFFICIENT-VDIT accelerates video generation by 7.8x using sparse attention and multi-step distillation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06155/cover.png"/></item><item><title>Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06782/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06782/</guid><description>Lumina-Video: Efficient and flexible video generation using a multi-scale Next-DiT architecture with motion control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06782/cover.png"/></item><item><title>TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06608/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06608/</guid><description>TripoSG: High-fidelity 3D shapes synthesized via large-scale rectified flow models, pushing image-to-3D generation to new heights.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06608/cover.png"/></item><item><title>3CAD: A Large-Scale Real-World 3C Product Dataset for Unsupervised Anomaly</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05761/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05761/</guid><description>3CAD: A new large-scale, real-world dataset with diverse 3C product anomalies boosts unsupervised anomaly detection, enabling superior algorithm development via a novel Coarse-to-Fine framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05761/cover.png"/></item><item><title>Dual Caption Preference Optimization for Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06023/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06023/</guid><description>Dual Caption Preference Optimization (DCPO) significantly boosts diffusion model image quality by using paired captions to resolve data distribution conflicts and irrelevant prompt issues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06023/cover.png"/></item><item><title>AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360° Unbounded Scene Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05176/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05176/</guid><description>AuraFusion360: High-quality 360° scene inpainting achieved via novel augmented unseen region alignment and a new benchmark dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05176/cover.png"/></item><item><title>FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/</guid><description>FlashVideo: Generate stunning high-resolution videos efficiently using a two-stage framework prioritizing fidelity and detail, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/cover.png"/></item><item><title>Goku: Flow Based Video Generative Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04896/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04896/</guid><description>Goku: a novel family of joint image-and-video generation models uses rectified flow Transformers, achieving industry-leading performance with a robust data pipeline and training infrastructure.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04896/cover.png"/></item><item><title>VideoRoPE: What Makes for Good Video Rotary Position Embedding?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/</guid><description>VideoRoPE enhances video processing in Transformer models by introducing a novel 3D rotary position embedding that preserves spatio-temporal relationships, resulting in superior performance across var&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/cover.png"/></item><item><title>Fast Video Generation with Sliding Tile Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04507/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04507/</guid><description>Sliding Tile Attention (STA) boosts video generation speed by 2.43-3.53x without losing quality by exploiting inherent data redundancy in video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04507/cover.png"/></item><item><title>MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04299/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04299/</guid><description>MotionCanvas lets users design cinematic video shots with intuitive controls for camera and object movements, translating scene-space intentions into video animations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04299/cover.png"/></item><item><title>Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03738/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03738/</guid><description>Smaller image patches improve vision transformer performance, defying conventional wisdom and revealing a new scaling law for enhanced visual understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03738/cover.png"/></item><item><title>DynVFX: Augmenting Real Videos with Dynamic Content</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03621/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03621/</guid><description>DynVFX: Effortlessly integrate dynamic content into real videos using simple text prompts. Zero-shot learning and novel attention mechanisms deliver seamless and realistic results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03621/cover.png"/></item><item><title>On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for Mobile Devices</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04363/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04363/</guid><description>On-device Sora makes high-quality, diffusion-based text-to-video generation possible on smartphones, overcoming computational and memory limitations through novel techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04363/cover.png"/></item><item><title>Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03639/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03639/</guid><description>This paper introduces PointVid, a 3D-aware video generation framework using 3D point regularization to enhance video realism and address common issues like object morphing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03639/cover.png"/></item><item><title>MotionLab: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02358/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02358/</guid><description>MotionLab: One framework to rule them all! Unifying human motion generation &amp;amp; editing via a novel Motion-Condition-Motion paradigm, boosting efficiency and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02358/cover.png"/></item><item><title>VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/</guid><description>VideoJAM enhances video generation by jointly learning appearance and motion representations, achieving state-of-the-art motion coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/cover.png"/></item><item><title>Improved Training Technique for Latent Consistency Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01441/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01441/</guid><description>Researchers significantly enhance latent consistency models&amp;rsquo; performance by introducing Cauchy loss, mitigating outlier effects, and employing novel training strategies, thus bridging the gap with dif&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01441/cover.png"/></item><item><title>Inverse Bridge Matching Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01362/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01362/</guid><description>Boosting Diffusion Bridge Models: A new distillation technique accelerates inference speed by 4x to 100x, sometimes even improving image quality!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01362/cover.png"/></item><item><title>LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01105/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01105/</guid><description>LayerTracer innovatively synthesizes cognitive-aligned layered SVGs via diffusion transformers, bridging the gap between AI and professional design standards by learning from a novel dataset of sequen&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01105/cover.png"/></item><item><title>OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human Animation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01061/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01061/</guid><description>OmniHuman-1: Scaling up one-stage conditioned human animation through novel mixed-condition training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01061/cover.png"/></item><item><title>A Study on the Performance of U-Net Modifications in Retroperitoneal Tumor Segmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00314/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00314/</guid><description>ViLU-Net, a novel U-Net modification using Vision-xLSTM, achieves superior retroperitoneal tumor segmentation accuracy and efficiency, exceeding existing state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00314/cover.png"/></item><item><title>DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16764/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16764/</guid><description>DIFFSPLAT repurposes 2D image diffusion models to natively generate high-quality 3D Gaussian splats, overcoming limitations in existing 3D generation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16764/cover.png"/></item><item><title>iFormer: Integrating ConvNet and Transformer for Mobile Application</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15369/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15369/</guid><description>iFormer: A new family of mobile hybrid vision networks that expertly blends ConvNeXt&amp;rsquo;s fast local feature extraction with the efficient global modeling of self-attention, achieving top-tier accuracy a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15369/cover.png"/></item><item><title>Relightable Full-Body Gaussian Codec Avatars</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14726/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14726/</guid><description>Relightable Full-Body Gaussian Codec Avatars: Realistic, animatable full-body avatars are now possible using learned radiance transfer and efficient 3D Gaussian splatting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14726/cover.png"/></item><item><title>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13926/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13926/</guid><description>Researchers significantly enhanced autoregressive image generation by integrating chain-of-thought reasoning strategies, achieving a remarkable +24% improvement on the GenEval benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13926/cover.png"/></item><item><title>EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13452/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13452/</guid><description>EchoVideo generates high-fidelity, identity-preserving videos by cleverly fusing text and image features, overcoming limitations of prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13452/cover.png"/></item><item><title>Improving Video Generation with Human Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13918/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13918/</guid><description>Human feedback boosts video generation! New VideoReward model &amp;amp; alignment algorithms significantly improve video quality and user prompt alignment, exceeding prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13918/cover.png"/></item><item><title>Temporal Preference Optimization for Long-Form Video Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13919/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13919/</guid><description>Boosting long-form video understanding, Temporal Preference Optimization (TPO) enhances video-LLMs by leveraging preference learning. It achieves this through a self-training method using preference &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13919/cover.png"/></item><item><title>Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13826/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13826/</guid><description>Video-MMMU benchmark systematically evaluates Large Multimodal Models’ knowledge acquisition from videos across multiple disciplines and cognitive stages, revealing significant gaps between human and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13826/cover.png"/></item><item><title>GPS as a Control Signal for Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12390/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12390/</guid><description>GPS-guided image generation is here! This paper leverages GPS data to create highly realistic images reflecting specific locations, even reconstructing 3D models from 2D photos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12390/cover.png"/></item><item><title>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12202/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12202/</guid><description>Hunyuan3D 2.0: A groundbreaking open-source system generating high-resolution, textured 3D assets using scalable diffusion models, exceeding state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12202/cover.png"/></item><item><title>TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/</guid><description>TokenVerse: Extract &amp;amp; combine visual concepts from multiple images for creative image generation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12224/cover.png"/></item><item><title>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12375/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12375/</guid><description>Video Depth Anything achieves consistent depth estimation for super-long videos by enhancing Depth Anything V2 with a spatial-temporal head and a novel temporal consistency loss, setting a new state-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12375/cover.png"/></item><item><title>EMO2: End-Effector Guided Audio-Driven Avatar Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10687/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10687/</guid><description>EMO2 achieves realistic audio-driven avatar video generation by employing a two-stage framework: first generating hand poses directly from audio and then using a diffusion model to synthesize full-bod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10687/cover.png"/></item><item><title>DiffuEraser: A Diffusion Model for Video Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10018/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10018/</guid><description>DiffuEraser: a novel video inpainting model based on stable diffusion, surpasses existing methods by using injected priors and temporal consistency improvements for superior results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10018/cover.png"/></item><item><title>GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09978/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09978/</guid><description>GaussianAvatar-Editor enables photorealistic, text-driven editing of animatable 3D heads, solving motion occlusion and ensuring temporal consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09978/cover.png"/></item><item><title>GSTAR: Gaussian Surface Tracking and Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10283/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10283/</guid><description>GSTAR: A novel method achieving photorealistic rendering, accurate reconstruction, and reliable 3D tracking of dynamic scenes with changing topology, even handling surfaces appearing, disappearing, or&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10283/cover.png"/></item><item><title>Textoon: Generating Vivid 2D Cartoon Characters from Text Descriptions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10020/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10020/</guid><description>Textoon: Generating vivid 2D cartoon characters from text descriptions in under a minute, revolutionizing animation workflow.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10020/cover.png"/></item><item><title>X-Dyna: Expressive Dynamic Human Image Animation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10021/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10021/</guid><description>X-Dyna: a novel diffusion-based pipeline generates realistic human image animation using a zero-shot approach by integrating a Dynamics-Adapter for dynamic detail preservation, exceeding state-of-the-&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10021/cover.png"/></item><item><title>AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09503/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09503/</guid><description>AnyStory: A unified framework enables high-fidelity personalized image generation for single and multiple subjects, addressing subject fidelity challenges in existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09503/cover.png"/></item><item><title>CaPa: Carve-n-Paint Synthesis for Efficient 4K Textured Mesh Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09433/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09433/</guid><description>CaPa: Carve-n-Paint Synthesis generates hyper-realistic 4K textured meshes in under 30 seconds, setting a new standard for efficient 3D asset creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09433/cover.png"/></item><item><title>Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09732/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09732/</guid><description>Boosting diffusion model performance at inference time, this research introduces a novel framework that goes beyond simply increasing denoising steps. By cleverly searching for better noise candidates&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09732/cover.png"/></item><item><title>Learnings from Scaling Visual Tokenizers for Reconstruction and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09755/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09755/</guid><description>Scaling visual tokenizers dramatically improves image and video generation, achieving state-of-the-art results and outperforming existing methods with fewer computations by focusing on decoder scaling&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09755/cover.png"/></item><item><title>SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09756/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09756/</guid><description>SynthLight: A novel diffusion model relights portraits realistically by learning to re-render synthetic faces, generalizing remarkably well to real photographs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09756/cover.png"/></item><item><title>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/</guid><description>VideoWorld shows AI can learn complex reasoning and planning skills from unlabeled videos alone, achieving professional-level performance in Go and robotics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/cover.png"/></item><item><title>CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08983/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08983/</guid><description>CityDreamer4D generates realistic, unbounded 4D city models by cleverly separating dynamic objects (like vehicles) from static elements (buildings, roads), using multiple neural fields for enhanced re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08983/cover.png"/></item><item><title>Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09019/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09019/</guid><description>Ouroboros-Diffusion: A novel tuning-free long video generation framework achieving unprecedented content consistency by cleverly integrating information across frames via latent sampling, cross-frame&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09019/cover.png"/></item><item><title>RepVideo: Rethinking Cross-Layer Representation for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08994/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08994/</guid><description>RepVideo enhances text-to-video generation by enriching feature representations, resulting in significantly improved temporal coherence and spatial detail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08994/cover.png"/></item><item><title>Do generative video models learn physical principles from watching videos?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09038/</link><pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09038/</guid><description>Generative video models struggle to understand physics despite producing visually realistic videos; Physics-IQ benchmark reveals this critical limitation, highlighting the need for improved physical r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09038/cover.png"/></item><item><title>GameFactory: Creating New Games with Generative Interactive Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08325/</link><pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08325/</guid><description>GameFactory uses AI to generate entirely new games within diverse, open-domain scenes by learning action controls from a small dataset and transferring them to pre-trained video models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08325/cover.png"/></item><item><title>An Empirical Study of Autoregressive Pre-training from Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/</guid><description>Toto, a new autoregressive video model, achieves competitive performance across various benchmarks by pre-training on over 1 trillion visual tokens, demonstrating the effectiveness of scaling video mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/cover.png"/></item><item><title>The GAN is dead; long live the GAN! A Modern GAN Baseline</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05441/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05441/</guid><description>R3GAN: A modernized GAN baseline achieves state-of-the-art results with a simple, stable loss function and modern architecture, debunking the myth that GANs are hard to train.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05441/cover.png"/></item><item><title>On Computational Limits and Provably Efficient Criteria of Visual Autoregressive Models: A Fine-Grained Complexity Analysis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04377/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04377/</guid><description>This paper unveils critical thresholds for efficient visual autoregressive model computation, proving sub-quartic time is impossible beyond a certain input matrix norm while establishing efficient app&amp;hellip;</description></item><item><title>SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04689/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04689/</guid><description>SPAR3D: Fast, accurate single-image 3D reconstruction via a novel two-stage approach using point clouds for high-fidelity mesh generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04689/cover.png"/></item><item><title>Chirpy3D: Continuous Part Latents for Creative 3D Bird Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04144/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04144/</guid><description>Chirpy3D: Generating creative, high-quality 3D birds with intricate details by learning a continuous part latent space from 2D images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04144/cover.png"/></item><item><title>Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/</guid><description>Diffusion as Shader (DaS) achieves versatile video control by using 3D tracking videos as control signals in a unified video diffusion model, enabling precise manipulation across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/cover.png"/></item><item><title>Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03916/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03916/</guid><description>DOLPHIN: AI automates scientific research from idea generation to experimental validation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03916/cover.png"/></item><item><title>MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03714/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03714/</guid><description>MoDec-GS: a novel framework achieving 70% model size reduction in dynamic 3D Gaussian splatting while improving visual quality by cleverly decomposing complex motions and optimizing temporal intervals&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03714/cover.png"/></item><item><title>MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02955/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02955/</guid><description>MotionBench, a new benchmark, reveals that existing video models struggle with fine-grained motion understanding. To address this, the authors propose TE Fusion, a novel architecture that improves mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02955/cover.png"/></item><item><title>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</guid><description>STAR: A novel approach uses text-to-video models for realistic, temporally consistent real-world video super-resolution, improving image quality and detail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/cover.png"/></item><item><title>Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</guid><description>Through-The-Mask uses mask-based motion trajectories to generate realistic videos from images and text, overcoming limitations of existing methods in handling complex multi-object motion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/cover.png"/></item><item><title>TransPixar: Advancing Text-to-Video Generation with Transparency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</guid><description>TransPixar generates high-quality videos with transparency by jointly training RGB and alpha channels, outperforming sequential generation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/cover.png"/></item><item><title>DepthMaster: Taming Diffusion Models for Monocular Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/</guid><description>DepthMaster tames diffusion models for faster, more accurate monocular depth estimation by aligning generative features with high-quality semantic features and adaptively balancing low and high-freque&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/cover.png"/></item><item><title>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</guid><description>GS-DiT: Generating high-quality videos with advanced 4D control through efficient dense 3D point tracking and pseudo 4D Gaussian fields.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/cover.png"/></item><item><title>MagicFace: High-Fidelity Facial Expression Editing with Action-Unit Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02260/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02260/</guid><description>MagicFace achieves high-fidelity facial expression editing via AU control, preserving identity and background using a diffusion model and ID encoder, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02260/cover.png"/></item><item><title>Ingredients: Blending Custom Photos with Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</guid><description>Ingredients: A new framework customizes videos by blending multiple photos with video diffusion transformers, enabling realistic and personalized video generation while maintaining consistent identity&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/cover.png"/></item><item><title>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</guid><description>LightningDiT resolves the optimization dilemma in latent diffusion models by aligning latent space with pre-trained vision models, achieving state-of-the-art ImageNet 256x256 generation with over 21x &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/cover.png"/></item><item><title>SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</guid><description>SeedVR: A novel diffusion transformer revolutionizes generic video restoration by efficiently handling arbitrary video lengths and resolutions, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/cover.png"/></item><item><title>SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/</guid><description>SeFAR: a novel semi-supervised framework for fine-grained action recognition, achieves state-of-the-art results by using dual-level temporal modeling, moderate temporal perturbation, and adaptive regu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/cover.png"/></item><item><title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</guid><description>VideoAnydoor: High-fidelity video object insertion with precise motion control, achieved via an end-to-end framework leveraging an ID extractor and a pixel warper for robust detail preservation and fi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/cover.png"/></item><item><title>MLLM-as-a-Judge for Image Safety without Human Labeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</guid><description>Zero-shot image safety judgment is achieved using MLLMs and a novel method called CLUE, objectifying safety rules, and significantly reducing the need for human labeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/cover.png"/></item><item><title>Edicho: Consistent Image Editing in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</guid><description>Edicho: a novel training-free method for consistent image editing across diverse images, achieving precise consistency by leveraging explicit correspondence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/cover.png"/></item><item><title>VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/</guid><description>VisionReward, a novel reward model, surpasses existing methods by precisely capturing multi-dimensional human preferences for image and video generation, enabling more accurate and stable model optimi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/cover.png"/></item><item><title>Bringing Objects to Life: 4D generation from 3D objects</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</link><pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</guid><description>3to4D: Animate any 3D object with text prompts, preserving visual quality and achieving realistic motion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/cover.png"/></item><item><title>VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</guid><description>VideoMaker achieves high-fidelity zero-shot customized video generation by cleverly harnessing the inherent power of video diffusion models, eliminating the need for extra feature extraction and injec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/cover.png"/></item><item><title>DepthLab: From Partial to Complete</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/</guid><description>DepthLab: a novel image-conditioned depth inpainting model enhances downstream 3D tasks by effectively completing partial depth information, showing superior performance and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/cover.png"/></item><item><title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</guid><description>DiTCtrl achieves state-of-the-art multi-prompt video generation without retraining by cleverly controlling attention in a diffusion transformer, enabling smooth transitions between video segments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/cover.png"/></item><item><title>Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</guid><description>Orient Anything: Learning robust object orientation estimation directly from rendered 3D models, achieving state-of-the-art accuracy on real images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/cover.png"/></item><item><title>PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</guid><description>PartGen generates compositional 3D objects with meaningful parts from text, images, or unstructured 3D data using multi-view diffusion models, enabling flexible 3D part editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/cover.png"/></item><item><title>SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/</guid><description>SBS Figures creates a massive, high-quality figure QA dataset via a novel stage-by-stage synthesis pipeline, enabling efficient pre-training of visual language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/cover.png"/></item><item><title>Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</guid><description>Distilled Decoding (DD) drastically speeds up image generation from autoregressive models by using flow matching to enable one-step sampling, achieving significant speedups while maintaining acceptabl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/cover.png"/></item><item><title>CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</guid><description>CLEAR: Conv-Like Linearization boosts pre-trained Diffusion Transformers, achieving 6.3x faster 8K image generation with minimal quality loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/cover.png"/></item><item><title>Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</guid><description>Affordance-Aware Object Insertion uses a novel Mask-Aware Dual Diffusion model &amp;amp; SAM-FB dataset to realistically place objects in scenes, considering contextual relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/cover.png"/></item><item><title>DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/</guid><description>DI-PCG uses a lightweight diffusion transformer to efficiently and accurately estimate parameters of procedural generators from images, enabling high-fidelity 3D asset creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/cover.png"/></item><item><title>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</guid><description>LeviTor: Revolutionizing image-to-video synthesis with intuitive 3D trajectory control, generating realistic videos from static images by abstracting object masks into depth-aware control points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/cover.png"/></item><item><title>Parallelized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</guid><description>Boosting autoregressive visual generation speed by 3.6-9.5x, this research introduces parallel processing while preserving model simplicity and generation quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/cover.png"/></item><item><title>UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</guid><description>UIP2P: Unsupervised instruction-based image editing achieves high-fidelity edits by enforcing Cycle Edit Consistency, eliminating the need for ground-truth data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/cover.png"/></item><item><title>AniDoc: Animation Creation Made Easier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</guid><description>AniDoc automates cartoon animation line art video colorization, making animation creation easier!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/cover.png"/></item><item><title>FashionComposer: Compositional Fashion Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</guid><description>FashionComposer revolutionizes fashion image creation through flexible composition of garments, faces, and poses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/cover.png"/></item><item><title>Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</guid><description>Prompting unlocks 4K metric depth from low-cost LiDAR.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/cover.png"/></item><item><title>ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</guid><description>ChatDiT enables zero-shot, multi-turn image generation using pretrained diffusion transformers and a novel multi-agent framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/cover.png"/></item><item><title>MIVE: New Design and Benchmark for Multi-Instance Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/</guid><description>Edit many objects at once in videos! MIVE does it accurately without affecting other areas, a big step for AI video editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/cover.png"/></item><item><title>Move-in-2D: 2D-Conditioned Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</guid><description>Move-in-2D generates realistic human motion sequences conditioned on a 2D scene image and text prompt, overcoming limitations of existing approaches and improving video synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/cover.png"/></item><item><title>VidTok: A Versatile and Open-Source Video Tokenizer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</guid><description>VidTok: an open-source, top performing video tokenizer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/cover.png"/></item><item><title>ColorFlow: Retrieval-Augmented Image Sequence Colorization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</guid><description>ColorFlow, a new AI model, accurately colorizes black-and-white image sequences while preserving character identity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/cover.png"/></item><item><title>IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/</guid><description>IDArb: A diffusion model for decomposing images into intrinsic components like albedo, normal, and material properties, handling varying views and lighting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/cover.png"/></item><item><title>MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/</guid><description>MOVIS enhances 3D scene generation by improving cross-view consistency in multi-object novel view synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/cover.png"/></item><item><title>Sequence Matters: Harnessing Video Models in 3D Super-Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/</guid><description>Leveraging video models, researchers achieve state-of-the-art 3D super-resolution by generating &amp;lsquo;video-like&amp;rsquo; sequences from unordered images, eliminating artifacts and computational demands.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/cover.png"/></item><item><title>StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/</guid><description>Create realistic 3D heads with specific hairstyles from text, no 3D hair data needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/cover.png"/></item><item><title>Wonderland: Navigating 3D Scenes from a Single Image</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/</guid><description>Generate wide-scope 3D scenes from single images in a snap!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/cover.png"/></item><item><title>GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</guid><description>Training-free method adds physical properties to 3D models using vision-language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/cover.png"/></item><item><title>BrushEdit: All-In-One Image Inpainting and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</guid><description>BrushEdit revolutionizes interactive image editing with instructions &amp;amp; inpainting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/cover.png"/></item><item><title>Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</guid><description>New attack fools breast ultrasound AI using subtle text prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/cover.png"/></item><item><title>Arbitrary-steps Image Super-resolution via Diffusion Inversion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/</guid><description>InvSR: a novel image super-resolution technique using diffusion inversion, enabling flexible sampling steps for efficient and high-fidelity results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/cover.png"/></item><item><title>DisPose: Disentangling Pose Guidance for Controllable Human Image Animation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/</guid><description>DisPose disentangles pose guidance for controllable human image animation, generating diverse animations while preserving appearance consistency using only sparse skeleton pose input, eliminating the &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/cover.png"/></item><item><title>EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/</guid><description>EasyRef uses multimodal LLMs to generate images from multiple references, overcoming limitations of prior methods by capturing consistent visual elements and offering improved zero-shot generalization&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/cover.png"/></item><item><title>FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</guid><description>Edit images precisely with AI, no masks needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/cover.png"/></item><item><title>FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</guid><description>FreeScale generates stunning 8K images and high-fidelity videos without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/cover.png"/></item><item><title>FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/</guid><description>FreeSplatter: a novel feed-forward framework reconstructs high-quality 3D scenes from uncalibrated sparse-view images, estimating camera poses in seconds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/cover.png"/></item><item><title>Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/</guid><description>Gaze-LLE achieves state-of-the-art gaze estimation by using a frozen DINOv2 encoder and a lightweight decoder, simplifying architecture and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/cover.png"/></item><item><title>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</guid><description>InstanceCap improves text-to-video generation through detailed, instance-aware captions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/cover.png"/></item><item><title>Learned Compression for Compressed Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09405/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09405/</guid><description>WaLLOC: a novel neural codec boosts compressed-domain learning by combining wavelet transforms with asymmetric autoencoders, achieving high compression ratios with minimal computation and uniform dime&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09405/cover.png"/></item><item><title>LoRACLR: Contrastive Adaptation for Customization of Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/</guid><description>LoRACLR merges multiple LoRA models for high-fidelity multi-concept image generation, using a contrastive objective to ensure concept distinctiveness and prevent interference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/cover.png"/></item><item><title>Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09593/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09593/</guid><description>Neural LightRig uses multi-light diffusion to accurately estimate object normals and materials from a single image, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09593/cover.png"/></item><item><title>DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/</guid><description>DiffSensei: A new framework generates customized manga with dynamic multi-character control using multi-modal LLMs and diffusion models, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/cover.png"/></item><item><title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</guid><description>Introducing Evaluation Agent, a faster, more flexible human-like framework for evaluating visual generative AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/cover.png"/></item><item><title>FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/</guid><description>FireFlow makes editing images faster and better.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/cover.png"/></item><item><title>FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/</guid><description>FiVA dataset and its adaptation framework enable unprecedented fine-grained control over visual attributes in text-to-image generation, empowering users to craft highly customized images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/cover.png"/></item><item><title>Mobile Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</guid><description>MobileVD: The first mobile-optimized video diffusion model, achieving 523x efficiency improvement over state-of-the-art with minimal quality loss, enabling realistic video generation on smartphones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/cover.png"/></item><item><title>ObjCtrl-2.5D: Training-free Object Control with Camera Poses</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</guid><description>ObjCtrl-2.5D: Training-free, precise image-to-video object control using 3D trajectories and camera poses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/cover.png"/></item><item><title>OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</guid><description>OmniDocBench, a novel benchmark, tackles limitations in current document parsing by introducing a diverse, high-quality dataset with comprehensive annotations, enabling fair multi-level evaluation of &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/cover.png"/></item><item><title>STIV: Scalable Text and Image Conditioned Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/</guid><description>STIV: A novel, scalable method for text and image-conditioned video generation, systematically improving model architectures, training, and data curation for superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/cover.png"/></item><item><title>UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/</guid><description>UniReal: a universal framework for image generation and editing, unifying diverse tasks via learning real-world dynamics from video data, achieving highly realistic and versatile results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/cover.png"/></item><item><title>Video Motion Transfer with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</guid><description>DiTFlow: training-free video motion transfer using Diffusion Transformers, enabling realistic motion control in synthesized videos via Attention Motion Flow.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/cover.png"/></item><item><title>EMOv2: Pushing 5M Vision Model Frontier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/</guid><description>EMOv2 achieves state-of-the-art performance in various vision tasks using a novel Meta Mobile Block, pushing the 5M parameter lightweight model frontier.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/cover.png"/></item><item><title>MoViE: Mobile Diffusion for Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</guid><description>MoViE: Mobile Diffusion for Video Editing achieves 12 FPS video editing on mobile phones by optimizing existing image editing models, achieving a major breakthrough in on-device video processing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/cover.png"/></item><item><title>LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</guid><description>LiFT leverages human feedback, including reasoning, to effectively align text-to-video models with human preferences, significantly improving video quality.</description></item><item><title>Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</guid><description>RAPL efficiently aligns robots with human preferences using minimal feedback by aligning visual representations before reward learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/cover.png"/></item><item><title>Mind the Time: Temporally-Controlled Multi-Event Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</guid><description>MinT: Generating coherent videos with precisely timed, multiple events via temporal control, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/cover.png"/></item><item><title>AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/</guid><description>AnyDressing: Customizable multi-garment virtual dressing via a novel latent diffusion model!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/cover.png"/></item><item><title>Hidden in the Noise: Two-Stage Robust Watermarking for Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/</guid><description>WIND: A novel, distortion-free image watermarking method leveraging diffusion models&amp;rsquo; initial noise for robust AI-generated content authentication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/cover.png"/></item><item><title>HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/</guid><description>HumanEdit: A new human-rewarded dataset revolutionizes instruction-based image editing by providing high-quality, diverse image pairs with detailed instructions, enabling precise model evaluation and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/cover.png"/></item><item><title>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/</guid><description>Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/cover.png"/></item><item><title>SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/</guid><description>SwiftEdit achieves lightning-fast, high-quality text-guided image editing in just 0.23 seconds via a novel one-step diffusion process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/cover.png"/></item><item><title>ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</guid><description>ZipAR accelerates autoregressive image generation by up to 91% through parallel decoding leveraging spatial locality in images, making high-resolution image generation significantly faster.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/cover.png"/></item><item><title>2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/</guid><description>2DGS-Room: Seed-guided 2D Gaussian splatting with geometric constraints achieves state-of-the-art high-fidelity indoor scene reconstruction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/cover.png"/></item><item><title>CleanDIFT: Diffusion Features without Noise</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/</guid><description>CleanDIFT revolutionizes diffusion feature extraction by leveraging clean images and a lightweight fine-tuning method, significantly boosting performance across various tasks without noise or timestep&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/cover.png"/></item><item><title>Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/</guid><description>ScoreLiDAR: Distilling diffusion models for 5x faster, higher-quality 3D LiDAR scene completion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/cover.png"/></item><item><title>Imagine360: Immersive 360 Video Generation from Perspective Anchor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/</guid><description>Imagine360: Generating immersive 360° videos from perspective videos, improving quality and accessibility of 360° content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/cover.png"/></item><item><title>MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/</guid><description>MIDI: a novel multi-instance diffusion model generates compositional 3D scenes from single images by simultaneously creating multiple 3D instances with accurate spatial relationships and high generali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/cover.png"/></item><item><title>Mimir: Improving Video Diffusion Models for Precise Text Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</guid><description>Mimir: A novel framework harmonizes LLMs and video diffusion models for precise text understanding in video generation, producing high-quality videos with superior text comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/cover.png"/></item><item><title>MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04106/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04106/</guid><description>MRGen, a novel diffusion-based data engine, controllably synthesizes MRI data for unannotated modalities, boosting segmentation model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04106/cover.png"/></item><item><title>MV-Adapter: Multi-view Consistent Image Generation Made Easy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/</guid><description>MV-Adapter easily transforms existing image generators into multi-view consistent image generators, improving efficiency and adaptability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/cover.png"/></item><item><title>NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</guid><description>NVComposer: A novel generative NVS model boosts synthesis quality by implicitly inferring spatial relationships from multiple sparse, unposed images, eliminating reliance on external alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/cover.png"/></item><item><title>OmniCreator: Self-Supervised Unified Generation with Universal Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</guid><description>OmniCreator: Self-supervised unified image+video generation &amp;amp; universal editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/cover.png"/></item><item><title>Scaling Image Tokenizers with Grouped Spherical Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/</guid><description>GSQ-GAN, a novel image tokenizer, achieves superior reconstruction quality with 16x downsampling using grouped spherical quantization, enabling efficient scaling for high-fidelity image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/cover.png"/></item><item><title>SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/</guid><description>SNOOPI supercharges one-step diffusion model distillation with enhanced guidance, achieving state-of-the-art performance by stabilizing training and enabling negative prompt control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/cover.png"/></item><item><title>VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</guid><description>VideoGen-of-Thought (VGoT) creates high-quality, multi-shot videos by collaboratively generating scripts, keyframes, and video clips, ensuring narrative consistency and visual coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/cover.png"/></item><item><title>Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</guid><description>Presto: a novel video diffusion model generates 15-second, high-quality videos with unparalleled long-range coherence and rich content, achieved through a segmented cross-attention mechanism and the L&amp;hellip;</description></item><item><title>Negative Token Merging: Image-based Adversarial Feature Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/</guid><description>NegToMe: Image-based adversarial guidance improves image generation diversity and reduces similarity to copyrighted content without training, simply by using images instead of negative text prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/cover.png"/></item><item><title>NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/</guid><description>NitroFusion achieves high-fidelity single-step image generation using a dynamic adversarial training approach with a specialized discriminator pool, dramatically improving speed and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/cover.png"/></item><item><title>One Shot, One Talk: Whole-body Talking Avatar from a Single Image</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01106/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01106/</guid><description>One-shot image to realistic, animatable talking avatar! Novel pipeline uses diffusion models and a hybrid 3DGS-mesh representation, achieving seamless generalization and precise control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01106/cover.png"/></item><item><title>PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</guid><description>PhysGame benchmark unveils video LLMs&amp;rsquo; weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/cover.png"/></item><item><title>Structured 3D Latents for Scalable and Versatile 3D Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/</guid><description>Unified 3D latent representation (SLAT) enables versatile high-quality 3D asset generation, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/cover.png"/></item><item><title>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</guid><description>SWITTI: a novel scale-wise transformer achieves 7x faster text-to-image generation than state-of-the-art diffusion models, while maintaining competitive image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/cover.png"/></item><item><title>TinyFusion: Diffusion Transformers Learned Shallow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</guid><description>TinyFusion, a novel learnable depth pruning method, crafts efficient shallow diffusion transformers with superior post-fine-tuning performance, achieving a 2x speedup with less than 7% of the original&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/cover.png"/></item><item><title>VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</guid><description>VideoLights: a novel framework for joint video highlight detection &amp;amp; moment retrieval, boosts performance via feature refinement, cross-modal &amp;amp; cross-task alignment, achieving state-of-the-art results&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/cover.png"/></item><item><title>VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</guid><description>VISTA synthesizes long-duration, high-resolution video instruction data, creating VISTA-400K and HRVideoBench to significantly boost video LMM performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/cover.png"/></item><item><title>AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/</guid><description>AlphaTablets: A novel 3D plane representation enabling accurate, consistent, and flexible 3D planar reconstruction from monocular videos, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/cover.png"/></item><item><title>DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19527/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19527/</guid><description>DisCoRD: Rectified flow decodes discrete motion tokens into continuous, natural movement, balancing faithfulness and realism.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19527/cover.png"/></item><item><title>Efficient Track Anything</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/</guid><description>EfficientTAMs achieve comparable video object segmentation accuracy to SAM 2 with ~2x speedup using lightweight ViTs and efficient cross-attention.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/cover.png"/></item><item><title>Open-Sora Plan: Open-Source Large Video Generation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</guid><description>Open-Sora Plan introduces an open-source large video generation model capable of producing high-resolution videos with long durations, based on various user inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/cover.png"/></item><item><title>Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</guid><description>TeaCache: a training-free method boosts video diffusion model speed by up to 4.41x with minimal quality loss by cleverly caching intermediate outputs.</description></item><item><title>Trajectory Attention for Fine-grained Video Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</guid><description>Trajectory Attention enhances video motion control by injecting trajectory information, improving precision and long-range consistency in video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/cover.png"/></item><item><title>Video Depth without Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</guid><description>RollingDepth: Achieving state-of-the-art video depth estimation without using complex video models, by cleverly extending a single-image depth estimator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/cover.png"/></item><item><title>AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</guid><description>AC3D achieves precise 3D camera control in video diffusion transformers by analyzing camera motion&amp;rsquo;s spectral properties, optimizing pose conditioning, and using a curated dataset of dynamic videos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/cover.png"/></item><item><title>Adaptive Blind All-in-One Image Restoration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18412/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18412/</guid><description>Adaptive Blind All-in-One Image Restoration (ABAIR) efficiently handles diverse image degradations, generalizes well to unseen distortions, and easily incorporates new ones via efficient fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18412/cover.png"/></item><item><title>CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18613/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18613/</guid><description>CAT4D: Create realistic 4D scenes from single-view videos using a novel multi-view video diffusion model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18613/cover.png"/></item><item><title>FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</guid><description>FAM Diffusion: Generate high-res images seamlessly from pre-trained diffusion models, solving structural and texture inconsistencies without retraining!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/cover.png"/></item><item><title>Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18197/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18197/</guid><description>Make-It-Animatable: Instantly create animation-ready 3D characters, regardless of pose or shape, using a novel data-driven framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18197/cover.png"/></item><item><title>ROICtrl: Boosting Instance Control for Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</guid><description>ROICtrl boosts visual generation&amp;rsquo;s instance control by using regional instance control via ROI-Align and a new ROI-Unpool operation, resulting in precise regional control and high efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/cover.png"/></item><item><title>TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</guid><description>TAPTRv3 achieves state-of-the-art long-video point tracking by cleverly using spatial and temporal context to enhance feature querying, surpassing previous methods and demonstrating strong performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/cover.png"/></item><item><title>TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</guid><description>TryOffDiff generates realistic garment images from single photos, solving virtual try-on limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/cover.png"/></item><item><title>AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</guid><description>AnchorCrafter animates cyber-anchors selling products via human-object interacting video generation, achieving high visual fidelity and controllable interactions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/cover.png"/></item><item><title>ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</guid><description>ChatGen-Evo automates text-to-image generation from freestyle chatting, simplifying the process and significantly improving performance over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/cover.png"/></item><item><title>Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</guid><description>Collaborative Decoding (CoDe) dramatically boosts visual auto-regressive model efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/cover.png"/></item><item><title>DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</guid><description>DreamCache enables efficient, high-quality personalized image generation without finetuning by caching reference image features and using lightweight conditioning adapters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/cover.png"/></item><item><title>DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</guid><description>DreamMix enhances image inpainting by disentangling object attributes for precise editing, enabling both identity preservation and flexible text-driven modifications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/cover.png"/></item><item><title>Identity-Preserving Text-to-Video Generation by Frequency Decomposition</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</guid><description>ConsisID achieves high-quality, identity-preserving text-to-video generation using a tuning-free diffusion transformer model that leverages frequency decomposition for effective identity control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/cover.png"/></item><item><title>MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17945/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17945/</guid><description>MARVEL-40M+ &amp;amp; MARVEL-FX3D: 40M+ high-quality 3D annotations &amp;amp; a fast two-stage text-to-3D pipeline enabling high-fidelity 3D model generation within 15 seconds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17945/cover.png"/></item><item><title>Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</guid><description>Omegance: One parameter precisely controls image detail in diffusion models, enabling flexible granularity adjustments without model changes or retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/cover.png"/></item><item><title>WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</guid><description>WF-VAE boosts video VAE performance with wavelet-driven energy flow and causal caching, enabling 2x higher throughput and 4x lower memory usage in latent video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/cover.png"/></item><item><title>Controllable Human Image Generation with Personalized Multi-Garments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</guid><description>BootComp: generate realistic human images wearing multiple garments using a novel synthetic data pipeline &amp;amp; diffusion model, enabling diverse applications like virtual try-on.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/cover.png"/></item><item><title>DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</guid><description>DREAMRUNNER generates high-quality storytelling videos by using LLMs for hierarchical planning, motion retrieval, and a novel spatial-temporal region-based diffusion model for fine-grained control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/cover.png"/></item><item><title>Factorized Visual Tokenization and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</guid><description>FQGAN revitalizes image generation by introducing Factorized Quantization, enabling scalable and stable visual tokenization with state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/cover.png"/></item><item><title>Learning 3D Representations from Procedural 3D Programs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17467/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17467/</guid><description>Self-supervised learning of 3D representations from procedurally generated synthetic shapes achieves comparable performance to models trained on real-world datasets, highlighting the potential of synt&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17467/cover.png"/></item><item><title>One Diffusion to Generate Them All</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</guid><description>OneDiffusion: A single diffusion model masters image synthesis &amp;amp; understanding across diverse tasks, from text-to-image to depth estimation, pushing the boundaries of AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/cover.png"/></item><item><title>Pathways on the Image Manifold: Image Editing via Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</guid><description>Image editing is revolutionized by Frame2Frame, which uses video generation to produce seamless and accurate edits, preserving image fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/cover.png"/></item><item><title>SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16856/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16856/</guid><description>SAR3D: Blazing-fast autoregressive 3D object generation and understanding using a multi-scale VQVAE, achieving sub-second generation and detailed multimodal comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16856/cover.png"/></item><item><title>SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16443/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16443/</guid><description>SplatFlow: A novel multi-view rectified flow model enabling direct 3D Gaussian splatting generation &amp;amp; training-free editing for diverse 3D tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16443/cover.png"/></item><item><title>VisualLens: Personalization through Visual History</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16034/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16034/</guid><description>VisualLens leverages user visual history for personalized recommendations, improving state-of-the-art by 5-10% and exceeding GPT-4&amp;rsquo;s performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16034/cover.png"/></item><item><title>Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15872/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15872/</guid><description>MedNeXt, a novel model ensemble, optimizes brain tumor segmentation in diverse populations, achieving state-of-the-art results on the BraTS 2024 SSA and pediatric datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15872/cover.png"/></item><item><title>Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</guid><description>New benchmark VCT² reveals limitations of AI-generated image detectors; Visual AI Index (VAI) provides a robust evaluation framework.</description></item><item><title>Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</guid><description>Diptych Prompting: a novel zero-shot subject-driven image generator leveraging large-scale text-to-image models and inpainting for precise subject alignment and high-quality image synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/cover.png"/></item><item><title>Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</guid><description>CoordTok: a novel video tokenizer drastically reduces token count for long videos, enabling memory-efficient training of diffusion models for high-quality, long video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/cover.png"/></item><item><title>Material Anything: Generating Materials for Any 3D Object via Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15138/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15138/</guid><description>Material Anything: Generate realistic materials for ANY 3D object via diffusion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15138/cover.png"/></item><item><title>Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</guid><description>Morph: a novel motion-free physics optimization framework drastically enhances human motion generation&amp;rsquo;s physical plausibility using synthetic data, achieving state-of-the-art quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/cover.png"/></item><item><title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</guid><description>OminiControl: A minimal, universal framework efficiently integrates image conditions into diffusion transformers, enabling diverse and precise control over image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/cover.png"/></item><item><title>Style-Friendly SNR Sampler for Style-Driven Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</guid><description>Style-friendly SNR sampler biases diffusion model training towards higher noise levels, enabling it to learn and generate images with higher style fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/cover.png"/></item><item><title>TEXGen: a Generative Diffusion Model for Mesh Textures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</guid><description>TEXGen: A groundbreaking generative diffusion model creates high-resolution 3D mesh textures directly from text and image prompts, exceeding prior methods in quality and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/cover.png"/></item><item><title>MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</guid><description>MagicDriveDiT generates high-resolution, long street-view videos with precise control, exceeding limitations of previous methods in autonomous driving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/cover.png"/></item><item><title>MyTimeMachine: Personalized Facial Age Transformation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</guid><description>MyTimeMachine personalizes facial age transformation using just 50 personal photos, outperforming existing methods by generating re-aged faces that closely match a person&amp;rsquo;s actual appearance at variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/cover.png"/></item><item><title>Novel View Extrapolation with Video Diffusion Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14208/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14208/</guid><description>ViewExtrapolator leverages Stable Video Diffusion to realistically extrapolate novel views far beyond training data, dramatically improving the quality of 3D scene generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14208/cover.png"/></item><item><title>SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14525/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14525/</guid><description>SegBook: a large-scale benchmark, reveals that fine-tuning full-body CT pre-trained models significantly improves performance on various downstream medical image segmentation tasks, particularly for s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14525/cover.png"/></item><item><title>Stable Flow: Vital Layers for Training-Free Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</guid><description>Stable Flow achieves diverse, consistent image editing without training by strategically injecting source image features into specific &amp;lsquo;vital&amp;rsquo; layers of a diffusion transformer model. This training-f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/cover.png"/></item><item><title>VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</guid><description>VBench++: A new benchmark suite meticulously evaluates video generative models across 16 diverse dimensions, aligning with human perception for improved model development and fairer comparisons.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/cover.png"/></item><item><title>Stylecodes: Encoding Stylistic Information For Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</guid><description>StyleCodes enables easy style sharing for image generation by encoding styles as compact strings, enhancing control and collaboration while minimizing quality loss.</description></item><item><title>Continuous Speculative Decoding for Autoregressive Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</guid><description>Researchers have developed Continuous Speculative Decoding, boosting autoregressive image generation speed by up to 2.33x while maintaining image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/cover.png"/></item><item><title>Generative World Explorer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11844/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11844/</guid><description>Generative World Explorer (Genex) enables agents to imaginatively explore environments, updating beliefs with generated observations for better decision-making.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11844/cover.png"/></item><item><title>ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12044/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12044/</guid><description>ITACLIP boosts training-free semantic segmentation by architecturally enhancing CLIP, integrating LLM-generated class descriptions, and employing image engineering; achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12044/cover.png"/></item><item><title>SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</guid><description>SAMURAI enhances the Segment Anything Model 2 for real-time, zero-shot visual object tracking by incorporating motion-aware memory and motion modeling, significantly improving accuracy and robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/cover.png"/></item><item><title>AnimateAnything: Consistent and Controllable Animation for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</guid><description>AnimateAnything: A unified approach enabling precise &amp;amp; consistent video manipulation via a novel optical flow representation and frequency stabilization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/cover.png"/></item><item><title>FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</guid><description>FitDiT boosts virtual try-on realism by enhancing garment details via Diffusion Transformers, improving texture and size accuracy for high-fidelity virtual fashion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/cover.png"/></item><item><title>Number it: Temporal Grounding Videos like Flipping Manga</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</guid><description>Boosting video temporal grounding, NumPro empowers Vid-LLMs by adding frame numbers, making temporal localization as easy as flipping through manga.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/cover.png"/></item><item><title>SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10161/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10161/</guid><description>SEAGULL: A novel network uses vision-language instruction tuning to assess image quality for regions of interest (ROIs) with high accuracy, leveraging masks and a new dataset for fine-grained IQA.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10161/cover.png"/></item><item><title>MagicQuill: An Intelligent Interactive Image Editing System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</guid><description>MagicQuill: an intelligent interactive image editing system enabling intuitive, precise image edits via brushstrokes and real-time intent prediction by a multimodal LLM.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/cover.png"/></item><item><title>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</guid><description>EgoVid-5M: First high-quality dataset for egocentric video generation, enabling realistic human-centric world simulations.</description></item><item><title>Sharingan: Extract User Action Sequence from Desktop Recordings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</guid><description>Sharingan extracts user action sequences from desktop recordings using novel VLM-based methods, achieving 70-80% accuracy and enabling RPA.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/cover.png"/></item><item><title>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08033/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08033/</guid><description>GaussianAnything: Interactive point cloud latent diffusion enables high-quality, editable 3D models from images or text, overcoming existing 3D generation limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08033/cover.png"/></item><item><title>Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08017/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08017/</guid><description>WaLa: a billion-parameter 3D generative model using wavelet encodings achieves state-of-the-art results, generating high-quality 3D shapes in seconds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08017/cover.png"/></item><item><title>Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</guid><description>Add-it: Training-free object insertion in images using pretrained diffusion models by cleverly balancing information from the scene, text prompt, and generated image, achieving state-of-the-art result&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/cover.png"/></item><item><title>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</guid><description>Edify Image: groundbreaking pixel-perfect photorealistic image generation using cascaded pixel-space diffusion models with a novel Laplacian diffusion process, enabling diverse applications including &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/cover.png"/></item><item><title>OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</guid><description>OmniEdit, a novel instruction-based image editing model, surpasses existing methods by leveraging specialist supervision and high-quality data, achieving superior performance across diverse editing ta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/cover.png"/></item><item><title>SAMPart3D: Segment Any Part in 3D Objects</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07184/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07184/</guid><description>SAMPart3D: Zero-shot 3D part segmentation across granularities, scaling to large datasets &amp;amp; handling part ambiguity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07184/cover.png"/></item><item><title>KMM: Key Frame Mask Mamba for Extended Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06481/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06481/</guid><description>KMM: Key Frame Mask Mamba generates extended, diverse human motion from text prompts by innovatively masking key frames in the Mamba architecture and using contrastive learning for improved text-motio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06481/cover.png"/></item><item><title>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</guid><description>StdGEN: Generate high-quality, semantically decomposed 3D characters from a single image in minutes, enabling flexible customization for various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/cover.png"/></item><item><title>DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/</guid><description>DimensionX generates photorealistic 3D and 4D scenes from a single image via controllable video diffusion, enabling precise manipulation of spatial structure and temporal dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/cover.png"/></item><item><title>GazeGen: Gaze-Driven User Interaction for Visual Content Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/</guid><description>GazeGen uses real-time gaze tracking to enable intuitive hands-free visual content creation and editing, setting a new standard for accessible AR/VR interaction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/cover.png"/></item><item><title>ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</guid><description>ReCapture generates videos with novel camera angles from user videos using masked video fine-tuning, preserving scene motion and plausibly hallucinating unseen parts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/cover.png"/></item><item><title>SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</guid><description>SG-I2V: Zero-shot controllable image-to-video generation using a self-guided approach that leverages pre-trained models for precise object and camera motion control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/cover.png"/></item><item><title>SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</guid><description>SVDQuant boosts 4-bit diffusion models by absorbing outliers via low-rank components, achieving 3.5x memory reduction and 3x speedup on 12B parameter models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/cover.png"/></item><item><title>Correlation of Object Detection Performance with Visual Saliency and Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02844/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02844/</guid><description>Visual saliency boosts object detection accuracy more than depth estimation, especially for larger objects, offering valuable insights for model and dataset improvement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02844/cover.png"/></item><item><title>GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/</guid><description>GarVerseLOD introduces a novel dataset and framework for high-fidelity 3D garment reconstruction from a single image, achieving unprecedented robustness via a hierarchical approach and leveraging a ma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/cover.png"/></item><item><title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</guid><description>Adaptive Caching (AdaCache) dramatically speeds up video generation with diffusion transformers by cleverly caching and reusing computations, tailoring the process to each video&amp;rsquo;s complexity and motio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/cover.png"/></item><item><title>GenXD: Generating Any 3D and 4D Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02319/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02319/</guid><description>GenXD: A unified model generating high-quality 3D &amp;amp; 4D scenes from any number of images, advancing the field of dynamic scene generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02319/cover.png"/></item><item><title>How Far is Video Generation from World Model: A Physical Law Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</guid><description>Scaling video generation models doesn&amp;rsquo;t guarantee they&amp;rsquo;ll learn physics; this study reveals they prioritize visual cues over true physical understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/cover.png"/></item><item><title>Training-free Regional Prompting for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</guid><description>Training-free Regional Prompting for FLUX boosts compositional text-to-image generation by cleverly manipulating attention mechanisms, achieving fine-grained control without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/cover.png"/></item><item><title>DreamPolish: Domain Score Distillation With Progressive Geometry Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01602/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01602/</guid><description>DreamPolish: A new text-to-3D model generates highly detailed 3D objects with polished surfaces and realistic textures using progressive geometry refinement and a novel domain score distillation tech&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01602/cover.png"/></item><item><title>Constant Acceleration Flow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</guid><description>Constant Acceleration Flow (CAF) dramatically speeds up diffusion model generation by using a constant acceleration equation, outperforming state-of-the-art methods with improved accuracy and few-step&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/cover.png"/></item><item><title>Randomized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</guid><description>Randomized Autoregressive Modeling (RAR) sets a new state-of-the-art in image generation by cleverly introducing randomness during training to improve the model&amp;rsquo;s ability to learn from bidirectional c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/cover.png"/></item><item><title>DELTA: Dense Efficient Long-range 3D Tracking for any video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24211/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24211/</guid><description>DELTA: A new method efficiently tracks every pixel in 3D space from monocular videos, enabling accurate motion estimation across entire videos with state-of-the-art accuracy and over 8x speed improvem&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24211/cover.png"/></item><item><title>In-Context LoRA for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</guid><description>In-Context LoRA empowers existing text-to-image models for high-fidelity multi-image generation by simply concatenating images and using minimal task-specific LoRA tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/cover.png"/></item><item><title>Learning Video Representations without Natural Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</guid><description>High-performing video representation models can be trained using only synthetic videos and images, eliminating the need for large natural video datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/cover.png"/></item><item><title>HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</guid><description>HelloMeme enhances text-to-image models by integrating spatial knitting attentions, enabling high-fidelity meme video generation while preserving model generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/cover.png"/></item><item><title>DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00836/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00836/</guid><description>DynaMath, a novel benchmark, reveals that state-of-the-art VLMs struggle with variations of simple math problems, showcasing their reasoning fragility. It offers 501 high-quality seed questions, dyna&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00836/cover.png"/></item><item><title>WHAC: World-grounded Humans and Cameras</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2403.12959/</link><pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2403.12959/</guid><description>WHAC: Grounding humans and cameras together!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2403.12959/cover.png"/></item></channel></rss>