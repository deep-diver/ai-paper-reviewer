<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Computer Vision on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/computer-vision/</link><description>Recent content in Computer Vision on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 AI Paper Reviews by AI</copyright><lastBuildDate>Mon, 06 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/computer-vision/index.xml" rel="self" type="application/rss+xml"/><item><title>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</guid><description>STAR: A novel approach uses text-to-video models for realistic, temporally consistent real-world video super-resolution, improving image quality and detail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/cover.png"/></item><item><title>Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/</guid><description>Through-The-Mask uses mask-based motion trajectories to generate realistic videos from images and text, overcoming limitations of existing methods in handling complex multi-object motion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03059/cover.png"/></item><item><title>TransPixar: Advancing Text-to-Video Generation with Transparency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</guid><description>TransPixar generates high-quality videos with transparency by jointly training RGB and alpha channels, outperforming sequential generation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/cover.png"/></item><item><title>DepthMaster: Taming Diffusion Models for Monocular Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/</guid><description>DepthMaster tames diffusion models for faster, more accurate monocular depth estimation by aligning generative features with high-quality semantic features and adaptively balancing low and high-freque&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02576/cover.png"/></item><item><title>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</guid><description>GS-DiT: Generating high-quality videos with advanced 4D control through efficient dense 3D point tracking and pseudo 4D Gaussian fields.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/cover.png"/></item><item><title>Ingredients: Blending Custom Photos with Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</guid><description>Ingredients: A new framework customizes videos by blending multiple photos with video diffusion transformers, enabling realistic and personalized video generation while maintaining consistent identity&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/cover.png"/></item><item><title>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/</guid><description>LightningDiT resolves the optimization dilemma in latent diffusion models by aligning latent space with pre-trained vision models, achieving state-of-the-art ImageNet 256x256 generation with over 21x &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01423/cover.png"/></item><item><title>SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</guid><description>SeedVR: A novel diffusion transformer revolutionizes generic video restoration by efficiently handling arbitrary video lengths and resolutions, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/cover.png"/></item><item><title>SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/</guid><description>SeFAR: a novel semi-supervised framework for fine-grained action recognition, achieves state-of-the-art results by using dual-level temporal modeling, moderate temporal perturbation, and adaptive regu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01245/cover.png"/></item><item><title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</guid><description>VideoAnydoor: High-fidelity video object insertion with precise motion control, achieved via an end-to-end framework leveraging an ID extractor and a pixel warper for robust detail preservation and fi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/cover.png"/></item><item><title>MLLM-as-a-Judge for Image Safety without Human Labeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</guid><description>Zero-shot image safety judgment is achieved using MLLMs and a novel method called CLUE, objectifying safety rules, and significantly reducing the need for human labeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/cover.png"/></item><item><title>Edicho: Consistent Image Editing in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</guid><description>Edicho: a novel training-free method for consistent image editing across diverse images, achieving precise consistency by leveraging explicit correspondence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/cover.png"/></item><item><title>VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/</guid><description>VisionReward, a novel reward model, surpasses existing methods by precisely capturing multi-dimensional human preferences for image and video generation, enabling more accurate and stable model optimi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21059/cover.png"/></item><item><title>Bringing Objects to Life: 4D generation from 3D objects</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</link><pubDate>Sun, 29 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/</guid><description>3to4D: Animate any 3D object with text prompts, preserving visual quality and achieving realistic motion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20422/cover.png"/></item><item><title>VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</guid><description>VideoMaker achieves high-fidelity zero-shot customized video generation by cleverly harnessing the inherent power of video diffusion models, eliminating the need for extra feature extraction and injec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/cover.png"/></item><item><title>DepthLab: From Partial to Complete</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/</guid><description>DepthLab: a novel image-conditioned depth inpainting model enhances downstream 3D tasks by effectively completing partial depth information, showing superior performance and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18153/cover.png"/></item><item><title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</guid><description>DiTCtrl achieves state-of-the-art multi-prompt video generation without retraining by cleverly controlling attention in a diffusion transformer, enabling smooth transitions between video segments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/cover.png"/></item><item><title>Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</guid><description>Orient Anything: Learning robust object orientation estimation directly from rendered 3D models, achieving state-of-the-art accuracy on real images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/cover.png"/></item><item><title>PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</guid><description>PartGen generates compositional 3D objects with meaningful parts from text, images, or unstructured 3D data using multi-view diffusion models, enabling flexible 3D part editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/cover.png"/></item><item><title>SBS Figures: Pre-training Figure QA from Stage-by-Stage Synthesized Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/</guid><description>SBS Figures creates a massive, high-quality figure QA dataset via a novel stage-by-stage synthesis pipeline, enabling efficient pre-training of visual language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17606/cover.png"/></item><item><title>Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/</guid><description>Distilled Decoding (DD) drastically speeds up image generation from autoregressive models by using flow matching to enable one-step sampling, achieving significant speedups while maintaining acceptabl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17153/cover.png"/></item><item><title>CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</link><pubDate>Fri, 20 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/</guid><description>CLEAR: Conv-Like Linearization boosts pre-trained Diffusion Transformers, achieving 6.3x faster 8K image generation with minimal quality loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16112/cover.png"/></item><item><title>Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/</guid><description>Affordance-Aware Object Insertion uses a novel Mask-Aware Dual Diffusion model &amp;amp; SAM-FB dataset to realistically place objects in scenes, considering contextual relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14462/cover.png"/></item><item><title>DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/</guid><description>DI-PCG uses a lightweight diffusion transformer to efficiently and accurately estimate parameters of procedural generators from images, enabling high-fidelity 3D asset creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15200/cover.png"/></item><item><title>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</guid><description>LeviTor: Revolutionizing image-to-video synthesis with intuitive 3D trajectory control, generating realistic videos from static images by abstracting object masks into depth-aware control points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/cover.png"/></item><item><title>Parallelized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/</guid><description>Boosting autoregressive visual generation speed by 3.6-9.5x, this research introduces parallel processing while preserving model simplicity and generation quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15119/cover.png"/></item><item><title>UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit Consistency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/</guid><description>UIP2P: Unsupervised instruction-based image editing achieves high-fidelity edits by enforcing Cycle Edit Consistency, eliminating the need for ground-truth data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15216/cover.png"/></item><item><title>AniDoc: Animation Creation Made Easier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</guid><description>AniDoc automates cartoon animation line art video colorization, making animation creation easier!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/cover.png"/></item><item><title>FashionComposer: Compositional Fashion Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/</guid><description>FashionComposer revolutionizes fashion image creation through flexible composition of garments, faces, and poses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14168/cover.png"/></item><item><title>Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</guid><description>Prompting unlocks 4K metric depth from low-cost LiDAR.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/cover.png"/></item><item><title>ChatDiT: A Training-Free Baseline for Task-Agnostic Free-Form Chatting with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/</guid><description>ChatDiT enables zero-shot, multi-turn image generation using pretrained diffusion transformers and a novel multi-agent framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12571/cover.png"/></item><item><title>MIVE: New Design and Benchmark for Multi-Instance Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/</guid><description>Edit many objects at once in videos! MIVE does it accurately without affecting other areas, a big step for AI video editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/cover.png"/></item><item><title>Move-in-2D: 2D-Conditioned Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</guid><description>Move-in-2D generates realistic human motion sequences conditioned on a 2D scene image and text prompt, overcoming limitations of existing approaches and improving video synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/cover.png"/></item><item><title>VidTok: A Versatile and Open-Source Video Tokenizer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</guid><description>VidTok: an open-source, top performing video tokenizer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/cover.png"/></item><item><title>ColorFlow: Retrieval-Augmented Image Sequence Colorization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/</guid><description>ColorFlow, a new AI model, accurately colorizes black-and-white image sequences while preserving character identity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11815/cover.png"/></item><item><title>IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/</guid><description>IDArb: A diffusion model for decomposing images into intrinsic components like albedo, normal, and material properties, handling varying views and lighting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12083/cover.png"/></item><item><title>MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/</guid><description>MOVIS enhances 3D scene generation by improving cross-view consistency in multi-object novel view synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11457/cover.png"/></item><item><title>Sequence Matters: Harnessing Video Models in 3D Super-Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/</guid><description>Leveraging video models, researchers achieve state-of-the-art 3D super-resolution by generating &amp;lsquo;video-like&amp;rsquo; sequences from unordered images, eliminating artifacts and computational demands.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11525/cover.png"/></item><item><title>StrandHead: Text to Strand-Disentangled 3D Head Avatars Using Hair Geometric Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/</guid><description>Create realistic 3D heads with specific hairstyles from text, no 3D hair data needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11586/cover.png"/></item><item><title>Wonderland: Navigating 3D Scenes from a Single Image</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/</guid><description>Generate wide-scope 3D scenes from single images in a snap!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12091/cover.png"/></item><item><title>GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</guid><description>Training-free method adds physical properties to 3D models using vision-language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/cover.png"/></item><item><title>BrushEdit: All-In-One Image Inpainting and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/</guid><description>BrushEdit revolutionizes interactive image editing with instructions &amp;amp; inpainting.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10316/cover.png"/></item><item><title>Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attacks on Breast Ultrasound Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/</guid><description>New attack fools breast ultrasound AI using subtle text prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09910/cover.png"/></item><item><title>Arbitrary-steps Image Super-resolution via Diffusion Inversion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/</guid><description>InvSR: a novel image super-resolution technique using diffusion inversion, enabling flexible sampling steps for efficient and high-fidelity results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09013/cover.png"/></item><item><title>DisPose: Disentangling Pose Guidance for Controllable Human Image Animation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/</guid><description>DisPose disentangles pose guidance for controllable human image animation, generating diverse animations while preserving appearance consistency using only sparse skeleton pose input, eliminating the &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09349/cover.png"/></item><item><title>EasyRef: Omni-Generalized Group Image Reference for Diffusion Models via Multimodal LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/</guid><description>EasyRef uses multimodal LLMs to generate images from multiple references, overcoming limitations of prior methods by capturing consistent visual elements and offering improved zero-shot generalization&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09618/cover.png"/></item><item><title>FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/</guid><description>Edit images precisely with AI, no masks needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09611/cover.png"/></item><item><title>FreeScale: Unleashing the Resolution of Diffusion Models via Tuning-Free Scale Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/</guid><description>FreeScale generates stunning 8K images and high-fidelity videos without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09626/cover.png"/></item><item><title>FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/</guid><description>FreeSplatter: a novel feed-forward framework reconstructs high-quality 3D scenes from uncalibrated sparse-view images, estimating camera poses in seconds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/cover.png"/></item><item><title>Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/</guid><description>Gaze-LLE achieves state-of-the-art gaze estimation by using a frozen DINOv2 encoder and a lightweight decoder, simplifying architecture and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/cover.png"/></item><item><title>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</guid><description>InstanceCap improves text-to-video generation through detailed, instance-aware captions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/cover.png"/></item><item><title>Learned Compression for Compressed Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09405/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09405/</guid><description>WaLLOC: a novel neural codec boosts compressed-domain learning by combining wavelet transforms with asymmetric autoencoders, achieving high compression ratios with minimal computation and uniform dime&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09405/cover.png"/></item><item><title>LoRACLR: Contrastive Adaptation for Customization of Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/</guid><description>LoRACLR merges multiple LoRA models for high-fidelity multi-concept image generation, using a contrastive objective to ensure concept distinctiveness and prevent interference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09622/cover.png"/></item><item><title>Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09593/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09593/</guid><description>Neural LightRig uses multi-light diffusion to accurately estimate object normals and materials from a single image, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09593/cover.png"/></item><item><title>DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/</guid><description>DiffSensei: A new framework generates customized manga with dynamic multi-character control using multi-modal LLMs and diffusion models, outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07589/cover.png"/></item><item><title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</guid><description>Introducing Evaluation Agent, a faster, more flexible human-like framework for evaluating visual generative AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/cover.png"/></item><item><title>FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/</guid><description>FireFlow makes editing images faster and better.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07517/cover.png"/></item><item><title>FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/</guid><description>FiVA dataset and its adaptation framework enable unprecedented fine-grained control over visual attributes in text-to-image generation, empowering users to craft highly customized images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07674/cover.png"/></item><item><title>Mobile Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</guid><description>MobileVD: The first mobile-optimized video diffusion model, achieving 523x efficiency improvement over state-of-the-art with minimal quality loss, enabling realistic video generation on smartphones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/cover.png"/></item><item><title>ObjCtrl-2.5D: Training-free Object Control with Camera Poses</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</guid><description>ObjCtrl-2.5D: Training-free, precise image-to-video object control using 3D trajectories and camera poses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/cover.png"/></item><item><title>OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</guid><description>OmniDocBench, a novel benchmark, tackles limitations in current document parsing by introducing a diverse, high-quality dataset with comprehensive annotations, enabling fair multi-level evaluation of &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/cover.png"/></item><item><title>STIV: Scalable Text and Image Conditioned Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/</guid><description>STIV: A novel, scalable method for text and image-conditioned video generation, systematically improving model architectures, training, and data curation for superior performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07730/cover.png"/></item><item><title>UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/</guid><description>UniReal: a universal framework for image generation and editing, unifying diverse tasks via learning real-world dynamics from video data, achieving highly realistic and versatile results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07774/cover.png"/></item><item><title>Video Motion Transfer with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</guid><description>DiTFlow: training-free video motion transfer using Diffusion Transformers, enabling realistic motion control in synthesized videos via Attention Motion Flow.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/cover.png"/></item><item><title>EMOv2: Pushing 5M Vision Model Frontier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/</guid><description>EMOv2 achieves state-of-the-art performance in various vision tasks using a novel Meta Mobile Block, pushing the 5M parameter lightweight model frontier.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/cover.png"/></item><item><title>MoViE: Mobile Diffusion for Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</guid><description>MoViE: Mobile Diffusion for Video Editing achieves 12 FPS video editing on mobile phones by optimizing existing image editing models, achieving a major breakthrough in on-device video processing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/cover.png"/></item><item><title>LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</guid><description>LiFT leverages human feedback, including reasoning, to effectively align text-to-video models with human preferences, significantly improving video quality.</description></item><item><title>Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</guid><description>RAPL efficiently aligns robots with human preferences using minimal feedback by aligning visual representations before reward learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/cover.png"/></item><item><title>Mind the Time: Temporally-Controlled Multi-Event Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</guid><description>MinT: Generating coherent videos with precisely timed, multiple events via temporal control, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/cover.png"/></item><item><title>AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/</guid><description>AnyDressing: Customizable multi-garment virtual dressing via a novel latent diffusion model!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04146/cover.png"/></item><item><title>Hidden in the Noise: Two-Stage Robust Watermarking for Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/</guid><description>WIND: A novel, distortion-free image watermarking method leveraging diffusion models&amp;rsquo; initial noise for robust AI-generated content authentication.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04653/cover.png"/></item><item><title>HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/</guid><description>HumanEdit: A new human-rewarded dataset revolutionizes instruction-based image editing by providing high-quality, diverse image pairs with detailed instructions, enabling precise model evaluation and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04280/cover.png"/></item><item><title>Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/</guid><description>Infinity, a novel bitwise autoregressive model, sets new records in high-resolution image synthesis, outperforming top diffusion models in speed and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04431/cover.png"/></item><item><title>SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/</guid><description>SwiftEdit achieves lightning-fast, high-quality text-guided image editing in just 0.23 seconds via a novel one-step diffusion process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04301/cover.png"/></item><item><title>ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</guid><description>ZipAR accelerates autoregressive image generation by up to 91% through parallel decoding leveraging spatial locality in images, making high-resolution image generation significantly faster.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/cover.png"/></item><item><title>2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/</guid><description>2DGS-Room: Seed-guided 2D Gaussian splatting with geometric constraints achieves state-of-the-art high-fidelity indoor scene reconstruction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/cover.png"/></item><item><title>CleanDIFT: Diffusion Features without Noise</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/</guid><description>CleanDIFT revolutionizes diffusion feature extraction by leveraging clean images and a lightweight fine-tuning method, significantly boosting performance across various tasks without noise or timestep&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03439/cover.png"/></item><item><title>Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/</guid><description>ScoreLiDAR: Distilling diffusion models for 5x faster, higher-quality 3D LiDAR scene completion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/cover.png"/></item><item><title>Imagine360: Immersive 360 Video Generation from Perspective Anchor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/</guid><description>Imagine360: Generating immersive 360Â° videos from perspective videos, improving quality and accessibility of 360Â° content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03552/cover.png"/></item><item><title>MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/</guid><description>MIDI: a novel multi-instance diffusion model generates compositional 3D scenes from single images by simultaneously creating multiple 3D instances with accurate spatial relationships and high generali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/cover.png"/></item><item><title>Mimir: Improving Video Diffusion Models for Precise Text Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</guid><description>Mimir: A novel framework harmonizes LLMs and video diffusion models for precise text understanding in video generation, producing high-quality videos with superior text comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/cover.png"/></item><item><title>MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04106/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04106/</guid><description>MRGen, a novel diffusion-based data engine, controllably synthesizes MRI data for unannotated modalities, boosting segmentation model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04106/cover.png"/></item><item><title>MV-Adapter: Multi-view Consistent Image Generation Made Easy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/</guid><description>MV-Adapter easily transforms existing image generators into multi-view consistent image generators, improving efficiency and adaptability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03632/cover.png"/></item><item><title>NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</guid><description>NVComposer: A novel generative NVS model boosts synthesis quality by implicitly inferring spatial relationships from multiple sparse, unposed images, eliminating reliance on external alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/cover.png"/></item><item><title>OmniCreator: Self-Supervised Unified Generation with Universal Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</guid><description>OmniCreator: Self-supervised unified image+video generation &amp;amp; universal editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/cover.png"/></item><item><title>Scaling Image Tokenizers with Grouped Spherical Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/</guid><description>GSQ-GAN, a novel image tokenizer, achieves superior reconstruction quality with 16x downsampling using grouped spherical quantization, enabling efficient scaling for high-fidelity image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02632/cover.png"/></item><item><title>SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/</guid><description>SNOOPI supercharges one-step diffusion model distillation with enhanced guidance, achieving state-of-the-art performance by stabilizing training and enabling negative prompt control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02687/cover.png"/></item><item><title>VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</guid><description>VideoGen-of-Thought (VGoT) creates high-quality, multi-shot videos by collaboratively generating scripts, keyframes, and video clips, ensuring narrative consistency and visual coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/cover.png"/></item><item><title>Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</guid><description>Presto: a novel video diffusion model generates 15-second, high-quality videos with unparalleled long-range coherence and rich content, achieved through a segmented cross-attention mechanism and the L&amp;hellip;</description></item><item><title>Negative Token Merging: Image-based Adversarial Feature Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/</guid><description>NegToMe: Image-based adversarial guidance improves image generation diversity and reduces similarity to copyrighted content without training, simply by using images instead of negative text prompts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01339/cover.png"/></item><item><title>NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/</guid><description>NitroFusion achieves high-fidelity single-step image generation using a dynamic adversarial training approach with a specialized discriminator pool, dramatically improving speed and quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02030/cover.png"/></item><item><title>One Shot, One Talk: Whole-body Talking Avatar from a Single Image</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01106/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01106/</guid><description>One-shot image to realistic, animatable talking avatar! Novel pipeline uses diffusion models and a hybrid 3DGS-mesh representation, achieving seamless generalization and precise control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01106/cover.png"/></item><item><title>PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</guid><description>PhysGame benchmark unveils video LLMs&amp;rsquo; weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/cover.png"/></item><item><title>Structured 3D Latents for Scalable and Versatile 3D Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/</guid><description>Unified 3D latent representation (SLAT) enables versatile high-quality 3D asset generation, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/cover.png"/></item><item><title>Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/</guid><description>SWITTI: a novel scale-wise transformer achieves 7x faster text-to-image generation than state-of-the-art diffusion models, while maintaining competitive image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01819/cover.png"/></item><item><title>TinyFusion: Diffusion Transformers Learned Shallow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/</guid><description>TinyFusion, a novel learnable depth pruning method, crafts efficient shallow diffusion transformers with superior post-fine-tuning performance, achieving a 2x speedup with less than 7% of the original&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01199/cover.png"/></item><item><title>VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</guid><description>VideoLights: a novel framework for joint video highlight detection &amp;amp; moment retrieval, boosts performance via feature refinement, cross-modal &amp;amp; cross-task alignment, achieving state-of-the-art results&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/cover.png"/></item><item><title>VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</guid><description>VISTA synthesizes long-duration, high-resolution video instruction data, creating VISTA-400K and HRVideoBench to significantly boost video LMM performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/cover.png"/></item><item><title>AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/</guid><description>AlphaTablets: A novel 3D plane representation enabling accurate, consistent, and flexible 3D planar reconstruction from monocular videos, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/cover.png"/></item><item><title>DisCoRD: Discrete Tokens to Continuous Motion via Rectified Flow Decoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19527/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19527/</guid><description>DisCoRD: Rectified flow decodes discrete motion tokens into continuous, natural movement, balancing faithfulness and realism.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19527/cover.png"/></item><item><title>Efficient Track Anything</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/</guid><description>EfficientTAMs achieve comparable video object segmentation accuracy to SAM 2 with ~2x speedup using lightweight ViTs and efficient cross-attention.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/cover.png"/></item><item><title>Open-Sora Plan: Open-Source Large Video Generation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/</guid><description>Open-Sora Plan introduces an open-source large video generation model capable of producing high-resolution videos with long durations, based on various user inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00131/cover.png"/></item><item><title>Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</guid><description>TeaCache: a training-free method boosts video diffusion model speed by up to 4.41x with minimal quality loss by cleverly caching intermediate outputs.</description></item><item><title>Trajectory Attention for Fine-grained Video Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</guid><description>Trajectory Attention enhances video motion control by injecting trajectory information, improving precision and long-range consistency in video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/cover.png"/></item><item><title>Video Depth without Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</guid><description>RollingDepth: Achieving state-of-the-art video depth estimation without using complex video models, by cleverly extending a single-image depth estimator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/cover.png"/></item><item><title>AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</guid><description>AC3D achieves precise 3D camera control in video diffusion transformers by analyzing camera motion&amp;rsquo;s spectral properties, optimizing pose conditioning, and using a curated dataset of dynamic videos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/cover.png"/></item><item><title>Adaptive Blind All-in-One Image Restoration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18412/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18412/</guid><description>Adaptive Blind All-in-One Image Restoration (ABAIR) efficiently handles diverse image degradations, generalizes well to unseen distortions, and easily incorporates new ones via efficient fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18412/cover.png"/></item><item><title>CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18613/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18613/</guid><description>CAT4D: Create realistic 4D scenes from single-view videos using a novel multi-view video diffusion model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18613/cover.png"/></item><item><title>FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/</guid><description>FAM Diffusion: Generate high-res images seamlessly from pre-trained diffusion models, solving structural and texture inconsistencies without retraining!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18552/cover.png"/></item><item><title>Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18197/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18197/</guid><description>Make-It-Animatable: Instantly create animation-ready 3D characters, regardless of pose or shape, using a novel data-driven framework.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18197/cover.png"/></item><item><title>ROICtrl: Boosting Instance Control for Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/</guid><description>ROICtrl boosts visual generation&amp;rsquo;s instance control by using regional instance control via ROI-Align and a new ROI-Unpool operation, resulting in precise regional control and high efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17949/cover.png"/></item><item><title>TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</guid><description>TAPTRv3 achieves state-of-the-art long-video point tracking by cleverly using spatial and temporal context to enhance feature querying, surpassing previous methods and demonstrating strong performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/cover.png"/></item><item><title>TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/</guid><description>TryOffDiff generates realistic garment images from single photos, solving virtual try-on limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18350/cover.png"/></item><item><title>AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</guid><description>AnchorCrafter animates cyber-anchors selling products via human-object interacting video generation, achieving high visual fidelity and controllable interactions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/cover.png"/></item><item><title>ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/</guid><description>ChatGen-Evo automates text-to-image generation from freestyle chatting, simplifying the process and significantly improving performance over existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17176/cover.png"/></item><item><title>Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/</guid><description>Collaborative Decoding (CoDe) dramatically boosts visual auto-regressive model efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17787/cover.png"/></item><item><title>DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/</guid><description>DreamCache enables efficient, high-quality personalized image generation without finetuning by caching reference image features and using lightweight conditioning adapters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17786/cover.png"/></item><item><title>DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/</guid><description>DreamMix enhances image inpainting by disentangling object attributes for precise editing, enabling both identity preservation and flexible text-driven modifications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17223/cover.png"/></item><item><title>Identity-Preserving Text-to-Video Generation by Frequency Decomposition</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/</guid><description>ConsisID achieves high-quality, identity-preserving text-to-video generation using a tuning-free diffusion transformer model that leverages frequency decomposition for effective identity control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17440/cover.png"/></item><item><title>MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17945/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17945/</guid><description>MARVEL-40M+ &amp;amp; MARVEL-FX3D: 40M+ high-quality 3D annotations &amp;amp; a fast two-stage text-to-3D pipeline enabling high-fidelity 3D model generation within 15 seconds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17945/cover.png"/></item><item><title>Omegance: A Single Parameter for Various Granularities in Diffusion-Based Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/</guid><description>Omegance: One parameter precisely controls image detail in diffusion models, enabling flexible granularity adjustments without model changes or retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17769/cover.png"/></item><item><title>WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</guid><description>WF-VAE boosts video VAE performance with wavelet-driven energy flow and causal caching, enabling 2x higher throughput and 4x lower memory usage in latent video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/cover.png"/></item><item><title>Controllable Human Image Generation with Personalized Multi-Garments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/</guid><description>BootComp: generate realistic human images wearing multiple garments using a novel synthetic data pipeline &amp;amp; diffusion model, enabling diverse applications like virtual try-on.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16801/cover.png"/></item><item><title>DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</guid><description>DREAMRUNNER generates high-quality storytelling videos by using LLMs for hierarchical planning, motion retrieval, and a novel spatial-temporal region-based diffusion model for fine-grained control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/cover.png"/></item><item><title>Factorized Visual Tokenization and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/</guid><description>FQGAN revitalizes image generation by introducing Factorized Quantization, enabling scalable and stable visual tokenization with state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16681/cover.png"/></item><item><title>Learning 3D Representations from Procedural 3D Programs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17467/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17467/</guid><description>Self-supervised learning of 3D representations from procedurally generated synthetic shapes achieves comparable performance to models trained on real-world datasets, highlighting the potential of synt&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17467/cover.png"/></item><item><title>One Diffusion to Generate Them All</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/</guid><description>OneDiffusion: A single diffusion model masters image synthesis &amp;amp; understanding across diverse tasks, from text-to-image to depth estimation, pushing the boundaries of AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16318/cover.png"/></item><item><title>Pathways on the Image Manifold: Image Editing via Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/</guid><description>Image editing is revolutionized by Frame2Frame, which uses video generation to produce seamless and accurate edits, preserving image fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16819/cover.png"/></item><item><title>SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16856/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16856/</guid><description>SAR3D: Blazing-fast autoregressive 3D object generation and understanding using a multi-scale VQVAE, achieving sub-second generation and detailed multimodal comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16856/cover.png"/></item><item><title>SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16443/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16443/</guid><description>SplatFlow: A novel multi-view rectified flow model enabling direct 3D Gaussian splatting generation &amp;amp; training-free editing for diverse 3D tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16443/cover.png"/></item><item><title>VisualLens: Personalization through Visual History</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16034/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16034/</guid><description>VisualLens leverages user visual history for personalized recommendations, improving state-of-the-art by 5-10% and exceeding GPT-4&amp;rsquo;s performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16034/cover.png"/></item><item><title>Optimizing Brain Tumor Segmentation with MedNeXt: BraTS 2024 SSA and Pediatrics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15872/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15872/</guid><description>MedNeXt, a novel model ensemble, optimizes brain tumor segmentation in diverse populations, achieving state-of-the-art results on the BraTS 2024 SSA and pediatric datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15872/cover.png"/></item><item><title>Visual Counter Turing Test (VCT^2): Discovering the Challenges for AI-Generated Image Detection and Introducing Visual AI Index (V_AI)</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</link><pubDate>Sun, 24 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16754/</guid><description>New benchmark VCTÂ² reveals limitations of AI-generated image detectors; Visual AI Index (VAI) provides a robust evaluation framework.</description></item><item><title>Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</link><pubDate>Sat, 23 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/</guid><description>Diptych Prompting: a novel zero-shot subject-driven image generator leveraging large-scale text-to-image models and inpainting for precise subject alignment and high-quality image synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15466/cover.png"/></item><item><title>Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</guid><description>CoordTok: a novel video tokenizer drastically reduces token count for long videos, enabling memory-efficient training of diffusion models for high-quality, long video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/cover.png"/></item><item><title>Material Anything: Generating Materials for Any 3D Object via Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15138/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15138/</guid><description>Material Anything: Generate realistic materials for ANY 3D object via diffusion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15138/cover.png"/></item><item><title>Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</guid><description>Morph: a novel motion-free physics optimization framework drastically enhances human motion generation&amp;rsquo;s physical plausibility using synthetic data, achieving state-of-the-art quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/cover.png"/></item><item><title>OminiControl: Minimal and Universal Control for Diffusion Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/</guid><description>OminiControl: A minimal, universal framework efficiently integrates image conditions into diffusion transformers, enabling diverse and precise control over image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15098/cover.png"/></item><item><title>Style-Friendly SNR Sampler for Style-Driven Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/</guid><description>Style-friendly SNR sampler biases diffusion model training towards higher noise levels, enabling it to learn and generate images with higher style fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14793/cover.png"/></item><item><title>TEXGen: a Generative Diffusion Model for Mesh Textures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/</guid><description>TEXGen: A groundbreaking generative diffusion model creates high-resolution 3D mesh textures directly from text and image prompts, exceeding prior methods in quality and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14740/cover.png"/></item><item><title>MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</guid><description>MagicDriveDiT generates high-resolution, long street-view videos with precise control, exceeding limitations of previous methods in autonomous driving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/cover.png"/></item><item><title>MyTimeMachine: Personalized Facial Age Transformation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/</guid><description>MyTimeMachine personalizes facial age transformation using just 50 personal photos, outperforming existing methods by generating re-aged faces that closely match a person&amp;rsquo;s actual appearance at variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14521/cover.png"/></item><item><title>Novel View Extrapolation with Video Diffusion Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14208/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14208/</guid><description>ViewExtrapolator leverages Stable Video Diffusion to realistically extrapolate novel views far beyond training data, dramatically improving the quality of 3D scene generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14208/cover.png"/></item><item><title>SegBook: A Simple Baseline and Cookbook for Volumetric Medical Image Segmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14525/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14525/</guid><description>SegBook: a large-scale benchmark, reveals that fine-tuning full-body CT pre-trained models significantly improves performance on various downstream medical image segmentation tasks, particularly for s&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14525/cover.png"/></item><item><title>Stable Flow: Vital Layers for Training-Free Image Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/</guid><description>Stable Flow achieves diverse, consistent image editing without training by strategically injecting source image features into specific &amp;lsquo;vital&amp;rsquo; layers of a diffusion transformer model. This training-f&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14430/cover.png"/></item><item><title>VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/</guid><description>VBench++: A new benchmark suite meticulously evaluates video generative models across 16 diverse dimensions, aligning with human perception for improved model development and fairer comparisons.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13503/cover.png"/></item><item><title>Stylecodes: Encoding Stylistic Information For Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12811/</guid><description>StyleCodes enables easy style sharing for image generation by encoding styles as compact strings, enhancing control and collaboration while minimizing quality loss.</description></item><item><title>Continuous Speculative Decoding for Autoregressive Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/</guid><description>Researchers have developed Continuous Speculative Decoding, boosting autoregressive image generation speed by up to 2.33x while maintaining image quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11925/cover.png"/></item><item><title>Generative World Explorer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11844/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11844/</guid><description>Generative World Explorer (Genex) enables agents to imaginatively explore environments, updating beliefs with generated observations for better decision-making.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11844/cover.png"/></item><item><title>ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12044/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12044/</guid><description>ITACLIP boosts training-free semantic segmentation by architecturally enhancing CLIP, integrating LLM-generated class descriptions, and employing image engineering; achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12044/cover.png"/></item><item><title>SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</guid><description>SAMURAI enhances the Segment Anything Model 2 for real-time, zero-shot visual object tracking by incorporating motion-aware memory and motion modeling, significantly improving accuracy and robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/cover.png"/></item><item><title>AnimateAnything: Consistent and Controllable Animation for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</guid><description>AnimateAnything: A unified approach enabling precise &amp;amp; consistent video manipulation via a novel optical flow representation and frequency stabilization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/cover.png"/></item><item><title>FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/</guid><description>FitDiT boosts virtual try-on realism by enhancing garment details via Diffusion Transformers, improving texture and size accuracy for high-fidelity virtual fashion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10499/cover.png"/></item><item><title>Number it: Temporal Grounding Videos like Flipping Manga</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</guid><description>Boosting video temporal grounding, NumPro empowers Vid-LLMs by adding frame numbers, making temporal localization as easy as flipping through manga.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/cover.png"/></item><item><title>SEAGULL: No-reference Image Quality Assessment for Regions of Interest via Vision-Language Instruction Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10161/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10161/</guid><description>SEAGULL: A novel network uses vision-language instruction tuning to assess image quality for regions of interest (ROIs) with high accuracy, leveraging masks and a new dataset for fine-grained IQA.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10161/cover.png"/></item><item><title>MagicQuill: An Intelligent Interactive Image Editing System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/</guid><description>MagicQuill: an intelligent interactive image editing system enabling intuitive, precise image edits via brushstrokes and real-time intent prediction by a multimodal LLM.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09703/cover.png"/></item><item><title>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</guid><description>EgoVid-5M: First high-quality dataset for egocentric video generation, enabling realistic human-centric world simulations.</description></item><item><title>Sharingan: Extract User Action Sequence from Desktop Recordings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</guid><description>Sharingan extracts user action sequences from desktop recordings using novel VLM-based methods, achieving 70-80% accuracy and enabling RPA.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/cover.png"/></item><item><title>GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08033/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08033/</guid><description>GaussianAnything: Interactive point cloud latent diffusion enables high-quality, editable 3D models from images or text, overcoming existing 3D generation limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08033/cover.png"/></item><item><title>Wavelet Latent Diffusion (Wala): Billion-Parameter 3D Generative Model with Compact Wavelet Encodings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08017/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08017/</guid><description>WaLa: a billion-parameter 3D generative model using wavelet encodings achieves state-of-the-art results, generating high-quality 3D shapes in seconds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08017/cover.png"/></item><item><title>Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/</guid><description>Add-it: Training-free object insertion in images using pretrained diffusion models by cleverly balancing information from the scene, text prompt, and generated image, achieving state-of-the-art result&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07232/cover.png"/></item><item><title>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/</guid><description>Edify Image: groundbreaking pixel-perfect photorealistic image generation using cascaded pixel-space diffusion models with a novel Laplacian diffusion process, enabling diverse applications including &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07126/cover.png"/></item><item><title>OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/</guid><description>OmniEdit, a novel instruction-based image editing model, surpasses existing methods by leveraging specialist supervision and high-quality data, achieving superior performance across diverse editing ta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07199/cover.png"/></item><item><title>SAMPart3D: Segment Any Part in 3D Objects</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07184/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07184/</guid><description>SAMPart3D: Zero-shot 3D part segmentation across granularities, scaling to large datasets &amp;amp; handling part ambiguity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07184/cover.png"/></item><item><title>KMM: Key Frame Mask Mamba for Extended Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06481/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06481/</guid><description>KMM: Key Frame Mask Mamba generates extended, diverse human motion from text prompts by innovatively masking key frames in the Mamba architecture and using contrastive learning for improved text-motio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06481/cover.png"/></item><item><title>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</guid><description>StdGEN: Generate high-quality, semantically decomposed 3D characters from a single image in minutes, enabling flexible customization for various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/cover.png"/></item><item><title>DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/</guid><description>DimensionX generates photorealistic 3D and 4D scenes from a single image via controllable video diffusion, enabling precise manipulation of spatial structure and temporal dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/cover.png"/></item><item><title>GazeGen: Gaze-Driven User Interaction for Visual Content Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/</guid><description>GazeGen uses real-time gaze tracking to enable intuitive hands-free visual content creation and editing, setting a new standard for accessible AR/VR interaction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/cover.png"/></item><item><title>ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</guid><description>ReCapture generates videos with novel camera angles from user videos using masked video fine-tuning, preserving scene motion and plausibly hallucinating unseen parts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/cover.png"/></item><item><title>SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/</guid><description>SG-I2V: Zero-shot controllable image-to-video generation using a self-guided approach that leverages pre-trained models for precise object and camera motion control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04989/cover.png"/></item><item><title>SVDQunat: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/</guid><description>SVDQuant boosts 4-bit diffusion models by absorbing outliers via low-rank components, achieving 3.5x memory reduction and 3x speedup on 12B parameter models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05007/cover.png"/></item><item><title>Correlation of Object Detection Performance with Visual Saliency and Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02844/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02844/</guid><description>Visual saliency boosts object detection accuracy more than depth estimation, especially for larger objects, offering valuable insights for model and dataset improvement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02844/cover.png"/></item><item><title>GarVerseLOD: High-Fidelity 3D Garment Reconstruction from a Single In-the-Wild Image using a Dataset with Levels of Details</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/</guid><description>GarVerseLOD introduces a novel dataset and framework for high-fidelity 3D garment reconstruction from a single image, achieving unprecedented robustness via a hierarchical approach and leveraging a ma&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03047/cover.png"/></item><item><title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</guid><description>Adaptive Caching (AdaCache) dramatically speeds up video generation with diffusion transformers by cleverly caching and reusing computations, tailoring the process to each video&amp;rsquo;s complexity and motio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/cover.png"/></item><item><title>GenXD: Generating Any 3D and 4D Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02319/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02319/</guid><description>GenXD: A unified model generating high-quality 3D &amp;amp; 4D scenes from any number of images, advancing the field of dynamic scene generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02319/cover.png"/></item><item><title>How Far is Video Generation from World Model: A Physical Law Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</guid><description>Scaling video generation models doesn&amp;rsquo;t guarantee they&amp;rsquo;ll learn physics; this study reveals they prioritize visual cues over true physical understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/cover.png"/></item><item><title>Training-free Regional Prompting for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/</guid><description>Training-free Regional Prompting for FLUX boosts compositional text-to-image generation by cleverly manipulating attention mechanisms, achieving fine-grained control without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02395/cover.png"/></item><item><title>DreamPolish: Domain Score Distillation With Progressive Geometry Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01602/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01602/</guid><description>DreamPolish: A new text-to-3D model generates highly detailed 3D objects with polished surfaces and realistic textures using progressive geometry refinement and a novel domain score distillation tech&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01602/cover.png"/></item><item><title>Constant Acceleration Flow</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/</guid><description>Constant Acceleration Flow (CAF) dramatically speeds up diffusion model generation by using a constant acceleration equation, outperforming state-of-the-art methods with improved accuracy and few-step&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00322/cover.png"/></item><item><title>Randomized Autoregressive Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/</guid><description>Randomized Autoregressive Modeling (RAR) sets a new state-of-the-art in image generation by cleverly introducing randomness during training to improve the model&amp;rsquo;s ability to learn from bidirectional c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00776/cover.png"/></item><item><title>DELTA: Dense Efficient Long-range 3D Tracking for any video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24211/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24211/</guid><description>DELTA: A new method efficiently tracks every pixel in 3D space from monocular videos, enabling accurate motion estimation across entire videos with state-of-the-art accuracy and over 8x speed improvem&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24211/cover.png"/></item><item><title>In-Context LoRA for Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/</guid><description>In-Context LoRA empowers existing text-to-image models for high-fidelity multi-image generation by simply concatenating images and using minimal task-specific LoRA tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23775/cover.png"/></item><item><title>Learning Video Representations without Natural Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</guid><description>High-performing video representation models can be trained using only synthetic videos and images, eliminating the need for large natural video datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/cover.png"/></item><item><title>HelloMeme: Integrating Spatial Knitting Attentions to Embed High-Level and Fidelity-Rich Conditions in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/</guid><description>HelloMeme enhances text-to-image models by integrating spatial knitting attentions, enabling high-fidelity meme video generation while preserving model generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22901/cover.png"/></item><item><title>DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00836/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00836/</guid><description>DynaMath, a novel benchmark, reveals that state-of-the-art VLMs struggle with variations of simple math problems, showcasing their reasoning fragility. It offers 501 high-quality seed questions, dyna&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00836/cover.png"/></item></channel></rss>