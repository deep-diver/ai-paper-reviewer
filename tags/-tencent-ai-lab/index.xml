<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Tencent AI Lab on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-tencent-ai-lab/</link><description>Recent content in üè¢ Tencent AI Lab on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 31 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-tencent-ai-lab/index.xml" rel="self" type="application/rss+xml"/><item><title>Expanding RL with Verifiable Rewards Across Diverse Domains</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-04-01/2503.23829/</link><pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-04-01/2503.23829/</guid><description>RL with Verifiable Rewards is now expanding to diverse domains like medicine!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-04-01/2503.23829/cover.png"/></item><item><title>The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08946/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08946/</guid><description>LLMs often fail to demonstrate true understanding of concepts, acting as &amp;lsquo;stochastic parrots&amp;rsquo; ‚Äì a phenomenon quantitatively proven by the PHYSICO benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08946/cover.png"/></item><item><title>Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18585/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18585/</guid><description>Large language models (LLMs) often prematurely abandon promising reasoning paths, a phenomenon called &amp;lsquo;underthinking&amp;rsquo;. This paper introduces a novel metric to quantify this issue and proposes a decodi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18585/cover.png"/></item><item><title>Autonomy-of-Experts Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13074/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13074/</guid><description>Revolutionizing large language models, Autonomy-of-Experts (AoE) empowers individual expert modules to autonomously select inputs, eliminating routers and boosting both efficiency and accuracy.</description></item><item><title>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12202/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12202/</guid><description>Hunyuan3D 2.0: A groundbreaking open-source system generating high-resolution, textured 3D assets using scalable diffusion models, exceeding state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12202/cover.png"/></item><item><title>CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08983/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08983/</guid><description>CityDreamer4D generates realistic, unbounded 4D city models by cleverly separating dynamic objects (like vehicles) from static elements (buildings, roads), using multiple neural fields for enhanced re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08983/cover.png"/></item><item><title>XMusic: Towards a Generalized and Controllable Symbolic Music Generation Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08809/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08809/</guid><description>XMusic: A new framework generates high-quality, emotionally controllable symbolic music from various prompts (images, videos, text, tags, humming).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08809/cover.png"/></item><item><title>Scaling Laws for Floating Point Quantization Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</guid><description>New scaling laws for efficient floating-point quantization training in LLMs are presented, showing optimal bit allocation and critical data size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/cover.png"/></item><item><title>VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/</guid><description>VideoMaker achieves high-fidelity zero-shot customized video generation by cleverly harnessing the inherent power of video diffusion models, eliminating the need for extra feature extraction and injec&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19645/cover.png"/></item><item><title>A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/</guid><description>This study reveals that gist token-based context compression in LLMs, while effective for some tasks, suffers from key failure patterns. The authors propose fine-grained autoencoding and segment-wise&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/cover.png"/></item><item><title>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</guid><description>DRT-01 leverages long chain-of-thought reasoning to significantly boost machine translation quality, particularly for complex sentences with metaphors and similes, achieving substantial improvements o&amp;hellip;</description></item><item><title>FreeSplatter: Pose-free Gaussian Splatting for Sparse-view 3D Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/</guid><description>FreeSplatter: a novel feed-forward framework reconstructs high-quality 3D scenes from uncalibrated sparse-view images, estimating camera poses in seconds.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09573/cover.png"/></item><item><title>EMOv2: Pushing 5M Vision Model Frontier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/</guid><description>EMOv2 achieves state-of-the-art performance in various vision tasks using a novel Meta Mobile Block, pushing the 5M parameter lightweight model frontier.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06674/cover.png"/></item><item><title>Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04432/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04432/</guid><description>Divot: A novel diffusion-powered video tokenizer enables unified video comprehension &amp;amp; generation with LLMs, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04432/cover.png"/></item><item><title>NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/</guid><description>NVComposer: A novel generative NVS model boosts synthesis quality by implicitly inferring spatial relationships from multiple sparse, unposed images, eliminating reliance on external alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03517/cover.png"/></item><item><title>Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</guid><description>Boosting LLMs&amp;rsquo; reasoning: A novel token-level contrastive estimation method automatically identifies and penalizes critical tokens leading to errors, significantly enhancing reasoning accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/cover.png"/></item><item><title>Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</guid><description>Self-VerIfication length Policy (SVIP) dynamically adjusts speculative decoding draft lengths based on token difficulty, achieving up to 20% faster large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/cover.png"/></item><item><title>AnchorCrafter: Animate CyberAnchors Saling Your Products via Human-Object Interacting Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/</guid><description>AnchorCrafter animates cyber-anchors selling products via human-object interacting video generation, achieving high visual fidelity and controllable interactions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17383/cover.png"/></item><item><title>Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</guid><description>Low-bit quantization excels for undertrained LLMs but struggles with fully-trained ones; new scaling laws reveal this, directing future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/cover.png"/></item><item><title>Morph: A Motion-free Physics Optimization Framework for Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/</guid><description>Morph: a novel motion-free physics optimization framework drastically enhances human motion generation&amp;rsquo;s physical plausibility using synthetic data, achieving state-of-the-art quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14951/cover.png"/></item><item><title>Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14432/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14432/</guid><description>Insight-V: A multi-agent system enhances multi-modal LLMs&amp;rsquo; visual reasoning by generating high-quality long-chain reasoning data and employing a two-stage training pipeline, achieving significant perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14432/cover.png"/></item><item><title>StdGEN: Semantic-Decomposed 3D Character Generation from Single Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/</guid><description>StdGEN: Generate high-quality, semantically decomposed 3D characters from a single image in minutes, enabling flexible customization for various applications.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05738/cover.png"/></item><item><title>Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</guid><description>Tencent unveils Hunyuan-Large, a groundbreaking open-source MoE LLM boasting 389B parameters and 52B activated parameters, surpassing existing models in performance across various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/cover.png"/></item></channel></rss>