<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/natural-language-processing/</link><description>Recent content in Natural Language Processing on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 23 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13629/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13629/</guid><description>SIGMA, a novel large language model, achieves &lt;strong>up to 33.36% faster inference speeds&lt;/strong> by using DiffQKV attention, which differentially optimizes query, key, and value components in the attention mech&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13629/cover.png"/></item><item><title>Autonomy-of-Experts Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.13074/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.13074/</guid><description>Revolutionizing large language models, Autonomy-of-Experts (AoE) empowers individual expert modules to autonomously select inputs, eliminating routers and boosting both efficiency and accuracy.</description></item><item><title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12948/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12948/</guid><description>DeepSeek-R1 significantly improves LLM reasoning by using reinforcement learning, achieving performance comparable to OpenAI&amp;rsquo;s top models while addressing previous challenges of poor readability and l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12948/cover.png"/></item><item><title>Kimi k1.5: Scaling Reinforcement Learning with LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12599/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12599/</guid><description>Kimi K1.5: A Multimodal LLM trained with RL achieves state-of-the-art reasoning by scaling long context RL training and improving policy optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12599/cover.png"/></item><item><title>O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12570/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12570/</guid><description>O1-Pruner efficiently prunes long-thought reasoning in LLMs by harmonizing reasoning length and accuracy via fine-tuning, significantly reducing inference time without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12570/cover.png"/></item><item><title>Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.13007/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.13007/</guid><description>Pairwise RM, a novel reward model with knockout tournaments, significantly boosts large language model accuracy in test-time scaling by comparing solution pairs, eliminating arbitrary scoring inconsis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.13007/cover.png"/></item><item><title>Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12895/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12895/</guid><description>Large language models (LLMs) are rapidly evolving, yet often struggle to adapt to human preferences quickly. This paper introduces Test-Time Preference Optimization (TPO), an innovative framework that&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.12895/cover.png"/></item><item><title>Debate Helps Weak-to-Strong Generalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13124/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13124/</guid><description>Debate-enhanced weak supervision boosts AI alignment by combining strong and weak models, enabling safer and more reliable AI systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13124/cover.png"/></item><item><title>MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12380/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12380/</guid><description>MMVU: a new benchmark pushes multimodal video understanding to expert level, revealing limitations of current models and paving the way for more advanced AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12380/cover.png"/></item><item><title>Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11425/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11425/</guid><description>Agent-R: A novel self-training framework enables language model agents to learn from errors by dynamically constructing training data that corrects erroneous actions, resulting in significantly improv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11425/cover.png"/></item><item><title>Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11733/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11733/</guid><description>Mobile-Agent-E: A self-evolving mobile assistant conquering complex tasks with hierarchical agents and a novel self-evolution module, significantly outperforming prior approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11733/cover.png"/></item><item><title>Reasoning Language Models: A Blueprint</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11223/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11223/</guid><description>Democratizing advanced reasoning in AI, this blueprint introduces a modular framework for building Reasoning Language Models (RLMs), simplifying development and enhancing accessibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11223/cover.png"/></item><item><title>IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.11067/</link><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.11067/</guid><description>IntellAgent: a novel open-source framework automating diverse conversational AI evaluation via policy-driven graph modeling, event generation, and user-agent simulations, enabling fine-grained diagnos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-23/2501.11067/cover.png"/></item><item><title>Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10799/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10799/</guid><description>STEP-KTO: A novel training framework boosts LLMs&amp;rsquo; mathematical reasoning by providing binary feedback on both intermediate steps and final answers. This ensures logical reasoning trajectories and impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10799/cover.png"/></item><item><title>ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10132/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10132/</guid><description>ComplexFuncBench, a new benchmark, rigorously evaluates LLMs&amp;rsquo; complex function-calling abilities across real-world scenarios involving multi-step processes, constraints, and long contexts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10132/cover.png"/></item><item><title>Evolving Deeper LLM Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09891/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09891/</guid><description>Mind Evolution, a novel evolutionary search strategy, significantly boosts Large Language Model (LLM) problem-solving by generating, recombining, and refining candidate solutions via an LLM, outperfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09891/cover.png"/></item><item><title>PaSa: An LLM Agent for Comprehensive Academic Paper Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10120/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10120/</guid><description>PaSa: An LLM agent autonomously performs comprehensive academic paper searches, outperforming existing methods by efficiently combining search tools, paper reading, and citation analysis, optimized vi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10120/cover.png"/></item><item><title>Bridging Language Barriers in Healthcare: A Study on Arabic LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09825/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09825/</guid><description>Arabic LLMs struggle with medical tasks; this study reveals optimal language ratios in training data for improved performance, highlighting challenges in simply translating medical data for different &amp;hellip;</description></item><item><title>Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09484/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09484/</guid><description>AI-powered medical consultations often struggle with the inquiry phase. This paper presents a novel patient simulator trained on real interactions, revealing that effective inquiry significantly impac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09484/cover.png"/></item><item><title>Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09775/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09775/</guid><description>LLM reasoning boosts self-confidence, even when answers are wrong, highlighting limitations in current evaluation metrics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09775/cover.png"/></item><item><title>Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09686/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09686/</guid><description>This survey paper explores the exciting new frontier of Large Reasoning Models (LRMs), focusing on how reinforcement learning and clever prompting techniques are boosting LLMs&amp;rsquo; reasoning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09686/cover.png"/></item><item><title>RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08617/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08617/</guid><description>RLHS, a novel alignment algorithm, leverages simulated hindsight feedback to mitigate misalignment in RLHF, significantly improving AI&amp;rsquo;s alignment with human values and goals.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08617/cover.png"/></item><item><title>Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04828/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04828/</guid><description>First-ever resources (NER dataset, dependency treebank, and corpus) and models for historical Turkish NLP are introduced, significantly advancing research capabilities in this underexplored field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04828/cover.png"/></item><item><title>EpiCoder: Encompassing Diversity and Complexity in Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04694/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04694/</guid><description>EpiCoder revolutionizes code generation by using feature trees to create diverse and complex training data, resulting in state-of-the-art performance on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04694/cover.png"/></item><item><title>LLM4SR: A Survey on Large Language Models for Scientific Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04306/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04306/</guid><description>LLMs revolutionize scientific research! This survey reveals their transformative potential across hypothesis discovery, experiment planning, writing, and peer review, guiding future research.</description></item><item><title>rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04519/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04519/</guid><description>Small language models can master complex math reasoning using self-evolved deep thinking via Monte Carlo Tree Search, surpassing larger models in performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04519/cover.png"/></item><item><title>URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/</guid><description>URSA-7B: A new multimodal model significantly improves chain-of-thought reasoning in mathematics!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/cover.png"/></item><item><title>Entropy-Guided Attention for Private LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03489/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03489/</guid><description>Boosting private LLMs&amp;rsquo; efficiency and security, this research introduces an entropy-guided attention mechanism and PI-friendly layer normalization to mitigate the overheads of nonlinear operations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03489/cover.png"/></item><item><title>PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03936/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03936/</guid><description>PPTAgent, a novel two-stage framework, significantly improves automatic presentation generation by leveraging an edit-based workflow and a new evaluation metric, outperforming existing end-to-end meth&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03936/cover.png"/></item><item><title>BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</guid><description>BoostStep enhances large language models&amp;rsquo; mathematical abilities by refining single-step reasoning through a novel step-level in-context learning strategy, achieving significant improvements on variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/cover.png"/></item><item><title>GeAR: Generation Augmented Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02772/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02772/</guid><description>GeAR, a new retrieval model, boosts accuracy by combining document retrieval with fine-grained information generation, leading to better understanding and improved localization.</description></item><item><title>Samba-asr state-of-the-art speech recognition leveraging structured state-space models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/</guid><description>Samba-ASR, a novel speech recognition model using Mamba architecture, surpasses existing transformer models in accuracy and efficiency, setting a new benchmark for future ASR research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/cover.png"/></item><item><title>Scaling Laws for Floating Point Quantization Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</guid><description>New scaling laws for efficient floating-point quantization training in LLMs are presented, showing optimal bit allocation and critical data size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/cover.png"/></item><item><title>Test-time Computing: from System-1 Thinking to System-2 Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/</guid><description>Unlocking LLM potential: This paper surveys test-time computing, showing how it boosts reasoning abilities by shifting from reactive System-1 to deliberate System-2 thinking, paving the way for more p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/cover.png"/></item><item><title>ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/</guid><description>ToolHop: New benchmark dataset rigorously evaluates LLMs&amp;rsquo; multi-hop tool use, revealing significant challenges and variations across different LLM families.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/cover.png"/></item><item><title>Personalized Graph-Based Retrieval for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/</guid><description>Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) significantly improves personalized text generation by leveraging user-centric knowledge graphs, especially in cold-start scenarios &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/cover.png"/></item><item><title>REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03262/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03262/</guid><description>REINFORCE++, a novel RLHF algorithm, achieves superior training stability and computational efficiency compared to existing methods like PPO and GRPO, while maintaining comparable performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03262/cover.png"/></item><item><title>Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/</guid><description>AUTO-RT automates LLM vulnerability discovery by using reinforcement learning to optimize complex attack strategies, achieving faster detection and higher success rates than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/cover.png"/></item><item><title>METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/</guid><description>METAGENE-1, a 7-billion parameter language model, achieves state-of-the-art results in pathogen detection and genomic embedding by leveraging a massive wastewater metagenomic dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/cover.png"/></item><item><title>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</guid><description>BoxingGym: A new benchmark rigorously evaluates AI agents&amp;rsquo; ability to design experiments and discover scientific models, revealing current LLMs&amp;rsquo; limitations and highlighting fertile research avenues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/cover.png"/></item><item><title>CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</guid><description>CODEELO benchmark uses CodeForces to fairly evaluate LLMs&amp;rsquo; coding abilities, providing human-comparable Elo ratings and addressing limitations of existing benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/cover.png"/></item><item><title>Dynamic Scaling of Unit Tests for Code Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</guid><description>Boosting code generation accuracy with more unit tests! This research shows that increasing the number of unit tests used to evaluate code generated by LLMs significantly improves accuracy, especially&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/cover.png"/></item><item><title>LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00874/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00874/</guid><description>LUSIFER: a novel zero-shot approach empowers English-centric LLM embedding models for multilingual tasks without explicit multilingual training data, significantly enhancing performance, especially fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00874/cover.png"/></item><item><title>Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</guid><description>Polarizing SSMs&amp;rsquo; state transition matrices enhances long-range dependency modeling by mitigating recency bias and over-smoothing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/cover.png"/></item><item><title>Efficiently Serving LLM Reasoning Programs with Certaindex</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</guid><description>Dynasor optimizes LLM reasoning by dynamically allocating compute based on a novel &amp;lsquo;certaindex&amp;rsquo; metric, reducing compute by up to 50% and increasing query rates by 3.3x.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/cover.png"/></item><item><title>Facilitating large language model Russian adaptation with Learned Embedding Propagation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</guid><description>Researchers introduce Learned Embedding Propagation (LEP), a novel technique that efficiently adapts large language models (LLMs) to new languages using minimal training data, thus overcoming limitati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/cover.png"/></item><item><title>HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</guid><description>New benchmarks, HumanEval Pro and MBPP Pro, reveal LLMs struggle with self-invoking code generation, highlighting a critical gap in current code reasoning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/cover.png"/></item><item><title>MapQaTor: A System for Efficient Annotation of Map Query Datasets</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/</guid><description>MAPQATOR: a web app that streamlines creation of reproducible geospatial QA datasets, boosting annotation speed by 30x!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/cover.png"/></item><item><title>TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/</guid><description>TANGOFLUX: Blazing-fast, high-fidelity text-to-audio generation using novel CLAP-Ranked Preference Optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/cover.png"/></item><item><title>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</link><pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</guid><description>OneKE: a dockerized, schema-guided LLM agent system efficiently extracts knowledge from diverse sources, offering adaptability and robust error handling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/cover.png"/></item><item><title>Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</guid><description>Boost fine-tuned LLMs&amp;rsquo; performance without sacrificing safety by merging pre- and post-tuning model weights!</description></item><item><title>Xmodel-2 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</guid><description>Xmodel-2: A 1.2B parameter LLM achieving state-of-the-art reasoning performance through efficient architecture and training, now publicly available!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/cover.png"/></item><item><title>Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/</guid><description>Molar: A novel multimodal LLM framework boosts sequential recommendation accuracy by cleverly aligning collaborative filtering with rich item representations from text and non-text data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/cover.png"/></item><item><title>Token-Budget-Aware LLM Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</guid><description>TALE: A novel framework dynamically adjusts token budgets in LLM reasoning prompts, slashing costs by ~70% with minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/cover.png"/></item><item><title>A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/</guid><description>This study reveals that gist token-based context compression in LLMs, while effective for some tasks, suffers from key failure patterns. The authors propose fine-grained autoencoding and segment-wise&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/cover.png"/></item><item><title>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</guid><description>B-STAR dynamically balances exploration and exploitation in self-taught reasoners, achieving superior performance in mathematical, coding, and commonsense reasoning tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/cover.png"/></item><item><title>Deliberation in Latent Space via Differentiable Cache Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</guid><description>Frozen LLMs get a performance boost by augmenting their key-value cache with latent embeddings generated by a differentiable offline coprocessor.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/cover.png"/></item><item><title>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</guid><description>DRT-01 leverages long chain-of-thought reasoning to significantly boost machine translation quality, particularly for complex sentences with metaphors and similes, achieving substantial improvements o&amp;hellip;</description></item><item><title>Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</guid><description>FoPE enhances attention&amp;rsquo;s periodic extension for better length generalization in language models by addressing spectral damage in RoPE using Fourier Series and zeroing out destructive frequencies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/cover.png"/></item><item><title>Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/</guid><description>Friends-MMC: A new dataset facilitates multi-modal multi-party conversation understanding by providing 24,000+ utterances with video, audio, and speaker annotations, enabling advancements in character&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/cover.png"/></item><item><title>In Case You Missed It: ARC 'Challenge' Is Not That Challenging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</guid><description>LLM evaluation on multiple-choice questions is flawed; considering all options simultaneously, not individually, reveals much higher accuracy and challenges existing benchmark rankings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/cover.png"/></item><item><title>YuLan-Mini: An Open Data-efficient Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</guid><description>YuLan-Mini: An open, data-efficient 2.42B parameter LLM achieving top-tier performance with innovative training techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/cover.png"/></item><item><title>OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</guid><description>OpenRFT adapts generalist reasoning models for domain-specific tasks using reinforcement fine-tuning, overcoming data scarcity and lack of reasoning step data via question augmentation, synthesized re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/cover.png"/></item><item><title>Revisiting In-Context Learning with Long Context Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</guid><description>Long-context models surprisingly show that simple random sampling of examples is as effective as sophisticated methods for in-context learning, shifting the focus to efficient context utilization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/cover.png"/></item><item><title>NILE: Internal Consistency Alignment in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</guid><description>NILE framework significantly boosts LLM performance by aligning instruction-tuning datasets with pre-trained internal knowledge, achieving up to 68.5% gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/cover.png"/></item><item><title>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</guid><description>AceMath achieves state-of-the-art results in mathematical reasoning by introducing highly effective instruction-tuned models and reward models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/cover.png"/></item><item><title>Fietje: An open, efficient LLM for Dutch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</guid><description>Fietje: an open-source, efficient Dutch language model outperforming larger models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/cover.png"/></item><item><title>How to Synthesize Text Data without Model Collapse?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</guid><description>Token-level editing prevents language model collapse from synthetic data by theoretically bounding test error and empirically improving model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/cover.png"/></item><item><title>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</guid><description>M-ALERT, a new multilingual benchmark, reveals significant safety inconsistencies across languages in top LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/cover.png"/></item><item><title>MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</guid><description>MixLLM achieves state-of-the-art LLM compression by using mixed-precision quantization between output features, improving accuracy and system efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/cover.png"/></item><item><title>Outcome-Refining Process Supervision for Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</guid><description>Boosting code generation accuracy, Outcome-Refining Process Supervision (ORPS) uses execution feedback and structured reasoning to refine code, achieving significant improvements across models and dat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/cover.png"/></item><item><title>ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</guid><description>ReMoE: Revolutionizing Mixture-of-Experts with fully differentiable ReLU routing, achieving superior scalability and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/cover.png"/></item><item><title>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</guid><description>ROBUSTFT tackles noisy data in LLM fine-tuning by using multi-expert noise detection and context-enhanced relabeling, significantly boosting model performance in noisy scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/cover.png"/></item><item><title>AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</guid><description>Auto-built benchmark with up-to-date knowledge ensures contamination-free LLM evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/cover.png"/></item><item><title>Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13795/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13795/</guid><description>Mix-LN boosts deep layer power in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13795/cover.png"/></item><item><title>RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</guid><description>First benchmark for RAG reward models reveals their limitations and the need for preference-aligned training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/cover.png"/></item><item><title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</guid><description>AI agents are tested in a simulated company, revealing their capability to automate tasks and shortcomings with complex workflows and interfaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/cover.png"/></item><item><title>Are Your LLMs Capable of Stable Reasoning?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</guid><description>G-Pass@k &amp;amp; LiveMathBench: Evaluating the stability of LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/cover.png"/></item><item><title>OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13018/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13018/</guid><description>OmniEval: Automatic benchmark for evaluating financial RAG systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13018/cover.png"/></item><item><title>RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/</guid><description>RetroLLM unifies retrieval &amp;amp; generation in LLMs, boosting accuracy and cutting costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/cover.png"/></item><item><title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</guid><description>SepLLM shrinks LLMs, speeding them up by over 50% without losing much accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/cover.png"/></item><item><title>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</guid><description>Self-play method SPAR enhances LLMs instruction following abilities, beating GPT-4 on IFEval</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/cover.png"/></item><item><title>Smaller Language Models Are Better Instruction Evolvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</guid><description>Smaller is better: SLMs outperform LLMs in evolving complex &amp;amp; diverse instructions for AI training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/cover.png"/></item><item><title>Byte Latent Transformer: Patches Scale Better Than Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</guid><description>BLT: tokenizer-free LLM for efficiency and robustness</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/cover.png"/></item><item><title>SCBench: A KV Cache-Centric Analysis of Long-Context Methods</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</guid><description>New benchmark for evaluating long-context models finds sub-O(n) methods lacking in real-world use cases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/cover.png"/></item><item><title>JuStRank: Benchmarking LLM Judges for System Ranking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09569/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09569/</guid><description>JuStRank: LLM system ranker benchmark reveals critical judge qualities (decisiveness, bias) impacting ranking accuracy, highlighting instance-level performance doesn&amp;rsquo;t guarantee accurate system-level&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09569/cover.png"/></item><item><title>Phi-4 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</guid><description>Phi-4: a 14B parameter LLM surpassing its teacher model (GPT-4) in STEM-focused QA through innovative synthetic data generation and post-training techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/cover.png"/></item><item><title>RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08972/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08972/</guid><description>RULEARENA, a new benchmark, rigorously evaluates large language models&amp;rsquo; ability to apply complex, real-world rules across diverse scenarios, revealing significant shortcomings in current LLMs&amp;rsquo; rule-gu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08972/cover.png"/></item><item><title>Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09025/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09025/</guid><description>Shiksha: A new multilingual translation dataset and model surpasses existing benchmarks for Indian languages, focusing on scientific, technical, and educational domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09025/cover.png"/></item><item><title>The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09460/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09460/</guid><description>Norwegians show that using copyrighted material improves LLMs, but raises legal and ethical issues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09460/cover.png"/></item><item><title>Word Sense Linking: Disambiguating Outside the Sandbox</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09370/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09370/</guid><description>Word Sense Linking (WSL) revolutionizes word sense disambiguation by tackling its real-world limitations. It combines span identification and sense linking in plain text, offering better integration &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09370/cover.png"/></item><item><title>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</guid><description>Fine-tuning small language models? Tweak the learning rate and batch size for a reasoning boost!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/cover.png"/></item><item><title>Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/</guid><description>Contextualized AI counterspeech significantly outperforms generic methods by adapting to the moderation context and user, improving persuasiveness without sacrificing other qualities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/cover.png"/></item><item><title>Granite Guardian</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/</guid><description>Granite Guardian: Open-source risk detection models for LLMs, surpassing existing models in accuracy and offering comprehensive coverage across multiple risk dimensions, promoting safer AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/cover.png"/></item><item><title>Training Large Language Models to Reason in a Continuous Latent Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</guid><description>LLMs are trained to reason using language, but COCONUT lets them reason directly in a continuous latent space, boosting performance on logical tasks requiring complex planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/cover.png"/></item><item><title>Fully Open Source Moxin-7B Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</guid><description>Moxin-LLM: A fully open-source 7B parameter LLM achieving superior zero-shot performance, promoting transparency and reproducibility in AI research.</description></item><item><title>DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04905/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04905/</guid><description>DEMO benchmark revolutionizes dialogue modeling by focusing on fine-grained elements (Prelude, Interlocution, Epilogue), enabling comprehensive evaluation and superior agent performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04905/cover.png"/></item><item><title>Evaluating and Aligning CodeLLMs on Human Preference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/</guid><description>CodeArena, a novel benchmark, evaluates code LLMs based on human preferences, revealing performance gaps between open-source and proprietary models, and a large-scale synthetic instruction corpus impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/cover.png"/></item><item><title>EXAONE 3.5: Series of Large Language Models for Real-world Use Cases</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/</guid><description>LG AI Research unveils EXAONE 3.5, a series of instruction-tuned language models (2.4B, 7.8B, and 32B parameters) excelling in real-world tasks, long-context understanding, and general benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/cover.png"/></item><item><title>Densing Law of LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</guid><description>LLMs&amp;rsquo; training quality is exponentially improving, enabling models with half the parameters to match state-of-the-art performance every 3 months, thus reducing inference costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/cover.png"/></item><item><title>Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/</guid><description>Marco-LLM: A groundbreaking multilingual LLM significantly enhances cross-lingual capabilities via massive multilingual training, bridging the performance gap between high- and low-resource languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/cover.png"/></item><item><title>Monet: Mixture of Monosemantic Experts for Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/</guid><description>MONET improves Transformer interpretability by using Mixture-of-Experts (MoE) with 262K monosemantic experts per layer, achieving parameter efficiency and enabling knowledge manipulation without perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/cover.png"/></item><item><title>Evaluating Language Models as Synthetic Data Generators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</guid><description>AGORABENCH: A new benchmark reveals surprising strengths &amp;amp; weaknesses of LMs as synthetic data generators, showing that problem-solving ability isn&amp;rsquo;t the sole indicator of data quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/cover.png"/></item><item><title>Robust Multi-bit Text Watermark with LLM-based Paraphrasers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/</guid><description>Researchers developed a robust multi-bit text watermarking method using LLMs for paraphrasing, achieving over 99.99% detection accuracy while maintaining semantic information and resisting common atta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/cover.png"/></item><item><title>Weighted-Reward Preference Optimization for Implicit Model Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/</guid><description>WRPO: Implicitly fuse LLMs, boosting performance without complex alignment or merging!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/cover.png"/></item><item><title>OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/</guid><description>Imperfect OCR hinders Retrieval-Augmented Generation (RAG). OHRBench, a new benchmark, reveals this cascading impact, showing current OCR solutions insufficient for high-quality RAG knowledge bases. &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/cover.png"/></item><item><title>Free Process Rewards without Process Labels</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</guid><description>Train high-performing Process Reward Models (PRMs) cheaply using only outcome-level labels, eliminating the need for costly step-by-step annotations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/cover.png"/></item><item><title>Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01408/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01408/</guid><description>Few-shot learning empowers cross-lingual audio abuse detection using pre-trained models, achieving high accuracy in low-resource Indian languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01408/cover.png"/></item><item><title>A dynamic parallel method for performance optimization on hybrid CPUs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/</guid><description>Dynamic parallel processing boosts LLM inference speed on hybrid CPUs by over 90% memory bandwidth, resolving performance bottlenecks caused by imbalanced hardware capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/cover.png"/></item><item><title>A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/</guid><description>Boost LLM accuracy exponentially by using a two-stage algorithm with provable scaling laws: generate multiple candidate solutions then compare them in a knockout tournament!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/cover.png"/></item><item><title>Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</guid><description>Boosting LLMs&amp;rsquo; reasoning: A novel token-level contrastive estimation method automatically identifies and penalizes critical tokens leading to errors, significantly enhancing reasoning accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/cover.png"/></item><item><title>INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/</guid><description>New multilingual LLM benchmark, INCLUDE, tackles regional knowledge gaps by using 197K QA pairs from 44 languages, improving cross-lingual evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/cover.png"/></item><item><title>KV Shifting Attention Enhances Language Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/</guid><description>KV Shifting Attention: A novel attention mechanism significantly enhances language modeling by simplifying induction heads, leading to improved performance and faster convergence, even in large-scale &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/cover.png"/></item><item><title>LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19638/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19638/</guid><description>Researchers developed a multilingual news topic classifier using a teacher-student framework and GPT-40 for automatic data annotation, achieving high performance without manual annotation.</description></item><item><title>o1-Coder: an o1 Replication for Coding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/</guid><description>O1-CODER replicates OpenAI&amp;rsquo;s o1 model for coding, integrating reinforcement learning and Monte Carlo Tree Search to enhance System-2 thinking and generate high-quality code with reasoning steps.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/cover.png"/></item><item><title>Puzzle: Distillation-Based NAS for Inference-Optimized LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</guid><description>Puzzle: a novel framework accelerates large language model inference by using neural architecture search and knowledge distillation, achieving a 2.17x speedup on a single GPU while preserving 98.4% ac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/cover.png"/></item><item><title>Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</guid><description>HiAR-ICL, a novel automated reasoning paradigm using Monte Carlo Tree Search, surpasses state-of-the-art accuracy in complex mathematical reasoning by shifting focus from specific examples to abstract&amp;hellip;</description></item><item><title>Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</guid><description>Self-VerIfication length Policy (SVIP) dynamically adjusts speculative decoding draft lengths based on token difficulty, achieving up to 20% faster large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/cover.png"/></item><item><title>Training and Evaluating Language Models with Template-based Data Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</guid><description>Researchers created TemplateGSM, a massive dataset of 7M+ grade-school math problems and solutions, using GPT-4 to generate templates, significantly advancing LLM training for mathematical reasoning.</description></item><item><title>LongKey: Keyphrase Extraction for Long Documents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17863/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17863/</guid><description>LongKey: A novel framework excels at extracting keyphrases from lengthy documents using an encoder-based language model and max-pooling, outperforming existing methods across diverse datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17863/cover.png"/></item><item><title>Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</guid><description>Low-bit quantization excels for undertrained LLMs but struggles with fully-trained ones; new scaling laws reveal this, directing future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/cover.png"/></item><item><title>Star Attention: Efficient LLM Inference over Long Sequences</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</guid><description>Star Attention: 11x faster LLM inference on long sequences with 95-100% accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/cover.png"/></item><item><title>From CISC to RISC: language-model guided assembly transpilation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/</guid><description>A novel LLM-based transpiler, CRT, efficiently converts x86 assembly to ARM and RISC-V assembly, achieving high accuracy and significant performance improvements over existing virtualization methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/cover.png"/></item><item><title>From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/</guid><description>LLMs are revolutionizing AI evaluation by offering nuanced judgments surpassing traditional methods. This paper provides a taxonomy, benchmark, and future directions for LLM-as-a-judge.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/cover.png"/></item><item><title>MH-MoE:Multi-Head Mixture-of-Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16205/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16205/</guid><description>MH-MoE: A novel implementation of Multi-Head Mixture-of-Experts achieves superior performance in large language models by enhancing efficiency without sacrificing model size or computational cost.</description></item><item><title>O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/</guid><description>Simple distillation from OpenAI&amp;rsquo;s API, combined with fine-tuning, surprisingly surpasses OpenAI&amp;rsquo;s O1-preview on complex mathematical reasoning, urging transparency in AI research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/cover.png"/></item><item><title>Predicting Emergent Capabilities by Finetuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</guid><description>Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/cover.png"/></item><item><title>MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/</guid><description>MolReFlect achieves state-of-the-art molecule-text alignment by using a teacher-student LLM framework that generates fine-grained alignments, improving accuracy and explainability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/cover.png"/></item><item><title>One to rule them all: natural language to bind communication, perception and action</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/</guid><description>AI-powered robots now understand and execute complex natural language commands, adapting seamlessly to dynamic environments thanks to a new architecture integrating LLMs, perception, and planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/cover.png"/></item><item><title>Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/</guid><description>LLMs&amp;rsquo; hallucinations stem from entity recognition: SAEs reveal model &amp;lsquo;self-knowledge&amp;rsquo;, causally affecting whether it hallucinates or refuses to answer. This mechanism is even repurposed by chat finet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/cover.png"/></item><item><title>Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/</guid><description>Marco-01: a novel large reasoning model surpasses existing LLMs by using Chain-of-Thought, Monte Carlo Tree Search, and reflection mechanisms to excel in open-ended problem-solving, particularly in co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/cover.png"/></item><item><title>UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/</guid><description>UnifiedCrawl efficiently harvests massive monolingual datasets for low-resource languages from Common Crawl, enabling affordable LLM adaptation via QLoRA, significantly improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/cover.png"/></item><item><title>A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/</guid><description>New data-free methodology creates effective, generalizable LLMs guardrails against off-topic prompts, significantly improving LLM safety and responsible use.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/cover.png"/></item><item><title>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/</guid><description>BALROG benchmark rigorously evaluates LLMs&amp;rsquo;/VLMs&amp;rsquo; abilities in complex games, revealing their strengths and weaknesses in long-term planning and decision-making, highlighting the need for improved vis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/cover.png"/></item><item><title>Hymba: A Hybrid-head Architecture for Small Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</guid><description>Hymba: Hybrid-head architecture boosts small language model performance by 11.67x cache size reduction and 3.49x throughput, surpassing existing models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/cover.png"/></item><item><title>ORID: Organ-Regional Information Driven Framework for Radiology Report Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13025/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13025/</guid><description>ORID framework leverages organ-regional information to boost radiology report generation, achieving state-of-the-art accuracy by integrating multi-modal data and reducing noise from unrelated organs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13025/cover.png"/></item><item><title>Patience Is The Key to Large Language Model Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</guid><description>Boosting Large Language Model (LLM) reasoning without massive datasets: A novel training method encourages &amp;lsquo;patient&amp;rsquo; reasoning, improving accuracy by up to 6.7% on benchmark tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/cover.png"/></item><item><title>When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/</guid><description>AnchorAttention enhances long-context LLMs by mitigating BFloat16&amp;rsquo;s disruptive effects on RoPE, improving performance and speeding up training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/cover.png"/></item><item><title>Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/</guid><description>SUTRA tokenizer outperforms other LLMs in Indian languages, improving efficiency and facilitating better model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/cover.png"/></item><item><title>RedPajama: an Open Dataset for Training Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/</guid><description>RedPajama, two massive open-source datasets, are released for training LLMs, improving transparency and facilitating the development of high-performing open-source models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/cover.png"/></item><item><title>Ultra-Sparse Memory Network</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/</guid><description>UltraMem, a novel ultra-sparse memory network, drastically speeds up LLM inference by 6x compared to MoE while maintaining performance, paving the way for efficient large-scale model deployment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/cover.png"/></item><item><title>Drowning in Documents: Consequences of Scaling Reranker Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11767/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11767/</guid><description>Scaling reranker inference surprisingly degrades retrieval quality beyond a certain point, prompting the need for more robust reranking techniques.</description></item><item><title>Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/</guid><description>Verifier engineering: A new post-training paradigm for foundation models using automated verifiers to provide effective supervision signals, enhancing capabilities beyond traditional data-centric meth&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/cover.png"/></item><item><title>LLÃ¤Mmlein: Compact and Competitive German-Only Language Models from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/</guid><description>New German-only LLMs, LLÃ¤Mmlein 120M &amp;amp; 1B, trained from scratch &amp;amp; openly released, show competitive performance and offer insights into efficient model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/cover.png"/></item><item><title>SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</guid><description>SageAttention2 achieves 4-bit accurate attention, boosting inference speed by 2x compared to FlashAttention2, while maintaining end-to-end accuracy across diverse models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/cover.png"/></item><item><title>SlimLM: An Efficient Small Language Model for On-Device Document Assistance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/</guid><description>SlimLM: Efficient small language models (SLMs) optimized for mobile document assistance, achieving comparable or superior performance to existing SLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/cover.png"/></item><item><title>Adaptive Decoding via Latent Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</guid><description>LLMs can dynamically adjust decoding temperature using Adaptive Decoding and Latent Preference Optimization, improving performance across creative and factual tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/cover.png"/></item><item><title>Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09213/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09213/</guid><description>MedRGB benchmark reveals current LLMs struggle with noisy medical data, emphasizing the need for robust RAG systems in healthcare AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09213/cover.png"/></item><item><title>LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</guid><description>LLaMA-Mesh: Unifying 3D mesh generation with LLMs by directly representing meshes as text, enabling efficient text-to-3D conversion within a single model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/cover.png"/></item><item><title>CamemBERT 2.0: A Smarter French Language Model Aged to Perfection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08868/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08868/</guid><description>CamemBERT 2.0: Two new French language models (CamemBERTav2 &amp;amp; CamemBERTv2) outperform predecessors by addressing temporal concept drift via larger, updated datasets and enhanced tokenization, demonstr&amp;hellip;</description></item><item><title>Can sparse autoencoders be used to decompose and interpret steering vectors?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/</guid><description>Sparse autoencoders fail to accurately decompose and interpret steering vectors due to distribution mismatch and the inability to handle negative feature projections; this paper identifies these issue&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/cover.png"/></item><item><title>Cut Your Losses in Large-Vocabulary Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/</guid><description>Cut Cross-Entropy (CCE) dramatically reduces the memory footprint of training large language models by cleverly computing the cross-entropy loss without materializing the full logit matrix.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/cover.png"/></item><item><title>Direct Preference Optimization Using Sparse Feature-Level Constraints</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/</guid><description>Feature-level constrained Preference Optimization (FPO) boosts LLM alignment efficiency and stability by using sparse autoencoders and feature-level constraints, achieving significant improvements ove&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/cover.png"/></item><item><title>Large Language Models Can Self-Improve in Long-context Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/</guid><description>LLMs can now self-improve long-context reasoning via SEALONG, a novel method leveraging multiple model outputs and minimum Bayes risk scoring to enable effective supervised fine-tuning or preference o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/cover.png"/></item><item><title>Top-$nÏ$: Not All Logits Are You Need</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/</guid><description>Top-Î·Ï: A novel LLM sampling method outperforms existing approaches by using a statistical threshold on pre-softmax logits, achieving higher accuracy while maintaining diversity, even at high temperat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/cover.png"/></item><item><title>Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/</guid><description>Chinese SimpleQA, a new benchmark, offers a comprehensive evaluation of the factuality of LLMs answering short questions in Chinese, exhibiting diversity, high quality, and ease of evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/cover.png"/></item><item><title>Stronger Models are NOT Stronger Teachers for Instruction Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/</guid><description>Larger language models aren&amp;rsquo;t always better teachers for instruction tuning; a new metric, CAR, predicts teacher model effectiveness better than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/cover.png"/></item><item><title>Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/</guid><description>Contrary to common belief, toxicity reduction in language models isn&amp;rsquo;t simply achieved by dampening toxic neurons; it&amp;rsquo;s a complex balancing act across multiple neuron groups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/cover.png"/></item><item><title>Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/</guid><description>WEB-DREAMER uses LLMs as world models for safe and efficient web agent planning, achieving substantial performance gains over reactive baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/cover.png"/></item><item><title>Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</guid><description>Golden Touchstone, a new bilingual benchmark, comprehensively evaluates financial LLMs across eight tasks, revealing model strengths and weaknesses and advancing FinLLM research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/cover.png"/></item><item><title>IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/</guid><description>IOPO empowers LLMs to master complex instructions via input-output preference optimization, boasting significant performance gains on a new benchmark, TRACE.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/cover.png"/></item><item><title>M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06176/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06176/</guid><description>M-LongDoc: a new benchmark and retrieval-aware tuning framework revolutionizes multimodal long document understanding, improving model accuracy by 4.6%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06176/cover.png"/></item><item><title>Balancing Pipeline Parallelism with Vocabulary Parallelism</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/</guid><description>Boost large language model training speed by 51% with Vocabulary Parallelism, a novel technique that balances computation and memory usage across pipeline stages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/cover.png"/></item><item><title>BitNet a4.8: 4-bit Activations for 1-bit LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/</guid><description>BitNet a4.8 achieves comparable performance to existing 1-bit LLMs, but with significantly faster inference, by using a hybrid quantization and sparsification strategy for 4-bit activations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/cover.png"/></item><item><title>DELIFT: Data Efficient Language model Instruction Fine Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04425/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04425/</guid><description>DELIFT: Data Efficient Language Model Instruction Fine-Tuning, drastically reduces the data needed for effective LLM fine-tuning without sacrificing performance.</description></item><item><title>Hardware and Software Platform Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/</guid><description>Researchers developed Hardware and Software Platform Inference (HSPI) to identify the underlying GPU and software stack used to serve LLMs, enhancing transparency in the industry.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/cover.png"/></item><item><title>Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/</guid><description>Can LLMs effectively handle information spread across vast, almost million-scale datasets? This research investigates this question by evaluating 17 LLMs on novel âneedle threadingâ tasks. These task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/cover.png"/></item><item><title>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/</guid><description>OpenCoder, a top-tier open-source code LLM, is introduced, providing not only model weights and code but also reproducible training data, data processing pipelines, and training protocols, enabling co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/cover.png"/></item><item><title>RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04752/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04752/</guid><description>RetrieveGPT enhances code-mixed information retrieval by merging GPT-3.5 Turbo prompts with a novel mathematical model, improving the accuracy of relevant document extraction from complex, sequenced c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04752/cover.png"/></item><item><title>HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02959/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02959/</guid><description>HtmlRAG boosts RAG system accuracy by using HTML, not plain text, to model retrieved knowledge, improving knowledge representation and mitigating LLM hallucination.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02959/cover.png"/></item><item><title>DynaSaur: Large Language Agents Beyond Predefined Actions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/</guid><description>DynaSaur: a novel LLM agent framework enabling dynamic action creation, surpassing prior methods with greater flexibility and top performance on the GAIA benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/cover.png"/></item><item><title>Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</guid><description>Tencent unveils Hunyuan-Large, a groundbreaking open-source MoE LLM boasting 389B parameters and 52B activated parameters, surpassing existing models in performance across various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/cover.png"/></item><item><title>Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/</guid><description>Boosting unit test generation efficiency, this study empirically evaluates various parameter-efficient fine-tuning methods on LLMs, demonstrating comparable performance to full fine-tuning at signific&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/cover.png"/></item><item><title>Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</guid><description>Researchers discovered predictable scaling laws for activation sparsity in LLMs, showing how data, architecture, and model size influence sparsity, paving the way for more efficient and interpretable &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/cover.png"/></item><item><title>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</guid><description>WEBRL: A self-evolving online curriculum reinforcement learning framework empowers open LLMs to excel as high-performing web agents, surpassing proprietary models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/cover.png"/></item><item><title>Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/</guid><description>Zebra-Llama, a context-aware LLM, democratizes rare disease knowledge by providing highly precise, context-rich information about Ehlers-Danlos Syndrome, significantly improving diagnostic support.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/cover.png"/></item><item><title>Sample-Efficient Alignment for LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/</guid><description>Sample-efficient LLM alignment achieved via a novel Thompson sampling algorithm (SEA), outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/cover.png"/></item><item><title>Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/</link><pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/</guid><description>Swan &amp;amp; ArabicMTEB: New dialect-aware Arabic embedding models and benchmark achieve state-of-the-art performance, addressing limitations of existing multilingual models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/cover.png"/></item><item><title>Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</guid><description>Specialized Sparse Autoencoders (SSAEs) decode foundation models&amp;rsquo; &amp;lsquo;dark matter&amp;rsquo; features, efficiently extracting rare subdomain concepts for improved interpretability and safety.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/cover.png"/></item><item><title>GRS-QA -- Graph Reasoning-Structured Question Answering Dataset</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00369/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00369/</guid><description>GRS-QA: New benchmark dataset reveals LLM reasoning limitations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00369/cover.png"/></item><item><title>LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/</guid><description>LibMoE: A new library streamlines MoE research by offering standardized training, evaluation, and a modular design, enabling efficient benchmarking of various MoE algorithms for LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/cover.png"/></item><item><title>BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/</guid><description>BitStack: Dynamic LLM sizing for variable memory!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/cover.png"/></item><item><title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</guid><description>Constraint Back-translation enhances complex instruction following in LLMs by leveraging inherent constraints in existing datasets for efficient high-quality data creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/cover.png"/></item><item><title>GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23825/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23825/</guid><description>GlotCC: Open multilingual corpus &amp;amp; pipeline for minority languages, exceeding 1000 languages.</description></item><item><title>LLaMo: Large Language Model-based Molecular Graph Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/</guid><description>LLaMo: a novel large molecular graph-language model seamlessly integrates molecular graph encoders and LLMs, achieving state-of-the-art performance in molecule description generation, property predict&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/cover.png"/></item><item><title>Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24218/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24218/</guid><description>Teaching AI agents with diverse and informative language feedback dramatically improves their learning, generalization, and adaptability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24218/cover.png"/></item><item><title>Controlling Language and Diffusion Models by Transporting Activations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/</guid><description>Steering large language and diffusion models is made easy and efficient via Activation Transport (ACT)! This novel framework uses optimal transport theory to precisely control model activations, leadi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/cover.png"/></item><item><title>A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22476/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22476/</guid><description>This research introduces MLMCID, a novel pointer network architecture that excels at jointly extracting multiple intent spans and detecting multi-label, multi-class intents from complex, multilingual &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22476/cover.png"/></item><item><title>AAAR-1.0: Assessing AI's Potential to Assist Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/</guid><description>AAAR-1.0 benchmark rigorously evaluates LLMs&amp;rsquo; ability to assist in four core research tasks, revealing both potential and limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/cover.png"/></item><item><title>M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/</guid><description>M2RC-EVAL: A new massively multilingual benchmark for repository-level code completion, featuring fine-grained annotations and a large instruction dataset, enabling better evaluation of code LLMs acro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/cover.png"/></item><item><title>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</guid><description>NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"/></item></channel></rss>