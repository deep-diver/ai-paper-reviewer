<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Natural Language Processing on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/natural-language-processing/</link><description>Recent content in Natural Language Processing on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 20 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14258/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14258/</guid><description>LLMs have &amp;lsquo;Temporal Heads&amp;rsquo; that process time-specific facts!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14258/cover.png"/></item><item><title>How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14502/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14502/</guid><description>Packing new knowledge into LoRA adapters can harm LLMs! A delicate balance is needed to prevent performance decline.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14502/cover.png"/></item><item><title>Unstructured Evidence Attribution for Long Context Query Focused Summarization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14409/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14409/</guid><description>LLMs struggle with positional bias and lack transparency when summarizing long contexts. This paper introduces SUnsET dataset and fine-tuning methods to improve unstructured evidence citation and summ&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.14409/cover.png"/></item><item><title>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13965/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13965/</guid><description>Autellix: Efficient LLM Serving for Agents</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13965/cover.png"/></item><item><title>Craw4LLM: Efficient Web Crawling for LLM Pretraining</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13347/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13347/</guid><description>CRAW4LLM: Efficiently crawls web pages for LLM pretraining by prioritizing influence scores, boosting data quality &amp;amp; cutting crawling waste.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13347/cover.png"/></item><item><title>Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13962/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13962/</guid><description>Test-time scaling + confidence = better QA!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13962/cover.png"/></item><item><title>LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13922/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13922/</guid><description>LongPO: Self-evolve LLMs to excel in long contexts via short-to-long preference optimization, boosting performance without sacrificing short-context skills.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13922/cover.png"/></item><item><title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13685/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13685/</guid><description>MoM: Enhancing linear sequence modeling via mixture-of-memories for improved recall and reduced memory interference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13685/cover.png"/></item><item><title>REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13622/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13622/</guid><description>REFIND: Detects LLM hallucinations by directly leveraging retrieved documents, using a novel Context Sensitivity Ratio.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13622/cover.png"/></item><item><title>Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13533/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13533/</guid><description>LORAM: Train small, infer large LLMs by memory-efficient LoRA training. Enables 70B parameter model training on a 20G HBM GPU, replacing A100-80G. Reduces parameter storage cost by 15.81x.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-20/2502.13533/cover.png"/></item><item><title>Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13063/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13063/</guid><description>LLMs can losslessly compress 1568 tokens into a single vector, surpassing prior methods by two orders of magnitude.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13063/cover.png"/></item><item><title>Crowd Comparative Reasoning: Unlocking Comprehensive Evaluations for LLM-as-a-Judge</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12501/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12501/</guid><description>Crowd-based comparative evaluation significantly boosts LLM-as-a-judge accuracy by using crowd responses to expose deeper details, resulting in more reliable and efficient auto-evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12501/cover.png"/></item><item><title>HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12574/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12574/</guid><description>HEADINFER achieves memory-efficient LLM inference by cleverly offloading key-value cache to the CPU, enabling 4 million token inference on a single consumer GPU.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12574/cover.png"/></item><item><title>How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.12769/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.12769/</guid><description>Multilingual LLMs Hallucinate! This study measures hallucination across 30 languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-21/2502.12769/cover.png"/></item><item><title>PAFT: Prompt-Agnostic Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12859/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12859/</guid><description>PAFT dynamically adjusts prompts during LLM fine-tuning, improving model robustness and generalization across diverse prompts without sacrificing performance or efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12859/cover.png"/></item><item><title>Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12669/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12669/</guid><description>Perovskite-LLM: a new knowledge-enhanced system boosts perovskite solar cell research by integrating a domain-specific knowledge graph, high-quality datasets, and specialized LLMs for superior knowled&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12669/cover.png"/></item><item><title>Rethinking Diverse Human Preference Learning through Principal Component Analysis</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13131/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13131/</guid><description>Decomposed Reward Models (DRMs) extract diverse human preferences from binary comparisons using PCA, enabling flexible and interpretable LLM alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.13131/cover.png"/></item><item><title>SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12464/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12464/</guid><description>SafeRoute efficiently enhances LLM safety by adaptively using smaller and larger safety guard models, maximizing accuracy while minimizing costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12464/cover.png"/></item><item><title>Atom of Thoughts for Markov LLM Test-Time Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12018/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12018/</guid><description>Atom of Thoughts (AOT) revolutionizes LLM test-time scaling by decomposing complex reasoning into independent sub-questions, drastically reducing computation while maintaining high accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12018/cover.png"/></item><item><title>Building A Proof-Oriented Programmer That Is 64% Better Than GPT-4o Under Data Scarsity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11901/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11901/</guid><description>PoPilot, a novel proof-oriented programming LLM, outperforms GPT-40 by 64% under data scarcity by using synthetic data augmentation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11901/cover.png"/></item><item><title>Continuous Diffusion Model for Language Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11564/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11564/</guid><description>RDLM: A novel continuous diffusion model for language modeling leverages the geometry of categorical distributions, outperforming existing discrete approaches and approaching autoregressive model perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.11564/cover.png"/></item><item><title>Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11578/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11578/</guid><description>LLMs&amp;rsquo; performance on language complexity tasks (LIX &amp;amp; ADD) reveals a strong correlation with general capabilities, suggesting complexity metrics as noisy zero-shot proxies for model evaluation.</description></item><item><title>Large Language Models and Mathematical Reasoning Failures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11574/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11574/</guid><description>Large language models struggle with mathematical word problems, demonstrating flaws in reasoning despite achieving high accuracy; a new study highlights these persistent gaps in generalization abiliti&amp;hellip;</description></item><item><title>PhysReason: A Comprehensive Benchmark towards Physics-Based Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12054/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12054/</guid><description>PhysReason benchmark evaluates physics-based reasoning in LLMs, revealing critical limitations and guiding future improvements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12054/cover.png"/></item><item><title>Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12215/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12215/</guid><description>Contrary to popular belief, longer reasoning chains don&amp;rsquo;t always boost Large Language Model (LLM) accuracy; this research reveals that parallel scaling with shorter solutions outperforms sequential sc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12215/cover.png"/></item><item><title>SAFE-SQL: Self-Augmented In-Context Learning with Fine-grained Example Selection for Text-to-SQL</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11438/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11438/</guid><description>SAFE-SQL boosts Text-to-SQL accuracy by intelligently generating and filtering self-augmented examples for in-context learning, surpassing existing methods in challenging scenarios.</description></item><item><title>System Message Generation for User Preferences using Open-Source Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11330/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11330/</guid><description>SYSGEN: A novel pipeline generates effective system messages for LLMs using open-source models, improving model responses and addressing data scarcity in supervised fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11330/cover.png"/></item><item><title>Cuckoo: An IE Free Rider Hatched by Massive Nutrition in LLM's Nest</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11275/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11275/</guid><description>Cuckoo: a novel information extraction (IE) model leverages LLM pre-training data, achieving superior performance in few-shot settings by reframing next-token prediction as token extraction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11275/cover.png"/></item><item><title>Dyve: Thinking Fast and Slow for Dynamic Process Verification</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11157/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11157/</guid><description>Dyve: A novel dynamic process verifier boosts LLM reasoning accuracy by cleverly combining fast, immediate checks with deeper, slower analyses for complex steps, achieving significant performance gain&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11157/cover.png"/></item><item><title>FinMTEB: Finance Massive Text Embedding Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10990/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10990/</guid><description>FinMTEB: A new benchmark reveals that general-purpose embedding models struggle in the finance domain; domain-specific models excel, and surprisingly, simple BoW outperforms sophisticated models on ce&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10990/cover.png"/></item><item><title>How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11196/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11196/</guid><description>LLMs&amp;rsquo; knowledge acquisition is unveiled through the lens of evolving knowledge circuits, revealing how new knowledge integration depends on relevance to existing knowledge, exhibiting distinct phases &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11196/cover.png"/></item><item><title>Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11089/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11089/</guid><description>NSA: a novel sparse attention mechanism achieves efficient long-context modeling by combining algorithmic innovations with hardware-aligned optimizations, surpassing full attention models across vario&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11089/cover.png"/></item><item><title>Talk Structurally, Act Hierarchically: A Collaborative Framework for LLM Multi-Agent Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11098/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11098/</guid><description>TalkHier, a novel framework for LLM multi-agent systems, uses structured communication and hierarchical refinement to achieve state-of-the-art performance on various tasks, improving collaboration and&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11098/cover.png"/></item><item><title>Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10708/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10708/</guid><description>This survey paper comprehensively analyzes methods for injecting domain-specific knowledge into LLMs, categorizing them into four key approaches and evaluating their trade-offs to enhance performance &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10708/cover.png"/></item><item><title>Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10852/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10852/</guid><description>XLM-SWCM: A novel framework efficiently adapts multilingual encoders for text generation in extremely low-resource languages by cleverly sharing weights between encoder and decoder, achieving superior&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.10852/cover.png"/></item><item><title>An Open Recipe: Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09056/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09056/</guid><description>Low-resource language LLMs gain strong reasoning abilities by merging with a high-resource reasoning model, achieving performance comparable to state-of-the-art models while maintaining target languag&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09056/cover.png"/></item><item><title>CoT-Valve: Length-Compressible Chain-of-Thought Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09601/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09601/</guid><description>CoT-Valve dynamically adjusts reasoning chain lengths based on task difficulty, significantly reducing inference costs in large language models without substantial accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09601/cover.png"/></item><item><title>CRANE: Reasoning with constrained LLM generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09061/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09061/</guid><description>CRANE: A novel constrained decoding algorithm boosts LLM reasoning accuracy by strategically alternating between unconstrained reasoning and constrained generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09061/cover.png"/></item><item><title>MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12170/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.12170/</guid><description>MUDDFormer boosts Transformer performance by dynamically generating connection weights, improving cross-layer information flow and surpassing models trained with significantly more compute.</description></item><item><title>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09604/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09604/</guid><description>SelfCite: A self-supervised approach boosts LLM citation accuracy via context ablation. By removing or isolating cited text, SelfCite trains LLMs to generate high-quality citations without manual ann&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09604/cover.png"/></item><item><title>Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09083/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09083/</guid><description>Fact-checkers need explainable AI: This study reveals how AI tools can better support fact-checkers by providing explanations tailored to their workflows, addressing unmet needs, and improving the eff&amp;hellip;</description></item><item><title>SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09390/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09390/</guid><description>SQUARE, a novel prompting technique, enhances LLM reasoning by prompting self-interrogation through sequential question answering, significantly outperforming traditional methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09390/cover.png"/></item><item><title>The Stochastic Parrot on LLM's Shoulder: A Summative Assessment of Physical Concept Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08946/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08946/</guid><description>LLMs often fail to demonstrate true understanding of concepts, acting as &amp;lsquo;stochastic parrots&amp;rsquo; â a phenomenon quantitatively proven by the PHYSICO benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08946/cover.png"/></item><item><title>Typhoon T1: An Open Thai Reasoning Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09042/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09042/</guid><description>Typhoon T1: Open Thai reasoning model improves complex task performance by generating long chains of thought, detailed methodology, and open-source resources are provided.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09042/cover.png"/></item><item><title>You Do Not Fully Utilize Transformer's Representation Capacity</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09245/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09245/</guid><description>Boosting Transformer performance, Layer-Integrated Memory (LIMe) enhances representation capacity by enabling access to earlier layers&amp;rsquo; hidden states, significantly improving performance across variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-19/2502.09245/cover.png"/></item><item><title>Better Embeddings with Coupled Adam</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08441/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08441/</guid><description>Coupled Adam: A novel optimizer fixes anisotropic word embeddings in LLMs, boosting model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.08441/cover.png"/></item><item><title>One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10454/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10454/</guid><description>New benchmark COUNTERMATH enhances LLMs&amp;rsquo; mathematical reasoning using counterexample-driven proofs, revealing current models&amp;rsquo; limitations and paving the way for improved mathematical capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10454/cover.png"/></item><item><title>Auditing Prompt Caching in Language Model APIs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07776/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07776/</guid><description>Researchers expose widespread prompt caching in LLMs via novel timing attacks, highlighting significant privacy risks and model architecture leakage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07776/cover.png"/></item><item><title>CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07316/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07316/</guid><description>CODEI/O: Condensing reasoning patterns from code into LLM training data for enhanced reasoning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07316/cover.png"/></item><item><title>LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07563/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07563/</guid><description>LASP-2 revolutionizes linear attention training by achieving 36.6% faster speeds than Ring Attention via a novel sequence parallelism method, boosting efficiency for very long sequences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07563/cover.png"/></item><item><title>LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07374/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07374/</guid><description>LLMs can be effectively taught complex reasoning via efficient fine-tuning on demonstration data focusing on &lt;em>structure&lt;/em>, not content, of the reasoning process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07374/cover.png"/></item><item><title>We Can't Understand AI Using our Existing Vocabulary</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07586/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07586/</guid><description>To understand AI, we need new words! This paper argues that developing neologismsânew words for human &amp;amp; machine conceptsâis key to bridging the communication gap and achieving better AI control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07586/cover.png"/></item><item><title>Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06703/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06703/</guid><description>Smaller LLMs can outperform larger ones by strategically increasing computation during inference, defying conventional LLM scaling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06703/cover.png"/></item><item><title>Expect the Unexpected: FailSafe Long Context QA for Finance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06329/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06329/</guid><description>FailSafeQA benchmark rigorously evaluates LLMs&amp;rsquo; resilience against diverse human-interaction variations, revealing critical weaknesses in even high-performing models, particularly regarding hallucinat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06329/cover.png"/></item><item><title>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/</guid><description>OREAL, a novel RL framework, achieves state-of-the-art mathematical reasoning in LLMs using only binary outcome rewards, demonstrating that a 7B model can match the performance of 32B models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/cover.png"/></item><item><title>Hephaestus: Improving Fundamental Agent Capabilities of Large Language Models through Continual Pre-Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06589/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06589/</guid><description>Hephaestus-Forge, a new large-scale pre-training corpus, significantly boosts LLM agent capabilities in API function calling, reasoning, and adaptability through continual pre-training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06589/cover.png"/></item><item><title>Ignore the KL Penalty! Boosting Exploration on Critical Tokens to Enhance RL Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06533/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06533/</guid><description>Boosting RL fine-tuning efficiency in LLMs: A novel KL penalty modification prioritizes exploration on critical tokens, dramatically improving model performance on arithmetic tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06533/cover.png"/></item><item><title>Matryoshka Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06786/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06786/</guid><description>Matryoshka Quantization (MatQuant) boosts low-precision model accuracy by up to 10% through a novel multi-scale training approach. It leverages the nested structure of integer data types, allowing a &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06786/cover.png"/></item><item><title>ReasonFlux: Hierarchical LLM Reasoning via Scaling Thought Templates</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06772/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06772/</guid><description>ReasonFlux boosts LLM mathematical reasoning by using hierarchical thought templates, outperforming top LLMs like OpenAI&amp;rsquo;s 01-preview and DeepSeek V3.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06772/cover.png"/></item><item><title>Steel-LLM:From Scratch to Open Source -- A Personal Journey in Building a Chinese-Centric LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06635/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06635/</guid><description>Steel-LLM: A fully open-source, resource-efficient Chinese LLM trained with transparency, achieving competitive performance despite limited resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06635/cover.png"/></item><item><title>SynthDetoxM: Modern LLMs are Few-Shot Parallel Detoxification Data Annotators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06394/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06394/</guid><description>SynthDetoxM generates high-quality multilingual parallel data for text detoxification using LLMs, outperforming existing datasets and models in few-shot settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06394/cover.png"/></item><item><title>LM2: Large Memory Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06049/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06049/</guid><description>LM2: Large Memory Models enhance Transformers by adding an auxiliary memory module, significantly improving multi-step reasoning and long-context information synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06049/cover.png"/></item><item><title>The Curse of Depth in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05795/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05795/</guid><description>Deep layers in LLMs underperform due to Pre-Layer Normalization; LayerNorm Scaling resolves this by controlling output variance, significantly improving training efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05795/cover.png"/></item><item><title>Training Language Models for Social Deduction with Multi-Agent Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06060/</link><pubDate>Sun, 09 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06060/</guid><description>Language models learn effective social deduction strategies in a virtual game by using their goal to predict useful information as a dense reward signal, doubling win rates compared to standard RL.</description></item><item><title>APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05431/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05431/</guid><description>APE: a novel method significantly speeds up context-augmented generation (CAG). By using adaptive parallel encoding, APE achieves a 4.5x speedup and maintains high accuracy even with 128K length cont&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05431/cover.png"/></item><item><title>ARR: Question Answering with Large Language Models via Analyzing, Retrieving, and Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04689/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04689/</guid><description>ARR: A novel zero-shot prompting method significantly boosts LLM performance on diverse question-answering tasks by explicitly incorporating question analysis, information retrieval, and step-by-step &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04689/cover.png"/></item><item><title>DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05163/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05163/</guid><description>DuoGuard: a novel two-player RL framework generates high-quality synthetic data, improving multilingual LLM safety by outperforming state-of-the-art models with a significantly smaller model size and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05163/cover.png"/></item><item><title>Generating Symbolic World Models via Test-time Scaling of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04728/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04728/</guid><description>LLMs excel at complex reasoning but struggle with planning; this paper introduces a test-time scaling approach that enhances LLMs&amp;rsquo; PDDL reasoning, enabling high-quality PDDL domain generation, outperf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04728/cover.png"/></item><item><title>QuEST: Stable Training of LLMs with 1-Bit Weights and Activations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05003/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05003/</guid><description>QuEST enables stable, accurate LLM training using only 1-bit weights and activations, achieving Pareto-optimal performance compared to higher-precision models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05003/cover.png"/></item><item><title>Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05171/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05171/</guid><description>Boost LLM reasoning power at test time by recursively processing latent information, enabling dramatic performance gains with fewer parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05171/cover.png"/></item><item><title>Beyond Prompt Content: Enhancing LLM Performance via Content-Format Integrated Prompt Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04295/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04295/</guid><description>Researchers jointly optimize prompt content and format to significantly boost Large Language Model (LLM) performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04295/cover.png"/></item><item><title>BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03860/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03860/</guid><description>BOLT bootstraps Long Chain-of-Thought reasoning in LLMs without distillation, achieving impressive results across various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03860/cover.png"/></item><item><title>CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04416/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04416/</guid><description>CMOE efficiently transforms dense LLMs into sparse MoE architectures via expert carving, enabling fast inference without extensive retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04416/cover.png"/></item><item><title>Linear Correlation in LM's Compositional Generalization and Hallucination</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04520/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04520/</guid><description>Language models surprisingly exhibit linear relationships when composing knowledge; this linearity, resilient to fine-tuning, predicts compositional generalization and hallucination.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04520/cover.png"/></item><item><title>Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04128/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04128/</guid><description>Llasa, a novel single-Transformer TTS model, achieves state-of-the-art performance by scaling both training and inference compute, improving naturalness, prosody and emotional expressiveness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04128/cover.png"/></item><item><title>MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus Expansion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04235/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04235/</guid><description>MAGA reformulates existing corpora to massively expand LLM pretraining data, boosting performance across various model sizes while maintaining quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04235/cover.png"/></item><item><title>PILAF: Optimal Human Preference Sampling for Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04270/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04270/</guid><description>PILAF optimizes human feedback in reward modeling for better LLM alignment by using a novel response sampling strategy that aligns reward modeling with value optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04270/cover.png"/></item><item><title>Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple Interactions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04322/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04322/</guid><description>Simple interactions can easily elicit harmful outputs from LLMs, which are often overlooked. The SPEAK EASY framework and HARMSCORE metric expose this vulnerability and provide tools for better safet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04322/cover.png"/></item><item><title>Analyze Feature Flow to Enhance Interpretation and Steering in Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03032/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03032/</guid><description>Researchers unveil a data-free method to visualize and control feature flow in LLMs, enhancing interpretability and enabling targeted model steering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03032/cover.png"/></item><item><title>DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04370/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04370/</guid><description>DreamDPO: Revolutionizing text-to-3D generation by directly aligning outputs with human preferences via innovative preference optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04370/cover.png"/></item><item><title>Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03544/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03544/</guid><description>AlphaGeometry2 surpasses average IMO gold medalists in solving geometry problems!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03544/cover.png"/></item><item><title>LIMO: Less is More for Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03387/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03387/</guid><description>LIMO: Few examples unlock complex reasoning in LLMs, challenging assumptions about data-hungry models and achieving state-of-the-art results with minimal training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03387/cover.png"/></item><item><title>Teaching Language Models to Critique via Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03492/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03492/</guid><description>LLMs learn to critique and refine their output via reinforcement learning, significantly improving code generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03492/cover.png"/></item><item><title>Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03275/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03275/</guid><description>Boosting language model reasoning: A novel hybrid approach using latent tokens drastically shortens reasoning traces, improving model performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03275/cover.png"/></item><item><title>On Teacher Hacking in Language Model Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02671/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02671/</guid><description>Language model distillation suffers from &amp;rsquo;teacher hacking&amp;rsquo;, where student models over-optimize flawed teacher models, degrading true performance. This paper identifies this issue and offers effective&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02671/cover.png"/></item><item><title>QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02584/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02584/</guid><description>QLASS boosts language agent inference by using Q-values to guide a stepwise search, improving efficiency and performance even with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02584/cover.png"/></item><item><title>Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02508/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02508/</guid><description>Satori: A novel 7B LLM achieves state-of-the-art mathematical reasoning via autoregressive search.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02508/cover.png"/></item><item><title>A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01618/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01618/</guid><description>Boosting Large Language Model (LLM) inference speed using probabilistic inference via particle-based Monte Carlo methods achieves 4-16x better scaling than deterministic search approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01618/cover.png"/></item><item><title>Almost Surely Safe Alignment of Large Language Models at Inference-Time</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01208/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01208/</guid><description>InferenceGuard ensures almost-sure safe LLM responses at inference time by framing safe generation as a constrained Markov Decision Process in the LLM&amp;rsquo;s latent space, achieving high safety rates witho&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01208/cover.png"/></item><item><title>ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual Attribution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00989/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00989/</guid><description>ChartCitor: A multi-agent LLM framework combats LLM hallucination in ChartQA by providing fine-grained visual citations, enhancing user trust and productivity.</description></item><item><title>DeepRAG: Thinking to Retrieval Step by Step for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01142/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01142/</guid><description>DeepRAG enhances LLM reasoning by strategically integrating retrieval, modeled as an MDP, improving accuracy by 21.99% and retrieval efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01142/cover.png"/></item><item><title>FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01068/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01068/</guid><description>FastKV: A novel KV cache compression method speeds up long-context LLM processing 2x by selectively propagating tokens and using GQA-aware compression, maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01068/cover.png"/></item><item><title>Jailbreaking with Universal Multi-Prompts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01154/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01154/</guid><description>JUMP outperforms existing methods by optimizing universal multi-prompts for jailbreaking LLMs, offering a more efficient and generalizable approach to LLM adversarial attacks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01154/cover.png"/></item><item><title>Lifelong Sequential Knowledge Editing without Model Degradation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01636/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01636/</guid><description>ENCORE enables lifelong sequential knowledge editing in LLMs without performance loss, achieving 10,000 edits while maintaining downstream accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01636/cover.png"/></item><item><title>PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01584/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01584/</guid><description>New benchmark challenges LLMs with general knowledge puzzles, revealing reasoning gaps and suggesting improvements for future models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01584/cover.png"/></item><item><title>PlotGen: Multi-Agent LLM-based Scientific Data Visualization via Multimodal Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00988/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00988/</guid><description>PlotGen: A novel multi-agent LLM framework automates accurate scientific data visualization via multimodal feedback, boosting novice productivity and improving visualization accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00988/cover.png"/></item><item><title>Process Reinforcement through Implicit Rewards</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01456/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01456/</guid><description>PRIME (Process Reinforcement through IMplicit rEwards) revolutionizes LLM training by efficiently using implicit process rewards from online policy rollouts and outcome labels, significantly boosting &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01456/cover.png"/></item><item><title>The Differences Between Direct Alignment Algorithms are a Blur</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01237/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01237/</guid><description>Direct alignment algorithms are a blur, but this paper shows how a simple SFT phase and a scaling parameter significantly improve alignment quality, regardless of the specific reward function used.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01237/cover.png"/></item><item><title>ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01100/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01100/</guid><description>LLMs struggle with complex logical reasoning; ZebraLogic benchmark reveals a &amp;lsquo;curse of complexity&amp;rsquo;, highlighting inherent limitations and guiding future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01100/cover.png"/></item><item><title>GuardReasoner: Towards Reasoning-based LLM Safeguards</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18492/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18492/</guid><description>GuardReasoner enhances LLM safety with reasoning-based guardrails, improving performance, explainability, and generalization on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18492/cover.png"/></item><item><title>Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18585/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18585/</guid><description>Large language models (LLMs) often prematurely abandon promising reasoning paths, a phenomenon called &amp;lsquo;underthinking&amp;rsquo;. This paper introduces a novel metric to quantify this issue and proposes a decodi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18585/cover.png"/></item><item><title>WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18511/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18511/</guid><description>WILDCHAT-50M: Largest public chat dataset refines LLM post-training, showing superior SFT performance with fewer samples.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18511/cover.png"/></item><item><title>Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17703/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17703/</guid><description>Critique Fine-Tuning (CFT) outperforms traditional supervised fine-tuning (SFT) in training language models, achieving comparable results with significantly less data and opening new avenues in AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17703/cover.png"/></item><item><title>Large Language Models Think Too Fast To Explore Effectively</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18009/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18009/</guid><description>Large language models underperform humans in open-ended exploration due to prioritizing immediate choices over long-term strategic thinking, but innovative models show promise.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18009/cover.png"/></item><item><title>Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17433/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17433/</guid><description>Virus: A new attack method easily bypasses LLM guardrails, highlighting the inadequacy of current safety measures and urging for more robust solutions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17433/cover.png"/></item><item><title>Histoires Morales: A French Dataset for Assessing Moral Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17117/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17117/</guid><description>HISTOIRESMORALES: a new French dataset tackles the crucial issue of aligning language models with human moral values, providing valuable resources for ethical AI research in a previously underserved l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17117/cover.png"/></item><item><title>Optimizing Large Language Model Training Using FP4 Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17116/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17116/</guid><description>First-ever FP4 training framework for LLMs achieves accuracy comparable to BF16 and FP8, enabling efficient ultra-low precision training.</description></item><item><title>Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16975/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16975/</guid><description>Boosting Large Language Model (LLM) performance, researchers introduce Over-Tokenized Transformers, decoupling input/output vocabularies to improve language modeling. Scaling input vocabularies improv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16975/cover.png"/></item><item><title>SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18636/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18636/</guid><description>SafeRAG: A new benchmark exposes critical security vulnerabilities in Retrieval-Augmented Generation (RAG) systems by introducing four novel attack types and a comprehensive dataset for evaluation, re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18636/cover.png"/></item><item><title>SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17161/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17161/</guid><description>Reinforcement learning (RL) surpasses supervised fine-tuning (SFT) in fostering generalization in foundation models, while SFT aids RL&amp;rsquo;s stability; a comparative study across text and visual domains r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17161/cover.png"/></item><item><title>Atla Selene Mini: A General Purpose Evaluation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17195/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17195/</guid><description>Atla Selene Mini: A state-of-the-art small LLM judge surpassing larger models in benchmark performance!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17195/cover.png"/></item><item><title>IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15747/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15747/</guid><description>IndicMMLU-Pro: a new benchmark rigorously evaluates large language models&amp;rsquo; multi-task language understanding capabilities across nine major Indian languages, pushing Indic language AI research forward&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15747/cover.png"/></item><item><title>ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15570/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15570/</guid><description>ARWKV: A novel RNN-attention-based language model, distilled from a larger model, achieves strong performance using significantly fewer resources, opening a new path in efficient language model develo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.15570/cover.png"/></item><item><title>Chain-of-Retrieval Augmented Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14342/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14342/</guid><description>CoRAG, a novel Chain-of-Retrieval Augmented Generation model, dynamically refines queries for improved accuracy in multi-hop question answering, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14342/cover.png"/></item><item><title>Humanity's Last Exam</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14249/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14249/</guid><description>Humanity&amp;rsquo;s Last Exam (HLE): a groundbreaking multi-modal benchmark pushing the boundaries of large language model (LLM) capabilities, revealing a significant gap between current LLMs and human experts&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14249/cover.png"/></item><item><title>RealCritic: Towards Effectiveness-Driven Evaluation of Language Model Critiques</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14492/</link><pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14492/</guid><description>RealCritic: A new benchmark effectively evaluates language models&amp;rsquo; critique abilities using a closed-loop methodology, showcasing advanced reasoning models&amp;rsquo; superiority in self and iterative critique.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.14492/cover.png"/></item><item><title>Low-Rank Adapters Meet Neural Architecture Search for LLM Compression</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16372/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16372/</guid><description>Low-rank adapters combined with neural architecture search revolutionize LLM compression, enabling efficient fine-tuning and significantly reduced memory footprint.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.16372/cover.png"/></item><item><title>Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13629/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13629/</guid><description>SIGMA, a novel large language model, achieves &lt;strong>up to 33.36% faster inference speeds&lt;/strong> by using DiffQKV attention, which differentially optimizes query, key, and value components in the attention mech&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13629/cover.png"/></item><item><title>Autonomy-of-Experts Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13074/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13074/</guid><description>Revolutionizing large language models, Autonomy-of-Experts (AoE) empowers individual expert modules to autonomously select inputs, eliminating routers and boosting both efficiency and accuracy.</description></item><item><title>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12948/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12948/</guid><description>DeepSeek-R1 significantly improves LLM reasoning by using reinforcement learning, achieving performance comparable to OpenAI&amp;rsquo;s top models while addressing previous challenges of poor readability and l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12948/cover.png"/></item><item><title>Kimi k1.5: Scaling Reinforcement Learning with LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12599/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12599/</guid><description>Kimi K1.5: A Multimodal LLM trained with RL achieves state-of-the-art reasoning by scaling long context RL training and improving policy optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12599/cover.png"/></item><item><title>O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12570/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12570/</guid><description>O1-Pruner efficiently prunes long-thought reasoning in LLMs by harmonizing reasoning length and accuracy via fine-tuning, significantly reducing inference time without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12570/cover.png"/></item><item><title>Pairwise RM: Perform Best-of-N Sampling with Knockout Tournament</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13007/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13007/</guid><description>Pairwise RM, a novel reward model with knockout tournaments, significantly boosts large language model accuracy in test-time scaling by comparing solution pairs, eliminating arbitrary scoring inconsis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13007/cover.png"/></item><item><title>Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12895/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12895/</guid><description>Large language models (LLMs) are rapidly evolving, yet often struggle to adapt to human preferences quickly. This paper introduces Test-Time Preference Optimization (TPO), an innovative framework that&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12895/cover.png"/></item><item><title>Debate Helps Weak-to-Strong Generalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13124/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13124/</guid><description>Debate-enhanced weak supervision boosts AI alignment by combining strong and weak models, enabling safer and more reliable AI systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13124/cover.png"/></item><item><title>MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12380/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12380/</guid><description>MMVU: a new benchmark pushes multimodal video understanding to expert level, revealing limitations of current models and paving the way for more advanced AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12380/cover.png"/></item><item><title>Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11425/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11425/</guid><description>Agent-R: A novel self-training framework enables language model agents to learn from errors by dynamically constructing training data that corrects erroneous actions, resulting in significantly improv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11425/cover.png"/></item><item><title>Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11733/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11733/</guid><description>Mobile-Agent-E: A self-evolving mobile assistant conquering complex tasks with hierarchical agents and a novel self-evolution module, significantly outperforming prior approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11733/cover.png"/></item><item><title>Reasoning Language Models: A Blueprint</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11223/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11223/</guid><description>Democratizing advanced reasoning in AI, this blueprint introduces a modular framework for building Reasoning Language Models (RLMs), simplifying development and enhancing accessibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11223/cover.png"/></item><item><title>Redundancy Principles for MLLMs Benchmarks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13953/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13953/</guid><description>This research proposes principles and a framework to tackle redundancy in MLLM benchmarks, enhancing efficiency and guiding future development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13953/cover.png"/></item><item><title>IntellAgent: A Multi-Agent Framework for Evaluating Conversational AI Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11067/</link><pubDate>Sun, 19 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11067/</guid><description>IntellAgent: a novel open-source framework automating diverse conversational AI evaluation via policy-driven graph modeling, event generation, and user-agent simulations, enabling fine-grained diagnos&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.11067/cover.png"/></item><item><title>Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10799/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10799/</guid><description>STEP-KTO: A novel training framework boosts LLMs&amp;rsquo; mathematical reasoning by providing binary feedback on both intermediate steps and final answers. This ensures logical reasoning trajectories and impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10799/cover.png"/></item><item><title>ComplexFuncBench: Exploring Multi-Step and Constrained Function Calling under Long-Context Scenario</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10132/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10132/</guid><description>ComplexFuncBench, a new benchmark, rigorously evaluates LLMs&amp;rsquo; complex function-calling abilities across real-world scenarios involving multi-step processes, constraints, and long contexts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10132/cover.png"/></item><item><title>Evolving Deeper LLM Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09891/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09891/</guid><description>Mind Evolution, a novel evolutionary search strategy, significantly boosts Large Language Model (LLM) problem-solving by generating, recombining, and refining candidate solutions via an LLM, outperfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09891/cover.png"/></item><item><title>PaSa: An LLM Agent for Comprehensive Academic Paper Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10120/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10120/</guid><description>PaSa: An LLM agent autonomously performs comprehensive academic paper searches, outperforming existing methods by efficiently combining search tools, paper reading, and citation analysis, optimized vi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10120/cover.png"/></item><item><title>Bridging Language Barriers in Healthcare: A Study on Arabic LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09825/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09825/</guid><description>Arabic LLMs struggle with medical tasks; this study reveals optimal language ratios in training data for improved performance, highlighting challenges in simply translating medical data for different &amp;hellip;</description></item><item><title>Exploring the Inquiry-Diagnosis Relationship with Advanced Patient Simulators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09484/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09484/</guid><description>AI-powered medical consultations often struggle with the inquiry phase. This paper presents a novel patient simulator trained on real interactions, revealing that effective inquiry significantly impac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09484/cover.png"/></item><item><title>Multiple Choice Questions: Reasoning Makes Large Language Models (LLMs) More Self-Confident Even When They Are Wrong</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09775/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09775/</guid><description>LLM reasoning boosts self-confidence, even when answers are wrong, highlighting limitations in current evaluation metrics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09775/cover.png"/></item><item><title>Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09686/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09686/</guid><description>This survey paper explores the exciting new frontier of Large Reasoning Models (LRMs), focusing on how reinforcement learning and clever prompting techniques are boosting LLMs&amp;rsquo; reasoning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09686/cover.png"/></item><item><title>RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08617/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08617/</guid><description>RLHS, a novel alignment algorithm, leverages simulated hindsight feedback to mitigate misalignment in RLHF, significantly improving AI&amp;rsquo;s alignment with human values and goals.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08617/cover.png"/></item><item><title>Building Foundations for Natural Language Processing of Historical Turkish: Resources and Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04828/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04828/</guid><description>First-ever resources (NER dataset, dependency treebank, and corpus) and models for historical Turkish NLP are introduced, significantly advancing research capabilities in this underexplored field.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04828/cover.png"/></item><item><title>EpiCoder: Encompassing Diversity and Complexity in Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04694/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04694/</guid><description>EpiCoder revolutionizes code generation by using feature trees to create diverse and complex training data, resulting in state-of-the-art performance on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04694/cover.png"/></item><item><title>LLM4SR: A Survey on Large Language Models for Scientific Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04306/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04306/</guid><description>LLMs revolutionize scientific research! This survey reveals their transformative potential across hypothesis discovery, experiment planning, writing, and peer review, guiding future research.</description></item><item><title>rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04519/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04519/</guid><description>Small language models can master complex math reasoning using self-evolved deep thinking via Monte Carlo Tree Search, surpassing larger models in performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04519/cover.png"/></item><item><title>URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/</guid><description>URSA-7B: A new multimodal model significantly improves chain-of-thought reasoning in mathematics!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04686/cover.png"/></item><item><title>Entropy-Guided Attention for Private LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03489/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03489/</guid><description>Boosting private LLMs&amp;rsquo; efficiency and security, this research introduces an entropy-guided attention mechanism and PI-friendly layer normalization to mitigate the overheads of nonlinear operations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03489/cover.png"/></item><item><title>PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03936/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03936/</guid><description>PPTAgent, a novel two-stage framework, significantly improves automatic presentation generation by leveraging an edit-based workflow and a new evaluation metric, outperforming existing end-to-end meth&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03936/cover.png"/></item><item><title>BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</guid><description>BoostStep enhances large language models&amp;rsquo; mathematical abilities by refining single-step reasoning through a novel step-level in-context learning strategy, achieving significant improvements on variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/cover.png"/></item><item><title>GeAR: Generation Augmented Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02772/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02772/</guid><description>GeAR, a new retrieval model, boosts accuracy by combining document retrieval with fine-grained information generation, leading to better understanding and improved localization.</description></item><item><title>Samba-asr state-of-the-art speech recognition leveraging structured state-space models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/</guid><description>Samba-ASR, a novel speech recognition model using Mamba architecture, surpasses existing transformer models in accuracy and efficiency, setting a new benchmark for future ASR research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02832/cover.png"/></item><item><title>Scaling Laws for Floating Point Quantization Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/</guid><description>New scaling laws for efficient floating-point quantization training in LLMs are presented, showing optimal bit allocation and critical data size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02423/cover.png"/></item><item><title>Test-time Computing: from System-1 Thinking to System-2 Thinking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/</guid><description>Unlocking LLM potential: This paper surveys test-time computing, showing how it boosts reasoning abilities by shifting from reactive System-1 to deliberate System-2 thinking, paving the way for more p&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02497/cover.png"/></item><item><title>ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/</guid><description>ToolHop: New benchmark dataset rigorously evaluates LLMs&amp;rsquo; multi-hop tool use, revealing significant challenges and variations across different LLM families.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02506/cover.png"/></item><item><title>Personalized Graph-Based Retrieval for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/</guid><description>Personalized Graph-based Retrieval-Augmented Generation (PGraphRAG) significantly improves personalized text generation by leveraging user-centric knowledge graphs, especially in cold-start scenarios &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02157/cover.png"/></item><item><title>REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03262/</link><pubDate>Sat, 04 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03262/</guid><description>REINFORCE++, a novel RLHF algorithm, achieves superior training stability and computational efficiency compared to existing methods like PPO and GRPO, while maintaining comparable performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03262/cover.png"/></item><item><title>Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/</guid><description>AUTO-RT automates LLM vulnerability discovery by using reinforcement learning to optimize complex attack strategies, achieving faster detection and higher success rates than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01830/cover.png"/></item><item><title>METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/</guid><description>METAGENE-1, a 7-billion parameter language model, achieves state-of-the-art results in pathogen detection and genomic embedding by leveraging a massive wastewater metagenomic dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02045/cover.png"/></item><item><title>BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/</guid><description>BoxingGym: A new benchmark rigorously evaluates AI agents&amp;rsquo; ability to design experiments and discover scientific models, revealing current LLMs&amp;rsquo; limitations and highlighting fertile research avenues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01540/cover.png"/></item><item><title>CodeElo: Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/</guid><description>CODEELO benchmark uses CodeForces to fairly evaluate LLMs&amp;rsquo; coding abilities, providing human-comparable Elo ratings and addressing limitations of existing benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01257/cover.png"/></item><item><title>Dynamic Scaling of Unit Tests for Code Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/</guid><description>Boosting code generation accuracy with more unit tests! This research shows that increasing the number of unit tests used to evaluate code generated by LLMs significantly improves accuracy, especially&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01054/cover.png"/></item><item><title>LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00874/</link><pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00874/</guid><description>LUSIFER: a novel zero-shot approach empowers English-centric LLM embedding models for multilingual tasks without explicit multilingual training data, significantly enhancing performance, especially fo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00874/cover.png"/></item><item><title>Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/</guid><description>Polarizing SSMs&amp;rsquo; state transition matrices enhances long-range dependency modeling by mitigating recency bias and over-smoothing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00658/cover.png"/></item><item><title>Efficiently Serving LLM Reasoning Programs with Certaindex</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/</guid><description>Dynasor optimizes LLM reasoning by dynamically allocating compute based on a novel &amp;lsquo;certaindex&amp;rsquo; metric, reducing compute by up to 50% and increasing query rates by 3.3x.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20993/cover.png"/></item><item><title>Facilitating large language model Russian adaptation with Learned Embedding Propagation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/</guid><description>Researchers introduce Learned Embedding Propagation (LEP), a novel technique that efficiently adapts large language models (LLMs) to new languages using minimal training data, thus overcoming limitati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21140/cover.png"/></item><item><title>HumanEval Pro and MBPP Pro: Evaluating Large Language Models on Self-invoking Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/</guid><description>New benchmarks, HumanEval Pro and MBPP Pro, reveal LLMs struggle with self-invoking code generation, highlighting a critical gap in current code reasoning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21199/cover.png"/></item><item><title>MapQaTor: A System for Efficient Annotation of Map Query Datasets</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/</guid><description>MAPQATOR: a web app that streamlines creation of reproducible geospatial QA datasets, boosting annotation speed by 30x!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21015/cover.png"/></item><item><title>TangoFlux: Super Fast and Faithful Text to Audio Generation with Flow Matching and Clap-Ranked Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/</guid><description>TANGOFLUX: Blazing-fast, high-fidelity text-to-audio generation using novel CLAP-Ranked Preference Optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21037/cover.png"/></item><item><title>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</link><pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</guid><description>OneKE: a dockerized, schema-guided LLM agent system efficiently extracts knowledge from diverse sources, offering adaptability and robust error handling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/cover.png"/></item><item><title>Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19512/</guid><description>Boost fine-tuned LLMs&amp;rsquo; performance without sacrificing safety by merging pre- and post-tuning model weights!</description></item><item><title>Xmodel-2 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</link><pubDate>Fri, 27 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/</guid><description>Xmodel-2: A 1.2B parameter LLM achieving state-of-the-art reasoning performance through efficient architecture and training, now publicly available!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19638/cover.png"/></item><item><title>Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/</guid><description>Molar: A novel multimodal LLM framework boosts sequential recommendation accuracy by cleverly aligning collaborative filtering with rich item representations from text and non-text data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18176/cover.png"/></item><item><title>Token-Budget-Aware LLM Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/</guid><description>TALE: A novel framework dynamically adjusts token budgets in LLM reasoning prompts, slashing costs by ~70% with minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18547/cover.png"/></item><item><title>A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/</guid><description>This study reveals that gist token-based context compression in LLMs, while effective for some tasks, suffers from key failure patterns. The authors propose fine-grained autoencoding and segment-wise&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17483/cover.png"/></item><item><title>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</guid><description>B-STAR dynamically balances exploration and exploitation in self-taught reasoners, achieving superior performance in mathematical, coding, and commonsense reasoning tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/cover.png"/></item><item><title>Deliberation in Latent Space via Differentiable Cache Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/</guid><description>Frozen LLMs get a performance boost by augmenting their key-value cache with latent embeddings generated by a differentiable offline coprocessor.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17747/cover.png"/></item><item><title>DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17498/</guid><description>DRT-01 leverages long chain-of-thought reasoning to significantly boost machine translation quality, particularly for complex sentences with metaphors and similes, achieving substantial improvements o&amp;hellip;</description></item><item><title>Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/</guid><description>FoPE enhances attention&amp;rsquo;s periodic extension for better length generalization in language models by addressing spectral damage in RoPE using Fourier Series and zeroing out destructive frequencies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17739/cover.png"/></item><item><title>Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/</guid><description>Friends-MMC: A new dataset facilitates multi-modal multi-party conversation understanding by providing 24,000+ utterances with video, audio, and speaker annotations, enabling advancements in character&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17295/cover.png"/></item><item><title>In Case You Missed It: ARC 'Challenge' Is Not That Challenging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/</guid><description>LLM evaluation on multiple-choice questions is flawed; considering all options simultaneously, not individually, reveals much higher accuracy and challenges existing benchmark rankings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17758/cover.png"/></item><item><title>YuLan-Mini: An Open Data-efficient Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/</guid><description>YuLan-Mini: An open, data-efficient 2.42B parameter LLM achieving top-tier performance with innovative training techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17743/cover.png"/></item><item><title>OpenRFT: Adapting Reasoning Foundation Model for Domain-specific Tasks with Reinforcement Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/</guid><description>OpenRFT adapts generalist reasoning models for domain-specific tasks using reinforcement fine-tuning, overcoming data scarcity and lack of reasoning step data via question augmentation, synthesized re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16849/cover.png"/></item><item><title>Revisiting In-Context Learning with Long Context Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</link><pubDate>Sun, 22 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/</guid><description>Long-context models surprisingly show that simple random sampling of examples is as effective as sophisticated methods for in-context learning, shifting the focus to efficient context utilization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16926/cover.png"/></item><item><title>NILE: Internal Consistency Alignment in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/</guid><description>NILE framework significantly boosts LLM performance by aligning instruction-tuning datasets with pre-trained internal knowledge, achieving up to 68.5% gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16686/cover.png"/></item><item><title>AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/</guid><description>AceMath achieves state-of-the-art results in mathematical reasoning by introducing highly effective instruction-tuned models and reward models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15084/cover.png"/></item><item><title>Fietje: An open, efficient LLM for Dutch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/</guid><description>Fietje: an open-source, efficient Dutch language model outperforming larger models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15450/cover.png"/></item><item><title>How to Synthesize Text Data without Model Collapse?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/</guid><description>Token-level editing prevents language model collapse from synthetic data by theoretically bounding test error and empirically improving model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14689/cover.png"/></item><item><title>LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/</guid><description>M-ALERT, a new multilingual benchmark, reveals significant safety inconsistencies across languages in top LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15035/cover.png"/></item><item><title>MixLLM: LLM Quantization with Global Mixed-precision between Output-features and Highly-efficient System Design</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/</guid><description>MixLLM achieves state-of-the-art LLM compression by using mixed-precision quantization between output features, improving accuracy and system efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14590/cover.png"/></item><item><title>Outcome-Refining Process Supervision for Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/</guid><description>Boosting code generation accuracy, Outcome-Refining Process Supervision (ORPS) uses execution feedback and structured reasoning to refine code, achieving significant improvements across models and dat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15118/cover.png"/></item><item><title>ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/</guid><description>ReMoE: Revolutionizing Mixture-of-Experts with fully differentiable ReLU routing, achieving superior scalability and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14711/cover.png"/></item><item><title>RobustFT: Robust Supervised Fine-tuning for Large Language Models under Noisy Response</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/</guid><description>ROBUSTFT tackles noisy data in LLM fine-tuning by using multi-expert noise detection and context-enhanced relabeling, significantly boosting model performance in noisy scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14922/cover.png"/></item><item><title>AntiLeak-Bench: Preventing Data Contamination by Automatically Constructing Benchmarks with Updated Real-World Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/</guid><description>Auto-built benchmark with up-to-date knowledge ensures contamination-free LLM evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13670/cover.png"/></item><item><title>Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13795/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13795/</guid><description>Mix-LN boosts deep layer power in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13795/cover.png"/></item><item><title>RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/</guid><description>First benchmark for RAG reward models reveals their limitations and the need for preference-aligned training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13746/cover.png"/></item><item><title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</guid><description>AI agents are tested in a simulated company, revealing their capability to automate tasks and shortcomings with complex workflows and interfaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/cover.png"/></item><item><title>Are Your LLMs Capable of Stable Reasoning?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</guid><description>G-Pass@k &amp;amp; LiveMathBench: Evaluating the stability of LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/cover.png"/></item><item><title>OmniEval: An Omnidirectional and Automatic RAG Evaluation Benchmark in Financial Domain</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13018/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13018/</guid><description>OmniEval: Automatic benchmark for evaluating financial RAG systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13018/cover.png"/></item><item><title>RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/</guid><description>RetroLLM unifies retrieval &amp;amp; generation in LLMs, boosting accuracy and cutting costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11919/cover.png"/></item><item><title>SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/</guid><description>SepLLM shrinks LLMs, speeding them up by over 50% without losing much accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12094/cover.png"/></item><item><title>SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</link><pubDate>Mon, 16 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/</guid><description>Self-play method SPAR enhances LLMs instruction following abilities, beating GPT-4 on IFEval</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11605/cover.png"/></item><item><title>Smaller Language Models Are Better Instruction Evolvers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/</guid><description>Smaller is better: SLMs outperform LLMs in evolving complex &amp;amp; diverse instructions for AI training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11231/cover.png"/></item><item><title>Byte Latent Transformer: Patches Scale Better Than Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/</guid><description>BLT: tokenizer-free LLM for efficiency and robustness</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09871/cover.png"/></item><item><title>SCBench: A KV Cache-Centric Analysis of Long-Context Methods</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/</guid><description>New benchmark for evaluating long-context models finds sub-O(n) methods lacking in real-world use cases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10319/cover.png"/></item><item><title>JuStRank: Benchmarking LLM Judges for System Ranking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09569/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09569/</guid><description>JuStRank: LLM system ranker benchmark reveals critical judge qualities (decisiveness, bias) impacting ranking accuracy, highlighting instance-level performance doesn&amp;rsquo;t guarantee accurate system-level&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09569/cover.png"/></item><item><title>Phi-4 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/</guid><description>Phi-4: a 14B parameter LLM surpassing its teacher model (GPT-4) in STEM-focused QA through innovative synthetic data generation and post-training techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08905/cover.png"/></item><item><title>RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08972/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08972/</guid><description>RULEARENA, a new benchmark, rigorously evaluates large language models&amp;rsquo; ability to apply complex, real-world rules across diverse scenarios, revealing significant shortcomings in current LLMs&amp;rsquo; rule-gu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08972/cover.png"/></item><item><title>Shiksha: A Technical Domain focused Translation Dataset and Model for Indian Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09025/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09025/</guid><description>Shiksha: A new multilingual translation dataset and model surpasses existing benchmarks for Indian languages, focusing on scientific, technical, and educational domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09025/cover.png"/></item><item><title>The Impact of Copyrighted Material on Large Language Models: A Norwegian Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09460/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09460/</guid><description>Norwegians show that using copyrighted material improves LLMs, but raises legal and ethical issues.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09460/cover.png"/></item><item><title>Word Sense Linking: Disambiguating Outside the Sandbox</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09370/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09370/</guid><description>Word Sense Linking (WSL) revolutionizes word sense disambiguation by tackling its real-world limitations. It combines span identification and sense linking in plain text, offering better integration &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09370/cover.png"/></item><item><title>SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/</guid><description>Fine-tuning small language models? Tweak the learning rate and batch size for a reasoning boost!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.08347/cover.png"/></item><item><title>Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/</guid><description>Contextualized AI counterspeech significantly outperforms generic methods by adapting to the moderation context and user, improving persuasiveness without sacrificing other qualities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/cover.png"/></item><item><title>Granite Guardian</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/</guid><description>Granite Guardian: Open-source risk detection models for LLMs, surpassing existing models in accuracy and offering comprehensive coverage across multiple risk dimensions, promoting safer AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/cover.png"/></item><item><title>Training Large Language Models to Reason in a Continuous Latent Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</guid><description>LLMs are trained to reason using language, but COCONUT lets them reason directly in a continuous latent space, boosting performance on logical tasks requiring complex planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/cover.png"/></item><item><title>Fully Open Source Moxin-7B Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</guid><description>Moxin-LLM: A fully open-source 7B parameter LLM achieving superior zero-shot performance, promoting transparency and reproducibility in AI research.</description></item><item><title>DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04905/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04905/</guid><description>DEMO benchmark revolutionizes dialogue modeling by focusing on fine-grained elements (Prelude, Interlocution, Epilogue), enabling comprehensive evaluation and superior agent performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04905/cover.png"/></item><item><title>Evaluating and Aligning CodeLLMs on Human Preference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/</guid><description>CodeArena, a novel benchmark, evaluates code LLMs based on human preferences, revealing performance gaps between open-source and proprietary models, and a large-scale synthetic instruction corpus impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/cover.png"/></item><item><title>EXAONE 3.5: Series of Large Language Models for Real-world Use Cases</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/</guid><description>LG AI Research unveils EXAONE 3.5, a series of instruction-tuned language models (2.4B, 7.8B, and 32B parameters) excelling in real-world tasks, long-context understanding, and general benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/cover.png"/></item><item><title>Densing Law of LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</guid><description>LLMs&amp;rsquo; training quality is exponentially improving, enabling models with half the parameters to match state-of-the-art performance every 3 months, thus reducing inference costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/cover.png"/></item><item><title>Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/</guid><description>Marco-LLM: A groundbreaking multilingual LLM significantly enhances cross-lingual capabilities via massive multilingual training, bridging the performance gap between high- and low-resource languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/cover.png"/></item><item><title>Monet: Mixture of Monosemantic Experts for Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/</guid><description>MONET improves Transformer interpretability by using Mixture-of-Experts (MoE) with 262K monosemantic experts per layer, achieving parameter efficiency and enabling knowledge manipulation without perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/cover.png"/></item><item><title>Evaluating Language Models as Synthetic Data Generators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</guid><description>AGORABENCH: A new benchmark reveals surprising strengths &amp;amp; weaknesses of LMs as synthetic data generators, showing that problem-solving ability isn&amp;rsquo;t the sole indicator of data quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/cover.png"/></item><item><title>Robust Multi-bit Text Watermark with LLM-based Paraphrasers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/</guid><description>Researchers developed a robust multi-bit text watermarking method using LLMs for paraphrasing, achieving over 99.99% detection accuracy while maintaining semantic information and resisting common atta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/cover.png"/></item><item><title>Weighted-Reward Preference Optimization for Implicit Model Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/</guid><description>WRPO: Implicitly fuse LLMs, boosting performance without complex alignment or merging!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/cover.png"/></item><item><title>OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/</guid><description>Imperfect OCR hinders Retrieval-Augmented Generation (RAG). OHRBench, a new benchmark, reveals this cascading impact, showing current OCR solutions insufficient for high-quality RAG knowledge bases. &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/cover.png"/></item><item><title>Free Process Rewards without Process Labels</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</guid><description>Train high-performing Process Reward Models (PRMs) cheaply using only outcome-level labels, eliminating the need for costly step-by-step annotations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/cover.png"/></item><item><title>Towards Cross-Lingual Audio Abuse Detection in Low-Resource Settings with Few-Shot Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01408/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01408/</guid><description>Few-shot learning empowers cross-lingual audio abuse detection using pre-trained models, achieving high accuracy in low-resource Indian languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01408/cover.png"/></item><item><title>A dynamic parallel method for performance optimization on hybrid CPUs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/</guid><description>Dynamic parallel processing boosts LLM inference speed on hybrid CPUs by over 90% memory bandwidth, resolving performance bottlenecks caused by imbalanced hardware capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/cover.png"/></item><item><title>A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/</guid><description>Boost LLM accuracy exponentially by using a two-stage algorithm with provable scaling laws: generate multiple candidate solutions then compare them in a knockout tournament!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/cover.png"/></item><item><title>Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</guid><description>Boosting LLMs&amp;rsquo; reasoning: A novel token-level contrastive estimation method automatically identifies and penalizes critical tokens leading to errors, significantly enhancing reasoning accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/cover.png"/></item><item><title>INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/</guid><description>New multilingual LLM benchmark, INCLUDE, tackles regional knowledge gaps by using 197K QA pairs from 44 languages, improving cross-lingual evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/cover.png"/></item><item><title>KV Shifting Attention Enhances Language Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/</guid><description>KV Shifting Attention: A novel attention mechanism significantly enhances language modeling by simplifying induction heads, leading to improved performance and faster convergence, even in large-scale &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/cover.png"/></item><item><title>LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19638/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19638/</guid><description>Researchers developed a multilingual news topic classifier using a teacher-student framework and GPT-40 for automatic data annotation, achieving high performance without manual annotation.</description></item><item><title>o1-Coder: an o1 Replication for Coding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/</guid><description>O1-CODER replicates OpenAI&amp;rsquo;s o1 model for coding, integrating reinforcement learning and Monte Carlo Tree Search to enhance System-2 thinking and generate high-quality code with reasoning steps.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/cover.png"/></item><item><title>Puzzle: Distillation-Based NAS for Inference-Optimized LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</guid><description>Puzzle: a novel framework accelerates large language model inference by using neural architecture search and knowledge distillation, achieving a 2.17x speedup on a single GPU while preserving 98.4% ac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/cover.png"/></item><item><title>Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</guid><description>HiAR-ICL, a novel automated reasoning paradigm using Monte Carlo Tree Search, surpasses state-of-the-art accuracy in complex mathematical reasoning by shifting focus from specific examples to abstract&amp;hellip;</description></item><item><title>Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</guid><description>Self-VerIfication length Policy (SVIP) dynamically adjusts speculative decoding draft lengths based on token difficulty, achieving up to 20% faster large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/cover.png"/></item><item><title>Training and Evaluating Language Models with Template-based Data Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</guid><description>Researchers created TemplateGSM, a massive dataset of 7M+ grade-school math problems and solutions, using GPT-4 to generate templates, significantly advancing LLM training for mathematical reasoning.</description></item><item><title>LongKey: Keyphrase Extraction for Long Documents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17863/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17863/</guid><description>LongKey: A novel framework excels at extracting keyphrases from lengthy documents using an encoder-based language model and max-pooling, outperforming existing methods across diverse datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17863/cover.png"/></item><item><title>Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</guid><description>Low-bit quantization excels for undertrained LLMs but struggles with fully-trained ones; new scaling laws reveal this, directing future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/cover.png"/></item><item><title>Star Attention: Efficient LLM Inference over Long Sequences</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</guid><description>Star Attention: 11x faster LLM inference on long sequences with 95-100% accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/cover.png"/></item><item><title>From CISC to RISC: language-model guided assembly transpilation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/</guid><description>A novel LLM-based transpiler, CRT, efficiently converts x86 assembly to ARM and RISC-V assembly, achieving high accuracy and significant performance improvements over existing virtualization methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/cover.png"/></item><item><title>From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/</guid><description>LLMs are revolutionizing AI evaluation by offering nuanced judgments surpassing traditional methods. This paper provides a taxonomy, benchmark, and future directions for LLM-as-a-judge.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/cover.png"/></item><item><title>MH-MoE:Multi-Head Mixture-of-Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16205/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16205/</guid><description>MH-MoE: A novel implementation of Multi-Head Mixture-of-Experts achieves superior performance in large language models by enhancing efficiency without sacrificing model size or computational cost.</description></item><item><title>O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/</guid><description>Simple distillation from OpenAI&amp;rsquo;s API, combined with fine-tuning, surprisingly surpasses OpenAI&amp;rsquo;s O1-preview on complex mathematical reasoning, urging transparency in AI research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/cover.png"/></item><item><title>Predicting Emergent Capabilities by Finetuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</guid><description>Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/cover.png"/></item><item><title>MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/</guid><description>MolReFlect achieves state-of-the-art molecule-text alignment by using a teacher-student LLM framework that generates fine-grained alignments, improving accuracy and explainability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/cover.png"/></item><item><title>One to rule them all: natural language to bind communication, perception and action</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/</guid><description>AI-powered robots now understand and execute complex natural language commands, adapting seamlessly to dynamic environments thanks to a new architecture integrating LLMs, perception, and planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/cover.png"/></item><item><title>Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/</guid><description>LLMs&amp;rsquo; hallucinations stem from entity recognition: SAEs reveal model &amp;lsquo;self-knowledge&amp;rsquo;, causally affecting whether it hallucinates or refuses to answer. This mechanism is even repurposed by chat finet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/cover.png"/></item><item><title>Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/</guid><description>Marco-01: a novel large reasoning model surpasses existing LLMs by using Chain-of-Thought, Monte Carlo Tree Search, and reflection mechanisms to excel in open-ended problem-solving, particularly in co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/cover.png"/></item><item><title>UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/</guid><description>UnifiedCrawl efficiently harvests massive monolingual datasets for low-resource languages from Common Crawl, enabling affordable LLM adaptation via QLoRA, significantly improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/cover.png"/></item><item><title>A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/</guid><description>New data-free methodology creates effective, generalizable LLMs guardrails against off-topic prompts, significantly improving LLM safety and responsible use.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/cover.png"/></item><item><title>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/</guid><description>BALROG benchmark rigorously evaluates LLMs&amp;rsquo;/VLMs&amp;rsquo; abilities in complex games, revealing their strengths and weaknesses in long-term planning and decision-making, highlighting the need for improved vis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/cover.png"/></item><item><title>Hymba: A Hybrid-head Architecture for Small Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</guid><description>Hymba: Hybrid-head architecture boosts small language model performance by 11.67x cache size reduction and 3.49x throughput, surpassing existing models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/cover.png"/></item><item><title>ORID: Organ-Regional Information Driven Framework for Radiology Report Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13025/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13025/</guid><description>ORID framework leverages organ-regional information to boost radiology report generation, achieving state-of-the-art accuracy by integrating multi-modal data and reducing noise from unrelated organs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13025/cover.png"/></item><item><title>Patience Is The Key to Large Language Model Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</guid><description>Boosting Large Language Model (LLM) reasoning without massive datasets: A novel training method encourages &amp;lsquo;patient&amp;rsquo; reasoning, improving accuracy by up to 6.7% on benchmark tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/cover.png"/></item><item><title>When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/</guid><description>AnchorAttention enhances long-context LLMs by mitigating BFloat16&amp;rsquo;s disruptive effects on RoPE, improving performance and speeding up training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/cover.png"/></item><item><title>Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/</guid><description>SUTRA tokenizer outperforms other LLMs in Indian languages, improving efficiency and facilitating better model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/cover.png"/></item><item><title>RedPajama: an Open Dataset for Training Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/</guid><description>RedPajama, two massive open-source datasets, are released for training LLMs, improving transparency and facilitating the development of high-performing open-source models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/cover.png"/></item><item><title>Ultra-Sparse Memory Network</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/</guid><description>UltraMem, a novel ultra-sparse memory network, drastically speeds up LLM inference by 6x compared to MoE while maintaining performance, paving the way for efficient large-scale model deployment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/cover.png"/></item><item><title>Drowning in Documents: Consequences of Scaling Reranker Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11767/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11767/</guid><description>Scaling reranker inference surprisingly degrades retrieval quality beyond a certain point, prompting the need for more robust reranking techniques.</description></item><item><title>Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/</guid><description>Verifier engineering: A new post-training paradigm for foundation models using automated verifiers to provide effective supervision signals, enhancing capabilities beyond traditional data-centric meth&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/cover.png"/></item><item><title>LLÃ¤Mmlein: Compact and Competitive German-Only Language Models from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/</guid><description>New German-only LLMs, LLÃ¤Mmlein 120M &amp;amp; 1B, trained from scratch &amp;amp; openly released, show competitive performance and offer insights into efficient model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/cover.png"/></item><item><title>SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</guid><description>SageAttention2 achieves 4-bit accurate attention, boosting inference speed by 2x compared to FlashAttention2, while maintaining end-to-end accuracy across diverse models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/cover.png"/></item><item><title>SlimLM: An Efficient Small Language Model for On-Device Document Assistance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/</guid><description>SlimLM: Efficient small language models (SLMs) optimized for mobile document assistance, achieving comparable or superior performance to existing SLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/cover.png"/></item><item><title>Adaptive Decoding via Latent Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</guid><description>LLMs can dynamically adjust decoding temperature using Adaptive Decoding and Latent Preference Optimization, improving performance across creative and factual tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/cover.png"/></item><item><title>Comprehensive and Practical Evaluation of Retrieval-Augmented Generation Systems for Medical Question Answering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09213/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09213/</guid><description>MedRGB benchmark reveals current LLMs struggle with noisy medical data, emphasizing the need for robust RAG systems in healthcare AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09213/cover.png"/></item><item><title>LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</guid><description>LLaMA-Mesh: Unifying 3D mesh generation with LLMs by directly representing meshes as text, enabling efficient text-to-3D conversion within a single model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/cover.png"/></item><item><title>CamemBERT 2.0: A Smarter French Language Model Aged to Perfection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08868/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08868/</guid><description>CamemBERT 2.0: Two new French language models (CamemBERTav2 &amp;amp; CamemBERTv2) outperform predecessors by addressing temporal concept drift via larger, updated datasets and enhanced tokenization, demonstr&amp;hellip;</description></item><item><title>Can sparse autoencoders be used to decompose and interpret steering vectors?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/</guid><description>Sparse autoencoders fail to accurately decompose and interpret steering vectors due to distribution mismatch and the inability to handle negative feature projections; this paper identifies these issue&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/cover.png"/></item><item><title>Cut Your Losses in Large-Vocabulary Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/</guid><description>Cut Cross-Entropy (CCE) dramatically reduces the memory footprint of training large language models by cleverly computing the cross-entropy loss without materializing the full logit matrix.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/cover.png"/></item><item><title>Direct Preference Optimization Using Sparse Feature-Level Constraints</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/</guid><description>Feature-level constrained Preference Optimization (FPO) boosts LLM alignment efficiency and stability by using sparse autoencoders and feature-level constraints, achieving significant improvements ove&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/cover.png"/></item><item><title>Large Language Models Can Self-Improve in Long-context Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/</guid><description>LLMs can now self-improve long-context reasoning via SEALONG, a novel method leveraging multiple model outputs and minimum Bayes risk scoring to enable effective supervised fine-tuning or preference o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/cover.png"/></item><item><title>Top-$nÏ$: Not All Logits Are You Need</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/</guid><description>Top-Î·Ï: A novel LLM sampling method outperforms existing approaches by using a statistical threshold on pre-softmax logits, achieving higher accuracy while maintaining diversity, even at high temperat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/cover.png"/></item><item><title>Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/</guid><description>Chinese SimpleQA, a new benchmark, offers a comprehensive evaluation of the factuality of LLMs answering short questions in Chinese, exhibiting diversity, high quality, and ease of evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/cover.png"/></item><item><title>Stronger Models are NOT Stronger Teachers for Instruction Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/</guid><description>Larger language models aren&amp;rsquo;t always better teachers for instruction tuning; a new metric, CAR, predicts teacher model effectiveness better than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/cover.png"/></item><item><title>Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/</guid><description>Contrary to common belief, toxicity reduction in language models isn&amp;rsquo;t simply achieved by dampening toxic neurons; it&amp;rsquo;s a complex balancing act across multiple neuron groups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/cover.png"/></item><item><title>Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/</guid><description>WEB-DREAMER uses LLMs as world models for safe and efficient web agent planning, achieving substantial performance gains over reactive baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/cover.png"/></item><item><title>Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</guid><description>Golden Touchstone, a new bilingual benchmark, comprehensively evaluates financial LLMs across eight tasks, revealing model strengths and weaknesses and advancing FinLLM research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/cover.png"/></item><item><title>IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/</guid><description>IOPO empowers LLMs to master complex instructions via input-output preference optimization, boasting significant performance gains on a new benchmark, TRACE.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/cover.png"/></item><item><title>M-Longdoc: A Benchmark For Multimodal Super-Long Document Understanding And A Retrieval-Aware Tuning Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06176/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06176/</guid><description>M-LongDoc: a new benchmark and retrieval-aware tuning framework revolutionizes multimodal long document understanding, improving model accuracy by 4.6%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06176/cover.png"/></item><item><title>Balancing Pipeline Parallelism with Vocabulary Parallelism</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/</guid><description>Boost large language model training speed by 51% with Vocabulary Parallelism, a novel technique that balances computation and memory usage across pipeline stages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/cover.png"/></item><item><title>BitNet a4.8: 4-bit Activations for 1-bit LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/</guid><description>BitNet a4.8 achieves comparable performance to existing 1-bit LLMs, but with significantly faster inference, by using a hybrid quantization and sparsification strategy for 4-bit activations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/cover.png"/></item><item><title>DELIFT: Data Efficient Language model Instruction Fine Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04425/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04425/</guid><description>DELIFT: Data Efficient Language Model Instruction Fine-Tuning, drastically reduces the data needed for effective LLM fine-tuning without sacrificing performance.</description></item><item><title>Hardware and Software Platform Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/</guid><description>Researchers developed Hardware and Software Platform Inference (HSPI) to identify the underlying GPU and software stack used to serve LLMs, enhancing transparency in the industry.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/cover.png"/></item><item><title>Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/</guid><description>Can LLMs effectively handle information spread across vast, almost million-scale datasets? This research investigates this question by evaluating 17 LLMs on novel âneedle threadingâ tasks. These task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/cover.png"/></item><item><title>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/</guid><description>OpenCoder, a top-tier open-source code LLM, is introduced, providing not only model weights and code but also reproducible training data, data processing pipelines, and training protocols, enabling co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/cover.png"/></item><item><title>RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04752/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04752/</guid><description>RetrieveGPT enhances code-mixed information retrieval by merging GPT-3.5 Turbo prompts with a novel mathematical model, improving the accuracy of relevant document extraction from complex, sequenced c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04752/cover.png"/></item><item><title>HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02959/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02959/</guid><description>HtmlRAG boosts RAG system accuracy by using HTML, not plain text, to model retrieved knowledge, improving knowledge representation and mitigating LLM hallucination.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02959/cover.png"/></item><item><title>DynaSaur: Large Language Agents Beyond Predefined Actions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/</guid><description>DynaSaur: a novel LLM agent framework enabling dynamic action creation, surpassing prior methods with greater flexibility and top performance on the GAIA benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/cover.png"/></item><item><title>Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</guid><description>Tencent unveils Hunyuan-Large, a groundbreaking open-source MoE LLM boasting 389B parameters and 52B activated parameters, surpassing existing models in performance across various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/cover.png"/></item><item><title>Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/</guid><description>Boosting unit test generation efficiency, this study empirically evaluates various parameter-efficient fine-tuning methods on LLMs, demonstrating comparable performance to full fine-tuning at signific&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/cover.png"/></item><item><title>Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</guid><description>Researchers discovered predictable scaling laws for activation sparsity in LLMs, showing how data, architecture, and model size influence sparsity, paving the way for more efficient and interpretable &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/cover.png"/></item><item><title>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</guid><description>WEBRL: A self-evolving online curriculum reinforcement learning framework empowers open LLMs to excel as high-performing web agents, surpassing proprietary models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/cover.png"/></item><item><title>Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/</guid><description>Zebra-Llama, a context-aware LLM, democratizes rare disease knowledge by providing highly precise, context-rich information about Ehlers-Danlos Syndrome, significantly improving diagnostic support.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/cover.png"/></item><item><title>Sample-Efficient Alignment for LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/</guid><description>Sample-efficient LLM alignment achieved via a novel Thompson sampling algorithm (SEA), outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/cover.png"/></item><item><title>Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/</link><pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/</guid><description>Swan &amp;amp; ArabicMTEB: New dialect-aware Arabic embedding models and benchmark achieve state-of-the-art performance, addressing limitations of existing multilingual models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/cover.png"/></item><item><title>Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</guid><description>Specialized Sparse Autoencoders (SSAEs) decode foundation models&amp;rsquo; &amp;lsquo;dark matter&amp;rsquo; features, efficiently extracting rare subdomain concepts for improved interpretability and safety.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/cover.png"/></item><item><title>GRS-QA -- Graph Reasoning-Structured Question Answering Dataset</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00369/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00369/</guid><description>GRS-QA: New benchmark dataset reveals LLM reasoning limitations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00369/cover.png"/></item><item><title>LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/</guid><description>LibMoE: A new library streamlines MoE research by offering standardized training, evaluation, and a modular design, enabling efficient benchmarking of various MoE algorithms for LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/cover.png"/></item><item><title>BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/</guid><description>BitStack: Dynamic LLM sizing for variable memory!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/cover.png"/></item><item><title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</guid><description>Constraint Back-translation enhances complex instruction following in LLMs by leveraging inherent constraints in existing datasets for efficient high-quality data creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/cover.png"/></item><item><title>GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23825/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23825/</guid><description>GlotCC: Open multilingual corpus &amp;amp; pipeline for minority languages, exceeding 1000 languages.</description></item><item><title>LLaMo: Large Language Model-based Molecular Graph Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/</guid><description>LLaMo: a novel large molecular graph-language model seamlessly integrates molecular graph encoders and LLMs, achieving state-of-the-art performance in molecule description generation, property predict&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/cover.png"/></item><item><title>Teaching Embodied Reinforcement Learning Agents: Informativeness and Diversity of Language Use</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24218/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24218/</guid><description>Teaching AI agents with diverse and informative language feedback dramatically improves their learning, generalization, and adaptability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24218/cover.png"/></item><item><title>Controlling Language and Diffusion Models by Transporting Activations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/</guid><description>Steering large language and diffusion models is made easy and efficient via Activation Transport (ACT)! This novel framework uses optimal transport theory to precisely control model activations, leadi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/cover.png"/></item><item><title>A Pointer Network-based Approach for Joint Extraction and Detection of Multi-Label Multi-Class Intents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22476/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22476/</guid><description>This research introduces MLMCID, a novel pointer network architecture that excels at jointly extracting multiple intent spans and detecting multi-label, multi-class intents from complex, multilingual &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22476/cover.png"/></item><item><title>AAAR-1.0: Assessing AI's Potential to Assist Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/</guid><description>AAAR-1.0 benchmark rigorously evaluates LLMs&amp;rsquo; ability to assist in four core research tasks, revealing both potential and limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/cover.png"/></item><item><title>M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/</guid><description>M2RC-EVAL: A new massively multilingual benchmark for repository-level code completion, featuring fine-grained annotations and a large instruction dataset, enabling better evaluation of code LLMs acro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/cover.png"/></item><item><title>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</guid><description>NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"/></item></channel></rss>