<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Carnegie Mellon University on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-carnegie-mellon-university/</link><description>Recent content in üè¢ Carnegie Mellon University on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Wed, 26 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-carnegie-mellon-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Unified Multimodal Discrete Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.20853/</link><pubDate>Wed, 26 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.20853/</guid><description>UniDisc: a unified multimodal discrete diffusion model for joint text and image generation, surpassing autoregressive models in quality &amp;amp; efficiency!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.20853/cover.png"/></item><item><title>Optimizing Test-Time Compute via Meta Reinforcement Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07572/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07572/</guid><description>LLMs can now reason more efficiently!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07572/cover.png"/></item><item><title>APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05431/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05431/</guid><description>APE: a novel method significantly speeds up context-augmented generation (CAG). By using adaptive parallel encoding, APE achieves a 4.5x speedup and maintains high accuracy even with 128K length cont&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05431/cover.png"/></item><item><title>Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17703/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17703/</guid><description>Critique Fine-Tuning (CFT) outperforms traditional supervised fine-tuning (SFT) in training language models, achieving comparable results with significantly less data and opening new avenues in AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17703/cover.png"/></item><item><title>TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/</guid><description>AI agents are tested in a simulated company, revealing their capability to automate tasks and shortcomings with complex workflows and interfaces.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14161/cover.png"/></item><item><title>MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05237/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05237/</guid><description>MAmmoTH-VL: A novel approach to instruction tuning at scale creates a 12M dataset eliciting chain-of-thought reasoning, yielding state-of-the-art multimodal reasoning capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05237/cover.png"/></item><item><title>Evaluating Language Models as Synthetic Data Generators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</guid><description>AGORABENCH: A new benchmark reveals surprising strengths &amp;amp; weaknesses of LMs as synthetic data generators, showing that problem-solving ability isn&amp;rsquo;t the sole indicator of data quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/cover.png"/></item><item><title>Video Depth without Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</guid><description>RollingDepth: Achieving state-of-the-art video depth estimation without using complex video models, by cleverly extending a single-image depth estimator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/cover.png"/></item><item><title>Soft Robotic Dynamic In-Hand Pen Spinning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/</guid><description>SWIFT, a new system, enables a soft robotic hand to learn dynamic pen spinning via real-world trial-and-error, achieving 100% success across diverse pen properties without explicit object modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/cover.png"/></item><item><title>VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04923/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04923/</guid><description>VideoGLaMM: a new large multimodal model achieves precise pixel-level visual grounding in videos by seamlessly integrating a dual vision encoder, a spatio-temporal decoder, and a large language model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04923/cover.png"/></item><item><title>Inference Optimal VLMs Need Only One Visual Token but Larger Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03312/</link><pubDate>Tue, 05 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03312/</guid><description>Inference-optimal Vision Language Models (VLMs) need only one visual token but larger models!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.03312/cover.png"/></item><item><title>Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</guid><description>Specialized Sparse Autoencoders (SSAEs) decode foundation models&amp;rsquo; &amp;lsquo;dark matter&amp;rsquo; features, efficiently extracting rare subdomain concepts for improved interpretability and safety.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/cover.png"/></item></channel></rss>