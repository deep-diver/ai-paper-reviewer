<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ðŸ¤— 24-10-18 on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-18/</link><description>Recent content in ðŸ¤— 24-10-18 on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Thu, 17 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-18/index.xml" rel="self" type="application/rss+xml"/><item><title>$Î³-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13859/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13859/</guid><description>Î³-MoD efficiently adapts Mixture-of-Depths to existing MLLMs, drastically reducing computational costs without significant performance loss, paving the way for more practical multimodal AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13859/cover.png"/></item><item><title>A Comparative Study on Reasoning Patterns of OpenAI's o1 Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13639/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13639/</guid><description>OpenAI&amp;rsquo;s o1 model surpasses other LLMs in reasoning tasks by employing unique inference strategies, revealing six novel reasoning patterns analyzed across various benchmarks.</description></item><item><title>A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13841/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13841/</guid><description>Researchers reveal a unified framework for editing post-trained large language model parameters, improving efficiency and performance by mathematically analyzing the impact of different editing operat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13841/cover.png"/></item><item><title>BenTo: Benchmark Task Reduction with In-Context Transferability</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13804/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13804/</guid><description>BENTO efficiently reduces LLM benchmark size by 95% using in-context transferability, achieving 97% evaluation accuracy, saving computational costs without compromising quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13804/cover.png"/></item><item><title>Can MLLMs Understand the Deep Implication Behind Chinese Images?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13854/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13854/</guid><description>CII-Bench, a new benchmark, reveals that current MLLMs struggle to understand the deeper implications within Chinese images, particularly those related to traditional culture, showcasing a significant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13854/cover.png"/></item><item><title>Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13334/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13334/</guid><description>LLM safety mechanisms, while aiming to prevent harmful outputs, paradoxically introduce biases that enable &amp;lsquo;jailbreaks&amp;rsquo;; this research quantifies these biases and proposes a novel defense.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13334/cover.png"/></item><item><title>DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13830/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13830/</guid><description>DreamVideo-2 achieves zero-shot video customization with precise motion control by using a novel mask-guided motion module and masked reference attention, overcoming the limitations of previous method&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13830/cover.png"/></item><item><title>Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13198/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13198/</guid><description>DARAG boosts ASR accuracy by 8-33% using synthetic data and retrieval augmentation to improve Generative Error Correction, overcoming limitations of traditional GEC models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13198/cover.png"/></item><item><title>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13863/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13863/</guid><description>FLUID, a 10.5B parameter autoregressive model using continuous tokens and random order generation, achieves state-of-the-art text-to-image generation, demonstrating that careful model design can unloc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13863/cover.png"/></item><item><title>Harnessing Webpage UIs for Text-Rich Visual Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13824/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13824/</guid><description>MultiUI, a massive dataset of 7.3M multimodal instructions synthesized from web UIs, significantly boosts text-rich visual understanding model performance across diverse tasks, exceeding specialized m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13824/cover.png"/></item><item><title>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13848/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13848/</guid><description>Janus, a novel autoregressive framework, unifies multimodal understanding and generation by decoupling visual encoding, surpassing previous unified models and achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13848/cover.png"/></item><item><title>LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13618/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13618/</guid><description>LoLDU, a novel parameter-efficient fine-tuning method, drastically reduces trainable parameters in large language models using Lower-Diag-Upper decomposition, achieving comparable performance to full &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13618/cover.png"/></item><item><title>MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13754/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13754/</guid><description>MixEval-X: a new benchmark standardizes multi-modal AI evaluations using real-world data mixtures, improving consistency and reducing bias in model rankings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13754/cover.png"/></item><item><title>MobA: A Two-Level Agent System for Efficient Mobile Task Automation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13757/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13757/</guid><description>MobA, a novel two-level agent system, significantly improves mobile task automation efficiency by combining multimodal LLMs with a sophisticated task planning and reflection mechanism.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13757/cover.png"/></item><item><title>PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13785/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13785/</guid><description>PopAlign improves LLM alignment by diversifying contrasting patterns across prompt, model, and pipeline levels, resulting in more comprehensive and robust alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13785/cover.png"/></item><item><title>Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13360/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13360/</guid><description>RAP-MLLMs: Personalize AI assistants in real-time without retraining, using a retrieval-augmented framework and a new dataset for infinite visual concept understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13360/cover.png"/></item><item><title>Retrospective Learning from Interactions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13852/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13852/</guid><description>RESPECT: a novel method improves language models by learning from implicit user feedback in multi-turn interactions, boosting task completion rates without external annotation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13852/cover.png"/></item><item><title>Roadmap towards Superhuman Speech Understanding using Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13268/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13268/</guid><description>New roadmap &amp;amp; benchmark for superhuman speech understanding using LLMs, revealing key limitations in handling abstract acoustic knowledge and non-semantic information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13268/cover.png"/></item><item><title>SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13293/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13293/</guid><description>SBI-RAG enhances math word problem solving by integrating schema-based instruction with a large language model, improving reasoning clarity and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13293/cover.png"/></item><item><title>VidPanos: Generative Panoramic Videos from Casual Panning Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13832/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13832/</guid><description>VidPanos generates realistic panoramic videos from casual panning videos by cleverly using generative video models to fill in unseen parts of the scene, offering a significant step towards immersive v&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13832/cover.png"/></item><item><title>AERO: Softmax-Only LLMs for Efficient Private Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/</guid><description>AERO achieves 4.23x communication and 1.94x latency reduction in private AI inference by developing a Softmax-only LLM architecture with novel entropy regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/cover.png"/></item><item><title>JudgeBench: A Benchmark for Evaluating LLM-based Judges</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/</guid><description>JudgeBench: a new benchmark objectively evaluates LLM-based judges on complex tasks, revealing that even top models struggle, highlighting the need for more advanced AI judges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/cover.png"/></item><item><title>Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/</guid><description>Long-LRM: A groundbreaking 3D reconstruction model generating photorealistic, wide-coverage scenes from 32 images in 1.3 seconds using a novel hybrid architecture.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/cover.png"/></item><item><title>MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/</guid><description>MMed-RAG significantly boosts medical vision-language model factuality by using domain-aware retrieval, adaptive context selection, and RAG-based preference fine-tuning, achieving an average 43.8% imp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/cover.png"/></item><item><title>MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/</guid><description>MuVi: a novel framework generating synchronized music for videos, achieving superior semantic alignment and rhythmic synchronization through contrastive learning and flow-matching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/cover.png"/></item><item><title>Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/</guid><description>Meta FAIR released OMat24, a massive open dataset of inorganic materials with 110M+ DFT calculations and state-of-the-art Equiformer V2 models, accelerating AI-driven materials discovery.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/cover.png"/></item><item><title>TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/</guid><description>TransAgent empowers vision-language models by collaboratively distilling knowledge from diverse expert agents, achieving state-of-the-art performance on visual recognition tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/cover.png"/></item><item><title>WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/</guid><description>WORLDCUISINES: a massive multilingual VQA benchmark on global cuisines, reveals cultural knowledge gaps in current vision-language models and provides a valuable resource for advancing research in thi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/cover.png"/></item><item><title>MoH: Multi-Head Attention as Mixture-of-Head Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11842/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11842/</guid><description>MoH improves Transformer efficiency by dynamically routing attention heads, enhancing inference speed and reducing computational costs without accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11842/cover.png"/></item><item><title>Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10210/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10210/</guid><description>High-quality data, not sheer volume, is key to unlocking LLMs&amp;rsquo; potential for generating long, coherent outputs, as demonstrated by significant performance improvements with minimal tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10210/cover.png"/></item><item><title>FlatQuant: Flatness Matters for LLM Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09426/</link><pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09426/</guid><description>FLATQUANT achieves state-of-the-art LLM quantization, minimizing accuracy loss (&amp;lt;1%) and latency (up to 2.3x speedup) through fast, learnable affine transformations and efficient kernel fusion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09426/cover.png"/></item><item><title>Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09347/</link><pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09347/</guid><description>Researchers developed Condition Contrastive Alignment (CCA), a novel guidance-free method for high-quality autoregressive visual generation, significantly boosting performance while slashing sampling &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09347/cover.png"/></item><item><title>MedMobile: A mobile-sized language model with expert-level clinical capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09019/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09019/</guid><description>MedMobile: A mobile-ready 3.8B parameter language model achieves expert-level clinical performance, surpassing USMLE benchmarks with unprecedented efficiency and accessibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09019/cover.png"/></item></channel></rss>