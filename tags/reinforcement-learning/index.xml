<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/reinforcement-learning/</link><description>Recent content in Reinforcement Learning on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 03 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>ACECODER: Acing Coder RL via Automated Test-Case Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-05/2502.01718/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-05/2502.01718/</guid><description>AceCoder uses automated test-case synthesis to create a large-scale dataset for training reward models, enabling effective reinforcement learning to significantly boost code generation model performan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-05/2502.01718/cover.png"/></item><item><title>Improving Transformer World Models for Data-Efficient RL</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-04/2502.01591/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-04/2502.01591/</guid><description>AI agents now master complex tasks with improved Transformer World Models, achieving a new state-of-the-art in data-efficient reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-04/2502.01591/cover.png"/></item><item><title>SRMT: Shared Memory for Multi-agent Lifelong Pathfinding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13200/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13200/</guid><description>SRMT: Shared Recurrent Memory Transformer boosts multi-agent coordination by implicitly sharing information via a global memory, significantly outperforming baselines in complex pathfinding tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13200/cover.png"/></item></channel></rss>