<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/reinforcement-learning/</link><description>Recent content in Reinforcement Learning on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Wed, 26 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Self-rewarding correction for mathematical reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.19613/</link><pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.19613/</guid><description>LLM can now reason and correct itself using self-generated data, achieving performance on par with external reward models!</description></item><item><title>Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.16944/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.16944/</guid><description>DVPO: A lean RLHF framework that decouples value &amp;amp; policy optimization with global value guidance, cutting GPU use by 40% and training time by 35%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.16944/cover.png"/></item><item><title>TAG: A Decentralized Framework for Multi-Agent Hierarchical Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.15425/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.15425/</guid><description>TAG: A decentralized framework for scalable multi-agent hierarchical reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.15425/cover.png"/></item><item><title>Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14768/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14768/</guid><description>Logic-RL unlocks LLM reasoning via rule-based reinforcement learning, generalizing to math problems after training on logic puzzles.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14768/cover.png"/></item><item><title>MLGym: A New Framework and Benchmark for Advancing AI Research Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14499/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14499/</guid><description>MLGYM: A new framework &amp;amp; benchmark to advance AI Research Agents</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14499/cover.png"/></item><item><title>AdaptiveStep: Automatically Dividing Reasoning Step through Model Confidence</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13943/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13943/</guid><description>AdaptiveStep: Divides reasoning steps automatically through model confidence, enhancing PRM training &amp;amp; performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13943/cover.png"/></item><item><title>S$^2$R: Teaching LLMs to Self-verify and Self-correct via Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12853/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12853/</guid><description>S2R: Teaches LLMs to self-verify and self-correct, boosting reasoning with efficient reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12853/cover.png"/></item><item><title>Memory, Benchmark &amp; Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10550/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10550/</guid><description>MIKASA, a new benchmark for memory-intensive reinforcement learning, provides a unified framework for evaluating memory capabilities in diverse scenarios, including complex robotic manipulation tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10550/cover.png"/></item><item><title>Agency Is Frame-Dependent</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04403/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04403/</guid><description>Agency, a key concept in AI, is shown to be relative to the observer&amp;rsquo;s perspective (frame-dependent), challenging traditional binary definitions and necessitating a more nuanced approach for AI system&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04403/cover.png"/></item><item><title>ACECODER: Acing Coder RL via Automated Test-Case Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01718/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01718/</guid><description>AceCoder uses automated test-case synthesis to create a large-scale dataset for training reward models, enabling effective reinforcement learning to significantly boost code generation model performan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01718/cover.png"/></item><item><title>Improving Transformer World Models for Data-Efficient RL</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01591/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01591/</guid><description>AI agents now master complex tasks with improved Transformer World Models, achieving a new state-of-the-art in data-efficient reinforcement learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01591/cover.png"/></item><item><title>SRMT: Shared Memory for Multi-agent Lifelong Pathfinding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13200/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13200/</guid><description>SRMT: Shared Recurrent Memory Transformer boosts multi-agent coordination by implicitly sharing information via a global memory, significantly outperforming baselines in complex pathfinding tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13200/cover.png"/></item></channel></rss>