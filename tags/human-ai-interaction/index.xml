<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Human-AI Interaction on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/human-ai-interaction/</link><description>Recent content in Human-AI Interaction on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Wed, 05 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/human-ai-interaction/index.xml" rel="self" type="application/rss+xml"/><item><title>EgoLife: Towards Egocentric Life Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-07/2503.03803/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-07/2503.03803/</guid><description>EgoLife: Ultra-long egocentric dataset &amp;amp; benchmark enabling AI assistants to understand and enhance daily life. Datasets and models released!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-07/2503.03803/cover.png"/></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15027/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15027/</guid><description>InterFeedback: LMMs need better human feedback to enhance AI assistants!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15027/cover.png"/></item><item><title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12326/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12326/</guid><description>UI-TARS, a novel native GUI agent, achieves state-of-the-art performance by solely using screenshots as input, eliminating the need for complex agent frameworks and expert-designed workflows.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12326/cover.png"/></item><item><title>A3: Android Agent Arena for Mobile GUI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</guid><description>Android Agent Arena (A3): A novel evaluation platform for mobile GUI agents offering diverse tasks, flexible action space, and automated LLM-based evaluation, advancing real-world AI agent research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/cover.png"/></item><item><title>PC Agent: While You Sleep, AI Works -- A Cognitive Journey into Digital World</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17589/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17589/</guid><description>PC Agent: While you sleep, AI works! This AI system uses human cognition transfer to perform complex digital tasks, exceeding the capabilities of existing digital agents by efficiently learning from h&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17589/cover.png"/></item><item><title>GUI Agents: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13501/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13501/</guid><description>A comprehensive survey of GUI agents, categorizing benchmarks, architectures, training methods, and open challenges, providing a unified framework for researchers.</description></item><item><title>SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00174/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00174/</guid><description>SOLAMI: enabling immersive, natural interactions with 3D characters via a unified social vision-language-action model and a novel synthetic multimodal dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00174/cover.png"/></item><item><title>SketchAgent: Language-Driven Sequential Sketch Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17673/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17673/</guid><description>SketchAgent uses a multimodal LLM to generate dynamic, sequential sketches from textual prompts, enabling collaborative drawing and chat-based editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17673/cover.png"/></item><item><title>The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10323/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10323/</guid><description>Claude 3.5 Computer Use: A groundbreaking AI model offering public beta graphical user interface (GUI) agent for computer use is comprehensively analyzed in this research. This study provides an out-o&amp;hellip;</description></item><item><title>GazeGen: Gaze-Driven User Interaction for Visual Content Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/</guid><description>GazeGen uses real-time gaze tracking to enable intuitive hands-free visual content creation and editing, setting a new standard for accessible AR/VR interaction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04335/cover.png"/></item><item><title>AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/</guid><description>ANDROIDLAB, a novel framework, systematically benchmarks Android autonomous agents, improving LLM and LMM success rates on 138 tasks via a unified environment and open-source dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/cover.png"/></item><item><title>Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24032/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24032/</guid><description>Collaborative Assistant for Personalized Exploration (CARE) enhances LLM chatbots for exploratory tasks by combining a multi-agent framework with a structured interface, delivering tailored solutions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24032/cover.png"/></item><item><title>Survey of User Interface Design and Interaction Techniques in Generative AI Applications</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22370/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22370/</guid><description>This study provides a comprehensive taxonomy of user interface design and interaction techniques in generative AI, offering valuable insights for developers and researchers aiming to enhance user expe&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22370/cover.png"/></item></channel></rss>