<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ UC Berkeley on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-uc-berkeley/</link><description>Recent content in üè¢ UC Berkeley on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 25 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-uc-berkeley/index.xml" rel="self" type="application/rss+xml"/><item><title>Scaling Vision Pre-Training to 4K Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19903/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19903/</guid><description>PS3 scales CLIP vision pre-training to 4K resolution with near-constant cost, achieving state-of-the-art performance in multi-modal LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-26/2503.19903/cover.png"/></item><item><title>TULIP: Towards Unified Language-Image Pretraining</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15485/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15485/</guid><description>TULIP enhances image-text pretraining by unifying generative data augmentation with contrastive learning, achieving state-of-the-art performance in visual understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15485/cover.png"/></item><item><title>Why Do Multi-Agent LLM Systems Fail?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13657/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13657/</guid><description>Multi-Agent Systems (MAS) often underperform despite enthusiasm. This paper analyzes 5 popular frameworks across 150+ tasks, identifying 14 failure modes categorized into specification/design, inter-a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13657/cover.png"/></item><item><title>Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20396/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20396/</guid><description>Sim-to-real RL recipe achieves robust vision-based dexterous humanoid manipulation without human demos!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20396/cover.png"/></item><item><title>S*: Test Time Scaling for Code Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14382/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14382/</guid><description>S*: Hybrid test-time scaling for code generation, boosting both coverage and selection accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14382/cover.png"/></item><item><title>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/</guid><description>Autellix: Efficient LLM Serving for Agents</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13965/cover.png"/></item><item><title>Pre-training Auto-regressive Robotic Models with 4D Representations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/</guid><description>ARM4R pre-trains autoregressive robotic models using low-level 4D representations from human videos, achieving efficient transfer learning and improved task performance across various environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/cover.png"/></item><item><title>LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07374/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07374/</guid><description>LLMs can be effectively taught complex reasoning via efficient fine-tuning on demonstration data focusing on &lt;em>structure&lt;/em>, not content, of the reasoning process.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07374/cover.png"/></item><item><title>Lifelong Sequential Knowledge Editing without Model Degradation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01636/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01636/</guid><description>ENCORE enables lifelong sequential knowledge editing in LLMs without performance loss, achieving 10,000 edits while maintaining downstream accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01636/cover.png"/></item><item><title>SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17161/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17161/</guid><description>Reinforcement learning (RL) surpasses supervised fine-tuning (SFT) in fostering generalization in foundation models, while SFT aids RL&amp;rsquo;s stability; a comparative study across text and visual domains r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17161/cover.png"/></item><item><title>FAST: Efficient Action Tokenization for Vision-Language-Action Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/</guid><description>FAST: A novel action tokenization method using discrete cosine transform drastically improves autoregressive vision-language-action models&amp;rsquo; training and performance, enabling dexterous and high-freque&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/cover.png"/></item><item><title>An Empirical Study of Autoregressive Pre-training from Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/</guid><description>Toto, a new autoregressive video model, achieves competitive performance across various benchmarks by pre-training on over 1 trillion visual tokens, demonstrating the effectiveness of scaling video mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/cover.png"/></item><item><title>Training Software Engineering Agents and Verifiers with SWE-Gym</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</guid><description>SWE-Gym, a novel environment for training real-world software engineering agents using 2,438 real-world Python task instances, achieves new state-of-the-art performance and is publicly available.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/cover.png"/></item><item><title>Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</guid><description>RAPL efficiently aligns robots with human preferences using minimal feedback by aligning visual representations before reward learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/cover.png"/></item><item><title>Predicting Emergent Capabilities by Finetuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</guid><description>Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/cover.png"/></item></channel></rss>