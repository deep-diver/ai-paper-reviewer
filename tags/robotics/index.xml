<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Robotics on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/robotics/</link><description>Recent content in Robotics on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Fri, 14 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/robotics/index.xml" rel="self" type="application/rss+xml"/><item><title>Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11646/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11646/</guid><description>ADC: Human-robot collaboration revolutionizes data collection, slashing data needs and boosting robot learning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11646/cover.png"/></item><item><title>API Agents vs. GUI Agents: Divergence and Convergence</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11069/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11069/</guid><description>API vs. GUI Agents: Understanding the divergence and convergence in LLM-based automation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11069/cover.png"/></item><item><title>KUDA: Keypoints to Unify Dynamics Learning and Visual Prompting for Open-Vocabulary Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.10546/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.10546/</guid><description>KUDA unifies dynamics learning and visual prompting with keypoints for open-vocabulary robot manipulation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-19/2503.10546/cover.png"/></item><item><title>BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05652/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05652/</guid><description>BRS: Streamlining real-world whole-body manipulation for household activities. It introduces a robot suite tackling robot dexterity with bimanual coordination, navigation, and end-effector reach.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05652/cover.png"/></item><item><title>CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01378/</link><pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01378/</guid><description>CognitiveDrone: A novel VLA model and benchmark for real-time cognitive UAV tasks, improving reasoning and control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01378/cover.png"/></item><item><title>Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20396/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20396/</guid><description>Sim-to-real RL recipe achieves robust vision-based dexterous humanoid manipulation without human demos!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20396/cover.png"/></item><item><title>Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16707/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16707/</guid><description>Reflect VLM: Improving robotic manipulation via vision-language models with a novel reflection mechanism and a diffusion model for imagined futures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16707/cover.png"/></item><item><title>Pre-training Auto-regressive Robotic Models with 4D Representations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/</guid><description>ARM4R pre-trains autoregressive robotic models using low-level 4D representations from human videos, achieving efficient transfer learning and improved task performance across various environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/cover.png"/></item><item><title>Learning Getting-Up Policies for Real-World Humanoid Robots</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12152/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12152/</guid><description>HUMANUP: A novel two-stage reinforcement learning framework enables real-world humanoid robots to autonomously recover from falls on various terrains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12152/cover.png"/></item><item><title>DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09614/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09614/</guid><description>DexTrack achieves highly generalizable neural tracking control for dexterous robot manipulation by iteratively training a controller using high-quality demonstrations refined via homotopy optimization&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09614/cover.png"/></item><item><title>Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/</guid><description>HMA: a novel approach for generating high-quality robotic videos 15x faster, enabling real-time policy evaluation and data augmentation for scaling robot learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/cover.png"/></item><item><title>FAST: Efficient Action Tokenization for Vision-Language-Action Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/</guid><description>FAST: A novel action tokenization method using discrete cosine transform drastically improves autoregressive vision-language-action models&amp;rsquo; training and performance, enabling dexterous and high-freque&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/cover.png"/></item><item><title>EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/</guid><description>EnerVerse: A novel framework seamlessly integrates convolutional and attention mechanisms to generate embodied future spaces for enhanced robotic manipulation, mitigating data scarcity with a generati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/cover.png"/></item><item><title>Training Software Engineering Agents and Verifiers with SWE-Gym</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</guid><description>SWE-Gym, a novel environment for training real-world software engineering agents using 2,438 real-world Python task instances, achieves new state-of-the-art performance and is publicly available.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/cover.png"/></item><item><title>Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/</guid><description>MoDE makes AI for robot control faster and more efficient.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/cover.png"/></item><item><title>Large Action Models: From Inception to Implementation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</guid><description>From language models to action models: building AI that &lt;em>does&lt;/em> things.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/cover.png"/></item><item><title>TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/</guid><description>TidyBot++: Low-cost, open-source holonomic mobile base makes robot learning easier.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/cover.png"/></item><item><title>CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06782/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06782/</guid><description>CARP: A novel visuomotor policy learning paradigm achieves high accuracy and 10x faster inference than state-of-the-art by combining autoregressive efficiency and diffusion model precision through a c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06782/cover.png"/></item><item><title>Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/</guid><description>RAPL efficiently aligns robots with human preferences using minimal feedback by aligning visual representations before reward learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04835/cover.png"/></item><item><title>Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04455/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04455/</guid><description>Code-as-Monitor (CaM) uses vision-language models and constraint-aware visual programming to achieve both reactive and proactive robotic failure detection in real-time, improving success rates and red&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04455/cover.png"/></item><item><title>Moto: Latent Motion Token as the Bridging Language for Robot Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04445/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04445/</guid><description>Moto: Bridging language for robot manipulation using latent motion tokens, achieving superior performance with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04445/cover.png"/></item><item><title>WildLMa: Long Horizon Loco-Manipulation in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15131/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15131/</guid><description>WildLMa enables robots to perform complex, long-horizon manipulation tasks in unstructured environments by combining language-conditioned imitation learning, a whole-body controller for efficient tele&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15131/cover.png"/></item><item><title>Soft Robotic Dynamic In-Hand Pen Spinning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/</guid><description>SWIFT, a new system, enables a soft robotic hand to learn dynamic pen spinning via real-world trial-and-error, achieving 100% success across diverse pen properties without explicit object modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/cover.png"/></item><item><title>DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04999/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04999/</guid><description>DynaMem empowers robots with online dynamic spatio-semantic memory, achieving a 2x improvement in pick-and-drop success rate on non-stationary objects compared to static systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04999/cover.png"/></item><item><title>DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/</guid><description>DeeR-VLA dynamically adjusts the size of a multimodal large language model based on task difficulty, significantly reducing computational cost and memory usage in robotic control without compromising &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/cover.png"/></item></channel></rss>