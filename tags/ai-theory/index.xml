<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Theory on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/ai-theory/</link><description>Recent content in AI Theory on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 24 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/ai-theory/index.xml" rel="self" type="application/rss+xml"/><item><title>I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18878/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18878/</guid><description>LLMs&amp;rsquo; reasoning is decoded via sparse autoencoders, revealing key features that, when steered, enhance performance. First mechanistic account of reasoning in LLMs!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18878/cover.png"/></item><item><title>Measuring AI Ability to Complete Long Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14499/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14499/</guid><description>AI progress is tracked with a new metric, 50%-task-completion time horizon, showing exponential growth with a doubling time of ~7 months, hinting at significant automation potential in the near future&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14499/cover.png"/></item><item><title>Why Do Multi-Agent LLM Systems Fail?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.13657/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.13657/</guid><description>Multi-Agent Systems (MAS) often underperform despite enthusiasm. This paper analyzes 5 popular frameworks across 150+ tasks, identifying 14 failure modes categorized into specification/design, inter-a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.13657/cover.png"/></item><item><title>Implicit Bias-Like Patterns in Reasoning Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.11572/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.11572/</guid><description>AI reasoning models reveal bias-like patterns, processing association-incompatible info with more computational effort, mirroring human implicit biases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.11572/cover.png"/></item><item><title>Group-robust Machine Unlearning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09330/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09330/</guid><description>Group-robust machine unlearning via MIU reduces perf. degradation in dominant groups after unlearning, preserving model robustness without compromising accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.09330/cover.png"/></item><item><title>Mixture of Experts Made Intrinsically Interpretable</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07639/</link><pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07639/</guid><description>MoE-X: An intrinsically interpretable Mixture-of-Experts language model that uses sparse, wide networks to enhance transparency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.07639/cover.png"/></item><item><title>Beyond Release: Access Considerations for Generative AI Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16701/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16701/</guid><description>AI system access is more than just release; it&amp;rsquo;s about how accessible system components are, impacting benefits, risks, and scalability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16701/cover.png"/></item><item><title>CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16614/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16614/</guid><description>CodeCriticBench: A new benchmark for holistic code critique by Large Language Models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16614/cover.png"/></item><item><title>Forecasting Open-Weight AI Model Growth on Hugging Face</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15987/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15987/</guid><description>Predicting open-weight AI model growth on Hugging Face using a citation-style model, revealing adoption dynamics and influencing factors.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15987/cover.png"/></item><item><title>Discovering highly efficient low-weight quantum error-correcting codes with reinforcement learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14372/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14372/</guid><description>RL optimizes quantum error-correcting codes, slashing physical qubit overhead for fault-tolerant quantum computing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14372/cover.png"/></item><item><title>Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15086/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15086/</guid><description>LLMs fail to act safely when considering user-specific safety standards, which were made to be solved via new benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15086/cover.png"/></item><item><title>LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15007/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15007/</guid><description>LLMs use punctuation in context memory, surprisingly boosting performance by using seemingly trivial tokens.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.15007/cover.png"/></item><item><title>Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13946/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13946/</guid><description>Aligned LLMs&amp;rsquo; safety often anchors in the template region, creating vulnerabilities. Detaching safety mechanisms shows promise in mitigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13946/cover.png"/></item><item><title>The snake in the Brownian sphere</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13074/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13074/</guid><description>Unveiling the Brownian snake within the Brownian sphere! This research constructs the inverse of the CVS bijection, mapping the sphere back to its underlying snake.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13074/cover.png"/></item><item><title>Presumed Cultural Identity: How Names Shape LLM Responses</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11995/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11995/</guid><description>LLMs personalize based on user names, but this study reveals that cultural presumptions in LLM responses risk reinforcing stereotypes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11995/cover.png"/></item><item><title>o3-mini vs DeepSeek-R1: Which One is Safer?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18438/</link><pubDate>Thu, 30 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18438/</guid><description>ASTRAL, a novel automated safety testing tool, reveals DeepSeek-R1&amp;rsquo;s significantly higher unsafe response rate compared to OpenAI&amp;rsquo;s o3-mini, highlighting critical safety concerns in advanced LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18438/cover.png"/></item><item><title>Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17749/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.17749/</guid><description>Researchers used ASTRAL to systematically test OpenAI&amp;rsquo;s 03-mini LLM&amp;rsquo;s safety, revealing key vulnerabilities and highlighting the need for continuous, robust safety mechanisms in large language models.</description></item><item><title>Evolution and The Knightian Blindspot of Machine Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13075/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13075/</guid><description>Machine learning overlooks robustness to an unknowable future; this paper contrasts reinforcement learning with biological evolution, revealing that ML&amp;rsquo;s formalisms limit engagement with unknown unkno&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13075/cover.png"/></item><item><title>Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08970/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08970/</guid><description>Machine learning models can enable secure computations previously impossible with cryptography, achieving privacy and efficiency in Trusted Capable Model Environments (TCMEs).</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08970/cover.png"/></item><item><title>Game-theoretic LLM: Agent Workflow for Negotiation Games</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05990/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05990/</guid><description>Game-theoretic LLMs: Agent Workflow for Negotiation Games enhances large language model (LLM) rationality in strategic decision-making through novel game-theoretic workflows.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05990/cover.png"/></item><item><title>Minimum Entropy Coupling with Bottleneck</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21666/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21666/</guid><description>A new lossy compression framework handles reconstruction distribution divergence by integrating a bottleneck, extending minimum entropy coupling and offering guaranteed performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21666/cover.png"/></item></channel></rss>