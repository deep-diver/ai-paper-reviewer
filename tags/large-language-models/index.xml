<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Large Language Models on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/large-language-models/</link><description>Recent content in Large Language Models on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 10 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Contextualized Counterspeech: Strategies for Adaptation, Personalization, and Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/</guid><description>Contextualized AI counterspeech significantly outperforms generic methods by adapting to the moderation context and user, improving persuasiveness without sacrificing other qualities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07338/cover.png"/></item><item><title>Granite Guardian</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/</guid><description>Granite Guardian: Open-source risk detection models for LLMs, surpassing existing models in accuracy and offering comprehensive coverage across multiple risk dimensions, promoting safer AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07724/cover.png"/></item><item><title>Training Large Language Models to Reason in a Continuous Latent Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</guid><description>LLMs are trained to reason using language, but COCONUT lets them reason directly in a continuous latent space, boosting performance on logical tasks requiring complex planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/cover.png"/></item><item><title>Fully Open Source Moxin-7B Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</guid><description>Moxin-LLM: A fully open-source 7B parameter LLM achieving superior zero-shot performance, promoting transparency and reproducibility in AI research.</description></item><item><title>Evaluating and Aligning CodeLLMs on Human Preference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/</guid><description>CodeArena, a novel benchmark, evaluates code LLMs based on human preferences, revealing performance gaps between open-source and proprietary models, and a large-scale synthetic instruction corpus impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05210/cover.png"/></item><item><title>EXAONE 3.5: Series of Large Language Models for Real-world Use Cases</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/</guid><description>LG AI Research unveils EXAONE 3.5, a series of instruction-tuned language models (2.4B, 7.8B, and 32B parameters) excelling in real-world tasks, long-context understanding, and general benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04862/cover.png"/></item><item><title>Densing Law of LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</guid><description>LLMs&amp;rsquo; training quality is exponentially improving, enabling models with half the parameters to match state-of-the-art performance every 3 months, thus reducing inference costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/cover.png"/></item><item><title>Marco-LLM: Bridging Languages via Massive Multilingual Training for Cross-Lingual Enhancement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/</guid><description>Marco-LLM: A groundbreaking multilingual LLM significantly enhances cross-lingual capabilities via massive multilingual training, bridging the performance gap between high- and low-resource languages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04003/cover.png"/></item><item><title>Monet: Mixture of Monosemantic Experts for Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/</guid><description>MONET improves Transformer interpretability by using Mixture-of-Experts (MoE) with 262K monosemantic experts per layer, achieving parameter efficiency and enabling knowledge manipulation without perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04139/cover.png"/></item><item><title>Evaluating Language Models as Synthetic Data Generators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/</guid><description>AGORABENCH: A new benchmark reveals surprising strengths &amp;amp; weaknesses of LMs as synthetic data generators, showing that problem-solving ability isn&amp;rsquo;t the sole indicator of data quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03679/cover.png"/></item><item><title>Robust Multi-bit Text Watermark with LLM-based Paraphrasers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/</guid><description>Researchers developed a robust multi-bit text watermarking method using LLMs for paraphrasing, achieving over 99.99% detection accuracy while maintaining semantic information and resisting common atta&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03123/cover.png"/></item><item><title>Weighted-Reward Preference Optimization for Implicit Model Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/</guid><description>WRPO: Implicitly fuse LLMs, boosting performance without complex alignment or merging!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03187/cover.png"/></item><item><title>OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/</guid><description>Imperfect OCR hinders Retrieval-Augmented Generation (RAG). OHRBench, a new benchmark, reveals this cascading impact, showing current OCR solutions insufficient for high-quality RAG knowledge bases. &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02592/cover.png"/></item><item><title>Free Process Rewards without Process Labels</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</guid><description>Train high-performing Process Reward Models (PRMs) cheaply using only outcome-level labels, eliminating the need for costly step-by-step annotations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/cover.png"/></item><item><title>A dynamic parallel method for performance optimization on hybrid CPUs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/</guid><description>Dynamic parallel processing boosts LLM inference speed on hybrid CPUs by over 90% memory bandwidth, resolving performance bottlenecks caused by imbalanced hardware capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19542/cover.png"/></item><item><title>A Simple and Provable Scaling Law for the Test-Time Compute of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/</guid><description>Boost LLM accuracy exponentially by using a two-stage algorithm with provable scaling laws: generate multiple candidate solutions then compare them in a knockout tournament!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19477/cover.png"/></item><item><title>Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/</guid><description>Boosting LLMs&amp;rsquo; reasoning: A novel token-level contrastive estimation method automatically identifies and penalizes critical tokens leading to errors, significantly enhancing reasoning accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19943/cover.png"/></item><item><title>INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/</guid><description>New multilingual LLM benchmark, INCLUDE, tackles regional knowledge gaps by using 197K QA pairs from 44 languages, improving cross-lingual evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19799/cover.png"/></item><item><title>KV Shifting Attention Enhances Language Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/</guid><description>KV Shifting Attention: A novel attention mechanism significantly enhances language modeling by simplifying induction heads, leading to improved performance and faster convergence, even in large-scale &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19574/cover.png"/></item><item><title>o1-Coder: an o1 Replication for Coding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/</guid><description>O1-CODER replicates OpenAI&amp;rsquo;s o1 model for coding, integrating reinforcement learning and Monte Carlo Tree Search to enhance System-2 thinking and generate high-quality code with reasoning steps.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00154/cover.png"/></item><item><title>Puzzle: Distillation-Based NAS for Inference-Optimized LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</guid><description>Puzzle: a novel framework accelerates large language model inference by using neural architecture search and knowledge distillation, achieving a 2.17x speedup on a single GPU while preserving 98.4% ac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/cover.png"/></item><item><title>Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</guid><description>HiAR-ICL, a novel automated reasoning paradigm using Monte Carlo Tree Search, surpasses state-of-the-art accuracy in complex mathematical reasoning by shifting focus from specific examples to abstract&amp;hellip;</description></item><item><title>Draft Model Knows When to Stop: A Self-Verification Length Policy for Speculative Decoding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/</guid><description>Self-VerIfication length Policy (SVIP) dynamically adjusts speculative decoding draft lengths based on token difficulty, achieving up to 20% faster large language model inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18462/cover.png"/></item><item><title>Training and Evaluating Language Models with Template-based Data Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</guid><description>Researchers created TemplateGSM, a massive dataset of 7M+ grade-school math problems and solutions, using GPT-4 to generate templates, significantly advancing LLM training for mathematical reasoning.</description></item><item><title>Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/</guid><description>Low-bit quantization excels for undertrained LLMs but struggles with fully-trained ones; new scaling laws reveal this, directing future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17691/cover.png"/></item><item><title>Star Attention: Efficient LLM Inference over Long Sequences</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</guid><description>Star Attention: 11x faster LLM inference on long sequences with 95-100% accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/cover.png"/></item><item><title>From CISC to RISC: language-model guided assembly transpilation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/</guid><description>A novel LLM-based transpiler, CRT, efficiently converts x86 assembly to ARM and RISC-V assembly, achieving high accuracy and significant performance improvements over existing virtualization methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16341/cover.png"/></item><item><title>From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/</guid><description>LLMs are revolutionizing AI evaluation by offering nuanced judgments surpassing traditional methods. This paper provides a taxonomy, benchmark, and future directions for LLM-as-a-judge.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16594/cover.png"/></item><item><title>MH-MoE:Multi-Head Mixture-of-Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16205/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16205/</guid><description>MH-MoE: A novel implementation of Multi-Head Mixture-of-Experts achieves superior performance in large language models by enhancing efficiency without sacrificing model size or computational cost.</description></item><item><title>O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/</guid><description>Simple distillation from OpenAI&amp;rsquo;s API, combined with fine-tuning, surprisingly surpasses OpenAI&amp;rsquo;s O1-preview on complex mathematical reasoning, urging transparency in AI research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16489/cover.png"/></item><item><title>Predicting Emergent Capabilities by Finetuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/</guid><description>Predicting emergent LLM capabilities is now possible by finetuning smaller models; this approach shifts the emergence point, enabling accurate predictions of future model performance, even with up to &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16035/cover.png"/></item><item><title>MolReFlect: Towards In-Context Fine-grained Alignments between Molecules and Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/</guid><description>MolReFlect achieves state-of-the-art molecule-text alignment by using a teacher-student LLM framework that generates fine-grained alignments, improving accuracy and explainability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14721/cover.png"/></item><item><title>One to rule them all: natural language to bind communication, perception and action</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/</guid><description>AI-powered robots now understand and execute complex natural language commands, adapting seamlessly to dynamic environments thanks to a new architecture integrating LLMs, perception, and planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15033/cover.png"/></item><item><title>Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/</guid><description>LLMs&amp;rsquo; hallucinations stem from entity recognition: SAEs reveal model &amp;lsquo;self-knowledge&amp;rsquo;, causally affecting whether it hallucinates or refuses to answer. This mechanism is even repurposed by chat finet&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14257/cover.png"/></item><item><title>Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/</guid><description>Marco-01: a novel large reasoning model surpasses existing LLMs by using Chain-of-Thought, Monte Carlo Tree Search, and reflection mechanisms to excel in open-ended problem-solving, particularly in co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14405/cover.png"/></item><item><title>UnifiedCrawl: Aggregated Common Crawl for Affordable Adaptation of LLMs on Low-Resource Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/</guid><description>UnifiedCrawl efficiently harvests massive monolingual datasets for low-resource languages from Common Crawl, enabling affordable LLM adaptation via QLoRA, significantly improving performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14343/cover.png"/></item><item><title>A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/</guid><description>New data-free methodology creates effective, generalizable LLMs guardrails against off-topic prompts, significantly improving LLM safety and responsible use.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12946/cover.png"/></item><item><title>BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/</guid><description>BALROG benchmark rigorously evaluates LLMs&amp;rsquo;/VLMs&amp;rsquo; abilities in complex games, revealing their strengths and weaknesses in long-term planning and decision-making, highlighting the need for improved vis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13543/cover.png"/></item><item><title>Hymba: A Hybrid-head Architecture for Small Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</guid><description>Hymba: Hybrid-head architecture boosts small language model performance by 11.67x cache size reduction and 3.49x throughput, surpassing existing models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/cover.png"/></item><item><title>Patience Is The Key to Large Language Model Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</guid><description>Boosting Large Language Model (LLM) reasoning without massive datasets: A novel training method encourages &amp;lsquo;patient&amp;rsquo; reasoning, improving accuracy by up to 6.7% on benchmark tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/cover.png"/></item><item><title>When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/</guid><description>AnchorAttention enhances long-context LLMs by mitigating BFloat16&amp;rsquo;s disruptive effects on RoPE, improving performance and speeding up training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13476/cover.png"/></item><item><title>Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/</guid><description>SUTRA tokenizer outperforms other LLMs in Indian languages, improving efficiency and facilitating better model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12240/cover.png"/></item><item><title>RedPajama: an Open Dataset for Training Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/</guid><description>RedPajama, two massive open-source datasets, are released for training LLMs, improving transparency and facilitating the development of high-performing open-source models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12372/cover.png"/></item><item><title>Ultra-Sparse Memory Network</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/</guid><description>UltraMem, a novel ultra-sparse memory network, drastically speeds up LLM inference by 6x compared to MoE while maintaining performance, paving the way for efficient large-scale model deployment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12364/cover.png"/></item><item><title>Search, Verify and Feedback: Towards Next Generation Post-training Paradigm of Foundation Models via Verifier Engineering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/</guid><description>Verifier engineering: A new post-training paradigm for foundation models using automated verifiers to provide effective supervision signals, enhancing capabilities beyond traditional data-centric meth&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11504/cover.png"/></item><item><title>LLÃ¤Mmlein: Compact and Competitive German-Only Language Models from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/</guid><description>New German-only LLMs, LLÃ¤Mmlein 120M &amp;amp; 1B, trained from scratch &amp;amp; openly released, show competitive performance and offer insights into efficient model training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11171/cover.png"/></item><item><title>SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</guid><description>SageAttention2 achieves 4-bit accurate attention, boosting inference speed by 2x compared to FlashAttention2, while maintaining end-to-end accuracy across diverse models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/cover.png"/></item><item><title>SlimLM: An Efficient Small Language Model for On-Device Document Assistance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/</guid><description>SlimLM: Efficient small language models (SLMs) optimized for mobile document assistance, achieving comparable or superior performance to existing SLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09944/cover.png"/></item><item><title>Adaptive Decoding via Latent Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</guid><description>LLMs can dynamically adjust decoding temperature using Adaptive Decoding and Latent Preference Optimization, improving performance across creative and factual tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/cover.png"/></item><item><title>LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</guid><description>LLaMA-Mesh: Unifying 3D mesh generation with LLMs by directly representing meshes as text, enabling efficient text-to-3D conversion within a single model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/cover.png"/></item><item><title>CamemBERT 2.0: A Smarter French Language Model Aged to Perfection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08868/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08868/</guid><description>CamemBERT 2.0: Two new French language models (CamemBERTav2 &amp;amp; CamemBERTv2) outperform predecessors by addressing temporal concept drift via larger, updated datasets and enhanced tokenization, demonstr&amp;hellip;</description></item><item><title>Can sparse autoencoders be used to decompose and interpret steering vectors?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/</guid><description>Sparse autoencoders fail to accurately decompose and interpret steering vectors due to distribution mismatch and the inability to handle negative feature projections; this paper identifies these issue&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08790/cover.png"/></item><item><title>Cut Your Losses in Large-Vocabulary Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/</guid><description>Cut Cross-Entropy (CCE) dramatically reduces the memory footprint of training large language models by cleverly computing the cross-entropy loss without materializing the full logit matrix.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09009/cover.png"/></item><item><title>Direct Preference Optimization Using Sparse Feature-Level Constraints</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/</guid><description>Feature-level constrained Preference Optimization (FPO) boosts LLM alignment efficiency and stability by using sparse autoencoders and feature-level constraints, achieving significant improvements ove&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07618/cover.png"/></item><item><title>Large Language Models Can Self-Improve in Long-context Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/</guid><description>LLMs can now self-improve long-context reasoning via SEALONG, a novel method leveraging multiple model outputs and minimum Bayes risk scoring to enable effective supervised fine-tuning or preference o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08147/cover.png"/></item><item><title>Top-$nÏ$: Not All Logits Are You Need</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/</guid><description>Top-Î·Ï: A novel LLM sampling method outperforms existing approaches by using a statistical threshold on pre-softmax logits, achieving higher accuracy while maintaining diversity, even at high temperat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07641/cover.png"/></item><item><title>Chinese SimpleQA: A Chinese Factuality Evaluation for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/</guid><description>Chinese SimpleQA, a new benchmark, offers a comprehensive evaluation of the factuality of LLMs answering short questions in Chinese, exhibiting diversity, high quality, and ease of evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07140/cover.png"/></item><item><title>Stronger Models are NOT Stronger Teachers for Instruction Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/</link><pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/</guid><description>Larger language models aren&amp;rsquo;t always better teachers for instruction tuning; a new metric, CAR, predicts teacher model effectiveness better than existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07133/cover.png"/></item><item><title>Ablation is Not Enough to Emulate DPO: How Neuron Dynamics Drive Toxicity Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/</guid><description>Contrary to common belief, toxicity reduction in language models isn&amp;rsquo;t simply achieved by dampening toxic neurons; it&amp;rsquo;s a complex balancing act across multiple neuron groups.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06424/cover.png"/></item><item><title>Is Your LLM Secretly a World Model of the Internet? Model-Based Planning for Web Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/</guid><description>WEB-DREAMER uses LLMs as world models for safe and efficient web agent planning, achieving substantial performance gains over reactive baselines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06559/cover.png"/></item><item><title>Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</guid><description>Golden Touchstone, a new bilingual benchmark, comprehensively evaluates financial LLMs across eight tasks, revealing model strengths and weaknesses and advancing FinLLM research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/cover.png"/></item><item><title>IOPO: Empowering LLMs with Complex Instruction Following via Input-Output Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/</guid><description>IOPO empowers LLMs to master complex instructions via input-output preference optimization, boasting significant performance gains on a new benchmark, TRACE.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06208/cover.png"/></item><item><title>Balancing Pipeline Parallelism with Vocabulary Parallelism</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/</link><pubDate>Fri, 08 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/</guid><description>Boost large language model training speed by 51% with Vocabulary Parallelism, a novel technique that balances computation and memory usage across pipeline stages.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05288/cover.png"/></item><item><title>BitNet a4.8: 4-bit Activations for 1-bit LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/</guid><description>BitNet a4.8 achieves comparable performance to existing 1-bit LLMs, but with significantly faster inference, by using a hybrid quantization and sparsification strategy for 4-bit activations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04965/cover.png"/></item><item><title>DELIFT: Data Efficient Language model Instruction Fine Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04425/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04425/</guid><description>DELIFT: Data Efficient Language Model Instruction Fine-Tuning, drastically reduces the data needed for effective LLM fine-tuning without sacrificing performance.</description></item><item><title>Hardware and Software Platform Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/</guid><description>Researchers developed Hardware and Software Platform Inference (HSPI) to identify the underlying GPU and software stack used to serve LLMs, enhancing transparency in the industry.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05197/cover.png"/></item><item><title>Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/</guid><description>Can LLMs effectively handle information spread across vast, almost million-scale datasets? This research investigates this question by evaluating 17 LLMs on novel âneedle threadingâ tasks. These task&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05000/cover.png"/></item><item><title>OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/</guid><description>OpenCoder, a top-tier open-source code LLM, is introduced, providing not only model weights and code but also reproducible training data, data processing pipelines, and training protocols, enabling co&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04905/cover.png"/></item><item><title>DynaSaur: Large Language Agents Beyond Predefined Actions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/</guid><description>DynaSaur: a novel LLM agent framework enabling dynamic action creation, surpassing prior methods with greater flexibility and top performance on the GAIA benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01747/cover.png"/></item><item><title>Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/</guid><description>Tencent unveils Hunyuan-Large, a groundbreaking open-source MoE LLM boasting 389B parameters and 52B activated parameters, surpassing existing models in performance across various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02265/cover.png"/></item><item><title>Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/</guid><description>Boosting unit test generation efficiency, this study empirically evaluates various parameter-efficient fine-tuning methods on LLMs, demonstrating comparable performance to full fine-tuning at signific&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02462/cover.png"/></item><item><title>Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</guid><description>Researchers discovered predictable scaling laws for activation sparsity in LLMs, showing how data, architecture, and model size influence sparsity, paving the way for more efficient and interpretable &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/cover.png"/></item><item><title>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</guid><description>WEBRL: A self-evolving online curriculum reinforcement learning framework empowers open LLMs to excel as high-performing web agents, surpassing proprietary models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/cover.png"/></item><item><title>Zebra-Llama: A Context-Aware Large Language Model for Democratizing Rare Disease Knowledge</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/</guid><description>Zebra-Llama, a context-aware LLM, democratizes rare disease knowledge by providing highly precise, context-rich information about Ehlers-Danlos Syndrome, significantly improving diagnostic support.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02657/cover.png"/></item><item><title>Sample-Efficient Alignment for LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/</link><pubDate>Sun, 03 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/</guid><description>Sample-efficient LLM alignment achieved via a novel Thompson sampling algorithm (SEA), outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01493/cover.png"/></item><item><title>Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/</link><pubDate>Sat, 02 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/</guid><description>Swan &amp;amp; ArabicMTEB: New dialect-aware Arabic embedding models and benchmark achieve state-of-the-art performance, addressing limitations of existing multilingual models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.01192/cover.png"/></item><item><title>Decoding Dark Matter: Specialized Sparse Autoencoders for Interpreting Rare Concepts in Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/</guid><description>Specialized Sparse Autoencoders (SSAEs) decode foundation models&amp;rsquo; &amp;lsquo;dark matter&amp;rsquo; features, efficiently extracting rare subdomain concepts for improved interpretability and safety.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00743/cover.png"/></item><item><title>LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/</link><pubDate>Fri, 01 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/</guid><description>LibMoE: A new library streamlines MoE research by offering standardized training, evaluation, and a modular design, enabling efficient benchmarking of various MoE algorithms for LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00918/cover.png"/></item><item><title>BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/</guid><description>BitStack: Dynamic LLM sizing for variable memory!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23918/cover.png"/></item><item><title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</guid><description>Constraint Back-translation enhances complex instruction following in LLMs by leveraging inherent constraints in existing datasets for efficient high-quality data creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/cover.png"/></item><item><title>GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23825/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23825/</guid><description>GlotCC: Open multilingual corpus &amp;amp; pipeline for minority languages, exceeding 1000 languages.</description></item><item><title>LLaMo: Large Language Model-based Molecular Graph Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/</guid><description>LLaMo: a novel large molecular graph-language model seamlessly integrates molecular graph encoders and LLMs, achieving state-of-the-art performance in molecule description generation, property predict&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.00871/cover.png"/></item><item><title>Controlling Language and Diffusion Models by Transporting Activations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/</guid><description>Steering large language and diffusion models is made easy and efficient via Activation Transport (ACT)! This novel framework uses optimal transport theory to precisely control model activations, leadi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23054/cover.png"/></item><item><title>AAAR-1.0: Assessing AI's Potential to Assist Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/</guid><description>AAAR-1.0 benchmark rigorously evaluates LLMs&amp;rsquo; ability to assist in four core research tasks, revealing both potential and limitations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22394/cover.png"/></item><item><title>M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/</guid><description>M2RC-EVAL: A new massively multilingual benchmark for repository-level code completion, featuring fine-grained annotations and a large instruction dataset, enabling better evaluation of code LLMs acro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21157/cover.png"/></item><item><title>NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/</guid><description>NeuZip dynamically compresses neural network weights, achieving memory-efficient training and inference without performance loss, significantly reducing the memory footprint of large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20650/cover.png"/></item></channel></rss>