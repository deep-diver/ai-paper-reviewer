<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ NVIDIA on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-nvidia/</link><description>Recent content in üè¢ NVIDIA on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 02 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-nvidia/index.xml" rel="self" type="application/rss+xml"/><item><title>VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01822/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01822/</guid><description>VLSI: Verbalized Layers-to-Interactions efficiently transfers knowledge from large to small VLMs using layer-wise natural language distillation, achieving significant performance gains without scaling&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01822/cover.png"/></item><item><title>Puzzle: Distillation-Based NAS for Inference-Optimized LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/</guid><description>Puzzle: a novel framework accelerates large language model inference by using neural architecture search and knowledge distillation, achieving a 2.17x speedup on a single GPU while preserving 98.4% ac&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19146/cover.png"/></item><item><title>Star Attention: Efficient LLM Inference over Long Sequences</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/</guid><description>Star Attention: 11x faster LLM inference on long sequences with 95-100% accuracy!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17116/cover.png"/></item><item><title>Hymba: A Hybrid-head Architecture for Small Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/</guid><description>Hymba: Hybrid-head architecture boosts small language model performance by 11.67x cache size reduction and 3.49x throughput, surpassing existing models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13676/cover.png"/></item></channel></rss>