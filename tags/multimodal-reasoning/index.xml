<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Multimodal Reasoning on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/multimodal-reasoning/</link><description>Recent content in Multimodal Reasoning on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Fri, 07 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/multimodal-reasoning/index.xml" rel="self" type="application/rss+xml"/><item><title>R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05379/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05379/</guid><description>R1-Omni: RLVR enhances multimodal emotion recognition, boosting reasoning and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05379/cover.png"/></item><item><title>R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20395/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20395/</guid><description>R2-T2: Boost multimodal MoE performance by re-routing experts in test-time, no retraining needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20395/cover.png"/></item><item><title>Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16033/</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16033/</guid><description>MMIR: A new benchmark to assess and improve multimodal reasoning models&amp;rsquo; ability to detect inconsistencies in real-world content.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16033/cover.png"/></item><item><title>InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11573/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11573/</guid><description>InfiR: Efficient, small AI models rival larger ones in reasoning, slashing costs and boosting privacy for wider AI use.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11573/cover.png"/></item><item><title>video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/</guid><description>video-SALMONN-01: An open-source audio-visual LLM enhances video understanding with a novel reasoning-intensive dataset and the pDPO method, achieving significant accuracy gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/cover.png"/></item><item><title>I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/</guid><description>ThinkDiff empowers text-to-image diffusion models with multimodal reasoning by aligning vision-language models to an LLM decoder, achieving state-of-the-art results on in-context reasoning benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/cover.png"/></item><item><title>The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01081/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01081/</guid><description>GPT models&amp;rsquo; multimodal reasoning abilities are tracked over time on challenging visual puzzles, revealing surprisingly steady improvement and cost trade-offs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01081/cover.png"/></item><item><title>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/</guid><description>Virgo: A new multimodal slow-thinking system, significantly improves MLLM reasoning by fine-tuning with text-based long-form thought data, demonstrating comparable performance to commercial systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/cover.png"/></item><item><title>Diving into Self-Evolving Training for Multimodal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</guid><description>M-STAR: a novel self-evolving training framework significantly boosts multimodal reasoning in large models without human annotation, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/cover.png"/></item><item><title>Progressive Multimodal Reasoning via Active Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/</guid><description>AR-MCTS: a novel framework boosting multimodal large language model reasoning by actively retrieving key supporting evidence and using Monte Carlo Tree Search for improved path selection and verificat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/cover.png"/></item></channel></rss>