<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Multimodal Reasoning on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/multimodal-reasoning/</link><description>Recent content in Multimodal Reasoning on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 27 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/multimodal-reasoning/index.xml" rel="self" type="application/rss+xml"/><item><title>Video-R1: Reinforcing Video Reasoning in MLLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21776/</link><pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21776/</guid><description>Video-R1: First to explore rule-based RL for video reasoning in MLLMs, enhancing performance on key benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-28/2503.21776/cover.png"/></item><item><title>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.19990/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.19990/</guid><description>MLLMs still struggle with spatial reasoning! LEGO-Puzzles benchmark reveals critical deficiencies, paving the way for AI advancement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.19990/cover.png"/></item><item><title>MAPS: A Multi-Agent Framework Based on Big Seven Personality and Socratic Guidance for Multimodal Scientific Problem Solving</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16905/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16905/</guid><description>MAPS solves multimodal scientific problems better by combining multiple agents and Socratic learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16905/cover.png"/></item><item><title>OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.17352/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.17352/</guid><description>OpenVLThinker: Iteratively refining vision-language models for complex reasoning, bridging the gap to R1-style capabilities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.17352/cover.png"/></item><item><title>OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.16081/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.16081/</guid><description>OThink-MR1 enhances MLLM reasoning via dynamic reinforcement learning, achieving remarkable cross-task generalization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.16081/cover.png"/></item><item><title>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/</guid><description>MicroVQA: A new benchmark to test visual-question-answering in microscopy-based research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13399/cover.png"/></item><item><title>Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13360/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13360/</guid><description>TVC mitigates visual forgetting in multimodal LLMs, enhancing reasoning by strategically re-introducing and compressing visual information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13360/cover.png"/></item><item><title>MPBench: A Comprehensive Multimodal Reasoning Benchmark for Process Errors Identification</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12505/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12505/</guid><description>MPBench: Multimodal benchmark to identify errors in reasoning processes.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12505/cover.png"/></item><item><title>Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12605/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12605/</guid><description>A comprehensive survey of multimodal chain-of-thought (MCoT) reasoning, bridging the gap in existing literature and fostering innovation towards multimodal AGI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12605/cover.png"/></item><item><title>ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06553/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06553/</guid><description>ProJudge: MLLM judges&amp;rsquo; benchmark for sci-reasoning &amp;amp; instruction-tuning data to boost performance!</description></item><item><title>R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcing Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05379/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05379/</guid><description>R1-Omni: RLVR enhances multimodal emotion recognition, boosting reasoning and generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05379/cover.png"/></item><item><title>R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20395/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20395/</guid><description>R2-T2: Boost multimodal MoE performance by re-routing experts in test-time, no retraining needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20395/cover.png"/></item><item><title>Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16033/</link><pubDate>Sat, 22 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16033/</guid><description>MMIR: A new benchmark to assess and improve multimodal reasoning models&amp;rsquo; ability to detect inconsistencies in real-world content.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16033/cover.png"/></item><item><title>InfiR : Crafting Effective Small Language Models and Multimodal Small Language Models in Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11573/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11573/</guid><description>InfiR: Efficient, small AI models rival larger ones in reasoning, slashing costs and boosting privacy for wider AI use.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11573/cover.png"/></item><item><title>video-SALMONN-o1: Reasoning-enhanced Audio-visual Large Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/</guid><description>video-SALMONN-01: An open-source audio-visual LLM enhances video understanding with a novel reasoning-intensive dataset and the pDPO method, achieving significant accuracy gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11775/cover.png"/></item><item><title>I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/</guid><description>ThinkDiff empowers text-to-image diffusion models with multimodal reasoning by aligning vision-language models to an LLM decoder, achieving state-of-the-art results on in-context reasoning benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/cover.png"/></item><item><title>The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01081/</link><pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01081/</guid><description>GPT models&amp;rsquo; multimodal reasoning abilities are tracked over time on challenging visual puzzles, revealing surprisingly steady improvement and cost trade-offs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.01081/cover.png"/></item><item><title>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/</guid><description>Virgo: A new multimodal slow-thinking system, significantly improves MLLM reasoning by fine-tuning with text-based long-form thought data, demonstrating comparable performance to commercial systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01904/cover.png"/></item><item><title>Diving into Self-Evolving Training for Multimodal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</guid><description>M-STAR: a novel self-evolving training framework significantly boosts multimodal reasoning in large models without human annotation, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/cover.png"/></item><item><title>Progressive Multimodal Reasoning via Active Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/</guid><description>AR-MCTS: a novel framework boosting multimodal large language model reasoning by actively retrieving key supporting evidence and using Monte Carlo Tree Search for improved path selection and verificat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14835/cover.png"/></item></channel></rss>