<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Northeastern University on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-northeastern-university/</link><description>Recent content in üè¢ Northeastern University on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 13 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-northeastern-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Distilling Diversity and Control in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10637/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10637/</guid><description>Distilling diffusion models?üí° This paper shows you how to retain base model diversity while keeping the distilled model&amp;rsquo;s speed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10637/cover.png"/></item><item><title>Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10708/</link><pubDate>Sat, 15 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10708/</guid><description>This survey paper comprehensively analyzes methods for injecting domain-specific knowledge into LLMs, categorizing them into four key approaches and evaluating their trade-offs to enhance performance &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10708/cover.png"/></item><item><title>Fully Open Source Moxin-7B Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06845/</guid><description>Moxin-LLM: A fully open-source 7B parameter LLM achieving superior zero-shot performance, promoting transparency and reproducibility in AI research.</description></item></channel></rss>