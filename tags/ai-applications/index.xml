<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Applications on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/ai-applications/</link><description>Recent content in AI Applications on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 27 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/ai-applications/index.xml" rel="self" type="application/rss+xml"/><item><title>SoRFT: Issue Resolving with Subtask-oriented Reinforced Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20127/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20127/</guid><description>SoRFT enhances LLMs for issue resolving via subtask-oriented reinforced fine-tuning, outperforming other open-source models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-28/2502.20127/cover.png"/></item><item><title>Reflective Planning: Vision-Language Models for Multi-Stage Long-Horizon Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.16707/</link><pubDate>Sun, 23 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.16707/</guid><description>Reflect VLM: Improving robotic manipulation via vision-language models with a novel reflection mechanism and a diffusion model for imagined futures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.16707/cover.png"/></item><item><title>Can Community Notes Replace Professional Fact-Checkers?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.14132/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.14132/</guid><description>Community moderation success relies on fact-checking!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-25/2502.14132/cover.png"/></item><item><title>Pre-training Auto-regressive Robotic Models with 4D Representations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/</guid><description>ARM4R pre-trains autoregressive robotic models using low-level 4D representations from human videos, achieving efficient transfer learning and improved task performance across various environments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13142/cover.png"/></item><item><title>RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13144/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13144/</guid><description>RAD: 3DGS-based RL advances autonomous driving, achieving a 3x lower collision rate!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13144/cover.png"/></item><item><title>FLAG-Trader: Fusion LLM-Agent with Gradient-based Reinforcement Learning for Financial Trading</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11433/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11433/</guid><description>FLAG-TRADER fuses LLMs &amp;amp; RL for enhanced financial trading, achieving superior performance compared to traditional methods by efficiently integrating multimodal data and adapting to market dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11433/cover.png"/></item><item><title>Learning Getting-Up Policies for Real-World Humanoid Robots</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12152/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12152/</guid><description>HUMANUP: A novel two-stage reinforcement learning framework enables real-world humanoid robots to autonomously recover from falls on various terrains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12152/cover.png"/></item><item><title>V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/</guid><description>V2V-LLM leverages multi-modal LLMs for safer cooperative autonomous driving by fusing perception data from multiple vehicles, answering driving-related questions, and improving trajectory planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09980/cover.png"/></item><item><title>DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09614/</link><pubDate>Thu, 13 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09614/</guid><description>DexTrack achieves highly generalizable neural tracking control for dexterous robot manipulation by iteratively training a controller using high-quality demonstrations refined via homotopy optimization&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.09614/cover.png"/></item><item><title>Learning Real-World Action-Video Dynamics with Heterogeneous Masked Autoregression</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/</guid><description>HMA: a novel approach for generating high-quality robotic videos 15x faster, enabling real-time policy evaluation and data augmentation for scaling robot learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04296/cover.png"/></item><item><title>Current Pathology Foundation Models are unrobust to Medical Center Differences</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18055/</link><pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18055/</guid><description>Current pathology foundation models struggle with center variations; this paper introduces a robustness index to quantify this, revealing model biases and advancing robust model development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.18055/cover.png"/></item><item><title>FAST: Efficient Action Tokenization for Vision-Language-Action Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/</guid><description>FAST: A novel action tokenization method using discrete cosine transform drastically improves autoregressive vision-language-action models&amp;rsquo; training and performance, enabling dexterous and high-freque&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09747/cover.png"/></item><item><title>EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/</guid><description>EnerVerse: A novel framework seamlessly integrates convolutional and attention mechanisms to generate embodied future spaces for enhanced robotic manipulation, mitigating data scarcity with a generati&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01895/cover.png"/></item><item><title>A3: Android Agent Arena for Mobile GUI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</guid><description>Android Agent Arena (A3): A novel evaluation platform for mobile GUI agents offering diverse tasks, flexible action space, and automated LLM-based evaluation, advancing real-world AI agent research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/cover.png"/></item><item><title>Training Software Engineering Agents and Verifiers with SWE-Gym</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/</guid><description>SWE-Gym, a novel environment for training real-world software engineering agents using 2,438 real-world Python task instances, achieves new state-of-the-art performance and is publicly available.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21139/cover.png"/></item><item><title>LearnLM: Improving Gemini for Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16429/</link><pubDate>Sat, 21 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16429/</guid><description>LearnLM enhances Gemini for education by training it to follow pedagogical instructions, leading to significant preference improvements over GPT-40, Claude 3.5, and Gemini 1.5 Pro in diverse learning &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.16429/cover.png"/></item><item><title>Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/</guid><description>MoDE makes AI for robot control faster and more efficient.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12953/cover.png"/></item><item><title>Large Action Models: From Inception to Implementation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</link><pubDate>Fri, 13 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/</guid><description>From language models to action models: building AI that &lt;em>does&lt;/em> things.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10047/cover.png"/></item><item><title>TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/</link><pubDate>Wed, 11 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/</guid><description>TidyBot++: Low-cost, open-source holonomic mobile base makes robot learning easier.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.10447/cover.png"/></item><item><title>CARP: Visuomotor Policy Learning via Coarse-to-Fine Autoregressive Prediction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06782/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06782/</guid><description>CARP: A novel visuomotor policy learning paradigm achieves high accuracy and 10x faster inference than state-of-the-art by combining autoregressive efficiency and diffusion model precision through a c&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06782/cover.png"/></item><item><title>Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04455/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04455/</guid><description>Code-as-Monitor (CaM) uses vision-language models and constraint-aware visual programming to achieve both reactive and proactive robotic failure detection in real-time, improving success rates and red&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04455/cover.png"/></item><item><title>Moto: Latent Motion Token as the Bridging Language for Robot Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04445/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04445/</guid><description>Moto: Bridging language for robot manipulation using latent motion tokens, achieving superior performance with limited data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04445/cover.png"/></item><item><title>DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15139/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15139/</guid><description>DiffusionDrive: a novel truncated diffusion model achieves real-time, high-quality end-to-end autonomous driving by leveraging multi-mode action distributions and significantly reducing computational &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15139/cover.png"/></item><item><title>WildLMa: Long Horizon Loco-Manipulation in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15131/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15131/</guid><description>WildLMa enables robots to perform complex, long-horizon manipulation tasks in unstructured environments by combining language-conditioned imitation learning, a whole-body controller for efficient tele&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.15131/cover.png"/></item><item><title>Soft Robotic Dynamic In-Hand Pen Spinning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/</link><pubDate>Tue, 19 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/</guid><description>SWIFT, a new system, enables a soft robotic hand to learn dynamic pen spinning via real-world trial-and-error, achieving 100% success across diverse pen properties without explicit object modeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.12734/cover.png"/></item><item><title>The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10323/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10323/</guid><description>Claude 3.5 Computer Use: A groundbreaking AI model offering public beta graphical user interface (GUI) agent for computer use is comprehensively analyzed in this research. This study provides an out-o&amp;hellip;</description></item><item><title>Hermes: A Large Language Model Framework on the Journey to Autonomous Networks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06490/</link><pubDate>Sun, 10 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06490/</guid><description>Hermes, a novel LLM-based framework, automates cellular network modeling by generating explainable &amp;lsquo;blueprints&amp;rsquo; for constructing Network Digital Twins (NDTs), paving the way for fully autonomous netwo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06490/cover.png"/></item><item><title>DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04999/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04999/</guid><description>DynaMem empowers robots with online dynamic spatio-semantic memory, achieving a 2x improvement in pick-and-drop success rate on non-stationary objects compared to static systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04999/cover.png"/></item><item><title>DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/</guid><description>DeeR-VLA dynamically adjusts the size of a multimodal large language model based on task difficulty, significantly reducing computational cost and memory usage in robotic control without compromising &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/cover.png"/></item><item><title>Navigating the Unknown: A Chat-Based Collaborative Interface for Personalized Exploratory Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24032/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24032/</guid><description>Collaborative Assistant for Personalized Exploration (CARE) enhances LLM chatbots for exploratory tasks by combining a multi-agent framework with a structured interface, delivering tailored solutions &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24032/cover.png"/></item></channel></rss>