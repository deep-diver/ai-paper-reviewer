<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Zhejiang University on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-zhejiang-university/</link><description>Recent content in üè¢ Zhejiang University on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 20 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-zhejiang-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.15851/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.15851/</guid><description>Zero-1-to-A: Animatable avatars from a single image using video diffusion, robust to spatial &amp;amp; temporal inconsistencies!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.15851/cover.png"/></item><item><title>MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.15451/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.15451/</guid><description>MotionStreamer: Streaming motion generation w/ diffusion-based autoregressive model in causal latent space.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.15451/cover.png"/></item><item><title>Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14478/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14478/</guid><description>Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLMs</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.14478/cover.png"/></item><item><title>DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12885/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12885/</guid><description>DreamRenderer: Taming attribute control in large-scale text-to-image models with a plug-and-play, training-free approach for enhanced content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.12885/cover.png"/></item><item><title>MagicID: Hybrid Preference Optimization for ID-Consistent and Dynamic-Preserved Video Customization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.12689/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.12689/</guid><description>MagicID: ID-consistent &amp;amp; dynamic-preserved video customization via hybrid preference optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-21/2503.12689/cover.png"/></item><item><title>ReCamMaster: Camera-Controlled Generative Rendering from A Single Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11647/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11647/</guid><description>ReCamMaster: Re-shoots videos via generative rendering, controlling camera movement from a single source, for novel perspectives and enhanced video creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.11647/cover.png"/></item><item><title>DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17157/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17157/</guid><description>DICEPTION: A generalist diffusion model for visual perceptual tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17157/cover.png"/></item><item><title>Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13533/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13533/</guid><description>LORAM: Train small, infer large LLMs by memory-efficient LoRA training. Enables 70B parameter model training on a 20G HBM GPU, replacing A100-80G. Reduces parameter storage cost by 15.81x.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13533/cover.png"/></item><item><title>How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11196/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11196/</guid><description>LLMs&amp;rsquo; knowledge acquisition is unveiled through the lens of evolving knowledge circuits, revealing how new knowledge integration depends on relevance to existing knowledge, exhibiting distinct phases &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11196/cover.png"/></item><item><title>DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04370/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04370/</guid><description>DreamDPO: Revolutionizing text-to-3D generation by directly aligning outputs with human preferences via innovative preference optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04370/cover.png"/></item><item><title>InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04575/</link><pubDate>Wed, 08 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04575/</guid><description>InfiGUIAgent, a novel multimodal GUI agent, leverages a two-stage training pipeline to achieve advanced reasoning and GUI interaction capabilities, outperforming existing models in benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.04575/cover.png"/></item><item><title>OneKE: A Dockerized Schema-Guided LLM Agent-based Knowledge Extraction System</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</link><pubDate>Sat, 28 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/</guid><description>OneKE: a dockerized, schema-guided LLM agent system efficiently extracts knowledge from diverse sources, offering adaptability and robust error handling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.20005/cover.png"/></item><item><title>Orient Anything: Learning Robust Object Orientation Estimation from Rendering 3D Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/</guid><description>Orient Anything: Learning robust object orientation estimation directly from rendered 3D models, achieving state-of-the-art accuracy on real images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18605/cover.png"/></item><item><title>Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/</guid><description>Prompting unlocks 4K metric depth from low-cost LiDAR.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14015/cover.png"/></item><item><title>ZipAR: Accelerating Autoregressive Image Generation through Spatial Locality</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/</guid><description>ZipAR accelerates autoregressive image generation by up to 91% through parallel decoding leveraging spatial locality in images, making high-resolution image generation significantly faster.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04062/cover.png"/></item><item><title>Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/</guid><description>ScoreLiDAR: Distilling diffusion models for 5x faster, higher-quality 3D LiDAR scene completion!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03515/cover.png"/></item></channel></rss>