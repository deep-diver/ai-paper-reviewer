<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Meta AI on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-meta-ai/</link><description>Recent content in üè¢ Meta AI on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 17 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-meta-ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Intuitive physics understanding emerges from self-supervised pretraining on natural videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/</guid><description>AI models learn intuitive physics from self-supervised video pretraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/cover.png"/></item><item><title>Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03275/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03275/</guid><description>Boosting language model reasoning: A novel hybrid approach using latent tokens drastically shortens reasoning traces, improving model performance and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03275/cover.png"/></item><item><title>VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/</guid><description>VideoJAM enhances video generation by jointly learning appearance and motion representations, achieving state-of-the-art motion coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/cover.png"/></item><item><title>MLLM-as-a-Judge for Image Safety without Human Labeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</link><pubDate>Tue, 31 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/</guid><description>Zero-shot image safety judgment is achieved using MLLMs and a novel method called CLUE, objectifying safety rules, and significantly reducing the need for human labeling.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.00192/cover.png"/></item><item><title>PartGen: Part-level 3D Generation and Reconstruction with Multi-View Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/</guid><description>PartGen generates compositional 3D objects with meaningful parts from text, images, or unstructured 3D data using multi-view diffusion models, enabling flexible 3D part editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18608/cover.png"/></item><item><title>Training Large Language Models to Reason in a Continuous Latent Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/</guid><description>LLMs are trained to reason using language, but COCONUT lets them reason directly in a continuous latent space, boosting performance on logical tasks requiring complex planning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06769/cover.png"/></item><item><title>Efficient Track Anything</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/</guid><description>EfficientTAMs achieve comparable video object segmentation accuracy to SAM 2 with ~2x speedup using lightweight ViTs and efficient cross-attention.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18933/cover.png"/></item><item><title>Adaptive Decoding via Latent Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/</guid><description>LLMs can dynamically adjust decoding temperature using Adaptive Decoding and Latent Preference Optimization, improving performance across creative and factual tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09661/cover.png"/></item><item><title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</guid><description>Adaptive Caching (AdaCache) dramatically speeds up video generation with diffusion transformers by cleverly caching and reusing computations, tailoring the process to each video&amp;rsquo;s complexity and motio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/cover.png"/></item></channel></rss>