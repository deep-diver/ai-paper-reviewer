<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Shanghai Artificial Intelligence Laboratory on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-shanghai-artificial-intelligence-laboratory/</link><description>Recent content in üè¢ Shanghai Artificial Intelligence Laboratory on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 21 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-shanghai-artificial-intelligence-laboratory/index.xml" rel="self" type="application/rss+xml"/><item><title>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12368/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12368/</guid><description>InternLM-XComposer2.5-Reward: A novel multi-modal reward model boosting Large Vision Language Model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12368/cover.png"/></item><item><title>Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/</guid><description>Introducing Evaluation Agent, a faster, more flexible human-like framework for evaluating visual generative AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09645/cover.png"/></item><item><title>Chimera: Improving Generalist Model with Domain-Specific Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05983/</link><pubDate>Sun, 08 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05983/</guid><description>Chimera boosts large multimodal models&amp;rsquo; performance on specialized tasks by cleverly integrating domain-specific expert models, achieving state-of-the-art results on multiple benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05983/cover.png"/></item><item><title>VLSBench: Unveiling Visual Leakage in Multimodal Safety</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19939/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19939/</guid><description>VLSBench exposes visual leakage in MLLM safety benchmarks, creating a new, leak-free benchmark to evaluate true multimodal safety.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19939/cover.png"/></item></channel></rss>