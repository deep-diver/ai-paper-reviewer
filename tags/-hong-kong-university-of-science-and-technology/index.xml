<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Hong Kong University of Science and Technology on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-hong-kong-university-of-science-and-technology/</link><description>Recent content in üè¢ Hong Kong University of Science and Technology on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Fri, 21 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-hong-kong-university-of-science-and-technology/index.xml" rel="self" type="application/rss+xml"/><item><title>Position: Interactive Generative Video as Next-Generation Game Engine</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.17359/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.17359/</guid><description>Interactive Generative Video (IGV) can revolutionize game creation by using AI to generate endless, novel content for next-gen game engines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.17359/cover.png"/></item><item><title>Temporal Regularization Makes Your Video Generator Stronger</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15417/</link><pubDate>Wed, 19 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15417/</guid><description>FluxFlow: Make your video generator stronger via temporal regularization!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.15417/cover.png"/></item><item><title>Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13070/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13070/</guid><description>Rewards Are Enough!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.13070/cover.png"/></item><item><title>Long-Video Audio Synthesis with Multi-Agent Collaboration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10719/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10719/</guid><description>LVAS-Agent: Multi-agent system conquers long-video audio synthesis with collaborative dubbing, script, design, &amp;amp; more!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.10719/cover.png"/></item><item><title>LightGen: Efficient Image Generation through Knowledge Distillation and Direct Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08619/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08619/</guid><description>LightGen: Efficient image generation via knowledge distillation and direct preference optimization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.08619/cover.png"/></item><item><title>Learning Few-Step Diffusion Models by Trajectory Distribution Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06674/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06674/</guid><description>TDM: a new diffusion distillation paradigm unifying trajectory distillation and distribution matching, surpassing teachers in a data-free manner with state-of-the-art performance and low training cost&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.06674/cover.png"/></item><item><title>RectifiedHR: Enable Efficient High-Resolution Image Generation via Energy Rectification</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02537/</link><pubDate>Tue, 04 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02537/</guid><description>RectifiedHR: Enables training-free high-resolution image generation via energy rectification, boosting both efficiency and effectiveness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.02537/cover.png"/></item><item><title>Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16894/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16894/</guid><description>GOAT: Adaptively boosts LoRA with SVD &amp;amp; MoE alignment, closing the gap with Full FT.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.16894/cover.png"/></item><item><title>Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13145/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13145/</guid><description>mmMamba: a novel framework creates linear-complexity multimodal models via distillation, drastically improving efficiency without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13145/cover.png"/></item><item><title>Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12669/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12669/</guid><description>Perovskite-LLM: a new knowledge-enhanced system boosts perovskite solar cell research by integrating a domain-specific knowledge graph, high-quality datasets, and specialized LLMs for superior knowled&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12669/cover.png"/></item><item><title>Atom of Thoughts for Markov LLM Test-Time Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12018/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12018/</guid><description>Atom of Thoughts (AOT) revolutionizes LLM test-time scaling by decomposing complex reasoning into independent sub-questions, drastically reducing computation while maintaining high accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.12018/cover.png"/></item><item><title>FinMTEB: Finance Massive Text Embedding Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10990/</link><pubDate>Sun, 16 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10990/</guid><description>FinMTEB: A new benchmark reveals that general-purpose embedding models struggle in the finance domain; domain-specific models excel, and surprisingly, simple BoW outperforms sophisticated models on ce&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10990/cover.png"/></item><item><title>I Think, Therefore I Diffuse: Enabling Multimodal In-Context Reasoning in Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/</link><pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/</guid><description>ThinkDiff empowers text-to-image diffusion models with multimodal reasoning by aligning vision-language models to an LLM decoder, achieving state-of-the-art results on in-context reasoning benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10458/cover.png"/></item><item><title>CodeI/O: Condensing Reasoning Patterns via Code Input-Output Prediction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07316/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07316/</guid><description>CODEI/O: Condensing reasoning patterns from code into LLM training data for enhanced reasoning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07316/cover.png"/></item><item><title>CustomVideoX: 3D Reference Attention Driven Dynamic Adaptation for Zero-Shot Customized Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/</guid><description>CustomVideoX: Zero-shot personalized video generation, exceeding existing methods in quality &amp;amp; consistency via 3D reference attention and dynamic adaptation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06527/cover.png"/></item><item><title>FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/</guid><description>FlashVideo: Generate stunning high-resolution videos efficiently using a two-stage framework prioritizing fidelity and detail, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/cover.png"/></item><item><title>Generating Symbolic World Models via Test-time Scaling of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04728/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04728/</guid><description>LLMs excel at complex reasoning but struggle with planning; this paper introduces a test-time scaling approach that enhances LLMs&amp;rsquo; PDDL reasoning, enabling high-quality PDDL domain generation, outperf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04728/cover.png"/></item><item><title>Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04128/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04128/</guid><description>Llasa, a novel single-Transformer TTS model, achieves state-of-the-art performance by scaling both training and inference compute, improving naturalness, prosody and emotional expressiveness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04128/cover.png"/></item><item><title>Weak-to-Strong Diffusion with Reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00473/</link><pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00473/</guid><description>W2SD: A novel framework boosts diffusion model quality by using the difference between weak and strong models to refine sampling trajectories, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.00473/cover.png"/></item><item><title>GaussianAvatar-Editor: Photorealistic Animatable Gaussian Head Avatar Editor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09978/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09978/</guid><description>GaussianAvatar-Editor enables photorealistic, text-driven editing of animatable 3D heads, solving motion occlusion and ensuring temporal consistency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09978/cover.png"/></item><item><title>Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/</guid><description>Diffusion as Shader (DaS) achieves versatile video control by using 3D tracking videos as control signals in a unified video diffusion model, enabling precise manipulation across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/cover.png"/></item><item><title>TransPixar: Advancing Text-to-Video Generation with Transparency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</guid><description>TransPixar generates high-quality videos with transparency by jointly training RGB and alpha channels, outperforming sequential generation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/cover.png"/></item><item><title>A3: Android Agent Arena for Mobile GUI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/</guid><description>Android Agent Arena (A3): A novel evaluation platform for mobile GUI agents offering diverse tasks, flexible action space, and automated LLM-based evaluation, advancing real-world AI agent research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01149/cover.png"/></item><item><title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</guid><description>VideoAnydoor: High-fidelity video object insertion with precise motion control, achieved via an end-to-end framework leveraging an ID extractor and a pixel warper for robust detail preservation and fi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/cover.png"/></item><item><title>Edicho: Consistent Image Editing in the Wild</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</link><pubDate>Mon, 30 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/</guid><description>Edicho: a novel training-free method for consistent image editing across diverse images, achieving precise consistency by leveraging explicit correspondence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.21079/cover.png"/></item><item><title>B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/</guid><description>B-STAR dynamically balances exploration and exploitation in self-taught reasoners, achieving superior performance in mathematical, coding, and commonsense reasoning tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17256/cover.png"/></item><item><title>Diving into Self-Evolving Training for Multimodal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</link><pubDate>Mon, 23 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/</guid><description>M-STAR: a novel self-evolving training framework significantly boosts multimodal reasoning in large models without human annotation, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.17451/cover.png"/></item><item><title>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</guid><description>LeviTor: Revolutionizing image-to-video synthesis with intuitive 3D trajectory control, generating realistic videos from static images by abstracting object masks into depth-aware control points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/cover.png"/></item><item><title>MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14475/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14475/</guid><description>MegaPairs synthesizes 26M+ high-quality multimodal retrieval training examples, enabling state-of-the-art zero-shot performance and surpassing existing methods trained on 70x more data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14475/cover.png"/></item><item><title>AniDoc: Animation Creation Made Easier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</guid><description>AniDoc automates cartoon animation line art video colorization, making animation creation easier!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/cover.png"/></item><item><title>Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14233/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14233/</guid><description>Enhance image captions significantly with DCE, a novel engine leveraging visual specialists to generate comprehensive, detailed descriptions surpassing LMM and human-annotated captions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14233/cover.png"/></item><item><title>GaussianProperty: Integrating Physical Properties to 3D Gaussians with LMMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</link><pubDate>Sun, 15 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/</guid><description>Training-free method adds physical properties to 3D models using vision-language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.11258/cover.png"/></item><item><title>Lyra: An Efficient and Speech-Centric Framework for Omni-Cognition</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09501/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09501/</guid><description>Lyra: An efficient, speech-centric framework for omni-cognition, achieving state-of-the-art results across various modalities while being highly efficient.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09501/cover.png"/></item><item><title>OmniCreator: Self-Supervised Unified Generation with Universal Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</guid><description>OmniCreator: Self-supervised unified image+video generation &amp;amp; universal editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/cover.png"/></item><item><title>VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</guid><description>VideoGen-of-Thought (VGoT) creates high-quality, multi-shot videos by collaboratively generating scripts, keyframes, and video clips, ensuring narrative consistency and visual coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/cover.png"/></item><item><title>MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</guid><description>MagicDriveDiT generates high-resolution, long street-view videos with precise control, exceeding limitations of previous methods in autonomous driving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/cover.png"/></item><item><title>Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating Financial Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</link><pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/</guid><description>Golden Touchstone, a new bilingual benchmark, comprehensively evaluates financial LLMs across eight tasks, revealing model strengths and weaknesses and advancing FinLLM research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.06272/cover.png"/></item></channel></rss>