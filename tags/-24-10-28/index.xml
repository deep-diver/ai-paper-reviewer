<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ðŸ¤— 24-10-28 on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-28/</link><description>Recent content in ðŸ¤— 24-10-28 on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Fri, 25 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-28/index.xml" rel="self" type="application/rss+xml"/><item><title>Counting Ability of Large Language Models and Impact of Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/</guid><description>LLM counting abilities are surprisingly sensitive to tokenization; carefully crafted tokenization strategies significantly improve accuracy, bridging the gap between theory and practice.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/cover.png"/></item><item><title>FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/</guid><description>FasterCache: a training-free strategy boosts video diffusion model inference speed by 1.67x without sacrificing video quality, using dynamic feature reuse and CFG-Cache.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/cover.png"/></item><item><title>Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/</guid><description>PREREQ-TUNE, a novel LLM fine-tuning strategy, disentangles skill and knowledge learning to significantly reduce hallucinations by mitigating knowledge inconsistency between pre-training and fine-tuni&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/cover.png"/></item><item><title>Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/</guid><description>LLMs can detect and correct substantial label errors in NLP datasets, significantly improving model performance and highlighting the importance of data quality in NLP.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/cover.png"/></item><item><title>Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/</guid><description>This work introduces a new framework that learns object dynamics directly from multi-view videos by explicitly considering robot actions, achieving accurate 3D action-conditioned video prediction and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/cover.png"/></item><item><title>Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/</guid><description>Researchers developed a hybrid approach to collecting preference data for AI alignment, cleverly routing instances to either human or AI annotators based on a predictive model, resulting in improved m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/cover.png"/></item><item><title>Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/</guid><description>Infinity-MM, a 40-million-sample multimodal instruction dataset, boosts open-source VLM performance to state-of-the-art levels by combining real-world and synthetic data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/cover.png"/></item><item><title>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/</guid><description>MMAU benchmark challenges multimodal LLMs with diverse audio tasks, revealing significant gaps in current audio understanding capabilities and driving future advancements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/cover.png"/></item><item><title>Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/</guid><description>SUPE leverages unlabeled prior data to pre-train skills and pseudo-label trajectories for efficient online RL exploration, significantly outperforming existing methods on challenging tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/cover.png"/></item><item><title>ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/</guid><description>ROCKET-1 masters open-world Minecraft interaction by using visual-temporal context prompting, enabling VLMs to effectively guide low-level policies for complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/cover.png"/></item><item><title>Continuous Speech Synthesis using per-token Latent Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/</guid><description>SALAD, a novel per-token latent diffusion model, achieves superior zero-shot speech synthesis, surpassing discrete methods in intelligibility while maintaining speech quality and speaker similarity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/cover.png"/></item><item><title>Reflection-Bench: probing AI intelligence with reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/</guid><description>Reflection-Bench, a new benchmark, reveals current LLMs lack true reflection abilities, highlighting a critical gap in achieving human-level AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/cover.png"/></item><item><title>Teach Multimodal LLMs to Comprehend Electrocardiographic Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/</guid><description>PULSE, a new MLLM, achieves state-of-the-art accuracy in ECG image interpretation, exceeding existing models by 15-30%, thanks to a novel million-sample instruction tuning dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/cover.png"/></item></channel></rss>