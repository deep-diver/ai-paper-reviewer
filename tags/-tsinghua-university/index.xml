<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Tsinghua University on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-tsinghua-university/</link><description>Recent content in üè¢ Tsinghua University on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Fri, 06 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-tsinghua-university/index.xml" rel="self" type="application/rss+xml"/><item><title>Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05271/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05271/</guid><description>InternVL 2.5, a new open-source multimodal LLM, surpasses 70% on the MMMU benchmark, rivaling top commercial models through model, data, and test-time scaling strategies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05271/cover.png"/></item><item><title>Densing Law of LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/</guid><description>LLMs&amp;rsquo; training quality is exponentially improving, enabling models with half the parameters to match state-of-the-art performance every 3 months, thus reducing inference costs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04315/cover.png"/></item><item><title>GenMAC: Compositional Text-to-Video Generation with Multi-Agent Collaboration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04440/</link><pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04440/</guid><description>GENMAC: Multi-agent collaboration revolutionizes compositional text-to-video generation, achieving state-of-the-art results by iteratively refining videos via specialized agents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04440/cover.png"/></item><item><title>2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/</guid><description>2DGS-Room: Seed-guided 2D Gaussian splatting with geometric constraints achieves state-of-the-art high-fidelity indoor scene reconstruction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03428/cover.png"/></item><item><title>MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/</guid><description>MIDI: a novel multi-instance diffusion model generates compositional 3D scenes from single images by simultaneously creating multiple 3D instances with accurate spatial relationships and high generali&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03558/cover.png"/></item><item><title>Free Process Rewards without Process Labels</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/</guid><description>Train high-performing Process Reward Models (PRMs) cheaply using only outcome-level labels, eliminating the need for costly step-by-step annotations!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01981/cover.png"/></item><item><title>Structured 3D Latents for Scalable and Versatile 3D Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/</guid><description>Unified 3D latent representation (SLAT) enables versatile high-quality 3D asset generation, significantly outperforming existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01506/cover.png"/></item><item><title>X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01824/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01824/</guid><description>X-Prompt: a novel autoregressive vision-language model achieves universal in-context image generation by efficiently compressing contextual information and using a unified training framework for super&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01824/cover.png"/></item><item><title>AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/</link><pubDate>Fri, 29 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/</guid><description>AlphaTablets: A novel 3D plane representation enabling accurate, consistent, and flexible 3D planar reconstruction from monocular videos, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19950/cover.png"/></item><item><title>Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18478/</guid><description>HiAR-ICL, a novel automated reasoning paradigm using Monte Carlo Tree Search, surpasses state-of-the-art accuracy in complex mathematical reasoning by shifting focus from specific examples to abstract&amp;hellip;</description></item><item><title>TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</guid><description>TAPTRv3 achieves state-of-the-art long-video point tracking by cleverly using spatial and temporal context to enhance feature querying, surpassing previous methods and demonstrating strong performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/cover.png"/></item><item><title>Training and Evaluating Language Models with Template-based Data Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18104/</guid><description>Researchers created TemplateGSM, a massive dataset of 7M+ grade-school math problems and solutions, using GPT-4 to generate templates, significantly advancing LLM training for mathematical reasoning.</description></item><item><title>Patience Is The Key to Large Language Model Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</link><pubDate>Wed, 20 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/</guid><description>Boosting Large Language Model (LLM) reasoning without massive datasets: A novel training method encourages &amp;lsquo;patient&amp;rsquo; reasoning, improving accuracy by up to 6.7% on benchmark tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13082/cover.png"/></item><item><title>SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</link><pubDate>Sun, 17 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/</guid><description>SageAttention2 achieves 4-bit accurate attention, boosting inference speed by 2x compared to FlashAttention2, while maintaining end-to-end accuracy across diverse models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10958/cover.png"/></item><item><title>AnimateAnything: Consistent and Controllable Animation for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</guid><description>AnimateAnything: A unified approach enabling precise &amp;amp; consistent video manipulation via a novel optical flow representation and frequency stabilization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/cover.png"/></item><item><title>Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10442/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10442/</guid><description>Boosting multimodal reasoning in LLMs, researchers developed Mixed Preference Optimization (MPO) and a large-scale dataset (MMPR), significantly improving reasoning accuracy and achieving performance &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10442/cover.png"/></item><item><title>LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</link><pubDate>Thu, 14 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/</guid><description>LLaMA-Mesh: Unifying 3D mesh generation with LLMs by directly representing meshes as text, enabling efficient text-to-3D conversion within a single model.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.09595/cover.png"/></item><item><title>Sharingan: Extract User Action Sequence from Desktop Recordings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</guid><description>Sharingan extracts user action sequences from desktop recordings using novel VLM-based methods, achieving 70-80% accuracy and enabling RPA.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/cover.png"/></item><item><title>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07975/</link><pubDate>Tue, 12 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07975/</guid><description>JanusFlow harmonizes autoregression and rectified flow for unified multimodal understanding and generation, achieving state-of-the-art results on standard benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.07975/cover.png"/></item><item><title>DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/</guid><description>DimensionX generates photorealistic 3D and 4D scenes from a single image via controllable video diffusion, enabling precise manipulation of spatial structure and temporal dynamics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.04928/cover.png"/></item><item><title>DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/</guid><description>DeeR-VLA dynamically adjusts the size of a multimodal large language model based on task difficulty, significantly reducing computational cost and memory usage in robotic control without compromising &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02359/cover.png"/></item><item><title>Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/</guid><description>Researchers discovered predictable scaling laws for activation sparsity in LLMs, showing how data, architecture, and model size influence sparsity, paving the way for more efficient and interpretable &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02335/cover.png"/></item><item><title>WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/</guid><description>WEBRL: A self-evolving online curriculum reinforcement learning framework empowers open LLMs to excel as high-performing web agents, surpassing proprietary models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02337/cover.png"/></item><item><title>AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/</guid><description>ANDROIDLAB, a novel framework, systematically benchmarks Android autonomous agents, improving LLM and LMM success rates on 138 tasks via a unified environment and open-source dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24024/cover.png"/></item><item><title>Constraint Back-translation Improves Complex Instruction Following of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/</guid><description>Constraint Back-translation enhances complex instruction following in LLMs by leveraging inherent constraints in existing datasets for efficient high-quality data creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24175/cover.png"/></item></channel></rss>