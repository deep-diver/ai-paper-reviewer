<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ðŸ”– 24-10-16 on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-16/</link><description>Recent content in ðŸ”– 24-10-16 on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Wed, 16 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-16/index.xml" rel="self" type="application/rss+xml"/><item><title>AERO: Softmax-Only LLMs for Efficient Private Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/</guid><description>AERO achieves 4.23x communication and 1.94x latency reduction in private AI inference by developing a Softmax-only LLM architecture with novel entropy regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/cover.png"/></item><item><title>Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</guid><description>KeyNMF, a novel topic modeling approach, effectively analyzes information dynamics in Chinese diaspora media, revealing the PRC&amp;rsquo;s potential influence on European elections.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/cover.png"/></item><item><title>DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12628/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12628/</guid><description>DocLayout-YOLO: Blazing-fast document layout analysis via diverse synthetic data and adaptive perception, exceeding state-of-the-art speed and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12628/cover.png"/></item><item><title>Exploring Model Kinship for Merging Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12613/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12613/</guid><description>Researchers improve large language model capabilities by introducing &amp;lsquo;model kinship&amp;rsquo; â€“ a metric measuring LLM similarity, which guides a novel merging strategy for enhanced performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12613/cover.png"/></item><item><title>HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12381/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12381/</guid><description>HumanEval-V: A new benchmark rigorously evaluates large multimodal models&amp;rsquo; visual understanding and reasoning abilities through carefully designed coding tasks, revealing significant limitations in cu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12381/cover.png"/></item><item><title>Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12491/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12491/</guid><description>Researchers used inverse reinforcement learning to reveal hidden reward functions in large language models, achieving up to 80% accuracy in predicting human preferences and offering new insights into &amp;hellip;</description></item><item><title>JudgeBench: A Benchmark for Evaluating LLM-based Judges</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/</guid><description>JudgeBench: a new benchmark objectively evaluates LLM-based judges on complex tasks, revealing that even top models struggle, highlighting the need for more advanced AI judges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/cover.png"/></item><item><title>Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/</guid><description>Long-LRM: A groundbreaking 3D reconstruction model generating photorealistic, wide-coverage scenes from 32 images in 1.3 seconds using a novel hybrid architecture.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/cover.png"/></item><item><title>Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</guid><description>Meta-Chunking boosts RAG performance by intelligently segmenting text into logically coherent chunks, improving knowledge retrieval and question answering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/cover.png"/></item><item><title>MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/</guid><description>MMed-RAG significantly boosts medical vision-language model factuality by using domain-aware retrieval, adaptive context selection, and RAG-based preference fine-tuning, achieving an average 43.8% imp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/cover.png"/></item><item><title>MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/</guid><description>MuVi: a novel framework generating synchronized music for videos, achieving superior semantic alignment and rhythmic synchronization through contrastive learning and flow-matching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/cover.png"/></item><item><title>Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/</guid><description>Meta FAIR released OMat24, a massive open dataset of inorganic materials with 110M+ DFT calculations and state-of-the-art Equiformer V2 models, accelerating AI-driven materials discovery.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/cover.png"/></item><item><title>ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12405/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12405/</guid><description>ProSA assesses LLM prompt sensitivity using a new metric, revealing that larger models are more robust but subjective evaluations are also affected by prompt variations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12405/cover.png"/></item><item><title>Revealing the Barriers of Language Agents in Planning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12409/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12409/</guid><description>Language agents struggle with planning due to limited constraint understanding and the diminishing influence of goals, hindering human-level performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12409/cover.png"/></item><item><title>Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12490/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12490/</guid><description>By stabilizing the latent space using a novel discrete image tokenizer, researchers achieve superior performance in image autoregressive modeling, surpassing previous state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12490/cover.png"/></item><item><title>The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12787/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12787/</guid><description>Large multimodal models are prone to hallucinations; this work systematically investigates these, pinpointing key causes and introducing a benchmark for improved model reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12787/cover.png"/></item><item><title>Tracking Universal Features Through Fine-Tuning and Model Merging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12391/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12391/</guid><description>Researchers tracked feature evolution in small language models through fine-tuning and model merging, discovering surprising feature instability and uncovering interpretable persistent features like v&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12391/cover.png"/></item><item><title>TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/</guid><description>TransAgent empowers vision-language models by collaboratively distilling knowledge from diverse expert agents, achieving state-of-the-art performance on visual recognition tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/cover.png"/></item><item><title>WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/</guid><description>WORLDCUISINES: a massive multilingual VQA benchmark on global cuisines, reveals cultural knowledge gaps in current vision-language models and provides a valuable resource for advancing research in thi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/cover.png"/></item><item><title>WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12722/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12722/</guid><description>WorldMedQA-V: a new multilingual, multimodal medical exam dataset helps fairly evaluate AI&amp;rsquo;s performance in diverse healthcare settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12722/cover.png"/></item></channel></rss>