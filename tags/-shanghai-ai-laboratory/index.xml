<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Shanghai AI Laboratory on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-shanghai-ai-laboratory/</link><description>Recent content in üè¢ Shanghai AI Laboratory on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 11 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-shanghai-ai-laboratory/index.xml" rel="self" type="application/rss+xml"/><item><title>LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-13/2502.07563/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-13/2502.07563/</guid><description>LASP-2 revolutionizes linear attention training by achieving 36.6% faster speeds than Ring Attention via a novel sequence parallelism method, boosting efficiency for very long sequences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-13/2502.07563/cover.png"/></item><item><title>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/</guid><description>OREAL, a novel RL framework, achieves state-of-the-art mathematical reasoning in LLMs using only binary outcome rewards, demonstrating that a 7B model can match the performance of 32B models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/cover.png"/></item><item><title>BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</guid><description>BoostStep enhances large language models&amp;rsquo; mathematical abilities by refining single-step reasoning through a novel step-level in-context learning strategy, achieving significant improvements on variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/cover.png"/></item><item><title>Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/</guid><description>Task Preference Optimization (TPO) significantly boosts multimodal large language models&amp;rsquo; visual understanding by aligning them with fine-grained visual tasks via learnable task tokens, achieving 14.6&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/cover.png"/></item><item><title>Are Your LLMs Capable of Stable Reasoning?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</guid><description>G-Pass@k &amp;amp; LiveMathBench: Evaluating the stability of LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/cover.png"/></item><item><title>OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</guid><description>OmniDocBench, a novel benchmark, tackles limitations in current document parsing by introducing a diverse, high-quality dataset with comprehensive annotations, enabling fair multi-level evaluation of &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/cover.png"/></item><item><title>OS-ATLAS: A Foundation Action Model for Generalist GUI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/</guid><description>OS-Atlas: A new open-source toolkit and model dramatically improves GUI agent performance by providing a massive dataset and innovative training methods, enabling superior generalization to unseen int&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/cover.png"/></item></channel></rss>