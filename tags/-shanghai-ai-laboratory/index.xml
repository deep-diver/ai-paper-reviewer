<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>üè¢ Shanghai AI Laboratory on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-shanghai-ai-laboratory/</link><description>Recent content in üè¢ Shanghai AI Laboratory on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>¬© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 27 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-shanghai-ai-laboratory/index.xml" rel="self" type="application/rss+xml"/><item><title>A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.21614/</link><pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.21614/</guid><description>Survey on improving efficiency in large reasoning models across language, multimodality, and beyond.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-31/2503.21614/cover.png"/></item><item><title>LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.21749/</link><pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.21749/</guid><description>LeX-Art: High-quality text-to-image generation via scalable data synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.21749/cover.png"/></item><item><title>Lumina-Image 2.0: A Unified and Efficient Image Generative Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.21758/</link><pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.21758/</guid><description>Lumina-Image 2.0: A unified &amp;amp; efficient image generative framework, outperforming previous models with only 2.6B parameters.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.21758/cover.png"/></item><item><title>LEGO-Puzzles: How Good Are MLLMs at Multi-Step Spatial Reasoning?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.19990/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.19990/</guid><description>MLLMs still struggle with spatial reasoning! LEGO-Puzzles benchmark reveals critical deficiencies, paving the way for AI advancement.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.19990/cover.png"/></item><item><title>TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-04-01/2503.19901/</link><pubDate>Tue, 25 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-04-01/2503.19901/</guid><description>TokenHSI: Unified Transformer for Physical Human-Scene Interactions through Task Tokenization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-04-01/2503.19901/cover.png"/></item><item><title>Aether: Geometric-Aware Unified World Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.18945/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.18945/</guid><description>AETHER: a unified framework enabling geometry-aware reasoning in world models, achieving zero-shot generalization from synthetic to real-world data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.18945/cover.png"/></item><item><title>CLS-RL: Image Classification with Rule-Based Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16188/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16188/</guid><description>CLS-RL: Rule-based RL tackles catastrophic forgetting in MLLM image classification, outperforming SFT with better generalization and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.16188/cover.png"/></item><item><title>Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05447/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05447/</guid><description>Linear-MoE: Integrates Linear Sequence Modeling with Mixture-of-Experts, achieving efficiency gains and competitive performance in large language models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05447/cover.png"/></item><item><title>Lost in Literalism: How Supervised Training Shapes Translationese in LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04369/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04369/</guid><description>LLMs show translationese due to supervised training biases. Polishing references and filtering unnatural instances can mitigate this issue.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04369/cover.png"/></item><item><title>Liger: Linearizing Large Language Models to Gated Recurrent Structures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01496/</link><pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01496/</guid><description>Liger: LLMs linearized to gated recurrent models, enabling efficient deployment via key matrix repurposing and LoRA fine-tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.01496/cover.png"/></item><item><title>MoM: Linear Sequence Modeling with Mixture-of-Memories</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13685/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13685/</guid><description>MoM: Enhancing linear sequence modeling via mixture-of-memories for improved recall and reduced memory interference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.13685/cover.png"/></item><item><title>LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07563/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07563/</guid><description>LASP-2 revolutionizes linear attention training by achieving 36.6% faster speeds than Ring Attention via a novel sequence parallelism method, boosting efficiency for very long sequences.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07563/cover.png"/></item><item><title>Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/</guid><description>OREAL, a novel RL framework, achieves state-of-the-art mathematical reasoning in LLMs using only binary outcome rewards, demonstrating that a 7B model can match the performance of 32B models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06781/cover.png"/></item><item><title>BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/</guid><description>BoostStep enhances large language models&amp;rsquo; mathematical abilities by refining single-step reasoning through a novel step-level in-context learning strategy, achieving significant improvements on variou&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03226/cover.png"/></item><item><title>Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/</link><pubDate>Thu, 26 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/</guid><description>Task Preference Optimization (TPO) significantly boosts multimodal large language models&amp;rsquo; visual understanding by aligning them with fine-grained visual tasks via learnable task tokens, achieving 14.6&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.19326/cover.png"/></item><item><title>Are Your LLMs Capable of Stable Reasoning?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/</guid><description>G-Pass@k &amp;amp; LiveMathBench: Evaluating the stability of LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13147/cover.png"/></item><item><title>OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/</guid><description>OmniDocBench, a novel benchmark, tackles limitations in current document parsing by introducing a diverse, high-quality dataset with comprehensive annotations, enabling fair multi-level evaluation of &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07626/cover.png"/></item><item><title>OS-ATLAS: A Foundation Action Model for Generalist GUI Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/</link><pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/</guid><description>OS-Atlas: A new open-source toolkit and model dramatically improves GUI agent performance by providing a massive dataset and innovative training methods, enabling superior generalization to unseen int&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.23218/cover.png"/></item></channel></rss>