<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ðŸ”– 2024-10-24 on AI Paper Reviews by AI</title>
    <link>http://localhost:1313/ai-paper-reviewer/tags/-2024-10-24/</link>
    <description>Recent content in ðŸ”– 2024-10-24 on AI Paper Reviews by AI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Â© 2024 AI Paper Reviews by AI</copyright>
    <lastBuildDate>Thu, 24 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/ai-paper-reviewer/tags/-2024-10-24/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/</guid>
      <description>CAMEL-Bench is a new open-source benchmark for evaluating large multimodal models in Arabic.  It addresses the lack of Arabic-centric LMM benchmarks by offering a diverse set of tasks across eight dom&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18976/cover.png" />
    </item>
    
    <item>
      <title>CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/</guid>
      <description>The paper introduces CCI3.0-HQ, a large-scale, high-quality Chinese dataset for pre-training LLMs.  Using a novel two-stage filtering pipeline, CCI3.0-HQ significantly outperforms existing Chinese dat&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18505/cover.png" />
    </item>
    
    <item>
      <title>Data Scaling Laws in Imitation Learning for Robotic Manipulation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/</guid>
      <description>This paper explores data scaling laws in imitation learning for robotic manipulation.  It finds that diverse data from many environments and object types is key to good generalization, following appro&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18647/cover.png" />
    </item>
    
    <item>
      <title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/</guid>
      <description>To mitigate Large Language Model (LLM) hallucinations, DeCoRe contrasts outputs from a base LLM and one with masked retrieval heads (identified as crucial for factual recall), dynamically adjusting co&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18860/cover.png" />
    </item>
    
    <item>
      <title>Distill Visual Chart Reasoning Ability from LLMs to MLLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/</guid>
      <description>Researchers created a new method called Code-as-Intermediary Translation (CIT) to improve multimodal large language models (MLLMs) understanding of charts. CIT uses code to translate visual charts int&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18798/cover.png" />
    </item>
    
    <item>
      <title>Framer: Interactive Frame Interpolation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/</guid>
      <description>Framer is a novel interactive frame interpolation method that lets users customize transitions between two images by manipulating keypoints. It uses a pre-trained video diffusion model and provides bo&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18978/cover.png" />
    </item>
    
    <item>
      <title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/</guid>
      <description>LOGO is a novel training strategy that improves the alignment of long-context models with human preferences by using preference optimization and overcoming GPU memory limitations through a reference-f&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18533/cover.png" />
    </item>
    
    <item>
      <title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/</guid>
      <description>MotionCLR is a novel attention-based diffusion model for human motion generation and editing. It leverages self- and cross-attention mechanisms for fine-grained control, enabling various training-free&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18977/cover.png" />
    </item>
    
    <item>
      <title>Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/</guid>
      <description>Current image watermarking struggles against advanced image editing. This paper introduces W-Bench, a benchmark to evaluate watermarking methods against various editing techniques, and VINE, a new met&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18775/cover.png" />
    </item>
    
    <item>
      <title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/</guid>
      <description>This paper comprehensively evaluates various language model editing methods, finding that they generally cause performance degradation and safety issues, especially when scaling to many edits.  Curren&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18785/cover.png" />
    </item>
    
    <item>
      <title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/</guid>
      <description>This paper presents Skywork-Reward, a novel reward model for LLMs.  It emphasizes data quality over quantity, creating a smaller, meticulously curated dataset using advanced filtering and selection te&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18451/cover.png" />
    </item>
    
    <item>
      <title>SMITE: Segment Me In TimE</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/</guid>
      <description>SMITE is a novel video segmentation method using a pre-trained text-to-image diffusion model with a tracking module and low-frequency regularization.  It achieves temporally consistent segmentations w&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18538/cover.png" />
    </item>
    
    <item>
      <title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/</guid>
      <description>Stable Consistency Tuning (SCT) improves consistency model training by reducing variance and discretization errors, leading to faster convergence and state-of-the-art image generation quality on CIFAR&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18958/cover.png" />
    </item>
    
    <item>
      <title>Taipan: Efficient and Expressive State Space Language Models with Selective Attention</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/</guid>
      <description>Taipan is a new hybrid language model that combines the efficiency of state-space models with the power of selective attention.  It significantly outperforms existing models on long-context tasks, han&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18572/cover.png" />
    </item>
    
    <item>
      <title>The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18441/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18441/</guid>
      <description>This paper presents enhanced mathematical formulations and probabilistic optimization methods for key Transformer model components in generative AI.  It offers novel approaches to subword encoding, hy&amp;hellip;..</description>
      
    </item>
    
    <item>
      <title>Unbounded: A Generative Infinite Game of Character Life Simulation</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/</guid>
      <description>UNBOUNDED is a novel generative infinite game using AI to simulate character life in real-time. It overcomes limitations of traditional games by employing a specialized LLM for dynamic game mechanics &amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18975/cover.png" />
    </item>
    
    <item>
      <title>Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/</guid>
      <description>ScaleQuest is a novel data synthesis method that uses small open-source LLMs to create a large, high-quality mathematical reasoning dataset.  This dataset significantly improves the performance of mai&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18693/cover.png" />
    </item>
    
    <item>
      <title>WAFFLE: Multi-Modal Model for Automated Front-End Development</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/</guid>
      <description>WAFFLE is a new fine-tuning approach for multi-modal language models that significantly improves automated front-end web development by enhancing their understanding of HTML structure and aligning the&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18362/cover.png" />
    </item>
    
    <item>
      <title>Why Does the Effective Context Length of LLMs Fall Short?</title>
      <link>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/</link>
      <pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/</guid>
      <description>Large language models (LLMs) don&amp;rsquo;t use their full context window due to a skewed distribution of positional information during training.  The authors introduce STRING, a training-free method that shif&amp;hellip;..</description>
      <media:content xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:1313/ai-paper-reviewer/paper-reviews/2410.18745/cover.png" />
    </item>
    
  </channel>
</rss>
