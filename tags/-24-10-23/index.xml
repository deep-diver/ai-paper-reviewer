<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ðŸ¤— 24-10-23 on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-23/</link><description>Recent content in ðŸ¤— 24-10-23 on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 22 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-23/index.xml" rel="self" type="application/rss+xml"/><item><title>Aligning Large Language Models via Self-Steering Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</guid><description>New Self-Steering Optimization (SSO) algorithm autonomously generates accurate preference signals for aligning LLMs, eliminating manual annotation and boosting performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/cover.png"/></item><item><title>Frontiers in Intelligent Colonoscopy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</guid><description>This study advances intelligent colonoscopy by creating ColonINST, a large multimodal dataset; ColonGPT, a multimodal language model; and a benchmark, pushing the boundaries of AI in colorectal cancer&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/cover.png"/></item><item><title>JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</guid><description>JMMMU, a new benchmark, rigorously evaluates large multimodal models&amp;rsquo; understanding of Japanese language and culture, revealing critical performance gaps and guiding future development.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/cover.png"/></item><item><title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</guid><description>Math Neurosurgery precisely isolates math reasoning in LLMs using only forward passes, boosting performance without harming other skills.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/cover.png"/></item><item><title>MiniPLM: Knowledge Distillation for Pre-Training Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</guid><description>MINIPLM: Efficiently pre-train smaller, high-performing language models via offline knowledge distillation, boosting performance across diverse tasks and model architectures.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/cover.png"/></item><item><title>PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</guid><description>PyramidDrop boosts Large Vision-Language Model efficiency by 40% in training and 55% in inference, dropping redundant visual tokens in deeper layers while maintaining performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/cover.png"/></item><item><title>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</guid><description>SpectroMotion: groundbreaking 3D reconstruction that accurately renders dynamic specular scenes, outperforming existing methods with its novel combination of 3D Gaussian Splatting, physically-based re&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/cover.png"/></item><item><title>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</guid><description>3DGS-Enhancer boosts 3D Gaussian splatting&amp;rsquo;s novel view synthesis by integrating view-consistent 2D diffusion priors, dramatically improving quality in sparse-view scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/cover.png"/></item><item><title>Improve Vision Language Model Chain-of-thought Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</guid><description>Boosting vision-language model reasoning: This paper proposes a novel two-fold approach using GPT-4-distilled rationales and reinforcement learning to significantly improve chain-of-thought reasoning &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/cover.png"/></item><item><title>LLM-based Optimization of Compound AI Systems: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</guid><description>LLMs are revolutionizing compound AI optimization by efficiently generating complex parameters without gradient computation, enabling end-to-end system tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/cover.png"/></item><item><title>Mitigating Object Hallucination via Concentric Causal Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</guid><description>Concentric Causal Attention (CCA) tackles LVLMs&amp;rsquo; object hallucination by cleverly reducing the distance between visual and instruction tokens, improving multimodal alignment and surpassing existing me&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/cover.png"/></item><item><title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</guid><description>xGen-MM-Vid (BLIP-3-Video) efficiently represents videos using only 32 tokens, achieving state-of-the-art video question answering accuracy with a smaller model size.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/cover.png"/></item><item><title>EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</guid><description>EvoPress: A novel evolutionary algorithm optimizes dynamic LLM compression, surpassing existing methods in accuracy and efficiency.</description></item></channel></rss>