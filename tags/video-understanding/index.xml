<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Video Understanding on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/video-understanding/</link><description>Recent content in Video Understanding on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Wed, 12 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/video-understanding/index.xml" rel="self" type="application/rss+xml"/><item><title>Reangle-A-Video: 4D Video Generation as Video-to-Video Translation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-13/2503.09151/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-13/2503.09151/</guid><description>Reangle-A-Video generates synchronized multi-view videos from a single video via video-to-video translation, surpassing existing methods without specialized 4D training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-13/2503.09151/cover.png"/></item><item><title>TPDiff: Temporal Pyramid Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-13/2503.09566/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-13/2503.09566/</guid><description>TPDiff accelerates video diffusion by progressively increasing frame rates during diffusion, optimizing computational efficiency with a novel stage-wise training strategy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-13/2503.09566/cover.png"/></item><item><title>AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08417/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08417/</guid><description>AnyMoLe: Generate character motion in-between frames for diverse characters by video diffusion models without external data. Code: project page.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08417/cover.png"/></item><item><title>QuoTA: Query-oriented Token Assignment via CoT Query Decouple for Long Video Comprehension</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08689/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08689/</guid><description>QuoTA: Task-aware token assignment boosts long video comprehension in LVLMs via query-decoupled processing, without extra training!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08689/cover.png"/></item><item><title>Tuning-Free Multi-Event Long Video Generation via Synchronized Coupled Sampling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08605/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08605/</guid><description>SynCoS: Synchronized sampling generates high-quality &amp;amp; coherent long videos from text, without extra training!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.08605/cover.png"/></item><item><title>DreamRelation: Relation-Centric Video Customization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07602/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07602/</guid><description>DreamRelation: Personalize videos by customizing relationships between subjects, generalizing to new domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-11/2503.07602/cover.png"/></item><item><title>MagicInfinite: Generating Infinite Talking Videos with Your Words and Voice</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05978/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05978/</guid><description>MagicInfinite: Infinite talking videos from words and voice!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-12/2503.05978/cover.png"/></item><item><title>TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05638/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05638/</guid><description>TrajectoryCrafter: Precisely control camera movement in monocular videos with a novel diffusion model for coherent 4D content generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05638/cover.png"/></item><item><title>VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05639/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05639/</guid><description>VideoPainter: Edit any video, any length, with user-guided instructions!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.05639/cover.png"/></item><item><title>AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04504/</link><pubDate>Thu, 06 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04504/</guid><description>AnyAnomaly: LVLM for customizable zero-shot video anomaly detection, adapting to diverse environments without retraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2503.04504/cover.png"/></item><item><title>Mobius: Text to Seamless Looping Video Generation via Latent Shift</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20307/</link><pubDate>Thu, 27 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20307/</guid><description>Mobius generates seamless looping videos from text using latent shift, repurposing pre-trained models without training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.20307/cover.png"/></item><item><title>VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17258/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17258/</guid><description>VideoGrain: Fine-grained video editing via space-time attention!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17258/cover.png"/></item><item><title>X-Dancer: Expressive Music to Human Dance Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17414/</link><pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17414/</guid><description>X-Dancer: Expressive dance video generation from music and a single image!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.17414/cover.png"/></item><item><title>Dynamic Concepts Personalization from Single Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14844/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14844/</guid><description>Personalizing video models for dynamic concepts is now achievable with Set-and-Sequence: enabling high-fidelity generation, editing, and composition!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.14844/cover.png"/></item><item><title>Intuitive physics understanding emerges from self-supervised pretraining on natural videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/</guid><description>AI models learn intuitive physics from self-supervised video pretraining.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11831/cover.png"/></item><item><title>MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11663/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.11663/</guid><description>MaskGWM: Improves driving world models by using video mask reconstruction for better generalization.</description></item><item><title>Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10248/</link><pubDate>Fri, 14 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10248/</guid><description>Step-Video-T2V: A 30B parameter text-to-video model generating high-quality videos up to 204 frames, pushing the boundaries of video foundation models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.10248/cover.png"/></item><item><title>Enhance-A-Video: Better Generated Video for Free</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07508/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07508/</guid><description>Enhance-A-Video boosts video generation quality without retraining, by enhancing cross-frame correlations in diffusion transformers, resulting in improved coherence and visual fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07508/cover.png"/></item><item><title>Next Block Prediction: Video Generation via Semi-Autoregressive Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07737/</link><pubDate>Tue, 11 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07737/</guid><description>Next-Block Prediction (NBP) revolutionizes video generation by using a semi-autoregressive model that predicts blocks of video content simultaneously, resulting in significantly faster inference.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.07737/cover.png"/></item><item><title>Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06155/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06155/</guid><description>EFFICIENT-VDIT accelerates video generation by 7.8x using sparse attention and multi-step distillation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06155/cover.png"/></item><item><title>Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06782/</link><pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06782/</guid><description>Lumina-Video: Efficient and flexible video generation using a multi-scale Next-DiT architecture with motion control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.06782/cover.png"/></item><item><title>FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/</guid><description>FlashVideo: Generate stunning high-resolution videos efficiently using a two-stage framework prioritizing fidelity and detail, achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05179/cover.png"/></item><item><title>VideoRoPE: What Makes for Good Video Rotary Position Embedding?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/</link><pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/</guid><description>VideoRoPE enhances video processing in Transformer models by introducing a novel 3D rotary position embedding that preserves spatio-temporal relationships, resulting in superior performance across var&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.05173/cover.png"/></item><item><title>Fast Video Generation with Sliding Tile Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04507/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04507/</guid><description>Sliding Tile Attention (STA) boosts video generation speed by 2.43-3.53x without losing quality by exploiting inherent data redundancy in video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04507/cover.png"/></item><item><title>MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04299/</link><pubDate>Thu, 06 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04299/</guid><description>MotionCanvas lets users design cinematic video shots with intuitive controls for camera and object movements, translating scene-space intentions into video animations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.04299/cover.png"/></item><item><title>DynVFX: Augmenting Real Videos with Dynamic Content</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03621/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03621/</guid><description>DynVFX: Effortlessly integrate dynamic content into real videos using simple text prompts. Zero-shot learning and novel attention mechanisms deliver seamless and realistic results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03621/cover.png"/></item><item><title>Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03639/</link><pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03639/</guid><description>This paper introduces PointVid, a 3D-aware video generation framework using 3D point regularization to enhance video realism and address common issues like object morphing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.03639/cover.png"/></item><item><title>VideoJAM: Joint Appearance-Motion Representations for Enhanced Motion Generation in Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/</link><pubDate>Tue, 04 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/</guid><description>VideoJAM enhances video generation by jointly learning appearance and motion representations, achieving state-of-the-art motion coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2502.02492/cover.png"/></item><item><title>EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13452/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13452/</guid><description>EchoVideo generates high-fidelity, identity-preserving videos by cleverly fusing text and image features, overcoming limitations of prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13452/cover.png"/></item><item><title>Improving Video Generation with Human Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13918/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13918/</guid><description>Human feedback boosts video generation! New VideoReward model &amp;amp; alignment algorithms significantly improve video quality and user prompt alignment, exceeding prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13918/cover.png"/></item><item><title>Temporal Preference Optimization for Long-Form Video Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13919/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13919/</guid><description>Boosting long-form video understanding, Temporal Preference Optimization (TPO) enhances video-LLMs by leveraging preference learning. It achieves this through a self-training method using preference &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13919/cover.png"/></item><item><title>Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13826/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13826/</guid><description>Video-MMMU benchmark systematically evaluates Large Multimodal Modelsâ knowledge acquisition from videos across multiple disciplines and cognitive stages, revealing significant gaps between human and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.13826/cover.png"/></item><item><title>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12375/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12375/</guid><description>Video Depth Anything achieves consistent depth estimation for super-long videos by enhancing Depth Anything V2 with a spatial-temporal head and a novel temporal consistency loss, setting a new state-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.12375/cover.png"/></item><item><title>DiffuEraser: A Diffusion Model for Video Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10018/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10018/</guid><description>DiffuEraser: a novel video inpainting model based on stable diffusion, surpasses existing methods by using injected priors and temporal consistency improvements for superior results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.10018/cover.png"/></item><item><title>VideoWorld: Exploring Knowledge Learning from Unlabeled Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/</link><pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/</guid><description>VideoWorld shows AI can learn complex reasoning and planning skills from unlabeled videos alone, achieving professional-level performance in Go and robotics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09781/cover.png"/></item><item><title>Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09019/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09019/</guid><description>Ouroboros-Diffusion: A novel tuning-free long video generation framework achieving unprecedented content consistency by cleverly integrating information across frames via latent sampling, cross-frame&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09019/cover.png"/></item><item><title>RepVideo: Rethinking Cross-Layer Representation for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08994/</link><pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08994/</guid><description>RepVideo enhances text-to-video generation by enriching feature representations, resulting in significantly improved temporal coherence and spatial detail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08994/cover.png"/></item><item><title>Do generative video models learn physical principles from watching videos?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09038/</link><pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09038/</guid><description>Generative video models struggle to understand physics despite producing visually realistic videos; Physics-IQ benchmark reveals this critical limitation, highlighting the need for improved physical r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.09038/cover.png"/></item><item><title>GameFactory: Creating New Games with Generative Interactive Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08325/</link><pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08325/</guid><description>GameFactory uses AI to generate entirely new games within diverse, open-domain scenes by learning action controls from a small dataset and transferring them to pre-trained video models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.08325/cover.png"/></item><item><title>An Empirical Study of Autoregressive Pre-training from Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/</link><pubDate>Thu, 09 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/</guid><description>Toto, a new autoregressive video model, achieves competitive performance across various benchmarks by pre-training on over 1 trillion visual tokens, demonstrating the effectiveness of scaling video mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.05453/cover.png"/></item><item><title>Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/</link><pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/</guid><description>Diffusion as Shader (DaS) achieves versatile video control by using 3D tracking videos as control signals in a unified video diffusion model, enabling precise manipulation across diverse tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03847/cover.png"/></item><item><title>MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02955/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02955/</guid><description>MotionBench, a new benchmark, reveals that existing video models struggle with fine-grained motion understanding. To address this, the authors propose TE Fusion, a novel architecture that improves mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02955/cover.png"/></item><item><title>STAR: Spatial-Temporal Augmentation with Text-to-Video Models for Real-World Video Super-Resolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/</guid><description>STAR: A novel approach uses text-to-video models for realistic, temporally consistent real-world video super-resolution, improving image quality and detail.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02976/cover.png"/></item><item><title>TransPixar: Advancing Text-to-Video Generation with Transparency</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</link><pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/</guid><description>TransPixar generates high-quality videos with transparency by jointly training RGB and alpha channels, outperforming sequential generation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.03006/cover.png"/></item><item><title>GS-DiT: Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</link><pubDate>Sun, 05 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/</guid><description>GS-DiT: Generating high-quality videos with advanced 4D control through efficient dense 3D point tracking and pseudo 4D Gaussian fields.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.02690/cover.png"/></item><item><title>Ingredients: Blending Custom Photos with Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</link><pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/</guid><description>Ingredients: A new framework customizes videos by blending multiple photos with video diffusion transformers, enabling realistic and personalized video generation while maintaining consistent identity&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01790/cover.png"/></item><item><title>SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/</guid><description>SeedVR: A novel diffusion transformer revolutionizes generic video restoration by efficiently handling arbitrary video lengths and resolutions, achieving state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01320/cover.png"/></item><item><title>VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/</guid><description>VideoAnydoor: High-fidelity video object insertion with precise motion control, achieved via an end-to-end framework leveraging an ID extractor and a pixel warper for robust detail preservation and fi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2501.01427/cover.png"/></item><item><title>DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</link><pubDate>Tue, 24 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/</guid><description>DiTCtrl achieves state-of-the-art multi-prompt video generation without retraining by cleverly controlling attention in a diffusion transformer, enabling smooth transitions between video segments.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.18597/cover.png"/></item><item><title>LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</link><pubDate>Thu, 19 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/</guid><description>LeviTor: Revolutionizing image-to-video synthesis with intuitive 3D trajectory control, generating realistic videos from static images by abstracting object masks into depth-aware control points.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.15214/cover.png"/></item><item><title>AniDoc: Animation Creation Made Easier</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</link><pubDate>Wed, 18 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/</guid><description>AniDoc automates cartoon animation line art video colorization, making animation creation easier!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.14173/cover.png"/></item><item><title>MIVE: New Design and Benchmark for Multi-Instance Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/</guid><description>Edit many objects at once in videos! MIVE does it accurately without affecting other areas, a big step for AI video editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.12877/cover.png"/></item><item><title>Move-in-2D: 2D-Conditioned Human Motion Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/</guid><description>Move-in-2D generates realistic human motion sequences conditioned on a 2D scene image and text prompt, overcoming limitations of existing approaches and improving video synthesis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13185/cover.png"/></item><item><title>VidTok: A Versatile and Open-Source Video Tokenizer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</link><pubDate>Tue, 17 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/</guid><description>VidTok: an open-source, top performing video tokenizer.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.13061/cover.png"/></item><item><title>Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/</guid><description>Gaze-LLE achieves state-of-the-art gaze estimation by using a frozen DINOv2 encoder and a lightweight decoder, simplifying architecture and improving efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09586/cover.png"/></item><item><title>InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</link><pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/</guid><description>InstanceCap improves text-to-video generation through detailed, instance-aware captions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.09283/cover.png"/></item><item><title>Mobile Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</guid><description>MobileVD: The first mobile-optimized video diffusion model, achieving 523x efficiency improvement over state-of-the-art with minimal quality loss, enabling realistic video generation on smartphones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/cover.png"/></item><item><title>ObjCtrl-2.5D: Training-free Object Control with Camera Poses</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</guid><description>ObjCtrl-2.5D: Training-free, precise image-to-video object control using 3D trajectories and camera poses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/cover.png"/></item><item><title>Video Motion Transfer with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</guid><description>DiTFlow: training-free video motion transfer using Diffusion Transformers, enabling realistic motion control in synthesized videos via Attention Motion Flow.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/cover.png"/></item><item><title>MoViE: Mobile Diffusion for Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</guid><description>MoViE: Mobile Diffusion for Video Editing achieves 12 FPS video editing on mobile phones by optimizing existing image editing models, achieving a major breakthrough in on-device video processing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/cover.png"/></item><item><title>LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</guid><description>LiFT leverages human feedback, including reasoning, to effectively align text-to-video models with human preferences, significantly improving video quality.</description></item><item><title>Mind the Time: Temporally-Controlled Multi-Event Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</guid><description>MinT: Generating coherent videos with precisely timed, multiple events via temporal control, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/cover.png"/></item><item><title>Mimir: Improving Video Diffusion Models for Precise Text Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</guid><description>Mimir: A novel framework harmonizes LLMs and video diffusion models for precise text understanding in video generation, producing high-quality videos with superior text comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/cover.png"/></item><item><title>OmniCreator: Self-Supervised Unified Generation with Universal Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</guid><description>OmniCreator: Self-supervised unified image+video generation &amp;amp; universal editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/cover.png"/></item><item><title>VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</guid><description>VideoGen-of-Thought (VGoT) creates high-quality, multi-shot videos by collaboratively generating scripts, keyframes, and video clips, ensuring narrative consistency and visual coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/cover.png"/></item><item><title>Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</guid><description>Presto: a novel video diffusion model generates 15-second, high-quality videos with unparalleled long-range coherence and rich content, achieved through a segmented cross-attention mechanism and the L&amp;hellip;</description></item><item><title>PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</guid><description>PhysGame benchmark unveils video LLMs&amp;rsquo; weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/cover.png"/></item><item><title>VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</guid><description>VideoLights: a novel framework for joint video highlight detection &amp;amp; moment retrieval, boosts performance via feature refinement, cross-modal &amp;amp; cross-task alignment, achieving state-of-the-art results&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/cover.png"/></item><item><title>VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</guid><description>VISTA synthesizes long-duration, high-resolution video instruction data, creating VISTA-400K and HRVideoBench to significantly boost video LMM performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/cover.png"/></item><item><title>Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</guid><description>TeaCache: a training-free method boosts video diffusion model speed by up to 4.41x with minimal quality loss by cleverly caching intermediate outputs.</description></item><item><title>Trajectory Attention for Fine-grained Video Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</guid><description>Trajectory Attention enhances video motion control by injecting trajectory information, improving precision and long-range consistency in video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/cover.png"/></item><item><title>Video Depth without Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</guid><description>RollingDepth: Achieving state-of-the-art video depth estimation without using complex video models, by cleverly extending a single-image depth estimator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/cover.png"/></item><item><title>AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</guid><description>AC3D achieves precise 3D camera control in video diffusion transformers by analyzing camera motion&amp;rsquo;s spectral properties, optimizing pose conditioning, and using a curated dataset of dynamic videos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/cover.png"/></item><item><title>TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</guid><description>TAPTRv3 achieves state-of-the-art long-video point tracking by cleverly using spatial and temporal context to enhance feature querying, surpassing previous methods and demonstrating strong performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/cover.png"/></item><item><title>WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</guid><description>WF-VAE boosts video VAE performance with wavelet-driven energy flow and causal caching, enabling 2x higher throughput and 4x lower memory usage in latent video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/cover.png"/></item><item><title>DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</guid><description>DREAMRUNNER generates high-quality storytelling videos by using LLMs for hierarchical planning, motion retrieval, and a novel spatial-temporal region-based diffusion model for fine-grained control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/cover.png"/></item><item><title>Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</guid><description>CoordTok: a novel video tokenizer drastically reduces token count for long videos, enabling memory-efficient training of diffusion models for high-quality, long video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/cover.png"/></item><item><title>MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</guid><description>MagicDriveDiT generates high-resolution, long street-view videos with precise control, exceeding limitations of previous methods in autonomous driving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/cover.png"/></item><item><title>SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</guid><description>SAMURAI enhances the Segment Anything Model 2 for real-time, zero-shot visual object tracking by incorporating motion-aware memory and motion modeling, significantly improving accuracy and robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/cover.png"/></item><item><title>AnimateAnything: Consistent and Controllable Animation for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</guid><description>AnimateAnything: A unified approach enabling precise &amp;amp; consistent video manipulation via a novel optical flow representation and frequency stabilization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/cover.png"/></item><item><title>Number it: Temporal Grounding Videos like Flipping Manga</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</guid><description>Boosting video temporal grounding, NumPro empowers Vid-LLMs by adding frame numbers, making temporal localization as easy as flipping through manga.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/cover.png"/></item><item><title>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</guid><description>EgoVid-5M: First high-quality dataset for egocentric video generation, enabling realistic human-centric world simulations.</description></item><item><title>Sharingan: Extract User Action Sequence from Desktop Recordings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</guid><description>Sharingan extracts user action sequences from desktop recordings using novel VLM-based methods, achieving 70-80% accuracy and enabling RPA.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/cover.png"/></item><item><title>ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</guid><description>ReCapture generates videos with novel camera angles from user videos using masked video fine-tuning, preserving scene motion and plausibly hallucinating unseen parts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/cover.png"/></item><item><title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</guid><description>Adaptive Caching (AdaCache) dramatically speeds up video generation with diffusion transformers by cleverly caching and reusing computations, tailoring the process to each video&amp;rsquo;s complexity and motio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/cover.png"/></item><item><title>How Far is Video Generation from World Model: A Physical Law Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</guid><description>Scaling video generation models doesn&amp;rsquo;t guarantee they&amp;rsquo;ll learn physics; this study reveals they prioritize visual cues over true physical understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/cover.png"/></item><item><title>Learning Video Representations without Natural Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</guid><description>High-performing video representation models can be trained using only synthetic videos and images, eliminating the need for large natural video datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/cover.png"/></item></channel></rss>