<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Video Understanding on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/video-understanding/</link><description>Recent content in Video Understanding on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 10 Dec 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/video-understanding/index.xml" rel="self" type="application/rss+xml"/><item><title>Mobile Video Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/</guid><description>MobileVD: The first mobile-optimized video diffusion model, achieving 523x efficiency improvement over state-of-the-art with minimal quality loss, enabling realistic video generation on smartphones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07583/cover.png"/></item><item><title>ObjCtrl-2.5D: Training-free Object Control with Camera Poses</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/</guid><description>ObjCtrl-2.5D: Training-free, precise image-to-video object control using 3D trajectories and camera poses.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07721/cover.png"/></item><item><title>Video Motion Transfer with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</link><pubDate>Tue, 10 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/</guid><description>DiTFlow: training-free video motion transfer using Diffusion Transformers, enabling realistic motion control in synthesized videos via Attention Motion Flow.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.07776/cover.png"/></item><item><title>MoViE: Mobile Diffusion for Video Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</link><pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/</guid><description>MoViE: Mobile Diffusion for Video Editing achieves 12 FPS video editing on mobile phones by optimizing existing image editing models, achieving a major breakthrough in on-device video processing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.06578/cover.png"/></item><item><title>LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.04814/</guid><description>LiFT leverages human feedback, including reasoning, to effectively align text-to-video models with human preferences, significantly improving video quality.</description></item><item><title>Mind the Time: Temporally-Controlled Multi-Event Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</link><pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/</guid><description>MinT: Generating coherent videos with precisely timed, multiple events via temporal control, surpassing existing methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.05263/cover.png"/></item><item><title>Mimir: Improving Video Diffusion Models for Precise Text Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</link><pubDate>Wed, 04 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/</guid><description>Mimir: A novel framework harmonizes LLMs and video diffusion models for precise text understanding in video generation, producing high-quality videos with superior text comprehension.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.03085/cover.png"/></item><item><title>OmniCreator: Self-Supervised Unified Generation with Universal Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/</guid><description>OmniCreator: Self-supervised unified image+video generation &amp;amp; universal editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02114/cover.png"/></item><item><title>VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</link><pubDate>Tue, 03 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/</guid><description>VideoGen-of-Thought (VGoT) creates high-quality, multi-shot videos by collaboratively generating scripts, keyframes, and video clips, ensuring narrative consistency and visual coherence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.02259/cover.png"/></item><item><title>Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01316/</guid><description>Presto: a novel video diffusion model generates 15-second, high-quality videos with unparalleled long-range coherence and rich content, achieved through a segmented cross-attention mechanism and the L&amp;hellip;</description></item><item><title>PhysGame: Uncovering Physical Commonsense Violations in Gameplay Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/</guid><description>PhysGame benchmark unveils video LLMs&amp;rsquo; weaknesses in understanding physical commonsense from gameplay videos, prompting the creation of PhysVLM, a knowledge-enhanced model that outperforms existing mo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01800/cover.png"/></item><item><title>VideoLights: Feature Refinement and Cross-Task Alignment Transformer for Joint Video Highlight Detection and Moment Retrieval</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</link><pubDate>Mon, 02 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/</guid><description>VideoLights: a novel framework for joint video highlight detection &amp;amp; moment retrieval, boosts performance via feature refinement, cross-modal &amp;amp; cross-task alignment, achieving state-of-the-art results&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.01558/cover.png"/></item><item><title>VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</link><pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/</guid><description>VISTA synthesizes long-duration, high-resolution video instruction data, creating VISTA-400K and HRVideoBench to significantly boost video LMM performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2412.00927/cover.png"/></item><item><title>Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19108/</guid><description>TeaCache: a training-free method boosts video diffusion model speed by up to 4.41x with minimal quality loss by cleverly caching intermediate outputs.</description></item><item><title>Trajectory Attention for Fine-grained Video Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/</guid><description>Trajectory Attention enhances video motion control by injecting trajectory information, improving precision and long-range consistency in video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19324/cover.png"/></item><item><title>Video Depth without Video Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</link><pubDate>Thu, 28 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/</guid><description>RollingDepth: Achieving state-of-the-art video depth estimation without using complex video models, by cleverly extending a single-image depth estimator.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.19189/cover.png"/></item><item><title>AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/</guid><description>AC3D achieves precise 3D camera control in video diffusion transformers by analyzing camera motion&amp;rsquo;s spectral properties, optimizing pose conditioning, and using a curated dataset of dynamic videos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18673/cover.png"/></item><item><title>TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/</guid><description>TAPTRv3 achieves state-of-the-art long-video point tracking by cleverly using spatial and temporal context to enhance feature querying, surpassing previous methods and demonstrating strong performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.18671/cover.png"/></item><item><title>WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</link><pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/</guid><description>WF-VAE boosts video VAE performance with wavelet-driven energy flow and causal caching, enabling 2x higher throughput and 4x lower memory usage in latent video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.17459/cover.png"/></item><item><title>DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</link><pubDate>Mon, 25 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/</guid><description>DREAMRUNNER generates high-quality storytelling videos by using LLMs for hierarchical planning, motion retrieval, and a novel spatial-temporal region-based diffusion model for fine-grained control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.16657/cover.png"/></item><item><title>Efficient Long Video Tokenization via Coordinated-based Patch Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</link><pubDate>Fri, 22 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/</guid><description>CoordTok: a novel video tokenizer drastically reduces token count for long videos, enabling memory-efficient training of diffusion models for high-quality, long video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.14762/cover.png"/></item><item><title>MagicDriveDiT: High-Resolution Long Video Generation for Autonomous Driving with Adaptive Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</link><pubDate>Thu, 21 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/</guid><description>MagicDriveDiT generates high-resolution, long street-view videos with precise control, exceeding limitations of previous methods in autonomous driving.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.13807/cover.png"/></item><item><title>SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</link><pubDate>Mon, 18 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/</guid><description>SAMURAI enhances the Segment Anything Model 2 for real-time, zero-shot visual object tracking by incorporating motion-aware memory and motion modeling, significantly improving accuracy and robustness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.11922/cover.png"/></item><item><title>AnimateAnything: Consistent and Controllable Animation for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</link><pubDate>Sat, 16 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/</guid><description>AnimateAnything: A unified approach enabling precise &amp;amp; consistent video manipulation via a novel optical flow representation and frequency stabilization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10836/cover.png"/></item><item><title>Number it: Temporal Grounding Videos like Flipping Manga</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</link><pubDate>Fri, 15 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/</guid><description>Boosting video temporal grounding, NumPro empowers Vid-LLMs by adding frame numbers, making temporal localization as easy as flipping through manga.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.10332/cover.png"/></item><item><title>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08380/</guid><description>EgoVid-5M: First high-quality dataset for egocentric video generation, enabling realistic human-centric world simulations.</description></item><item><title>Sharingan: Extract User Action Sequence from Desktop Recordings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</link><pubDate>Wed, 13 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/</guid><description>Sharingan extracts user action sequences from desktop recordings using novel VLM-based methods, achieving 70-80% accuracy and enabling RPA.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.08768/cover.png"/></item><item><title>ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</link><pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/</guid><description>ReCapture generates videos with novel camera angles from user videos using masked video fine-tuning, preserving scene motion and plausibly hallucinating unseen parts.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.05003/cover.png"/></item><item><title>Adaptive Caching for Faster Video Generation with Diffusion Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/</guid><description>Adaptive Caching (AdaCache) dramatically speeds up video generation with diffusion transformers by cleverly caching and reusing computations, tailoring the process to each video&amp;rsquo;s complexity and motio&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02397/cover.png"/></item><item><title>How Far is Video Generation from World Model: A Physical Law Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</link><pubDate>Mon, 04 Nov 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/</guid><description>Scaling video generation models doesn&amp;rsquo;t guarantee they&amp;rsquo;ll learn physics; this study reveals they prioritize visual cues over true physical understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2411.02385/cover.png"/></item><item><title>Learning Video Representations without Natural Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</link><pubDate>Thu, 31 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/</guid><description>High-performing video representation models can be trained using only synthetic videos and images, eliminating the need for large natural video datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.24213/cover.png"/></item></channel></rss>