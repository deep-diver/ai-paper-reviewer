<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ðŸ¤— 24-10-21 on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-21/</link><description>Recent content in ðŸ¤— 24-10-21 on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Fri, 18 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/tags/-24-10-21/index.xml" rel="self" type="application/rss+xml"/><item><title>Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</guid><description>AI-generated text detection models struggle in real-world scenarios due to flawed training datasets; this paper provides a systematic review and proposes robust evaluation methods for these datasets.</description></item><item><title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</guid><description>BiGR: a novel conditional image generation model using compact binary codes, unifying generative and discriminative tasks for improved visual representation and zero-shot generalization across multipl&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/cover.png"/></item><item><title>How Do Training Methods Influence the Utilization of Vision Models?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</guid><description>Training methods dramatically alter which parts of a vision model are actually used for decisions, revealing surprising variations in layer importance across different training techniques.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/cover.png"/></item><item><title>Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</guid><description>Montessori-Instruct optimizes synthetic data generation for LLMs by aligning teacher models with student learning preferences, significantly improving student model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/cover.png"/></item><item><title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</guid><description>NaturalBench: a new benchmark reveals vision-language models struggle with simple, natural images and questions, highlighting biases and prompting development of more robust models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/cover.png"/></item><item><title>Teaching Models to Balance Resisting and Accepting Persuasion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</guid><description>LLMs can be easily persuaded; this paper introduces Persuasion-Balanced Training (PBT) to make them both resistant to misinformation and receptive to helpful influence, leading to improved performance&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/cover.png"/></item><item><title>DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</guid><description>DAWN: a novel non-autoregressive diffusion framework for all-at-once generation of dynamic talking head videos, achieving higher quality and speed than autoregressive methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/cover.png"/></item><item><title>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</guid><description>Diffusion Curriculum Learning (DisCL) generates high-quality synthetic data via image-guided diffusion, significantly boosting accuracy in long-tail and low-quality data classification.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/cover.png"/></item><item><title>DPLM-2: A Multimodal Diffusion Protein Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</guid><description>DPLM-2: a new multimodal model revolutionizes protein design by simultaneously generating protein sequences and 3D structures, surpassing existing methods in accuracy and diversity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/cover.png"/></item><item><title>FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</guid><description>FiTv2, an enhanced vision transformer, enables efficient and high-quality image generation at arbitrary resolutions and aspect ratios, surpassing existing diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/cover.png"/></item><item><title>Looking Inward: Language Models Can Learn About Themselves by Introspection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</guid><description>Language models can learn about themselves through introspection, outperforming other models in self-prediction tasks, showcasing a surprising new capability and challenging prevailing assumptions abo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/cover.png"/></item><item><title>MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</guid><description>MagicTailor empowers text-to-image models with component-level control, enabling precise customization of generated images by modifying specific visual elements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/cover.png"/></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</guid><description>SeerAttention learns to automatically identify and leverage inherent attention sparsity in LLMs, drastically boosting inference speed and scalability while maintaining accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/cover.png"/></item><item><title>UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</guid><description>UCFE benchmark realistically evaluates LLMs&amp;rsquo; financial expertise via user-centric design and dynamic interactions, revealing performance gaps and highlighting the need for more robust, human-aligned m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/cover.png"/></item><item><title>Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</guid><description>Boosting LLM-based web agents&amp;rsquo; performance, this study introduces World-Model-Augmented agents that simulate action outcomes for improved decision-making in complex web navigation tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/cover.png"/></item><item><title>Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</guid><description>KeyNMF, a novel topic modeling method, reveals how Chinese diaspora media&amp;rsquo;s information dynamics correlate with major European political events, highlighting the PRC&amp;rsquo;s potential influence.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/cover.png"/></item><item><title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</guid><description>Mini-Omni2: An open-source, multi-modal AI model closely replicating GPT-40&amp;rsquo;s vision, speech, and text capabilities, offering valuable insights for future research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/cover.png"/></item><item><title>SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</guid><description>Shakti: a 2.5B parameter LLM optimized for edge AI, boasts high performance and efficiency on resource-constrained devices via novel VGQA, SwiGLU, and RoPE.</description></item><item><title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</guid><description>HART, a novel hybrid autoregressive transformer, generates high-quality 1024x1024 images efficiently, rivaling diffusion models while being significantly faster.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/cover.png"/></item></channel></rss>