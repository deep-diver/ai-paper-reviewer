<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-01-28s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/</link><description>Recent content in 2025-01-28s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 27 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/index.xml" rel="self" type="application/rss+xml"/><item><title>Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15907/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15907/</guid><description>Emilia-Pipe and its resulting datasets, Emilia and Emilia-Large, offer the largest open-source, multilingual speech corpus, enabling more natural and spontaneous AI speech generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15907/cover.png"/></item><item><title>ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language Model Born from Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15570/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15570/</guid><description>ARWKV: A novel RNN-attention-based language model, distilled from a larger model, achieves strong performance using significantly fewer resources, opening a new path in efficient language model develo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15570/cover.png"/></item><item><title>Baichuan-Omni-1.5 Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15368/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15368/</guid><description>Baichuan-Omni-1.5: An open-source omni-modal LLM achieving SOTA performance across multiple modalities.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15368/cover.png"/></item><item><title>iFormer: Integrating ConvNet and Transformer for Mobile Application</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15369/</link><pubDate>Sun, 26 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15369/</guid><description>iFormer: A new family of mobile hybrid vision networks that expertly blends ConvNeXt&amp;rsquo;s fast local feature extraction with the efficient global modeling of self-attention, achieving top-tier accuracy a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-28/2501.15369/cover.png"/></item></channel></rss>