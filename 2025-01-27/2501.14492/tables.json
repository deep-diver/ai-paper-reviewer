[{"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" id=\"S1.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.1.1.1\">Benchmark</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.2.1.1.1\">Output</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.2.1.2.1\">Format</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.3\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.3.1.1.1\">Critique</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.3.1.2.1\">Type</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.4\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.4.1.1.1\">Evaluation</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.4.1.2.1\">Method</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.5\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.5.1.1.1\">Iterative</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.5.1.2.1\">Support</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.6\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.6.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.6.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.6.1.1.1\">Human</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.6.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.6.1.2.1\">Anno.</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.7\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.7.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.7.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.7.1.1.1\">Test Data</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.7.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.7.1.2.1\">Size</td>\n</tr>\n</table>\n</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\" id=\"S1.T1.1.1.1.8\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.8.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.8.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.8.1.1.1\">Public</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.8.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.8.1.2.1\">Release</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S1.T1.1.1.2.1\">CriticBench-Google</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.2\">C+V</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.3\">Self+Cross</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.4\">Verdict Judging</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.5\">\u2717</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.6\">\u2717</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.7\">0</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.2.8\">\u2717</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S1.T1.1.1.3.1\">CriticBench-THU</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.2\">C+V</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.3\">Cross</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.4\">Verdict Judging</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.5\">\u2717</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.6\">\u2713</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.3.7\">3,825</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.3.8\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S1.T1.1.1.4.1\">CriticEval</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.2\">C+S</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.3\">Cross</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.4\">Score Judging</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.5\">\u2717</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.6\">\u2717</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.4.7\">3,608</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.4.8\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S1.T1.1.1.5.1\">Shepherd</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.2\">C</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.3\">Cross</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.4\">GPT-4 Comparison</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.5\">\u2717</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.6\">\u2717</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.5.7\">352</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.5.8\">\u2717</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S1.T1.1.1.6.1\">Auto-J</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.2\">C</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.3\">Cross</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.4\">GPT-4 Comparison</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.5\">\u2717</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.6\">\u2717</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S1.T1.1.1.6.7\">232</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.6.8\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S1.T1.1.1.7.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S1.T1.1.1.7.1.1\">RealCritic</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.7.2\">C+V+Corr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.7.3\">Self+Cross</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.7.4\">Correction Matching</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.7.5\">\u2713</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.7.6\">\u2717</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.7.7\">2,093</td>\n<td class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S1.T1.1.1.7.8\">\u2713</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" colspan=\"8\" id=\"S1.T1.1.1.8.1\">Note: C = Critique, V = Verdict, S = Score, Corr = Correction. \u2713= Yes, \u2717 = No.</td>\n</tr>\n</table>", "caption": "Table 1: Comparison of critique evaluation approaches. Our RealCritic framework introduces several key innovations while providing a large-scale, publicly available benchmark for comprehensive critique ability evaluation.", "description": "Table 1 compares different approaches to evaluating the quality of critiques generated by large language models (LLMs).  It highlights key features of each approach, such as the type of output (critique, verdict, score, correction), the type of critique (self, cross, iterative), the evaluation method used, and whether the benchmark offers iterative support, a specified test data size, and public data release.  It showcases the novel aspects of the RealCritic framework which includes a large-scale publicly available benchmark and a comprehensive evaluation of critique abilities.", "section": "1 Introduction"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.2.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.2.1.1.1\">Output</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.2.1.2.1\">Format</td>\n</tr>\n</table>", "caption": "Table 2: Quality analysis of critiques in CriticBench-THU. All values are in percentages (%). Despite achieving correct verdicts in many cases (55.4%), the majority of critiques (68.8%) are still of low quality, highlighting the challenge in generating effective critiques.", "description": "Table 2 presents a detailed analysis of the critique quality within the CriticBench-THU dataset.  Despite a high accuracy rate (55.4%) in predicting whether a given solution is correct or incorrect, the overall quality of the critiques themselves is rather low (68.8% are classified as low quality). This discrepancy highlights the difficulty of generating critiques that are both accurate and truly helpful in improving the quality of solutions.", "section": "2 Toward Effectiveness-Driven Evaluation"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.3.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.3.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.3.1.1.1\">Critique</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.3.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.3.1.2.1\">Type</td>\n</tr>\n</table>", "caption": "Table 3: The total number of questions in each subset of RealCritic. Along with how many questions come with incorrect solutions and how many come with correct solutions.", "description": "Table 3 shows the breakdown of questions within the RealCritic benchmark dataset.  It details the total number of questions in each of the eight subsets, categorized by whether the associated solution is correct or incorrect. This provides insight into the dataset's composition and balance of problem types.", "section": "3.3 Dataset Collection"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.4.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.4.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.4.1.1.1\">Evaluation</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.4.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.4.1.2.1\">Method</td>\n</tr>\n</table>", "caption": "Table 4: Performance comparison across different evaluation modes. Direct Solution represents the model\u2019s ability to solve problems directly. Self-Critique shows performance when the model critiques its own solutions. Cross-Model Critique indicates performance when critiquing other models\u2019 solutions. \u0394(Self vs. Direct) shows the improvement from self-critique over direct solution. \u0394(Cross vs. Random) shows how much better the model performs compared to random chance (50%) in cross-model critique tasks. Gray rows highlight the delta metrics. All numbers are in percentages (%).", "description": "Table 4 presents a comprehensive comparison of various large language models (LLMs) across three different evaluation modes: direct problem-solving, self-critique, and cross-model critique.  For each model, the table shows the percentage accuracy achieved in each mode.  The \u0394(Self vs. Direct) column quantifies the performance improvement (or decline) observed when the model self-critiques its own solutions compared to directly solving the problem. The \u0394(Cross vs. Random) column reveals the improvement (or decline) of the model's cross-model critique performance compared to a random baseline of 50%.  Gray rows highlight these delta metrics for easier interpretation.  All values are expressed as percentages.", "section": "4.2 Results and Analysis"}, {"content": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S1.T1.1.1.1.5.1\">\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.5.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.5.1.1.1\">Iterative</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S1.T1.1.1.1.5.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_center\" id=\"S1.T1.1.1.1.5.1.2.1\">Support</td>\n</tr>\n</table>", "caption": "Table 5: Performance comparison of O1-mini and Qwen2.5-72B-Instruct across different evaluation metrics", "description": "This table presents a detailed comparison of the performance of the O1-mini and Qwen2.5-72B-Instruct large language models across various critique evaluation metrics.  It shows how each model performs on self-critique tasks (evaluating its own generated solutions), cross-critique tasks (evaluating solutions generated by other models), and direct chain-of-thought generation (solving the problem without critique).  The results are broken down by dataset, highlighting the strengths and weaknesses of each model in different scenarios. The performance is measured as the percentage of correct solutions or as the delta between the performances with/without critiques, offering a comprehensive view of their critique abilities.", "section": "4.2 Results and Analysis"}]