[{"figure_path": "2410.17891/tables/table_5_0.md", "caption": "Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models.", "description": "Table 1 presents a comprehensive evaluation of various diffusion language models and their autoregressive counterparts.  It compares models of different sizes (127M, 355M, 1.3B, and 7B parameters) and types (Autoregressive (AR), Discrete Diffusion (DD), and Continuous Diffusion (CD)).  The models are assessed across several tasks: Question Answering (TrivQA), word prediction (Lambada), common sense reasoning (HellaSwag, Winogrande, SIQA, PIQA), math problem solving (GSM8K - requiring finetuning), story infilling (ROCStories), and code infilling.  Performance is measured using accuracy for most tasks, and ROUGE scores for infilling. The table highlights the best-performing diffusion models for each task and indicates when a diffusion model outperforms its corresponding autoregressive counterpart.", "section": "4.2 EVALUATION SETUP"}, {"figure_path": "2410.17891/tables/table_7_0.md", "caption": "Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models.", "description": "This table presents a comprehensive evaluation of various diffusion language models (DLMs) and their autoregressive (AR) counterparts.  It compares the performance of these models across multiple tasks, including question answering (TrivQA, PIQA), word prediction (Lambada), common sense reasoning (HellaSwag, Winogrande, SIQA), math problem solving (GSM8K), story infilling (ROCStories), and code generation.  The table shows model size, model type (AR, DD, or CD for discrete or continuous diffusion), and the performance metrics (accuracy or ROUGE score) for each task.  The results highlight that larger DLMs generally perform better and that, in some cases, DLMs surpass the performance of their AR counterparts, particularly on infilling tasks.", "section": "4.2 Evaluation Setup"}, {"figure_path": "2410.17891/tables/table_8_0.md", "caption": "Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models.", "description": "The table presents a comprehensive evaluation of various diffusion language models and their autoregressive counterparts, all trained with comparable parameter scales.  It compares performance across several tasks, including question answering (TriQA, PIQA), word prediction (Lambada), commonsense reasoning (HellaSwag, Winogrande, SIQA), mathematical reasoning (GSM8K), story infilling (ROCStories), and code generation.  Model types are categorized as autoregressive (AR), discrete diffusion (DD), and continuous diffusion (CD).  Performance metrics vary by task (accuracy for most, ROUGE scores for infilling), and zero-shot performance is primarily shown, with the exception of GSM8K, where models are fine-tuned.  The table highlights the best-performing diffusion models for each task and notes when a diffusion model outperforms its autoregressive counterpart.", "section": "4.2 EVALUATION SETUP"}, {"figure_path": "2410.17891/tables/table_9_0.md", "caption": "Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models.", "description": "Table 1 presents a comprehensive evaluation of various diffusion language models and their autoregressive counterparts.  It compares models of different sizes (127M, 355M, 1.3B, and 7B parameters) and types (autoregressive (AR), discrete diffusion (DD), and continuous diffusion (CD)). The table assesses model performance across several tasks: TriviaQA (question answering), Lambada (word prediction), HellaSwag, Winogrande, SIQA, and PIQA (common sense reasoning), GSM8K (mathematics reasoning - finetuned), ROCStories (story infilling), and code infilling.  Evaluation metrics include accuracy for most tasks and ROUGE-1/2/L scores for infilling.  The table highlights the best-performing diffusion models for each task and notes when a diffusion model outperforms its corresponding autoregressive counterpart.", "section": "4.2 EVALUATION SETUP"}, {"figure_path": "2410.17891/tables/table_20_0.md", "caption": "Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models.", "description": "Table 1 presents a comprehensive evaluation of various diffusion language models and their autoregressive counterparts, all trained with comparable parameter scales.  The models are categorized into three types: autoregressive (AR), discrete diffusion (DD), and continuous diffusion (CD).  The table assesses model performance across a range of tasks, including question answering (TriQA), word prediction (Lambada), commonsense reasoning (HellaSwag, Winogrande, SIQA, PIQA), math problem solving (GSM8K*), story infilling (ROCStories), and code generation.  Evaluation metrics vary depending on the task: accuracy (%) for most tasks and ROUGE-1/2/L scores for the infilling tasks. GSM8K* results indicate that finetuning was performed on the models for that specific task, whereas all other results represent zero-shot performance.  Bold text highlights the best-performing diffusion model for each task, and underlined results denote cases where the diffusion model outperforms its autoregressive equivalent.", "section": "4.2 EVALUATION SETUP"}, {"figure_path": "2410.17891/tables/table_22_0.md", "caption": "Comprehensive evaluation of different diffusion language models and the same scale pre-trained autoregressive models. There are 3 types of these models: AR for autoregressive, DD for discrete diffusion and CD for continuous diffusion. For the infilling task, we use ROUGE-1/2/L score; for other tasks, we use the accuracy (%) metric. * indicates we finetune GSM8K on models; other tasks are all in zero-shot setting. Numbers in the () indicate that AR models are only given prefix for infilling tasks. We bold the best performance among diffusion language models and underline results that surpass their base models.", "description": "Table 1 presents a comparative evaluation of various diffusion language models and their autoregressive counterparts, all trained using the same amount of data.  It includes autoregressive (AR), discrete diffusion (DD), and continuous diffusion (CD) models of different sizes (127M, 355M, 1.3B, and 7B parameters).  The table evaluates model performance across multiple tasks: TriviaQA and Lambada (language modeling), HellaSwag, Winogrande, SIQA, and PIQA (commonsense reasoning), GSM8K (mathematics reasoning, with finetuning), ROCStories and Code (infilling).  Performance is measured using accuracy for most tasks and ROUGE-1/2/L scores for infilling. The table highlights the best-performing diffusion models for each task and indicates when a diffusion model outperforms its autoregressive counterpart.", "section": "4.2 EVALUATION SETUP"}]