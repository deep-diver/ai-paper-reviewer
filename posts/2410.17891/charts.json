[{"figure_path": "2410.17891/charts/charts_6_0.png", "caption": "Training Loss", "description": "The chart displays the training loss curves for three different sized diffusion language models: DiffuGPT-127M, DiffuGPT-355M, and DiffuLLaMA-7B.  The x-axis represents the amount of training tokens in billions, while the y-axis shows the training loss.  The curves reveal that the training loss decreases as the number of training tokens increases for all three models. DiffuLLaMA-7B exhibits a lower initial loss compared to the other two models and converges to a lower loss, suggesting improved performance with increased model size and training data. DiffuGPT-127M shows a higher initial loss but plateaus at a higher loss value than the other two. DiffuGPT-355M's loss curve falls between the other two, reflecting its intermediate size and performance.", "section": "4.1 ADAPTATION SETUP"}, {"figure_path": "2410.17891/charts/charts_8_0.png", "caption": "Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity.", "description": "The chart displays the quality of unconditionally generated text by various diffusion language models, evaluating both perplexity (measured using GPT-2 large) and distinct 2-gram diversity.  It shows the perplexity and diversity for different models (SEDD-S, SEDD-M, DiffuGPT-S, DiffuGPT-M, Plaid1B, and GPT2-M) at varying decoding steps (32, 64, 128, 256, 512, and 1024).  Lower perplexity suggests better fluency, while higher distinct 2-gram diversity indicates more varied text generation. The chart illustrates the tradeoff between perplexity and diversity, with increased decoding steps generally leading to lower perplexity but potentially less diversity.", "section": "Unconditional Generation"}, {"figure_path": "2410.17891/charts/charts_9_0.png", "caption": "Single batch decoding speed (seconds) for different models using flash-attention 2.", "description": "The chart displays the single batch decoding time in seconds for different language models and varying generation lengths.  The models compared are LLaMA2 and DiffuLLaMA with different diffusion timesteps (T = 64, 128, 256, 512).  The generation lengths are 512, 1024, and 2048 tokens.  The chart shows that DiffuLLaMA generally has faster decoding times than LLaMA2, especially for longer generation sequences, and that increasing the diffusion timesteps (T) in DiffuLLaMA increases the decoding time, while decreasing T significantly reduces decoding time.", "section": "4.3 LANGUAGE MODELING CAPACITIES"}, {"figure_path": "2410.17891/charts/charts_22_0.png", "caption": "Quality evaluation for unconditional generation, with perplexity measured by GPT2 large and distinct 2-gram diversity.", "description": "The chart displays the results of an evaluation of unconditional text generation quality using two metrics: generative perplexity (measured using GPT-2 large) and distinct 2-gram diversity.  It compares different diffusion models (DiffuGPT-S, DiffuGPT-M, Plaid1B, GPT2-M, MD4-S, MD4-M) across various decoding step counts (from 32 to 1024). Generative perplexity generally decreases while distinct 2-gram diversity slightly increases as the number of decoding steps increases. The chart indicates that the larger DiffuGPT models outperform other models in terms of both perplexity and diversity, particularly at lower step counts.  The chart also displays the distinct 2-gram diversity which reflects the text generation diversity.", "section": "Unconditional Generation"}, {"figure_path": "2410.17891/charts/charts_22_1.png", "caption": "Figure 6: Finetune GSM8K data with discrete diffusion objectives, using a base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains a lower loss.", "description": "This chart is a pair of line graphs, showing the training loss over training steps for both GPT2 and DiffuGPT models.  Each graph represents a different model size (small and medium).  The x-axis of each graph represents the number of training steps, while the y-axis represents the training loss.  Both graphs demonstrate that DiffuGPT converges to a lower training loss more rapidly than GPT2 for both small and medium model sizes.", "section": "4.5 DISCUSSIONS"}]