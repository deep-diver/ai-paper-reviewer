{"references": [{" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLaMA, a powerful and efficient large language model that serves as a foundation for the authors' work.  Adapting LLaMA to a diffusion model is a significant contribution, demonstrating the potential of diffusion models to compete with leading autoregressive models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This highly influential paper explores methods for aligning language models with human preferences and instructions, a crucial aspect for enhancing the capabilities of both autoregressive and diffusion models.  Its techniques can be applied to improve the performance and alignment of adapted diffusion models.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Jacob Austin", "paper_title": "Structured denoising diffusion models in discrete state-spaces", "reason": "This paper is foundational to the understanding of discrete diffusion models, which are crucial for text generation. The authors' work builds upon this foundation by adapting autoregressive models to the discrete diffusion framework, leading to improvements in efficiency and scaling.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Jascha Sohl-Dickstein", "paper_title": "Deep unsupervised learning using nonequilibrium thermodynamics", "reason": "This seminal paper introduces the core concepts of diffusion models, providing a fundamental theoretical basis for the authors' work.  Its concepts of forward and reverse processes form the foundation of the adaptation strategy, laying the groundwork for adapting autoregressive models into diffusion models.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Yang Song", "paper_title": "Generative modeling by estimating gradients of the data distribution", "reason": "This paper provides a fundamental theoretical framework for understanding generative models, particularly the concept of estimating the gradient of the data distribution.  This concept is central to the training process of diffusion models, which the authors adapt and build upon.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Diederik Kingma", "paper_title": "Variational diffusion models", "reason": "This paper provides a mathematical foundation for diffusion models, focusing on the variational approach to training. The evidence lower bound (ELBO) derivation in this paper is referenced in the preliminary section and provides the theoretical basis for the training methodology used in the authors' work.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Diffusion-lm improves controllable text generation", "reason": "This paper focuses on controllable text generation, a key advantage of diffusion models. By adapting autoregressive models, the authors aim to improve controllability and extend these capabilities in DLMs, building on the previous work in continuous diffusion models for text generation.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Jonathan Ho", "paper_title": "Denoising diffusion probabilistic models", "reason": "This paper is a seminal work in diffusion models, introducing the core concepts and framework. The authors' work directly builds upon these concepts, extending them to the text domain and adapting the training process for efficient conversion from autoregressive models.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Alexander Quinn Nichol", "paper_title": "Improved denoising diffusion probabilistic models", "reason": "This paper introduces key improvements to the denoising diffusion models, enhancing their performance and efficiency. These improvements directly benefit the adaptation process described in the paper, making it more efficient and effective in generating high-quality text.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This foundational paper introduced the Transformer architecture, a key component of modern language models. The authors leverage the Transformer architecture in their adaptation process, making it a crucial component of their methodology.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Ishaan Gulrajani", "paper_title": "Likelihood-based diffusion language models", "reason": "This paper is directly related to the scaling challenges in diffusion models for text. It provides valuable insights into the resource requirements for training diffusion models, contextualizing the authors' innovative adaptation approach.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Aaron Lou", "paper_title": "Discrete diffusion language modeling by estimating the ratios of the data distribution", "reason": "This paper focuses on discrete diffusion models, which are particularly relevant to text data. The authors build on this work by adapting existing large autoregressive language models to the discrete diffusion framework, leading to significant performance improvements.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Jiaxin Shi", "paper_title": "Simplified and generalized masked diffusion for discrete data", "reason": "This paper presents improvements to discrete diffusion models, making them more efficient and suitable for text generation.  The authors' approach builds on these improvements by adapting pre-trained models, resulting in a more efficient training process compared to training from scratch.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama 2: Open foundation and fine-tuned chat models", "reason": "This paper introduces LLaMA 2, a significant improvement upon the original LLaMA model, providing a more robust and powerful foundation for the authors' adaptation work. Using LLaMA 2 as a base model enhances the resulting diffusion model's capabilities.", "section_number": 4}, {" publication_date": "2019", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This seminal paper established the effectiveness of large language models in few-shot learning settings.  The authors' work builds on this finding by adapting pre-trained AR models to diffusion models, demonstrating that adapted diffusion models can also achieve strong performance in various few-shot settings.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Raymond Li", "paper_title": "Starcoder: may the source be with you!", "reason": "This paper introduces Starcoder, a large language model that is directly relevant to the authors' adaptation approach, as it offers a robust foundation model for conversion to a diffusion model.  The comparison between Starcoder and the adapted models sheds light on the effectiveness of the adaptation technique.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Mohammad Samragh", "paper_title": "Scaling smart: Accelerating large language model pre-training with small model initialization", "reason": "This paper addresses the computational challenges of training large language models, directly relevant to the resource-intensive nature of training diffusion models. The authors' approach aims to circumvent these challenges through adaptation, making it more efficient than training from scratch.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Shansan Gong", "paper_title": "DiffuSeq: Sequence to sequence text generation with diffusion models", "reason": "This paper is highly relevant because it is a previous work by the same authors that explores text diffusion models. It lays the foundation for the current study, providing insights into the challenges and opportunities in adapting diffusion models, while highlighting the limitations of existing models.", "section_number": 5}, {" publication_date": "2018", "fullname_first_author": "Jiatao Gu", "paper_title": "Non-autoregressive neural machine translation", "reason": "This paper explores non-autoregressive generation, a crucial concept for understanding the advantages of diffusion models over traditional autoregressive approaches. The authors' work demonstrates how the non-autoregressive nature of diffusion models can lead to significant improvements in efficiency and generation flexibility.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Wilson Wu", "paper_title": "Do language models plan ahead for future tokens?", "reason": "This paper is highly relevant due to its exploration of planning capabilities in large language models, which is directly relevant to the advantages of diffusion models over autoregressive models.  It highlights the inherent limitations of autoregressive models in planning and suggests that diffusion models might provide a solution.", "section_number": 5}]}