[{"figure_path": "2410.17891/figures/figures_3_0.png", "caption": "The overview of our approach to adapt autoregressive (AR) models to diffusion models. Left: The shift operation in AR models enables the output layer hi to approximate the distribution of next tokens Xi+1 in hidden representations through the cross entropy (CE) loss. Middle: We remove the causal mask gradually during training eventually making our model bi-directional. Right: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on hi would be with respect to xi+1), while perceptually, the diffusion models are still functioning as recovering the original signals (since hi corresponds to xi+1 in AR loss).", "description": "The figure illustrates the adaptation process from autoregressive (AR) language models to diffusion language models.  The left panel shows the standard shift operation in AR models, where the output layer predicts the next token using cross-entropy loss. The middle panel depicts the gradual annealing of the causal mask in the AR model, transitioning from a strictly left-to-right attention mechanism to a fully bidirectional one. This is a key step in adapting the model for diffusion modeling. The right panel shows the resulting diffusion model architecture. Here, the logits are shifted to align with the next token during loss computation, mirroring the shift operation in the AR model, while retaining the bi-directional attention.  This approach bridges the difference between AR and diffusion model objectives.", "section": "3 MODEL"}]