{"references": [{" publication_date": "2017", "fullname_first_author": "Yash Goyal", "paper_title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "reason": "This paper is highly relevant to the CAMEL-Bench evaluation as it focuses on visual question answering (VQA), a core task within the benchmark.  The paper's emphasis on image understanding is particularly important for a multimodal benchmark like CAMEL-Bench, which requires models to effectively integrate visual and textual information.  The introduction of novel VQA metrics and approaches within this paper influenced how the evaluation was designed in the CAMEL-Bench framework. ", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Junnan Li", "paper_title": "Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "This paper's introduction of BLIP, a unified vision-language model, is highly relevant to CAMEL-Bench, which evaluates both unified and specialized models. BLIP's advancements in vision-language tasks are directly relevant to the benchmark's various sub-domains, like image captioning and visual question answering. The methodology of pre-training and fine-tuning described in this paper is directly applicable to the evaluation of LMMs in CAMEL-Bench.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen technical report", "reason": "This paper presents Qwen, a large language model used in CAMEL-Bench's filtering and verification process to compare semantic similarity between English and Arabic question-answer pairs.  The model's ability to assess semantic equivalence is crucial for evaluating the quality and consistency of the translated data, ensuring the validity and reliability of CAMEL-Bench.  The technical details of Qwen provided in this paper are vital for understanding the filtering process and its impact on benchmark results.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "This paper describes Qwen-VL, a large vision-language model, offering valuable insights into the capabilities of current vision-language models.  Given CAMEL-Bench's focus on evaluating Arabic LMMs, this paper provides a reference to evaluate the performance of similar models. The architectural design and performance characteristics of Qwen-VL are directly relevant to understanding the state-of-the-art in multimodal understanding.  This paper informs the design of the evaluation framework used in CAMEL-Bench.", "section_number": 3}, {" publication_date": "2019", "fullname_first_author": "Drew A Hudson", "paper_title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "reason": "This paper introduced the GQA dataset, which is highly relevant to CAMEL-Bench as it focuses on visual reasoning and compositional question answering.  These are key tasks within the multimodal understanding domains of CAMEL-Bench.  Understanding the design and challenges of GQA helps in understanding the design choices made in CAMEL-Bench's multimodal understanding section and provides a comparison point for evaluating model performance.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Jeffrey P Bigham", "paper_title": "Vizwiz: nearly real-time answers to visual questions", "reason": "This paper introduces the VizWiz dataset, a valuable resource for VQA research, relevant to CAMEL-Bench as it addresses challenges related to visual question answering in real-world scenarios.  The real-world context and image diversity of VizWiz are directly relevant to the creation of realistic and diverse visual questions in CAMEL-Bench.  The paper contributes to understanding the complexities of visual reasoning in varied contexts, influencing the design and evaluation methodology of the CAMEL-Bench.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chaoyou Fu", "paper_title": "MME: A comprehensive evaluation benchmark for multimodal large language models", "reason": "The introduction of MME benchmark in this paper provides a critical comparison for CAMEL-Bench, another multimodal language model benchmark.  Understanding MME's structure, challenges, and strengths helps to contextualize the novelty and significance of CAMEL-Bench in the evaluation of multimodal large language models.  Comparing these benchmarks helps to clarify CAMEL-Bench's unique contributions to the field.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "This paper details Blip-2, a significant advancement in vision-language models, and therefore highly relevant to CAMEL-Bench. The comparison of Blip-2 with other models in CAMEL-Bench provides valuable insights into current LMM performance and facilitates a better understanding of the strengths and weaknesses of various models.  Blip-2's capabilities within the vision-language space are directly relevant to numerous tasks in CAMEL-Bench.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Jaemin Cho", "paper_title": "Unifying vision-and-language tasks via text generation", "reason": "This paper is crucial because it explores the unification of vision-and-language tasks, a central theme of CAMEL-Bench which evaluates multimodal models' ability to handle diverse tasks. The methods and findings presented in this paper offer valuable insights into the design and evaluation of a comprehensive benchmark like CAMEL-Bench.  Understanding the approach to unifying these tasks is paramount for developing robust multimodal models.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Google AI", "paper_title": "Gemini: A family of highly capable multimodal models", "reason": "This paper introduces the Gemini family of models, which are directly used in CAMEL-Bench's evaluation. Understanding Gemini's capabilities and limitations, particularly in Arabic, is a key factor for the benchmark's interpretation. Gemini serves as a standard against which CAMEL-Bench evaluates other models' performance, highlighting strengths and weaknesses of both open-source and closed-source models.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Rocktim Jyoti Das", "paper_title": "Exams-V: A multi-discipline multilingual multimodal exam benchmark for evaluating vision language models", "reason": "The Exams-V benchmark, introduced in this paper, serves as a direct comparison point for CAMEL-Bench in the context of evaluating vision-language models. Comparing these benchmarks allows for a clear understanding of their respective strengths and weaknesses, highlighting the specific focus and contribution of CAMEL-Bench to the field of Arabic LMM evaluation. The multilingual aspect is also highly relevant.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Fajri Koto", "paper_title": "ArabicMMLU: Assessing massive multitask language understanding in Arabic", "reason": "This paper introduces the ArabicMMLU benchmark, providing another relevant comparison for CAMEL-Bench. The similarity in objectives allows for evaluating the unique contributions of CAMEL-Bench in terms of comprehensive task coverage and data quality. It also informs the design and evaluation methodology for similar benchmarks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Fakhraddin Alwajih", "paper_title": "Peacock: A family of arabic multimodal large language models and benchmarks", "reason": "This paper introduces the Peacock benchmark, offering a direct comparison to CAMEL-Bench in the context of Arabic multimodal LMM evaluation. Comparing the two benchmarks helps to highlight the unique characteristics and contributions of CAMEL-Bench, including its scale, diversity of tasks, and rigorous evaluation methodology. The comparative analysis adds value to the overall discussion of the importance of such benchmarks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "reason": "This paper presents InternVL, an open-source vision-language model, allowing for a direct comparison with other models in CAMEL-Bench\u2019s evaluations. Analyzing InternVL\u2019s performance against closed-source models like GPT-4 helps illuminate the current gap between open-source and closed-source models in the field of Arabic LMMs, guiding future research and development efforts. The model is used in CAMEL-Bench\u2019s evaluation.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Haotian Liu", "paper_title": "LLaVa-NeXt: Improved reasoning, OCR, and world knowledge", "reason": "This paper introduces LLaVa-NeXt, an open-source vision-language model, offering a valuable comparison point in the CAMEL-Bench evaluation.  Evaluating LLaVa-NeXt against GPT-4 and other models highlights the current capabilities of open-source models compared to leading closed-source models.  This aids in identifying areas for improvement in open-source models and informing future development.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Xingyu Fu", "paper_title": "Blink: Multimodal large language models can see but not perceive", "reason": "This paper introduces the Blink dataset, which is directly relevant to CAMEL-Bench as it focuses on evaluating the multimodal capabilities of large language models, particularly in understanding visual information. The findings and methodologies of this research contribute to a broader understanding of the challenges involved in developing high-performing multimodal models, which helps in interpreting the findings and results of CAMEL-Bench.", "section_number": 2}, {" publication_date": "2009", "fullname_first_author": "Jia Deng", "paper_title": "Imagenet: A large-scale hierarchical image database", "reason": "ImageNet is a fundamental dataset used in many computer vision tasks, including those evaluated in CAMEL-Bench.  Understanding ImageNet\u2019s structure, scale, and challenges is important for understanding the design and performance of models evaluated in CAMEL-Bench.  Its influence on the development of modern vision-language models makes it highly relevant to understanding the context of CAMEL-Bench.", "section_number": 2}, {" publication_date": "2014", "fullname_first_author": "Tsung-Yi Lin", "paper_title": "Microsoft coco: Common objects in context", "reason": "COCO is a widely used dataset for object detection and image captioning, both relevant tasks in CAMEL-Bench's multimodal understanding domains. The paper's contribution to the computer vision field directly impacts the development and evaluation of models tested on CAMEL-Bench.  Analyzing COCO helps to evaluate the performance of models across multiple vision-based tasks within the benchmark.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Muhammad Awais", "paper_title": "AgroGPT: Efficient agricultural vision-language model with expert tuning", "reason": "This paper introduces AgroGPT, a model specifically designed for agricultural vision-language tasks, making it directly relevant to the Agricultural Image Understanding domain within CAMEL-Bench. Understanding AgroGPT's design, methodology, and performance helps in evaluating the capabilities of specialized models within CAMEL-Bench and highlights the challenges of agricultural image understanding.  This adds to the comprehensive benchmark evaluation.", "section_number": 2}]}