[{"figure_path": "2410.18976/tables/table_2_0.md", "caption": "Comparison of our CAMEL-Bench with existing Arabic LMM benchmarks: Exams-V [13], CVQA [46], Henna[4], and KHATT [34]. Here * denotes that only Arabic part of benchmark is counted.", "description": "The table compares CAMEL-Bench with three other existing Arabic LMM benchmarks: Exams-V, CVQA, and KHATT.  It shows a checkmark (\u2713) indicating which benchmark includes each of eight characteristics or domains (Multimodal Understanding & Reasoning, OCR & Docs Understanding, Charts & Diagrams Understanding, Video Understanding, Medical Image Understanding, Agricultural Image Understanding, Remote Sensing Understanding, and Cultural-Specific Understanding).  It also notes whether each benchmark uses open-source components and lists the number of questions included in each.  CAMEL-Bench stands out with significantly more questions (29K) than the others, and it is the only one to include all eight domains.", "section": "1. Introduction"}, {"figure_path": "2410.18976/tables/table_3_0.md", "caption": "Table 2. Different data sources used for 38 sub-domains corresponding to eight domains, with around 29k questions in total. The different data sources include: MME [15], MMBench [30], MMT-Bench-MI [56], SEED [23], MMMU [58], MMMU-Pro [60], CountBench [39], POPE [26], MathVista [33], Exams-V (Arabic portion) [13], ScienceQA-IMG [32], GQA [20], VizWiz [10], VQAv2 [17], BLINK [16], MuirBench [50], COCO [27], Imagenet [14], Mocheg [55], Snli-Ve [54], Pinterest [42], RealWorldQA [53], PATS-01 [3], KHATT [34], PATD [40], Historical Arabic Handwritten Text Recognition Dataset [37], ISI-PPT-Dataset [52], EvArEST [18], MTVQA [49], ChartQA [35], IconQA [31], BEC-Arabic [47], Claude-3.5 [5], arab-celeb-dataset [36], arabic-food-101 [6], Countries and landmarks [41, 51, 57], Pexel [41], AgroGPT [7], GeoChat [22]. These data sources are carefully translated and verified to ensure quality and relevance.", "description": "Table 2 presents a detailed breakdown of the data sources used for the 38 sub-domains within the eight domains of the CAMEL-Bench benchmark.  It lists each domain, its corresponding sub-domains, the specific data sources utilized for each sub-domain (with citation numbers referencing the paper's bibliography), and the number of questions sourced from each. The table highlights the diversity of data sources, including both existing datasets and newly created or adapted materials, to ensure a comprehensive and robust benchmark.  The sources encompass various types of data including images, text, and video, and span various areas such as visual question answering, OCR, chart and diagram understanding, and specialized domains like medical imaging and remote sensing.", "section": "2. CAMEL-Bench"}, {"figure_path": "2410.18976/tables/table_5_0.md", "caption": "Table 3. Performance comparison of different closed-and open-source LMMs on CAMEL-Bench. We present per-domain results of seven LMMs: GPT-40 [38], GPT-40-mini [38], Gemini-1.5-Pro [2], Gemini-1.5-Flash [2], Pangea-7B [59], Qwen2-VL [9], InternVL2-8B [11], and LLaVaNeXt-7B [29]. GPT-40 excels in most domains, while GPT-40-mini offers an impressive balance of performance and model size. All models struggle with remote sensing, medical imaging, OCR & document understanding, and general multimodal understanding and reasoning domains. Open-source models like InternVL2-8B and LLaVaNeXt-7B show a decline in performance across domains, with their best results in video understanding.", "description": "This table presents a comparative evaluation of seven different large multimodal models (LMMs) on the CAMEL-Bench benchmark.  The models, including both closed-source (GPT-40, GPT-40-mini, Gemini-1.5-Pro, Gemini-1.5-Flash) and open-source (Pangea-7B, Qwen2-VL-2B, InternVL2-8B, LLaVa-NeXt-7B) models are evaluated across eight domains of the CAMEL-Bench: Multimodal Understanding & Reasoning, OCR & Document Understanding, Charts & Diagram Understanding, Video Understanding, Cultural Specific Understanding, Medical Imaging, Agricultural Image Understanding, and Remote Sensing Understanding.  The table shows the performance of each model in each domain as a percentage score, highlighting the relative strengths and weaknesses of each model across diverse multimodal tasks.  GPT-4o shows superior performance across most domains, while open-source models exhibit lower scores, particularly struggling with remote sensing and medical image understanding.", "section": "3. CAMEL-Bench Benchmark Evaluation"}]