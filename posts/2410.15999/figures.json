[{"figure_path": "2410.15999/figures/figures_1_0.png", "caption": "In the event of a knowledge conflict, the model can rely on the context or on the parametric knowledge. The figure presents the predictions of Llama2-7B steered by SPARE.", "description": "The figure shows two examples of how the SPARE model resolves knowledge conflicts. In the first example, given the context that Geoffrey Hinton won the Nobel Prize in Physics in 2024, the model correctly identifies the Nobel Prize as the correct answer when instructed to use the context, and it gives the Turing Award as the answer when instructed to use memory. In the second example, given the context that Geoffrey Hinton wrote the song Shake It Off and won the Nobel Prize in Physics in 2024, the model correctly gives Geoffrey Hinton as the answer when instructed to use context and gives Taylor Swift as the answer when instructed to use memory.  These examples illustrate the ability of SPARE to control the model's knowledge selection behavior, allowing it to prioritize either contextual or parametric knowledge as needed.", "section": "Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/figures/figures_4_0.png", "caption": "Figure 3: The workflow of SPARE steers the knowledge selection behaviour. The figure presents an example of steering the model to use parametric knowledge. First, the SAE encoder fe encodes hidden state h into the SAE activation z. Then, it determines the values of SAE activations z\u00af and z+ for editing (Eq. (2) and Eq. (3)). Finally, we edit the hidden state using the features extracted from the SAE decoder go (Eq. (4)).", "description": "This figure illustrates the workflow of the SPARE method for steering knowledge selection behaviour in LLMs. It begins with a hidden state (h) from the LLM, which is encoded by a sparse autoencoder (SAE) into a sparse vector (z).  Based on pre-calculated mean vectors (ZC and ZM) representing contextual and parametric knowledge selection respectively,  the SAE activations are adjusted by subtracting values (z-) and adding values (z+). These adjusted activations are then decoded by the SAE to generate modified hidden states (h'), which are combined with the original hidden states (h) to produce a final output that steers the LLM towards either contextual or parametric knowledge based on the user defined hyperparameter (\u03b1).", "section": "4 Resolving Knowledge Conflicts by Representation Engineering"}]