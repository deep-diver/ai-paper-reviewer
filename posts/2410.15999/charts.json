[{"figure_path": "2410.15999/charts/charts_3_0.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "This figure displays the results of probing experiments conducted on two large language models, Llama2-7B and Gemma2-9B, using the NQSwap dataset. The experiments aimed to detect knowledge conflicts within the models by training logistic regression models to classify whether activations from different layers (hidden states, MLP, or Self-Attention) originated from instances with or without knowledge conflicts. The x-axis represents the layer number, while the y-axis shows the Area Under the Receiver Operating Characteristic curve (AUROC), a metric indicating the models' ability to distinguish between conflict and non-conflict activations. The different activation types are represented by different colors: red for hidden states, blue for MLP, and green for Self-Attention.  The figure shows that the probing accuracy increases from the first layer to the middle layers for both models, indicating that the signal of a knowledge conflict is registered at mid-layers. However, accuracy decreases in the later layers, suggesting that the signals from the conflict aren't further processed by the later layers.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_7_0.png", "caption": "Figure 4: Detailed evaluation results of controlling capability on NQSwap. We use different colours for different methods and use different shapes for different models. The upper-right area indicates a high performance for all figures. (a) presents the capability of changing the behaviour of LLMs, where x-axis and y-axis are EMC\u2192M and EMM\u2192C, measuring the capability of changing the answer from C to M and from M to C, respectively; (b) presents the capability of maintaining the behaviour when steering to the same behaviour as the original behaviour, where x-axis and y-axis are EMM\u2192M and EMC\u2192C, measuring the maintaining capability of generating M and C, respectively; (c) present the ablation analysis of SPARE, x-axis and y-axis are EMM and EMC.", "description": "This figure presents a multi-faceted evaluation of the SPARE model and several baseline methods across three key aspects of controlling LLM behavior in knowledge conflict scenarios.  Subfigure (a) assesses the models' ability to *change* the LLM's knowledge selection behavior from generating either a contextual answer (C) to a parametric answer (M) (EMC\u2192M) or vice versa (EMM\u2192C). Subfigure (b) measures how well each model *maintains* the original knowledge selection behavior when attempting to steer the LLM towards the same type of response (EMM\u2192M and EMC\u2192C).  Finally, subfigure (c) shows an ablation study of the SPARE model, examining its performance by removing or adding parts of the algorithm, showing the impact of each component on the final results for both contextual (EMC) and parametric (EMM) knowledge selection.", "section": "5.3 Multi-Perspective Controlling Analysis"}, {"figure_path": "2410.15999/charts/charts_8_0.png", "caption": "Figure 5: Effectiveness of SPARE on editing different layers individually.", "description": "This chart displays the performance of SPARE when editing different layers individually on two different LLMs, Llama3-8B and Gemma2-9B.  The x-axis represents the layer number, and the y-axis represents the exact match (EM) score. Two lines are plotted for each LLM: one representing the EM score when steering towards contextual knowledge (EM<sub>C</sub>, blue line), and the other representing the EM score when steering towards parametric knowledge (EM<sub>M</sub>, red line). The chart visually shows how the effectiveness of SPARE in controlling the LLM's knowledge selection behavior changes across different layers, indicating that certain layers are more influential than others.", "section": "5.3 Multi-Perspective Controlling Analysis"}, {"figure_path": "2410.15999/charts/charts_8_1.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "The chart displays the results of a knowledge conflict probing experiment conducted on two large language models, Llama2-7B and Gemma2-9B, using the NQSwap dataset.  The experiment involved training logistic regression models to classify whether activations from different layers (hidden states, MLP, and self-attention) contained a knowledge conflict signal. The x-axis represents the layers of the model, and the y-axis represents the area under the receiver operating characteristic curve (AUROC). Separate lines depict AUROC scores for each activation type (hidden states, MLP, and self-attention) for each model, allowing for a comparison across layers and activation types. Higher AUROC values signify better performance in detecting knowledge conflicts.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_8_2.png", "caption": "Figure 6: The residual stream changes after applying SPARE to Llama3-8B at the 15th layer.", "description": "This figure shows two sub-figures, (a) and (b), illustrating the changes in the residual stream of Llama3-8B after applying SPARE at the 15th layer.  Subfigure (a) presents AUROC probing results, showing the performance of a classifier trained to distinguish between instances with and without knowledge conflicts.  The lines represent the AUROC values for different hidden states (hidden, MLP, and attention) across layers, with three scenarios depicted: without controlling knowledge selection, steering to use contextual knowledge, and steering to use parametric knowledge. Subfigure (b) displays the kurtosis values of these hidden states across layers for the three scenarios. The kurtosis values provide insight into the distribution of residual stream activations and how the application of SPARE alters that distribution.  This comparison is used to analyze the effect of SPARE on controlling the selection of different knowledge sources.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_14_0.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "The chart displays the Area Under the Receiver Operating Characteristic Curve (AUROC) for detecting knowledge conflicts in language models Llama2-7B and Gemma2-9B across different layers.  The AUROC is plotted for three different activation types: hidden states, MLP (Multi-Layer Perceptron), and attention (attn) layers. The x-axis represents the layer number of the model, and the y-axis shows the AUROC, indicating the model's ability to distinguish between instances with and without knowledge conflicts.  Higher AUROC values signify better performance in detecting knowledge conflicts.  The chart reveals that the AUROC generally increases from lower to middle layers for all activation types and then decreases in the upper layers, implying that knowledge conflicts' signals are strongest in the middle layers of both models.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_14_1.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "The chart displays the results of a knowledge conflict probing experiment conducted on two large language models, Llama2-7B and Gemma2-9B, using the NQSwap dataset.  The experiment assesses the ability to detect knowledge conflicts within the models by analyzing different types of activations (hidden states, MLP, and self-attention) across various layers.  The AUROC (Area Under the Receiver Operating Characteristic curve) is plotted on the y-axis against the layer number on the x-axis for each activation type.  Different colored lines represent different activation types.  The chart shows that probing accuracy generally increases from the first layers to the middle layers and then decreases again. This suggests that the signal of knowledge conflict is strongest in the middle layers of the models.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_14_2.png", "caption": "Figure 8: Knowledge conflict probing results using Llama2-7B on NQSwap.", "description": "This figure presents the results of probing knowledge conflicts on the Llama2-7B model using the NQSwap dataset.  The AUPRC (Area Under the Precision-Recall Curve) is plotted against the layer number for three different types of activations within the model: hidden, MLP, and attention.  The chart shows the performance of a binary classification task to detect knowledge conflicts. The x-axis represents the layer number, ranging from 0 to 30, while the y-axis shows the AUPRC values, which range from approximately 0.75 to 0.97. Each activation type is represented by a different colored line; a higher AUPRC indicates better performance in detecting knowledge conflicts. The chart suggests that the ability to detect knowledge conflicts varies across layers and activation types, with certain layers exhibiting notably higher AUPRC.", "section": "A More Analysis of Knowledge Conflict Probing"}, {"figure_path": "2410.15999/charts/charts_14_3.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "This chart displays the Area Under the Receiver Operating Characteristic Curve (AUROC) for detecting knowledge conflicts in LLMs across different layers.  The AUROC is shown separately for Llama2-7B and Gemma2-9B models, with results broken down by activation type (hidden, MLP, and attention).  The x-axis represents the layer number, while the y-axis shows the AUROC value, indicating the model's performance at detecting knowledge conflicts.  Higher AUROC values represent better performance.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_14_4.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "The chart displays the Area Under the Receiver Operating Characteristic Curve (AUROC) scores for detecting knowledge conflicts in the Llama2-7B and Gemma2-9B language models across different layers.  The AUROC is plotted against the layer number (1-30) and separate lines represent different activation types (hidden, MLP, and attention).  The results indicate that the AUROC generally increases from the first layer to the middle layers and then decreases in the later layers, suggesting the signal of conflict is most effectively registered in the middle layers. The shaded areas around the lines represent standard deviations across multiple runs.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_14_5.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "The chart displays the Area Under the Precision-Recall Curve (AUPRC) for knowledge conflict detection in Llama2-7B and Gemma2-9B language models across different layers.  Three activation types are shown: hidden states, MLP (multi-layer perceptron), and attention.  The x-axis represents the layer number, and the y-axis represents the AUPRC, indicating the model's ability to distinguish between instances with and without knowledge conflicts.  The chart shows AUPRC values for each activation type across all layers, revealing the performance trends in different model components and highlighting potential layers where knowledge conflict signals are most prominent.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_16_0.png", "caption": "Figure 10: The impact of the number of the collected hidden states N on the controlling performance.", "description": "This figure shows two line charts illustrating the impact of the number of collected hidden states (N) on the performance of controlling knowledge selection behavior in LLMs.  The left chart displays results using 0-250 examples, while the right chart uses 0-120 examples. Both charts plot Exact Match (EM) scores for steering towards contextual knowledge (EMC) and parametric knowledge (EMM), comparing controlled results with uncontrolled baselines.  The charts show the trend of improved performance in controlling knowledge selection with increasing N, especially for steering towards contextual knowledge, although this improvement levels off after a certain number of examples.", "section": "C Implementation Details"}, {"figure_path": "2410.15999/charts/charts_19_0.png", "caption": "Figure 11: Proportion of accumulated mutual Information (K) on Gemma2-9B", "description": "This chart displays the relationship between the proportion of accumulated mutual information (K) and the number of activations selected for three different layers (23, 24, and 25) in the Gemma2-9B model.  Each subplot represents a layer, showing a curve that depicts how the number of selected activations increases as the proportion of accumulated mutual information increases.  The x-axis represents the proportion of accumulated mutual information, while the y-axis shows the number of activations.", "section": "E Distribution of Mutual Information"}, {"figure_path": "2410.15999/charts/charts_19_1.png", "caption": "Figure 11: Proportion of accumulated mutual Information (K) on Gemma2-9B", "description": "This chart displays the relationship between the proportion of accumulated mutual information (K) and the number of selected activations (k) for three different layers (23, 24, and 25) of the Gemma2-9B model.  Each subplot represents a layer, showing a curve that starts with a steep rise and gradually flattens as K increases. This indicates that a small percentage of total mutual information accounts for a large number of activations, highlighting that selecting a relatively small subset of highly informative activations can capture the crucial information for knowledge selection behavior.", "section": "E Distribution of Mutual Information"}, {"figure_path": "2410.15999/charts/charts_19_2.png", "caption": "Figure 11: Proportion of accumulated mutual Information (K) on Gemma2-9B", "description": "This figure shows three subplots displaying the relationship between the proportion of accumulated mutual information and the number of activations for three different layers (23, 24, and 25) of the Gemma2-9B model. Each subplot contains a line graph where the x-axis represents the proportion of accumulated mutual information (ranging from 0 to 0.175) and the y-axis shows the number of activations. The line graphs illustrate that as the proportion of accumulated mutual information increases, the number of activations needed also increases, indicating a positive correlation between these two variables across the different layers.", "section": "E Distribution of Mutual Information"}, {"figure_path": "2410.15999/charts/charts_19_3.png", "caption": "Figure 6: The residual stream changes after applying SPARE to Llama3-8B at the 15th layer.", "description": "This chart displays the changes in the residual stream of Llama3-8B at the 15th layer after applying the SPARE method.  It shows probing results (AUROC) when steering towards parametric knowledge (green line) and contextual knowledge (blue line) use, in comparison to the original behavior without control (black line). The chart demonstrates how SPARE impacts the signal of knowledge conflict in the residual stream, affecting the model's ability to distinguish between conflicting information sources.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_19_4.png", "caption": "Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.", "description": "This chart displays the skewness of the hidden states of the Llama2-7B language model on the NQSwap dataset, categorized by whether the model uses parametric knowledge (DM) or contextual knowledge (DC) to answer questions. The x-axis represents the layer number of the model, and the y-axis shows the Hoyer skewness value for each layer. The lines in the chart show that the distribution of the hidden states when using the contextual knowledge tends to be more skewed than using the parametric knowledge in most of the layers.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_19_5.png", "caption": "Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.", "description": "This line chart illustrates the L1 and L2 norms of hidden states for Llama2-7B on the NQSwap dataset, categorized by whether the model selects parametric knowledge (DM) or contextual knowledge (DC) during answer generation.  The x-axis represents the layer number, ranging from 0 to 30, while the y-axis shows the norm values.  Separate lines depict the L1 and L2 norms for both DM and DC, revealing a distinct pattern of divergence in the L1 and L2 norms across different layers. The difference between DM and DC in L1 norm is relatively small, while in L2 norm, the difference is larger. Both L1 and L2 norms for DM show fluctuations and generally smaller values than DC, which tends to show a higher norm and also a more stable pattern.", "section": "F.2 L1 Norm and L2 Norm Pattern"}, {"figure_path": "2410.15999/charts/charts_19_6.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "The chart displays the results of probing experiments conducted on Llama2-7B and Gemma2-9B language models using the NQSwap dataset, which contains instances with knowledge conflicts.  The Area Under the Receiver Operating Characteristic curve (AUROC) is used as a metric to assess the models' ability to detect knowledge conflicts. The x-axis represents the layer number in the models, while the y-axis represents the AUROC values.  Separate lines depict the AUROC for different activation types within the models: hidden states, Multi-Layer Perceptrons (MLPs), and Self-Attention.  The chart shows that the AUROC increases from the first layer to middle layers and then decreases in later layers, regardless of the activation type used, indicating that signals of knowledge conflict are present in the middle layers.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_19_7.png", "caption": "Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.", "description": "The chart displays the skewness of the hidden states of the Llama2-7B language model on the NQSwap dataset.  It shows the Hoyer skewness across different layers (x-axis) of the model for two groups of instances:  those where the model uses parametric knowledge (DM, red line) and those where it uses contextual knowledge (Dc, blue line). Error bars represent the standard deviation. The plot illustrates how the skewness differs across layers and between the two knowledge source selection behaviours, suggesting a difference in the representation of the information processed when using each source.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_19_8.png", "caption": "Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.", "description": "The chart displays the L1 and L2 norms of hidden states from Llama2-7B model on the NQSwap dataset.  It shows the trends of these norms across different layers of the model, distinguishing between two groups: \nDM (model uses parametric knowledge to generate answer M) and Dc (model uses contextual knowledge to generate answer C). The x-axis represents the layers of the model, while the y-axis represents the L1 or L2 norm values.  Separate lines are plotted for the DM and Dc groups, allowing for a layer-by-layer comparison of the L1 and L2 norm values between the two groups of knowledge selection behaviours.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_20_0.png", "caption": "Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.", "description": "The chart displays the skewness of hidden states across different layers (0-30) of the Llama2-7B language model on the NQSwap dataset.  Two lines represent two distinct groups of instances: those where the model uses parametric knowledge (DM, red line) and those where it uses contextual knowledge (DC, blue line).  The y-axis shows the Kurtosis values, a measure of the tailedness of the probability distribution of a real-valued random variable. It shows that the skewness is significantly different for the two groups, especially in some of the layers.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_20_1.png", "caption": "Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.", "description": "The chart displays the layer-wise skewness of hidden states for Llama2-7B on the NQSwap dataset, comparing two groups:  DM (model uses parametric knowledge) and DC (model uses contextual knowledge).  The y-axis represents the Hoyer skewness, a measure of sparsity, ranging from approximately 0.2 to 0.47. The x-axis represents the layer number, ranging from 0 to 30.  The lines show that the skewness values fluctuate across layers for both groups, with DC generally exhibiting higher skewness than DM. The trends show different patterns for each group across layers, suggesting distinct representation characteristics in how each group processes knowledge.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_20_2.png", "caption": "Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.", "description": "The chart displays the L1 and L2 norm values of the hidden states of the Llama2-7B language model on the NQSwap dataset.  The x-axis represents the layers of the model, ranging from 0 to 30, while the y-axis shows the norm values. Two lines are presented, one red representing the DM group and one blue representing the DC group, allowing for a comparison of norm values between these two groups across different layers of the model. ", "section": "F.2 L1 Norm and L2 Norm Pattern"}, {"figure_path": "2410.15999/charts/charts_20_3.png", "caption": "Figure 2: The knowledge conflict probing results of Llama2-7B and Gemma2-9B on NQSwap (Longpre et al., 2021). The probing results on hidden states, MLP and Self-Attention activations are coloured differently.", "description": "The chart displays the results of probing experiments conducted on two large language models, Llama2-7B and Gemma2-9B, to assess their ability to detect knowledge conflicts within the NQSwap dataset.  The results are presented as Area Under the Receiver Operating Characteristic curve (AUROC) values across different layers of the models, for three distinct activation types: hidden states, Multi-Layer Perceptrons (MLPs), and Self-Attention mechanisms.  For each model and activation type, a line graph illustrates the AUROC scores across each layer, revealing how the models' ability to detect knowledge conflict evolves across different processing stages.  The y-axis represents the AUROC scores (ranging from 0.5 to 1.0), indicating the model's performance in classifying knowledge conflict instances, while the x-axis represents the layer number, indicating the depth of the model\u2019s processing.", "section": "3 Detection of Knowledge Conflicts"}, {"figure_path": "2410.15999/charts/charts_20_4.png", "caption": "Figure 14: Skewness of the hidden states of Llama2-7B on NQSwap.", "description": "The chart displays the Hoyer skewness of hidden states across 31 layers for two groups: DM (model uses parametric knowledge) and DC (model uses contextual knowledge). The x-axis represents the layer number (0-31), while the y-axis shows the Hoyer skewness. Both groups exhibit a fluctuating pattern, but DC consistently shows higher skewness than DM across almost all layers.  This difference suggests a distinct representation pattern when selecting contextual versus parametric knowledge.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_20_5.png", "caption": "Gini", "description": "The chart displays the Gini index values across different layers of a model, categorized into two groups: \nDM and Dc.  The x-axis represents the layer number, ranging from 0 to 30, while the y-axis shows the Gini index. The lines for DM and Dc show distinct patterns, indicating different levels of inequality or concentration for each group across the layers.  The DM line exhibits a sharp initial decrease and subsequent fluctuations, while the Dc line shows a more gradual decrease with subsequent rises.", "section": "6.2 Analysing the Residual Stream"}, {"figure_path": "2410.15999/charts/charts_20_6.png", "caption": "L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.", "description": "This chart is a line graph showing the L1 and L2 norms of the hidden states for Llama2-7B on the NQSwap dataset.  The x-axis represents the layers of the model, ranging from 0 to 30.  The y-axis on the left shows the L1 norm, and the y-axis on the right shows the L2 norm. Two lines are plotted for each norm: one for instances where the model uses parametric knowledge (red, labeled  \"D<sub>M</sub>\") and one for instances where the model uses contextual knowledge (blue, labeled \"D<sub>C</sub>\"). Both the L1 and L2 norms generally increase across layers, with the norm for contextual knowledge (D<sub>C</sub>) often exceeding that for parametric knowledge (D<sub>M</sub>). The difference between the two lines varies across layers, suggesting differing levels of reliance on the two knowledge sources at different processing stages.", "section": "F.2 L1 Norm and L2 Norm Pattern"}, {"figure_path": "2410.15999/charts/charts_20_7.png", "caption": "Figure 19: L1 norm and L2 norm of the hidden states of Llama2-7B on NQSwap.", "description": "This chart is a line graph showing the L1 and L2 norms of the hidden states of the Llama2-7B language model on the NQSwap dataset.  The x-axis represents the layer number, ranging from 0 to 30. The y-axis represents the L1 norm (left panel) and L2 norm (right panel) values. Two lines are plotted in each panel, one for the \"DM\" group (red line) and another for the \"DC\" group (blue line).  The \"DM\" group corresponds to instances where the model uses parametric knowledge to answer questions, and the \"DC\" group represents instances where the model uses contextual knowledge to generate the answers.  The lines show the trend of the L1 and L2 norm values across the layers for both groups.  Overall, the values appear to increase with increasing layer depth for both the L1 and L2 norms.", "section": "F.2 L1 Norm and L2 Norm Pattern"}]