[{"figure_path": "2410.17637/figures/figures_2_0.png", "caption": "Figure 1: (a) Overview of MIA-DPO. We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization. (b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks.", "description": "The figure is composed of two subfigures. Subfigure (a) illustrates the MIA-DPO framework, showing how single-image data is augmented with noisy images to create multi-image data.  Language descriptions are used to focus the model on specific images within the collage. Attention values are then used to identify and filter out hallucinated responses, improving the accuracy of chosen/rejected pairs for DPO optimization. Subfigure (b) presents benchmark results, comparing the performance of MIA-DPO against other methods across five multi-image benchmarks and seven single-image benchmarks.  The results visually demonstrate that MIA-DPO consistently outperforms existing methods in multi-image scenarios, while maintaining comparable performance in single-image tasks.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17637/figures/figures_4_0.png", "caption": "Figure 2: Examples of Multi-Image Hallucinations. Top: Sequence Confusion that the model is confused about the order in which the images should be referenced. Bottom: Element Interference. The model incorrectly identified the attributes due to visual element interference across different images. Attention values illustrate how the model's focus was dispersed across different images, resulting in the hallucination response.", "description": "This figure presents two examples of multi-image hallucinations observed in Large Vision-Language Models (LVLMs). The top example illustrates \"Sequence Confusion,\" where the model incorrectly references an image out of order, focusing its attention on the wrong image in the sequence instead of the one specified in the prompt.  The bottom example displays \"Element Interference,\" where the model's attention is scattered across multiple images, leading to inaccurate identification of object attributes by mixing visual details from different images, thus causing a hallucination. In both cases, attention maps are visualized to highlight how the model's focus is distributed across the images, directly relating the attention distribution to the type of hallucination.", "section": "3.2 ANALYSIS ON MULTI-IMAGE HALLUCINATIONS"}, {"figure_path": "2410.17637/figures/figures_5_0.png", "caption": "Figure 3: MIA-DPO Framework. We extend the single-image dataset to multi-image datasets by inserting irrelevant images and using attention values to filter out the hallucination responses for rejected samples of the DPO algorithm.", "description": "This figure provides a visual overview of the MIA-DPO framework. It starts with a single-image dataset (like LLaVA 665k), which is augmented by adding noisy or unrelated images to create multi-image data in three formats: sequence, grid collage, and pic-in-pic.  The generated multi-image prompts are then input to a Large Vision Language Model (LVLM). Attention values from the LVLM are used to filter out rejected responses (hallucinations) via an attention-aware selection mechanism.  These chosen and rejected pairs are then used in the Direct Preference Optimization (DPO) algorithm to refine the LVLM.  A post-selection step further refines data based on perplexity (PPL), length ratio, and edit distance.  The result is a stronger LVLM better able to handle multi-image scenarios.", "section": "3.3 MIA-DPO FRAMEWORK"}, {"figure_path": "2410.17637/figures/figures_6_0.png", "caption": "Figure 4: Multi-Images DPO Data Format. To address multi-image hallucinations mentioned in Fig. 2, we construct our multi-image prompts in three formats: (a) Sequence. (b) Grid Collage. (c) Pic-in-Pic.", "description": "This figure illustrates three different methods for transforming single-image data into multi-image data for use in the MIA-DPO model.  (a) Sequence shows multiple images presented sequentially, where the question focuses on a specific image within the sequence. (b) Grid Collage presents a collage of images, with the question targeting a specific image within the collage using text descriptions.  (c) Pic-in-pic shows one image overlaid onto another larger image, with the question focusing on the smaller image. Each example includes the prompt, the chosen and rejected answers, and the attention visualization. These three methods, sequence, grid collage, and pic-in-pic, aim to address different types of multi-image hallucinations.", "section": "3.3 MIA-DPO FRAMEWORK"}, {"figure_path": "2410.17637/figures/figures_6_2.png", "caption": "Figure 2: Examples of Multi-Image Hallucinations. Top: Sequence Confusion that the model is confused about the order in which the images should be referenced. Bottom: Element Interference. The model incorrectly identified the attributes due to visual element interference across different images. Attention values illustrate how the model's focus was dispersed across different images, resulting in the hallucination response.", "description": "This figure showcases two examples of multi-image hallucinations. The top example illustrates \"Sequence Confusion,\" where the model incorrectly identifies the image being referenced in a question, focusing its attention on an incorrect image in a sequence. The bottom example shows \"Element Interference,\" where the model misidentifies the attributes of an object due to interference from visual elements in other images.  Attention maps are included to visualize how the model's attention is distributed across the multiple images.", "section": "3.2 ANALYSIS ON MULTI-IMAGE HALLUCINATIONS"}, {"figure_path": "2410.17637/figures/figures_6_3.png", "caption": "Figure 3: MIA-DPO Framework. We extend the single-image dataset to multi-image datasets by inserting irrelevant images and using attention values to filter out the hallucination responses for rejected samples of the DPO algorithm.", "description": "The figure provides a visual overview of the MIA-DPO framework. It begins with a single-image dataset, like LLaVA665k, which is augmented by adding noisy or unrelated images to create multi-image data in sequence, grid collage, or pic-in-pic formats.  The prompts then specify the target image within the multi-image context.  LVLMs process these inputs, and their attention values are analyzed to identify and filter out rejected responses potentially caused by hallucinations. This attention-aware selection process constructs chosen/rejected pairs for the DPO optimization algorithm. The final output is a stronger LVLM model that is better at handling multi-image inputs.", "section": "3.3 MIA-DPO FRAMEWORK"}, {"figure_path": "2410.17637/figures/figures_10_0.png", "caption": "Figure 6: Attention Difference Before and After DPO. We present the attention distribution in the intermediate layers for the original LLaVa-v1.5 (top row), MIA-DPO + LLaVa-v1.5 (second row), and the difference value (bottom row), respectively.", "description": "The figure visualizes the attention distribution in intermediate layers of the LLaVa-v1.5 model before and after applying MIA-DPO.  Three examples are shown, each with different prompts and images. The top row displays the original attention distribution for LLaVa-v1.5, illustrating how attention is spread across the images. The second row shows the attention distribution after applying MIA-DPO, highlighting a more focused attention on the relevant image regions specified in the prompt. The bottom row presents the difference between these two distributions, indicating which areas received increased attention after using MIA-DPO. This visual representation demonstrates how MIA-DPO refines the model's attention mechanism, leading to a more accurate focus on the relevant aspects of the images.", "section": "4.5 VISUALIZATION OBSERVATIONS"}, {"figure_path": "2410.17637/figures/figures_23_0.png", "caption": "Figure 3: MIA-DPO Framework. We extend the single-image dataset to multi-image datasets by inserting irrelevant images and using attention values to filter out the hallucination responses for rejected samples of the DPO algorithm.", "description": "The figure presents a flowchart illustrating the MIA-DPO framework. It starts with a single-image dataset from LLaVa665k, which is then augmented by adding noisy or unrelated images, resulting in three types of multi-image data: sequence, grid collage, and pic-in-pic.  These data types are processed by a Large Vision Language Model (LVLM), and attention values are extracted from the intermediate layer. An attention-aware selection process filters out rejected responses based on attention values, generating chosen/rejected pairs for Direct Preference Optimization (DPO). Post-selection further refines the data using perplexity, length ratio, and edit distance metrics before the DPO algorithm optimizes the LVLM, enhancing its multi-image understanding capability.", "section": "3.3 MIA-DPO FRAMEWORK"}, {"figure_path": "2410.17637/figures/figures_24_0.png", "caption": "Figure 1: (a) Overview of MIA-DPO. We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization. (b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks.", "description": "The figure is composed of two subfigures. Subfigure (a) illustrates the MIA-DPO framework, showing how single-image data is augmented with unrelated images to create multi-image data.  Language descriptions are used to specify the target image within the collage, and attention values from the LVLMs are used to identify and filter out hallucinated responses before feeding into the direct preference optimization (DPO) process. Subfigure (b) presents benchmark results comparing MIA-DPO against other methods on five multi-image and seven single-image benchmarks.  The bar charts display performance gains for MIA-DPO, showing improvement on all multi-image benchmarks and maintaining competitiveness on single-image tasks.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17637/figures/figures_24_1.png", "caption": "Examples of Multi-Image Hallucinations. Top: Sequence Confusion that the model is confused about the order in which the images should be referenced. Bottom: Element Interference. The model incorrectly identified the attributes due to visual element interference across different images. Attention values illustrate how the model's focus was dispersed across different images, resulting in the hallucination response.", "description": "The figure shows two examples of multi-image hallucinations. The top example illustrates \"Sequence Confusion,\" where the model incorrectly identifies the image being referenced in a question, focusing its attention on a different image than the one specified. The bottom example demonstrates \"Element Interference,\" where the model incorrectly identifies an attribute of an object due to mixing visual details from different images. In both examples, attention values visualize the model's focus across images, illustrating how the model's scattered attention contributes to hallucination.", "section": "3.2 ANALYSIS ON MULTI-IMAGE HALLUCINATIONS"}, {"figure_path": "2410.17637/figures/figures_24_2.png", "caption": "Figure 1: (a) Overview of MIA-DPO. We transform single-image data (e.g., LLaVA 665k) into multi-image data by adding noisy or unrelated images and using language descriptions to specify the target image. Attention values are then used to detect hallucinations in multi-image contexts, filtering out rejected data for DPO optimization. (b) Benchmark Results. MIA-DPO excels across five multi-image benchmarks while maintaining competitive performance on seven single-image benchmarks, demonstrating its robustness in both single and multi-image tasks.", "description": "This figure provides a visual overview of the MIA-DPO framework and its performance on benchmark datasets.  Panel (a) illustrates the process of MIA-DPO, showing how single-image data is augmented with unrelated images to create multi-image inputs.  The use of attention weights to identify and filter out erroneous model responses is also highlighted. Panel (b) presents a comparison of MIA-DPO's performance against other methods across five multi-image and seven single-image benchmark datasets.  The results demonstrate MIA-DPO's superior performance on multi-image tasks while maintaining competitiveness on single-image tasks.", "section": "1 INTRODUCTION"}]