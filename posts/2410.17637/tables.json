[{"figure_path": "2410.17637/tables/table_8_0.md", "caption": "Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "The table presents the main results of the MIA-DPO model and other DPO algorithms on five multi-image benchmarks: MMMU, BLINK, Mantis, NLVR2, and MVBench.  It compares the average performance across these benchmarks for several different large vision-language models (LVLMs), including LLaVa-v1.5 and InternLM-XC2.5, both with and without the MIA-DPO method applied.  The table also shows the performance improvement achieved by MIA-DPO compared to other DPO methods and baselines (LLaVA-RLHF, HA-DPO, POVID) on each benchmark, indicating the effectiveness of the MIA-DPO approach for enhancing multi-image understanding in LVLMs.", "section": "4.2 Results on Multi-Images Benchmarks"}, {"figure_path": "2410.17637/tables/table_9_0.md", "caption": "Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "This table presents the performance comparison of various models, including MIA-DPO and other DPO methods, across five multi-image benchmarks: MMMU, BLINK, Mantis, NLVR2, and MVBench.  For each model and benchmark, the table shows a performance score (presumably accuracy or a similar metric). The table highlights that MIA-DPO significantly improves performance on both LLaVa-v1.5 and InternLM-XC2.5 compared to other single-image DPO methods, indicating its effectiveness in handling multi-image tasks.  The \"\u0394\" rows show the improvement achieved by adding MIA-DPO to the baseline models.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17637/tables/table_10_0.md", "caption": "Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "This table presents the main results of the MIA-DPO model and other DPO algorithms on five multi-image benchmarks: MMMU, BLINK, Mantis, NLVR2, and MVBench.  The table compares the average performance across these benchmarks for several models, including LLaVa-v1.5 and InternLM-XC2.5, both with and without MIA-DPO, as well as other comparative models like LLaVA-RLHF, HA-DPO, and POVID. The results show the performance improvement achieved by MIA-DPO on these benchmarks, highlighting its effectiveness in handling multi-image tasks while maintaining competitive performance on single-image tasks compared to existing methods.  The \u0394 row indicates the improvement in performance obtained with MIA-DPO compared to the baseline model.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17637/tables/table_16_0.md", "caption": "Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "This table presents the main results of the MIA-DPO model and several other Direct Preference Optimization (DPO) approaches across five multi-image benchmarks (MMMU, BLINK, Mantis, NLVR2, MVBench).  For each benchmark, the table shows the average performance of several large vision-language models (LVLMs) including LLaVa-v1.5 and InternLM-XC2.5, both with and without MIA-DPO or other DPO methods applied. The results are presented as percentages, representing the model's accuracy on each benchmark.  The table also includes a row showing the average performance improvement achieved by MIA-DPO across all benchmarks for each model, highlighting its effectiveness in improving the performance of LVLMs on multi-image tasks compared to existing DPO approaches.  The \u0394 row indicates the improvement brought by MIA-DPO compared to the base model performance.", "section": "4 Experiments"}, {"figure_path": "2410.17637/tables/table_16_1.md", "caption": "Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "Table 1 presents a comparison of different models' performance across five multi-image benchmarks: MMMU, BLINK, Mantis, NLVR2, and MVBench.  The models compared include several existing large vision-language models (LLaVAs, Qwen-VL-Chat, etc.), as well as variations of LLaVa-v1.5 enhanced by different preference optimization techniques (LLaVa-RLHF, HA-DPO, POVID). The table displays each model's performance (as a percentage) on each benchmark and then shows the improvement achieved by the MIA-DPO method compared to the baseline LLaVa-v1.5 and InternLM-XC2.5 models.  The results highlight MIA-DPO's significant performance gains across all benchmarks compared to previous approaches. The \u0394 row shows the performance improvement that MIA-DPO brings to both LLaVA-v1.5 and InternLM-XC2.5.", "section": "4.2 RESULTS ON MULTI-IMAGES BENCHMARKS"}, {"figure_path": "2410.17637/tables/table_17_0.md", "caption": "Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "This table presents the main results of the proposed MIA-DPO model and compares it with other direct preference optimization (DPO) methods across five multi-image benchmarks: MMMU, BLINK, Mantis, NLVR2, and MVBench.  The table shows the average performance across these benchmarks for various large vision-language models (LVLMs), including LLaVA-v1.5 and InternLM-XC2.5, both with and without MIA-DPO applied.  The results are displayed as percentages, allowing for easy comparison between models and the impact of MIA-DPO.  The table also includes a row showing the performance difference (\u0394) achieved by adding MIA-DPO to each base model.", "section": "4 Experiments"}, {"figure_path": "2410.17637/tables/table_17_1.md", "caption": "Main results on multi-image benchmarks. We compare our MIA-DPO along with other DPO algorithms across five multi-image benchmarks. Our method brings significant performance improvements to both the classic LLaVa-v1.5 and the recent InternLM-XC2.5. In contrast, other single-image DPO methods perform poorly on multi-image benchmarks.", "description": "The table presents the main results of the MIA-DPO model and other Direct Preference Optimization (DPO) approaches on five multi-image benchmarks: MMMU, BLINK, Mantis, NLVR2, and MVBench.  It compares the average performance across these benchmarks for various large vision-language models (LVLMs), including LLaVa-v1.5, InternLM-XC2.5 and others, both with and without the application of different DPO techniques (including LLaVA-RLHF, HA-DPO, POVID, and MIA-DPO).  The table highlights the significant performance gains achieved by MIA-DPO on both LLaVa-v1.5 and InternLM-XC2.5 compared to other methods, showcasing its effectiveness in handling multi-image tasks. The delta values (\u0394) show the improvement in performance achieved by adding MIA-DPO to the baseline model.", "section": "4.2 Results on Multi-Images Benchmarks"}, {"figure_path": "2410.17637/tables/table_18_0.md", "caption": "Table 8: DPO Data Statistic. We listed in the table the data volume used for DPO with LLaVa-v1.5 and InternLM-XC2d5, along with the proportion of each type of data.", "description": "This table presents a statistical overview of the data used for Direct Preference Optimization (DPO) in the MIA-DPO framework, broken down by model and data type.  It shows the total amount of data used for DPO training, along with the specific number of samples used for each of the three data types employed in the MIA-DPO approach: Sequence, Grid Collage, and Pic-in-Pic. The table compares data usage statistics for two different large vision-language models (LVLMs): LLaVa-v1.5 and InternLM-XC2.5, highlighting the differing amounts of data used for each model and data type.", "section": "4.1 EXPERIMENTAL SETUP"}]