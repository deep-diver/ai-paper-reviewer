{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "GPT-4 technical report", "reason": "This paper is foundational as it introduces GPT-4, a highly performant proprietary model that serves as a benchmark for open-source LVLMs.  The paper's description of GPT-4's capabilities, especially in multi-image contexts, is crucial for understanding the gap that MIA-DPO aims to bridge and provides context for the performance improvements of MIA-DPO.  Without this benchmark, evaluating MIA-DPO's effectiveness would be significantly limited.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly influential as it introduces Reinforcement Learning from Human Feedback (RLHF), a crucial technique used in aligning LLMs with human preferences.  The paper's description of RLHF and its success in improving the quality of LLM responses provides crucial context and theoretical foundation for the use of preference alignment (specifically, DPO) in MIA-DPO, which aims to extend RLHF to multi-image scenarios.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An open-source framework for training large autoregressive vision-language models", "reason": "This paper is significant as it presents OpenFlamingo, a prominent open-source LVLMs that is often compared against proprietary models and serves as a baseline for evaluating the performance of MIA-DPO.  Understanding the capabilities and limitations of OpenFlamingo provides crucial context for assessing the improvements achieved by MIA-DPO.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "reason": "This paper introduces Qwen-VL, another significant open-source LVLMs that serves as a key comparison point for MIA-DPO.  The model's performance is directly relevant for evaluating the advancement provided by MIA-DPO in handling multi-image contexts. Its mention provides a specific, open-source comparative model for the authors to showcase their work's improvements.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Anas Awadalla", "paper_title": "MINT-1T: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens", "reason": "This paper is highly important due to its introduction of a massive multimodal dataset, MINT-1T.  The scale and diversity of this dataset are directly relevant to the challenges of data scarcity addressed by MIA-DPO, which leverages existing single-image data to create multi-image training data. This dataset demonstrates the potential impact of large-scale data on multi-image LVLMs, providing context for the importance of MIA-DPO's data-efficient techniques.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from ai feedback", "reason": "This paper is crucial as it introduces Reinforcement Learning from AI Feedback (RLAIF), an alternative to RLHF that is relevant to visual preference alignment.  The paper's comparison of RLHF and RLAIF provides valuable context for the authors' choice of using DPO, a technique related to both RLHF and RLAIF, as the optimization method for MIA-DPO.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Quan Sun", "paper_title": "Generative multimodal models are in-context learners", "reason": "This paper is relevant because it discusses LLaVA-RLHF, a visual preference alignment method that serves as a baseline for comparison with MIA-DPO. The paper's focus on mitigating hallucinations in single-image scenarios and the comparison of their method with MIA-DPO provide a clear illustration of the advances made by the proposed model in handling multi-image contexts.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lin Chen", "paper_title": "Are we on the right way for evaluating large vision-language models?", "reason": "This paper is relevant to the experimental setup of MIA-DPO because it provides a comprehensive review of existing benchmarks for LVLMs. The authors' discussion of various benchmark datasets and metrics helps provide context and justification for the selection of benchmarks used in the evaluation of MIA-DPO.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Anas Awadalla", "paper_title": "OpenFlamingo: An open-source framework for training large autoregressive vision-language models", "reason": "This paper introduces OpenFlamingo, a key model used for comparison in evaluating MIA-DPO.  Its significance lies in its role as a prominent open-source LVLMs, providing a baseline to assess the improvements achieved by MIA-DPO in handling multi-image data.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-VL: A frontier large vision-language model with versatile abilities", "reason": "This paper is highly significant as it describes Qwen-VL, a major open-source LVLMs used as a comparison model for MIA-DPO.  The comparison highlights the progress made by MIA-DPO in handling multi-image tasks, making it a critical reference point for evaluating the proposed method.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhiqing Sun", "paper_title": "Aligning large multimodal models with factually augmented rlhf", "reason": "This paper is essential as it presents LLaVA-RLHF, a crucial baseline method for visual preference alignment that is directly compared with MIA-DPO.  The paper's detailed description of LLaVA-RLHF and its performance on multi-image tasks is critical for understanding the advancements introduced by MIA-DPO.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "reason": "This is a highly important paper that introduces Direct Preference Optimization (DPO), the core optimization algorithm used in MIA-DPO.  The paper's detailed explanation of DPO's principles and its effectiveness in visual preference alignment is crucial for understanding the theoretical underpinnings of MIA-DPO and its ability to align model preferences with human preferences efficiently.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "reason": "This paper introduces Proximal Policy Optimization (PPO), an important reinforcement learning algorithm. Although MIA-DPO uses DPO, understanding PPO's context is crucial because DPO builds upon the core concepts of reinforcement learning algorithms like PPO.  This paper is therefore important for providing a fundamental understanding of the optimization techniques used in MIA-DPO.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Zhiyuan Zhao", "paper_title": "Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization", "reason": "This paper is highly relevant as it introduces HA-DPO, another visual preference alignment method that is directly compared to MIA-DPO. The performance comparison of HA-DPO and MIA-DPO highlights the strengths of the latter in handling multi-image scenarios and its cost efficiency.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Yiyang Zhou", "paper_title": "Aligning modalities in vision large language models via preference fine-tuning", "reason": "This paper is important because it explores preference fine-tuning in vision-language models, a topic closely related to MIA-DPO. The exploration of alternative preference alignment techniques provides valuable context for understanding the innovation and specific contributions of MIA-DPO.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Dongfu Jiang", "paper_title": "Interleaved multi-image instruction tuning", "reason": "This paper is relevant to MIA-DPO because it presents MANTIS, a method for multi-image instruction tuning in LVLMs. This is directly relevant to the goals of MIA-DPO, which aims to improve the alignment of LVLMs with human preferences in multi-image scenarios. The comparison between the two methods helps to contextualize the contribution of MIA-DPO in data efficiency and cost effectiveness.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "OBELICS: An open web-scale filtered dataset of interleaved image-text documents", "reason": "This paper introduces OBELICS, a valuable dataset relevant to MIA-DPO because of its focus on multi-image contexts. The dataset\u2019s characteristics, such as the interleaving of image and text data, are highly relevant to the challenges addressed by MIA-DPO, which aims to improve the handling of complex, multi-image scenarios in LVLMs.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Pan Lu", "paper_title": "BLINK: Multimodal large language models can see but not perceive", "reason": "This paper is relevant because it introduces the BLINK benchmark, a critical dataset used in the experimental evaluation of MIA-DPO.  BLINK's focus on visual perception tasks is directly relevant to the goals of MIA-DPO, which aims to improve visual understanding in LVLMs, making it a key dataset for evaluating MIA-DPO's performance.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Alane Suhr", "paper_title": "A corpus for reasoning about natural language grounded in photographs", "reason": "This paper is relevant because it introduces NLVR2, a benchmark dataset used in the evaluation of MIA-DPO.  NLVR2's focus on joint reasoning involving natural language and images is directly relevant to the goals of MIA-DPO, making it a crucial dataset for evaluating the model's performance on complex, multi-modal reasoning tasks.", "section_number": 4}]}