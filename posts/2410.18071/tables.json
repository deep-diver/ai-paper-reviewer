[{"figure_path": "2410.18071/tables/table_3_0.md", "caption": "Similar prompt changes have different effects on two models for helmet anomaly detection task in MMT-Bench.", "description": "The table presents a comparison of the performance of two different large language models, LLaVA and DeepSeek, on a helmet anomaly detection task within the MMT-Bench benchmark.  Three variations of a prompt asking whether a person in an image is wearing a helmet are used.  Each prompt variation differs slightly in wording, aiming to test the models' sensitivity to nuances in prompt phrasing. The table shows the accuracy scores obtained by each model for each prompt variation, illustrating how subtle changes in prompt wording can significantly affect the models' performance and highlighting a discrepancy in how the models respond to the same task under differently phrased prompts.", "section": "Multimodal Large Language Model Evaluation"}, {"figure_path": "2410.18071/tables/table_7_0.md", "caption": "Table 2: Overall result for MMT-S. All three models exhibited significant performance improvements across a substantial number of tasks following prompt customization.", "description": "Table 2 presents the overall performance results of three multimodal large language models (LLaVA-1.5-7B, DeepSeek-VL-7B, and Mini-InternVL-Chat-4B-V1-5) on the MMT-S benchmark after prompt optimization using the TP-Eval framework.  For each model, it shows the original score before prompt optimization, the improved score after optimization using TP-Eval, the number of tasks where the score improved, and the ratio of improved tasks to the total number of tasks. The results indicate substantial performance gains across the models, suggesting the effectiveness of the proposed TP-Eval method in mitigating prompt-induced bias and improving model evaluation.", "section": "5.2 MAIN RESULTS"}, {"figure_path": "2410.18071/tables/table_10_0.md", "caption": "Similar prompt changes have different effects on two models for helmet anomaly detection task in MMT-Bench.", "description": "The table presents a comparison of the performance of two different models, LLaVA and DeepSeek, on a helmet anomaly detection task within the MMT-Bench benchmark.  It showcases how subtle changes in prompt wording can significantly impact the accuracy of each model. Three variations of prompts are shown, each progressively more detailed. LLaVA exhibits a substantial accuracy increase (from 0.65 to 0.88) as the prompt becomes more specific, indicating sensitivity to prompt wording. In contrast, DeepSeek's accuracy changes are less pronounced and even shows a slight decrease (from 0.79 to 0.61) with the most detailed prompt.", "section": "2 MULTIMODAL LARGE LANGUAGE MODEL EVALUATION"}]