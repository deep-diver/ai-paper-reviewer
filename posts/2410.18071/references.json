{"references": [{" publication_date": "2023", "fullname_first_author": "Josh Achiam", "paper_title": "Gpt-4 technical report", "reason": "This paper is highly relevant as it introduces GPT-4, a powerful multimodal LLM that serves as a benchmark model and is directly related to the topic of MLLM evaluation which is the focus of the paper.", "section_number": 2}, {" publication_date": "2024a", "fullname_first_author": "Haotian Liu", "paper_title": "Visual instruction tuning", "reason": "This paper explores visual instruction tuning, a technique relevant to multimodal learning and model training, which is directly relevant to the topic of evaluating the capabilities of MLLMs, a key element of the current paper.", "section_number": 2}, {" publication_date": "2024b", "fullname_first_author": "Yuan Liu", "paper_title": "Mmbench: Is your multi-modal model an all-around player?", "reason": "This benchmark is explicitly mentioned and used for comparison in the paper. Understanding its strengths and weaknesses is crucial for evaluating the proposed TP-Eval framework.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Haoyu Lu", "paper_title": "Deepseek-vl: Towards real-world vision-language understanding", "reason": "DeepSeek is one of the models evaluated in the experiments.  Understanding its characteristics and performance is critical to evaluating the proposed TP-Eval framework.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Archiki Prasad", "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models", "reason": "This paper introduces a prompt optimization technique which is directly relevant to the method proposed in the current paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Reid Pryzant", "paper_title": "Automatic prompt optimization with\" gradient descent\" and beam search", "reason": "This paper proposes an automatic prompt optimization method using gradient descent and beam search, which is directly related to the technical approach of the current paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Melanie Sclar", "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting", "reason": "This research directly addresses the prompt sensitivity issue and its impact on model evaluation, providing a strong theoretical foundation for the current paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Xinyu Tang", "paper_title": "Unleashing the potential of large language models as prompt optimizers: An analogical analysis with gradient-based model optimizers", "reason": "This paper explores using LLMs as prompt optimizers, an advanced approach that is conceptually similar to the methods used in TP-Eval and offers valuable insights into the optimization process.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Chengrun Yang", "paper_title": "Large Language Models as Optimizers", "reason": "This paper explores the use of LLMs as optimizers, a technique relevant to the method proposed in the current paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Heng Yang", "paper_title": "Instoptima: Evolutionary multi-objective instruction optimization via large language model-based instruction operators", "reason": "This research explores evolutionary algorithms for prompt optimization which is an alternative approach compared to the gradient based method in the current paper.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Kaining Ying", "paper_title": "Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi", "reason": "MMT-Bench is a benchmark dataset used in the experiments. A thorough understanding of this benchmark is critical for interpreting and evaluating the results of TP-Eval.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Weihao Yu", "paper_title": "Mm-vet: Evaluating large multimodal models for integrated capabilities", "reason": "This paper introduces another benchmark dataset for multimodal LLMs, providing additional context and comparison points for evaluating the proposed method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Xiang Yue", "paper_title": "Mmmu: A massive multi-discipline multi-modal understanding and reasoning benchmark for expert agi", "reason": "MMMU is another benchmark dataset used in the experiments. Its characteristics are important for evaluating the performance and generalizability of TP-Eval.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Pengwei Zhan", "paper_title": "Mitigating the inconsistency between word saliency and model confidence with pathological contrastive training", "reason": "This paper discusses issues with LLMs and their sensitivity to various factors. This is a key consideration when customizing prompts as suggested in the current paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Pengwei Zhan", "paper_title": "Contrastive learning with adversarial examples for alleviating pathology of language model", "reason": "This paper builds upon the previous work by Zhan et al., further exploring the challenges and techniques related to improving the performance and reliability of LLMs, which is relevant to the current paper's context.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Pengwei Zhan", "paper_title": "Rethinking word-level adversarial attack: The trade-off between efficiency, effectiveness, and imperceptibility", "reason": "This paper continues the line of research by Zhan et al., focusing on adversarial attacks against LLMs, which helps provide insight into the challenges of designing robust and reliable LLM evaluations, a key element discussed in the current paper.", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Brian Lester", "paper_title": "The power of scale for parameter-efficient prompt tuning", "reason": "This paper discusses parameter-efficient prompt tuning, a technique relevant to the efficient customization of prompts which is the central idea of the current paper.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Bohao Li", "paper_title": "Seed-bench: Benchmarking multimodal llms with generative comprehension", "reason": "This paper introduces SEED-Bench, a benchmark dataset for multimodal LLMs which is relevant for comparison to the results achieved in the current paper.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Prefix-tuning: Optimizing continuous prompts for generation", "reason": "This paper explores prefix tuning, an optimization technique for prompts which is conceptually similar to the approach taken in TP-Eval.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tianjun Zhang", "paper_title": "Test-time prompting via reinforcement learning", "reason": "This paper explores test-time prompting using reinforcement learning which is directly related to the automatic prompt customization process in the current paper.", "section_number": 3}]}