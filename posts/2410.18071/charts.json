[{"figure_path": "2410.18071/charts/charts_7_0.png", "caption": "Figure 3: Results of different models on MMT-S (L2-category). Accuracy improvement is calculated by accuracy using the optimized prompt divided by accuracy using the original prompt. Three models showed varying improvement across different task types, while performance gains differ between models, highlighting the underestimation and bias introduced by original prompts and the effectiveness of our method.", "description": "The bar chart displays the accuracy improvement percentage for three different Multimodal Large Language Models (MLLMs): LLaVA, DeepSeek, and InternVL, across various tasks within the MMT-S benchmark.  Each bar represents a specific task, and the height of the bar indicates the percentage increase in accuracy achieved by using optimized prompts compared to the original prompts.  The chart shows varied levels of improvement across different models and tasks, highlighting the impact of prompt customization on model performance. Some tasks show significant improvement for all models, while others show little to no change, demonstrating the varying effects of prompt customization across different tasks and models.", "section": "5.2 Main Results"}, {"figure_path": "2410.18071/charts/charts_8_0.png", "caption": "Figure 4: Overall performance with different prompt methods on MMMU with LLaVA. In most cases, the results after optimization surpass those achieved with the initial prompts, and they generally outperform the original questions as well.", "description": "The bar chart displays the overall performance of LLaVA on the MMMU benchmark across six different categories (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering), plus an aggregated \"ALL\" category, using three different prompt methods: the original question, an initial prefix prompt, and an optimal prefix prompt generated by their method. For each category and prompt method, a bar represents the accuracy achieved. The chart reveals that the optimal prefix prompts consistently yield the highest accuracy across all categories, significantly outperforming both the original questions and the initial prefix prompts.  The improvement is particularly noticeable in the \"ALL\" category, suggesting the effectiveness of their proposed prompt optimization technique in enhancing LLaVA's performance on the MMMU benchmark.", "section": "5.2 MAIN RESULTS"}, {"figure_path": "2410.18071/charts/charts_8_1.png", "caption": "Figure 5: Result of applying optimized prompts to other models. Applying customized prompts from one model to another yields performance changes that differ from each model's inherent characteristics.", "description": "This heatmap visualizes the performance changes observed when applying prompts optimized for one model to other models. Each cell in the heatmap represents the performance difference (in percentage points) of a target model when using prompts optimized for a source model compared to its original performance. The rows and columns represent target models and source models, respectively.  The color intensity reflects the magnitude of the performance change, with darker reds indicating positive changes and lighter oranges indicating negative changes.  For instance, the top-left cell shows that when using prompts optimized for LLaVA on LLaVA, there is a 3.97% performance increase.  Conversely, the bottom-right cell displays a 2.29% improvement when using InternVL's optimized prompts on InternVL.", "section": "5.2.2 OPTIMALITY ANALYSIS"}, {"figure_path": "2410.18071/charts/charts_9_0.png", "caption": "Figure 6: Performance on whether to use introspection or not.", "description": "The bar chart displays the performance comparison of three methods on three different tasks: artwork_emotion_recognition, helmet_anomaly_detection, and behavior_anomaly_detection.  The three methods are 'Origin' (original prompt), 'No Introspection' (without introspection), and 'Ours' (the proposed method with introspection). For each task, three bars represent the accuracy achieved by each method, showing the impact of using introspection on the performance. The chart illustrates that using introspection ('Ours') generally leads to improved accuracy compared to using the original prompts ('Origin') and not incorporating introspection ('No Introspection'), although in one instance (behavior_anomaly_detection), the 'Origin' method performs slightly better than the proposed method without introspection.", "section": "5.3 ABLATION STUDY"}, {"figure_path": "2410.18071/charts/charts_9_1.png", "caption": "Figure 7: Influence of re-ranking. Both excessively high and low a can lead to a reduction in performance, and each model achieves optimal performance with a \u2208 [0.5, 0.6].", "description": "The chart displays the impact of the re-ranking parameter 'a*' on the accuracy of three different multi-modal large language models (MLLMs): LLaVA, DeepSeek, and InternVL.  The x-axis represents different values of 'a*', ranging from 0.4 to 0.8. The y-axis shows the accuracy achieved by each model for each 'a*' value. Each MLLM is represented by a different line style (dashed, dotted, and dash-dotted) and color (steelblue, darkorange, and mediumseagreen). The chart shows that for all three models, there's an optimal range for 'a*' (between 0.5 and 0.6) where accuracy is maximized, while values outside this range lead to lower accuracy, indicating that a balance must be struck between optimization and maintaining semantic similarity during the prompt customization process.", "section": "5.3 ABLATION STUDY"}]