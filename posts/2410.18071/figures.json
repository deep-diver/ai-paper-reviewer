[{"figure_path": "2410.18071/figures/figures_1_0.png", "caption": "Figure 1: (a) shows underestimation caused by unsuitable prompts in MMT-Bench, (b) shows our proposed evaluation framework resolving this by customizing prompts.", "description": "The figure is composed of two subfigures. Subfigure (a) illustrates the prompt sensitivity issue in the MMT-Bench benchmark. It shows how different phrasings of the same question about image similarity lead to vastly different accuracy results for the LLaVA model, highlighting the problem of underestimation caused by unsuitable prompts. Subfigure (b) presents the proposed TP-Eval framework, which addresses this issue by incorporating a prompt customizer module. This module takes the original prompts and a few examples from the test set as input to generate customized prompts for different models, aiming to improve evaluation accuracy and reduce bias.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18071/figures/figures_5_0.png", "caption": "Figure 2: The overview of our automatic prompt customization structure.", "description": "This figure illustrates the overall framework of the automatic prompt customization method used in TP-Eval. It starts with the initial text prompt (p0) from a multimodal evaluation dataset and utilizes GPT-40 mini as an optimizer (M0) and a few examples (Dfew) to obtain an optimal prompt (p*) for the MLLM (MT) being evaluated. The process involves a scorer which consists of the MLLM and an answer analyzer (MA, also GPT-40 mini) to generate scores and introspection,  which are used to iteratively construct meta-prompts for the optimizer to create refined prompt sets (P1, P2,... PN). Finally, the optimal prompt (p*) is selected through a re-ranking step based on scores and semantic similarity.", "section": "4 METHOD"}]