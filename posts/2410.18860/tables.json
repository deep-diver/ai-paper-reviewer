[{"figure_path": "2410.18860/tables/table_6_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on several faithfulness evaluation tasks.  The tasks include XSum (summarization), MemoTrap (instruction following avoiding memorization), IFEval (instruction following), NQ-Open (open-book question answering), and NQ-Swap (question answering with swapped answer entity).  For each LLM (Llama3-8b-Instruct and Llama3-70b-Instruct), the table shows the performance of the base model and several decoding methods including ITI, CAD, DoLA (low and high), AD, DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite. The performance metrics vary depending on the task, including ROUGE-L, BERTScore-F1, factKB, macro accuracy, micro accuracy, prompt accuracy, instruction accuracy, and exact match. The best performing method for each model is highlighted in bold, with the second-best underlined.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_6_1.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of various LLMs and decoding methods on several faithfulness evaluation tasks.  It shows ROUGE-L, BERTScore-F1, factKB scores for XSum, macro and micro accuracy scores for MemoTrap, and prompt and instruction accuracy scores for Instruction-Following Eval. For NQ-Open and NQ-Swap, the EM scores are shown.  The best performing model for each base model (Llama3-8b-Instruct and Llama3-70b-Instruct) is shown in bold, with the second best underlined. The table allows for a comparison of different decoding methods against the base models and provides insights into their relative effectiveness for various tasks.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_7_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks.  Specifically, it shows the ROUGE-L, BERTScore-F1, and factKB scores for the XSum summarization task, macro and micro-averaged accuracy for the MemoTrap instruction-following task, prompt-level and instruction-level accuracy for the Instruction-Following Eval (IFEval) task, and exact match (EM) scores for the Open Book NQ-Open and NQ-Swap reading comprehension tasks.  The results are broken down by model (Llama3-8B-Instruct and Llama3-70B-Instruct) and decoding method (Greedy Decoding, Contrastive Decoding, Context-Aware Decoding, Decoding by Contrasting Layers (low and high), Activation Decoding, Inference-Time Intervention, DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite). The best performance for each model is indicated in bold, and the second-best is underlined, enabling a direct comparison of different methods' effectiveness on various faithfulness evaluation metrics.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_19_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on various faithfulness evaluation tasks.  The tasks include XSum (summarization), MemoTrap (instruction following), IFEval (instruction following), NQ-Open (open-book question answering), and NQ-Swap (reading comprehension with swapped answers).  For each LLM (Llama3-8B-Instruct and Llama3-70B-Instruct), results are shown for several decoding methods: Greedy decoding, ITI, CAD, DoLA (low and high), AD, DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite. Multiple metrics are used to evaluate performance on each task, including ROUGE-L, BERTScore-F1, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruct Accuracy, and EM. The best performing method for each LLM and task is highlighted in bold, with the second best underlined. The table demonstrates the effectiveness of DeCoRe (particularly DeCoReentropy) in improving the faithfulness of LLM outputs on various tasks.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_19_1.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 1 presents the performance of various LLMs and decoding methods on several faithfulness evaluation tasks: XSum (summarization), MemoTrap (instruction following), IFEval (instruction following), NQ-Open (open-book question answering), and NQ-Swap (question answering with swapped answer entity).  For each LLM (Llama3-8B-Instruct and Llama3-70B-Instruct), the table shows the results for a baseline model with greedy decoding and several other decoding methods: ITI, CAD, DoLA (low and high), AD, DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite.  The metrics used vary depending on the specific task, but generally include ROUGE-L, BERTScore, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruct Accuracy, and Exact Match. The best performing method for each LLM on each task is highlighted in bold, and the second-best is underlined.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_19_2.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance of various LLMs and decoding methods on several faithfulness evaluation tasks, including XSum, MemoTrap, IFEval, NQ-Open, and NQ-Swap.  For each model (Llama3-8B-Instruct and Llama3-70B-Instruct), results are shown for several baselines (Greedy decoding, Contrastive Decoding, Context-Aware Decoding, Decoding by Contrasting Layers, Activation Decoding, and Inference-Time Intervention) and the proposed DeCoRe method (with static and dynamic entropy variants).  The metrics used vary depending on the specific task and include ROUGE-L, BERTScore, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruction Accuracy, and Exact Match.  The best performance for each base model is highlighted in bold, and the second-best is underlined, allowing for a direct comparison of the effectiveness of different decoding methods across multiple tasks.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_21_0.md", "caption": "Table 7: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on factuality evaluation tasks.", "description": "This table presents the performance comparison of Llama3-8B-Instruct model on factuality evaluation tasks (TruthfulQA (MC), TriviaQA, PopQA, and NQ-Open) with varying numbers of masked retrieval heads.  For each task, the table shows the performance metrics (MC1\u2191, MC2\u2191, MC3\u2191, EM\u2191, EM\u2191, EM\u2191) achieved by the model with 0 (baseline), 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100 masked retrieval heads.  The best performance for each metric and model is highlighted in bold, while the second-best is underlined. This allows for a direct comparison of model performance across different numbers of masked retrieval heads and across different factuality datasets.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_21_1.md", "caption": "Table 2: Performance of different models and decoding methods on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 2 presents the results of factuality evaluation experiments across multiple datasets, including TruthfulQA (MC, Generation), TriviaQA, PopQA, and NQ-Open.  For each base model (Llama3-8B-Instruct and Llama3-70B-Instruct), the table compares the performance of DeCoRe (DeCoRestatic, DeCoReentropy, DeCoReentropy-lite) against several baselines (Greedy decoding, Contrastive Decoding, Context-Aware Decoding, Decoding by Contrasting Layers, Activation Decoding, and Inference-Time Intervention).  The metrics used vary depending on the dataset; these include multi-label classification accuracy (MC1, MC2, MC3 for TruthfulQA), exact match (EM) scores for TriviaQA, PopQA, and NQ-Open, percentage of truthful and informative responses, and percentage of tokens of interest for the TruthfulQA Generation task.  The best and second-best performing methods for each base model are highlighted in bold and underlined, respectively, to demonstrate DeCoRe's improvement on factuality.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_22_0.md", "caption": "Table 9: Performance comparison of Llama3-8B-Instruct with different number of masked retrieval heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "This table presents the performance of the Llama3-8B-Instruct model on the MuSiQue dataset, a multi-hop reasoning task, using different numbers of masked retrieval heads (from 0 to 100).  The results are broken down by whether Chain-of-Thought (CoT) prompting was used (with CoT or without CoT) and whether the setting was closed-book or open-book.  For each condition (number of masked heads, use of CoT, and book type), the Exact Match (EM) score is provided, representing the accuracy of the model's answers.  The baseline represents the performance without any retrieval heads masked.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_22_1.md", "caption": "Table 10: Performance comparison of Llama3-8B-Instruct with different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "This table presents the performance comparison of the Llama3-8B-Instruct model on the MuSiQue dataset, a multi-hop reasoning task, using different numbers of masked random heads. The experiments were conducted with and without Chain-of-Thought (CoT) prompting, in both closed-book and open-book settings.  The table shows the EM (Exact Match) scores for each experimental setup,  including the baseline (0 masked heads) and for 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100 masked random heads.  Results are presented with standard deviations indicating the variability in performance across multiple runs.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_23_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the results of faithfulness evaluation tasks using different models and decoding methods.  The faithfulness evaluation tasks include XSum (summarization), MemoTrap (instruction-following avoiding memorization traps), Instruction-Following Eval (IFEval), and two open-book question answering datasets: NQ-Open and NQ-Swap.  Performance metrics vary depending on the specific task and include ROUGE-L, BERTScore, factKB, macro-averaged accuracy, micro-averaged accuracy, prompt-level accuracy, instruction-level accuracy, and exact match (EM).  For each base model (Llama3-8b-Instruct and Llama3-70b-Instruct), the best performing method is shown in bold, and the second-best method is underlined. The table compares the performance of DeCoRe (static and dynamic versions) against several baseline methods, such as greedy decoding, Contrastive Decoding, Context-Aware Decoding, Decoding by Contrasting Layers, Activation Decoding, and Inference-Time Intervention. ", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_24_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods across various faithfulness evaluation tasks, including XSum, MemoTrap, IFEval, NQ-Open and NQ-Swap.  For each LLM (Llama3-8b-Instruct and Llama3-70b-Instruct), the table shows the performance of several decoding methods such as greedy decoding, ITI, CAD, DoLA (low and high), AD, DeCoRestatic, and DeCoReentropy. The metrics used for evaluation vary depending on the task and include ROUGE-L, BERTScore-F1, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruct Accuracy, and EM. The best and second-best performance for each LLM are indicated in bold and underlined, respectively. The table enables a comparison of the different methods in improving faithfulness across different tasks and model sizes.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_26_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance of various LLMs and decoding methods on several faithfulness evaluation tasks, including XSum, MemoTrap, IFEval, NQ-Open, and NQ-Swap.  For each model (Llama3-8B-Instruct and Llama3-70B-Instruct), results are shown for different decoding methods:  Greedy Decoding, ITI (Li et al., 2024b), CAD (Shi et al., 2024), DoLA (Chuang et al., 2023), AD (Chen et al., 2024), DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite.  The table reports multiple metrics depending on the task, such as ROUGE-L, BERTScore-F1, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruct Accuracy, and EM, allowing for a comprehensive comparison of model performance across different decoding strategies. The best performing decoding method for each base model is highlighted in bold, while the second-best is underlined.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_26_1.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance of various LLMs and decoding methods on five different faithfulness evaluation tasks: XSum (ROUGE-L, BERTScore-F1, factKB), MemoTrap (Macro Acc, Micro Acc), and Instruction Following Eval (Prompt Acc, Instruct Acc), NQ-Open (EM), and NQ-Swap (EM).  The table compares the baseline Llama3-8B-Instruct and Llama3-70B-Instruct models against several other decoding methods, including ITI, CAD, DoLA, AD, DeCoRestatic, DeCoReentropy and DeCoReentropy-lite.  For each model, the best and second-best performance across the various metrics are highlighted in bold and underlined respectively, allowing for a comparison of the effectiveness of each decoding method in improving the faithfulness of LLM outputs on these different tasks.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_27_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the results of faithfulness evaluation tasks using different models and decoding methods.  The faithfulness evaluation tasks include XSum (summarization), MemoTrap (instruction following), IFEval (instruction following), NQ-Open (open-book question answering), and NQ-Swap (question answering with swapped answers). The table shows the performance metrics for each task and each model, including ROUGE-L, BERTScore-F1, factKB, macro-averaged accuracy, micro-averaged accuracy, prompt-level strict accuracy, instruction-level strict accuracy, exact match (EM) for NQ-Open, and EM for NQ-Swap.  For each base model (Llama-8B and Llama-70B), the best performing decoding method is highlighted in bold, and the second best is underlined. This allows for a direct comparison of the effectiveness of DeCoRe against several baseline decoding methods across various faithfulness-focused tasks and model sizes.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_27_1.md", "caption": "Table 2: Performance of different models and decoding methods on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 2 presents the results of factuality evaluation experiments across various models and decoding methods.  It shows the performance on TruthfulQA (a multi-label classification task with four sub-tasks: MC1, MC2, MC3, and a generation task, Gen) and three question-answering datasets: TriviaQA, PopQA, and NQ-Open.  For each dataset, the table displays the performance metrics (e.g., EM for TriviaQA, PopQA, and NQ-Open, accuracy for TruthfulQA MC) for the Llama3-8B-Instruct and Llama3-70B-Instruct base models, along with results after applying various hallucination mitigation methods: ITI, CD, CAD, DoLA (low and high), AD, DeCoRestatic, and DeCoReentropy. The best performing method for each base model is highlighted in bold, with the second-best underlined, providing a direct comparison of DeCoRe's performance against established baselines on factuality evaluation tasks.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_28_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different language models and decoding methods on various faithfulness evaluation tasks.  The tasks include XSum (summarization), MemoTrap (instruction following), IFEval (instruction following), NQ-Open (open-domain question answering), and NQ-Swap (reading comprehension).  For each task, multiple metrics are reported, including ROUGE-L, BERTScore-F1, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruction Accuracy, and EM. The results are shown for two base language models (Llama3-8b-Instruct and Llama3-70b-Instruct) and several decoding methods, including ITI, CAD, DoLA (low and high), AD, DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite.  For each base model, the best performing decoding method is highlighted in bold, and the second-best is underlined.  The table allows for a comparison of the effectiveness of various decoding strategies in improving the faithfulness of language models across different tasks and model sizes.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_28_1.md", "caption": "Table 19: Performance comparison across different numbers of masked random heads on MuSiQue, a multi-hop reasoning dataset, with and without CoT prompting in both closed-book and open-book settings.", "description": "This table presents the performance of Llama3-8B-Instruct model using DeCoRe entropy on MuSiQue dataset with varying numbers of masked random heads. The experiments are conducted in both closed-book and open-book settings, with and without chain-of-thought (CoT) prompting.  For each setting, the table shows the EM (Exact Match) score which is a metric of accuracy.  The baseline performance (0 masked heads) is shown for comparison, alongside the results for masking 10, 20, 30, 40, 50, 60, 70, 80, 90, and 100 random heads.  The table highlights how the performance changes in different multi-hop reasoning scenarios when a varying number of random heads are masked.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_29_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on several faithfulness evaluation tasks, including XSum (summarization), MemoTrap (instruction following), and NQ-Open/NQ-Swap (question answering).  For each LLM (Llama3-8B-Instruct and Llama3-70B-Instruct), it shows the performance of several baselines (Greedy decoding, Contrastive Decoding, Context-Aware Decoding, Decoding by Contrasting Layers, Activation Decoding, and ITI), along with DeCoRe's variants (DeCoRe static, DeCoRe entropy, and DeCoRe entropy-lite).  The metrics used vary depending on the task, including ROUGE-L, BERTScore, factKB for XSum; macro and micro-averaged accuracy for MemoTrap; prompt and instruction level accuracy for IFEval; and exact match for NQ-Open and NQ-Swap. The best performance for each base model is indicated in bold, and the second best is underlined.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_30_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on several faithfulness evaluation tasks.  The tasks include XSum (summarization), MemoTrap (instruction following), IFEval (instruction following), NQ-Open (open-book question answering), and NQ-Swap (question answering with swapped answers). For each model (Llama3-8b-Instruct and Llama3-70b-Instruct), the table shows the results for different decoding methods including the baseline (greedy decoding), ITI, CAD, DoLA (low and high), AD, DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite. The evaluation metrics used vary by task and include ROUGE-L, BERTScore-F1, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruct Accuracy, and Exact Match (EM). The best performance for each model is highlighted in bold, and the second-best is underlined, allowing for a direct comparison of different methods' effectiveness in mitigating hallucination in faithfulness tasks.", "section": "4 Results"}, {"figure_path": "2410.18860/tables/table_30_1.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs (Llama3-8B-Instruct and Llama3-70B-Instruct) and decoding methods on faithfulness evaluation tasks.  It includes metrics such as ROUGE-L, BERTScore-F1, and factKB for the XSum summarization dataset; macro and micro-averaged accuracy for the MemoTrap instruction following dataset; prompt and instruction accuracy for the Instruction-Following Eval dataset; and exact match (EM) for the NQ-Open and NQ-Swap question answering datasets. For each base LLM, the best performing decoding method is highlighted in bold, while the second-best is underlined. The table allows for a direct comparison of various decoding strategies (ITI, CAD, DoLA, AD, DeCoRestatic, DeCoReentropy, DeCoReentropy-lite) in terms of their effectiveness in improving the faithfulness of LLM outputs across different tasks and model sizes.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_31_0.md", "caption": "Table 23: Performance of Llama3-8b-Instruct with DeCoRestatic on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "Table 23 presents the results of an ablation study on the DeCoRestatic variant of the DeCoRe model, evaluating its performance on several faithfulness tasks across different values of the scaling factor \u03b1. The table shows the performance metrics (ROUGE-L, BERTScore-F1, factKB, Macro Acc, Micro Acc, Instruct Acc, Prompt Acc, EM) for each task (XSum, MemoTrap, IFEval, NQ-Open, NQ-Swap) and across different values of \u03b1 (-0.5, 0.0, 0.5, 1.0, 2.0, 4.0, 8.0). For each task and base model, the best performance is shown in bold and the second-best is underlined.  This demonstrates how different values for the scaling parameter affect the model's performance in mitigating hallucinations across different faithfulness-oriented tasks.", "section": "I. Ablation of DeCoRestatic"}, {"figure_path": "2410.18860/tables/table_32_0.md", "caption": "Performance of Llama3-8b-Instruct with DeCoRestatic on factuality evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the results of the factuality evaluation tasks using the Llama3-8b-Instruct model with the DeCoRestatic decoding method.  It shows the performance across different values of the hyperparameter 'alpha' (\u03b1), which controls the weight of the contrastive decoding.  The metrics reported include MC1, MC2, and MC3 scores from the TruthfulQA dataset, and EM scores from TriviaQA, PopQA, and NQ-Open datasets. The best performance for each metric and model is indicated in bold, while the second-best performance is underlined, offering a comparison of model performance under varying levels of contrastive decoding.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_32_1.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different Language Models (LLMs) and decoding methods on several faithfulness evaluation tasks.  The tasks include XSum (summarization), MemoTrap (instruction following), IFEval (instruction following), NQ-Open (open-domain question answering), and NQ-Swap (question answering with swapped answer entity).  For each LLM (Llama3-8b-Instruct and Llama3-70b-Instruct), the table shows the performance of several decoding methods (Greedy, ITI, CAD, DoLA-low, DoLA-high, AD, DeCoRestatic, DeCoReentropy, and DeCoReentropy-lite).  The evaluation metrics vary depending on the task and include ROUGE-L, BERTScore-F1, factKB, Macro Accuracy, Micro Accuracy, Prompt Accuracy, Instruct Accuracy, and Exact Match (EM). The best performing method for each base model is highlighted in bold, while the second-best is underlined.", "section": "4 RESULTS"}, {"figure_path": "2410.18860/tables/table_33_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the performance comparison of different LLMs and decoding methods on several faithfulness evaluation tasks.  The tasks include XSum (summarization), MemoTrap (instruction following), IFEval (instruction following), NQ-Open (open-domain question answering), and NQ-Swap (reading comprehension with swapped answers).  For each LLM (Llama-3 8B and Llama-3 70B), the performance is shown using various metrics depending on the task: ROUGE-L, BERTScore, factKB (for XSum), macro and micro-averaged accuracy (for MemoTrap), prompt and instruction accuracy (for IFEval), and EM (exact match) (for NQ-Open and NQ-Swap).  The table also includes several decoding methods as baselines for comparison: Greedy Decoding, Contrastive Decoding (CD), Context-Aware Decoding (CAD), Decoding by Contrasting Layers (DoLA), Activation Decoding (AD), and Inference-Time Intervention (ITI).  The best-performing method for each LLM is highlighted in bold, with the second-best underlined.", "section": "3.1 Datasets and Evaluation Metrics"}, {"figure_path": "2410.18860/tables/table_34_0.md", "caption": "Table 1: Performance of different models and decoding methods on faithfulness evaluation tasks. For each base model, the best performance is indicated in bold, and the second-best is underlined.", "description": "This table presents the results of faithfulness evaluation tasks using different models and decoding methods.  It shows the performance on XSum (ROUGE-L, BERTScore-F1, factKB), MemoTrap (Macro Accuracy, Micro Accuracy), Instruction-Following Eval (Prompt Acc, Instruct Acc), NQ-Open (EM), and NQ-Swap (EM). The best performance for each base model (Llama3-8B-Instruct and Llama3-70B-Instruct) is highlighted in bold, while the second-best is underlined.  The table compares the performance of the base models with several baseline methods including ITI, CAD, DoLA (low and high), AD, DeCoRestatic, and DeCoReentropy. DeCoReentropy-lite results are also included for the Llama3-8B-Instruct model.  Each metric is assessed using appropriate metrics for that task (e.g., ROUGE scores for XSum, Accuracy scores for MemoTrap, and EM for NQ tasks).", "section": "4 Results"}]