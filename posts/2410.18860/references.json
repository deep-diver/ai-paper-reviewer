{"references": [{" publication_date": "2023", "fullname_first_author": "Muhammad Aurangzeb Ahmad", "paper_title": "Creating trustworthy llms: Dealing with hallucinations in healthcare ai", "reason": "This paper is highly relevant due to its focus on hallucinations in LLMs, specifically within the healthcare domain. The authors' exploration of techniques to mitigate hallucinations, while different from DeCoRe's approach, provides valuable insights and context for understanding the challenges associated with hallucination mitigation in LLMs.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom Brown", "paper_title": "Language models are few-shot learners", "reason": "This foundational paper introduces the concept of few-shot learning in large language models, a critical concept that influences the approach and evaluation of DeCoRe, which is a training-free method. It's fundamental to understanding the capabilities and limitations of LLMs, as DeCoRe aims to improve their reliability without additional training.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Ziwei Ji", "paper_title": "Survey of hallucination in natural language generation", "reason": "This survey paper provides a comprehensive overview of the existing research on hallucinations in natural language generation. It's crucial in contextualizing DeCoRe's contribution and understanding the current state of knowledge in the field, thereby highlighting the novelty and significance of DeCoRe's proposed approach.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Vipula Rawte", "paper_title": "A survey of hallucination in large foundation models", "reason": "Similar to the previous reference, this survey paper offers a comprehensive review of research on hallucinations in large language models.  Its importance lies in its broad coverage of existing methods for mitigating hallucinations, providing context for understanding the novelty and unique features of DeCoRe's training-free approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Junyi Li", "paper_title": "The dawn after the dark: An empirical study on factuality hallucination in large language models", "reason": "This paper directly addresses factuality hallucinations in LLMs, a key aspect of the problem DeCoRe aims to solve.  Its empirical study provides valuable insights into the nature and characteristics of factuality hallucinations, providing context and a benchmark for evaluating the improvements achieved by DeCoRe.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Wenhao Wu", "paper_title": "Retrieval head mechanistically explains long-context factuality", "reason": "This paper is a cornerstone for DeCoRe because it identifies specific attention heads ('retrieval heads') within the LLM architecture responsible for accessing and retrieving context information. DeCoRe directly leverages this insight, building upon Wu et al.'s findings to propose a novel technique for mitigating hallucinations by manipulating these retrieval heads.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This seminal paper introduces the Transformer architecture, the foundation of many large language models.  DeCoRe is intrinsically linked to the Transformer architecture, as it directly manipulates specific attention heads within this framework. Understanding the Transformer architecture is essential to grasping the core concept and methodology of DeCoRe.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiang Lisa Li", "paper_title": "Contrastive decoding: Open-ended text generation as optimization", "reason": "This paper introduces contrastive decoding, a crucial technique that DeCoRe builds upon.  Contrastive decoding, by contrasting the outputs of different models, directly influences DeCoRe's approach to comparing the base LLM's output with a masked LLM's output to reduce hallucinations. Understanding contrastive decoding is central to understanding DeCoRe's core mechanism.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Andrey Malinin", "paper_title": "Uncertainty estimation in autoregressive structured prediction", "reason": "This paper introduces uncertainty quantification methods, which are central to DeCoRe's dynamic weighting of LLM outputs. DeCoRe leverages conditional entropy (a measure of uncertainty) to dynamically adjust the weighting of the base LLM and masked LLM outputs. Therefore, understanding uncertainty estimation is key to fully grasping DeCoRe's methodology.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Lookback lens: Detecting and mitigating contextual hallucinations in large language models using only attention maps", "reason": "This paper focuses on detecting and mitigating hallucinations using attention maps, a related approach to DeCoRe, which uses masking of specific attention heads to induce hallucinations. Comparing the methods highlights the strengths and limitations of different approaches to address this issue.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Shiqi Chen", "paper_title": "In-context sharpness as alerts: An inner representation perspective for hallucination mitigation", "reason": "This paper provides insights into the inner workings of LLMs, specifically regarding the relationship between model representations and hallucination. Understanding this relationship complements the understanding of DeCoRe, which directly targets specific components (retrieval heads) within the model architecture for hallucination mitigation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yung-Sung Chuang", "paper_title": "Decoding by contrasting layers improves factuality in large language models", "reason": "This work presents DoLA, a contrasting method, which DeCoRe directly compares with in the experiments.  Understanding DoLa's approach, strengths, and weaknesses helps to contextualize DeCoRe's performance and advantages, highlighting the unique aspects of DeCoRe, such as its training-free nature and dynamic weighting based on conditional entropy.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Lei Huang", "paper_title": "A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions", "reason": "This survey paper provides a thorough overview of the research on hallucinations in LLMs, contextualizing DeCoRe within the broader research landscape.  It helps in understanding the challenges, taxonomy, and open research questions associated with hallucinations, which are important for evaluating DeCoRe's contributions.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Muru Zhang", "paper_title": "How language model hallucinations can snowball", "reason": "This paper discusses the 'snowball effect' of hallucinations, where initial errors compound in long-generation tasks, leading to increased inaccuracies. This is directly relevant to DeCoRe's experimental setup and evaluation as it highlights a critical challenge in long-sequence generation tasks, where DeCoRe aims to improve accuracy and reliability.", "section_number": 3}, {" publication_date": "2018", "fullname_first_author": "Shashi Narayan", "paper_title": "Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization", "reason": "This paper introduces the XSum dataset, one of the key benchmarks used to evaluate DeCoRe's performance.  The XSum dataset is specifically designed to assess abstractive summarization capabilities, a task where DeCoRe demonstrates significant improvements in mitigating hallucinations and improving accuracy.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Alisa Liu", "paper_title": "The memotrap dataset", "reason": "This paper introduces the MemoTrap dataset, another key benchmark used to assess DeCoRe's performance.  The MemoTrap dataset focuses on instruction following, and DeCoRe's ability to improve performance in this task is a crucial indicator of its efficacy in mitigating hallucinations.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Jeffrey Zhou", "paper_title": "Instruction-following evaluation for large language models", "reason": "This paper presents the Instruction-Following Eval (IFEval) benchmark, a critical component of the experimental setup used to evaluate DeCoRe.  IFEval directly assesses the model's ability to accurately follow instructions, a task where DeCoRe demonstrates improved performance, validating its effectiveness in mitigating hallucinations.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly relevant to DeCoRe because it introduces a method for training LLMs to follow instructions using human feedback. Although DeCoRe is a training-free method, understanding instruction-following and its challenges, as detailed in this paper, helps to evaluate DeCoRe's impact on tasks involving instruction following.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "William Merrill", "paper_title": "The Parallelism Tradeoff: Limitations of Log-Precision Transformers", "reason": "This paper discusses the limitations of log-precision transformers, which is relevant to the context of DeCoRe, as it provides a deeper understanding of the challenges associated with achieving accuracy and efficiency in large language model outputs, especially in tasks requiring extended sequences and reasoning steps.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Giwon Hong", "paper_title": "The hallucinations leaderboard-an open effort to measure hallucinations in large language models", "reason": "This paper presents a comprehensive benchmark for measuring hallucinations in LLMs, which is highly relevant to DeCoRe. The hallucinations leaderboard provides a standardized framework for comparing different approaches to mitigate hallucinations, enabling a more objective assessment of DeCoRe's effectiveness and contribution to the field.", "section_number": 3}]}