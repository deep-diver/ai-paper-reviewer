{"reason": "This paper introduces DeCoRe, a decoding strategy that mitigates hallucinations in LLMs by contrasting the outputs of a base LLM and a masked LLM (with retrieval heads masked).  DeCoRe dynamically adjusts the contrast based on the conditional entropy of the base LLM's next-token distribution, thus enhancing contextual faithfulness and factual consistency. Experiments across summarization, instruction following, and open-book question answering tasks show that DeCoRe significantly improves performance.", "takeaways": ["DeCoRe, a training-free decoding method, significantly reduces LLM hallucinations by contrasting the outputs of a base LLM and one with masked retrieval heads.", "Masking retrieval heads induces hallucinations, and contrasting outputs guided by conditional entropy amplifies accurate information.", "DeCoRe improves contextual faithfulness and factual accuracy across multiple tasks, demonstrating broad effectiveness."], "tldr": "To mitigate Large Language Model (LLM) hallucinations, DeCoRe contrasts outputs from a base LLM and one with masked retrieval heads (identified as crucial for factual recall), dynamically adjusting contrast based on conditional entropy.  This training-free method substantially improves performance on tasks demanding high contextual faithfulness, such as summarization and open-book QA."}