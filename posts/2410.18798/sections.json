[{"page_end_idx": 1, "page_start_idx": 1, "section_number": 1, "section_title": "INTRODUCTION", "details": {"details": "Multimodal large language models (MLLMs) have shown significant advancements in visual recognition tasks.  However, they struggle with complex chart understanding, which involves reasoning-intensive questions.  Existing benchmarks highlight the need for improved visual reasoning abilities in current MLLMs.  Analysis of errors in the ChartQA dataset reveals that 62% of errors are due to misrecognition, while 36% stem from reasoning mistakes after correct recognition, indicating a critical need to address both aspects.  One promising strategy is to distill the rationales of reasoning from human experts or stronger models; however, creating high-quality training data for chart-related tasks is expensive and time-consuming.  The difficulty lies in both constructing the chart data and generating relevant questions.  While some approaches try automating Q&A generation, they often overlook crucial visual features, leading to suboptimal results.  This highlights the challenges in creating effective training datasets that effectively address visual recognition and reasoning aspects simultaneously.", "first_cons": "Creating high-quality training data for chart-related tasks is costly and time-consuming, hindering the development of more advanced MLLMs.", "first_pros": "MLLMs have made significant achievements in visual recognition tasks, showing potential for improved performance with further development.", "keypoints": ["MLLMs struggle with complex chart understanding (reasoning-intensive questions).", "62% of errors in ChartQA stem from misrecognition; 36% from reasoning mistakes.", "Creating high-quality training data for chart-related tasks is costly and time-consuming.", "Distilling reasoning rationales from experts is a promising strategy but data creation is challenging"], "second_cons": "Existing Q&A synthesis methods often overlook critical visual features like color and layout in charts, which limits their effectiveness.", "second_pros": "Addressing both visual recognition and reasoning is crucial for creating more robust and generalized visual reasoning abilities in MLLMs.", "summary": "Current multimodal large language models (MLLMs) excel at simple visual recognition but struggle with complex chart understanding which demands strong visual reasoning abilities.  Existing datasets highlight a need for improved MLLMs with enhanced visual reasoning capabilities,  but creating such datasets is expensive and time-consuming due to the difficulty of simultaneously addressing visual recognition and reasoning errors (62% and 36% respectively in ChartQA). Data synthesis is a promising area to improve MLLMs but existing attempts lack focus on visual features, impacting performance."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "BACKGROUND", "details": {"details": "Existing chart-related datasets suffer from several limitations hindering the development of advanced visual reasoning models.  These datasets fall into two categories: benchmarks for evaluation and training data for model improvement.  Many existing datasets rely on manually collected data or use simplistic templates to generate questions, limiting their visual diversity and the complexity of reasoning tasks.  While some datasets aim for greater visual diversity and reasoning complexity, they frequently lack scalability due to manual annotation processes.  The resulting limitations include insufficient quantity and variety of chart types, a lack of complex reasoning questions, and high costs associated with manual data creation.  The use of LLMs for dataset augmentation offers a more scalable approach but overlooks crucial visual features, frequently utilizing only data tables and neglecting the importance of visual elements like layout and structure. The limited ability to capture these features may result in the generation of inaccurate or simplistic Q&A pairs.", "first_cons": "Many existing chart-related datasets are limited in their visual diversity and the complexity of the reasoning questions they offer, hindering the training of visual reasoning models.", "first_pros": "The use of LLMs for synthesizing data offers a more scalable and potentially more cost-effective approach to creating large-scale chart datasets.", "keypoints": ["Most existing datasets focus on basic visual recognition tasks rather than advanced reasoning.", "Datasets like ChartQA and OpenCQA feature limited visual diversity and uniform styles.", "Recent efforts employ LLMs for chart synthesis but often neglect visual features, focusing primarily on data tables.", "Manual annotation processes are costly and time-consuming, limiting the scalability of existing datasets."], "second_cons": "Existing methods of synthesizing data using LLMs often overlook critical visual features of charts, such as color, layout, and structure, potentially leading to inaccurate or simplistic Q&A pairs.", "second_pros": "Datasets like CharXiv and MMC that include complex scientific charts offer richer visual complexity, more accurately reflecting the diversity found in real-world scenarios.", "summary": "Current chart-related datasets have significant limitations, including a lack of visual diversity, insufficient reasoning complexity, and scalability issues due to manual annotation.  While LLMs offer a promising approach for generating larger datasets, current methods underutilize visual features, often relying solely on data tables.  This creates a need for improved data synthesis techniques that better capture visual complexity and enhance reasoning tasks."}}, {"page_end_idx": 5, "page_start_idx": 4, "section_number": 3, "section_title": "REACHQA: SYNTHESIZING CHART Q&A WITH CIT", "details": {"details": "The REACHQA dataset is created using a novel Code-as-Intermediary Translation (CIT) method.  This method uses code (specifically Python code using the Matplotlib library) as an intermediary between visual chart representations and textual question-answer pairs.  The process starts with 33 seed codes from the Matplotlib gallery, which are then expanded using the Self-Instruct method to generate a diverse set of chart-plotting codes.  The Evol-Instruct method further increases the complexity and diversity by evolving these codes.  Charts are generated by executing the code, and a self-repair mechanism is employed to fix any errors.  Finally,  instructions (questions and answers) are generated using LLMs, with both recognition-oriented and reasoning-oriented tasks included.  The entire process, starting from seed codes, results in a dataset of 3,249 reasoning-intensive charts and 19,963 Q&A pairs, achieved at a low cost of approximately \\$300. A multimodal validation step using multiple open-source LLMs ensures data quality.", "first_cons": "The reliance on LLMs for data generation might introduce biases or inconsistencies, which could affect the quality and generalizability of the dataset.", "first_pros": "The CIT approach is cost-effective and scalable, enabling the generation of a large and diverse dataset at low cost (approximately $300).", "keypoints": ["Uses Code-as-Intermediary Translation (CIT) to synthesize multimodal instruction data.", "Starts with 33 seed codes and expands using Self-Instruct and Evol-Instruct, resulting in diverse and complex charts.", "Contains 3,249 reasoning-intensive charts and 19,963 Q&A pairs (8k recognition-oriented and 12k reasoning-oriented).", "Low cost of dataset creation: approximately $300.", "Includes a multimodal validation step to ensure data quality."], "second_cons": "The reliance on a specific library (Matplotlib) might limit the variety of chart types and styles that can be generated.", "second_pros": "The use of code as an intermediary facilitates the adoption of text-based instruction augmentation strategies, resulting in high-quality and complex Q&A pairs.", "summary": "REACHQA is a novel multimodal chart question-answering dataset synthesized using a Code-as-Intermediary Translation (CIT) method.  CIT leverages the strengths of LLMs by using code as a bridge between visual charts and textual Q&A pairs, resulting in a cost-effective and scalable method for creating a diverse and complex dataset (3,249 charts and 19,963 Q&A pairs, costing approximately $300) while ensuring high quality through multimodal validation."}}, {"page_end_idx": 7, "page_start_idx": 6, "section_number": 4, "section_title": "EXPERIMENTS", "details": {"details": "This section details the experimental setup and results of evaluating various multimodal large language models (MLLMs) on chart-related tasks.  Three categories of benchmarks were used: traditional chart-related benchmarks focusing on recognition (ChartQA, ChartBench, ChartX); novel benchmarks assessing both recognition and reasoning (CharXiv, REACHQA); and general multimodal reasoning benchmarks (MathVista, MATH-Vision).  Multiple models were tested, including proprietary models (GPT-4, GPT-4 mini, Claude 3.5), chart-augmented open-source models, and general open-source models.  Supervised fine-tuning (SFT) using the REACHQA dataset was performed on general open-source models, and results showed that fine-tuning not only improved performance on chart benchmarks but also on general mathematical reasoning benchmarks.  Analysis of results showed the benefits of combining recognition and reasoning-focused training data.  Finally, an interpretability study was conducted to gain insights into the model's attention mechanisms. ", "first_cons": "The study primarily focuses on the performance of LLMs on chart-related tasks, potentially limiting the generalizability of the findings to other multimodal reasoning tasks.", "first_pros": "The comprehensive evaluation across various benchmark datasets and model categories provides a robust assessment of the current capabilities of MLLMs in chart understanding and reasoning.", "keypoints": ["Three categories of benchmarks were used for a comprehensive evaluation: traditional chart-related benchmarks focusing on recognition, novel benchmarks assessing both recognition and reasoning, and general multimodal reasoning benchmarks.", "Multiple model categories were evaluated, including proprietary, chart-augmented open-source, and general open-source models, providing a broader perspective on MLLM capabilities.", "Fine-tuning with the REACHQA dataset significantly improved not only performance on chart-related benchmarks but also on general mathematical reasoning benchmarks (MathVista, MATH-Vision), highlighting the transferability of the learned abilities.", "The study showed the effectiveness of combining both recognition and reasoning-oriented data for training, with the combined dataset yielding the best performance improvements (at least 15% across models).", "An interpretability study using attention visualization provided insights into how the fine-tuned model focuses on relevant visual elements during the reasoning process."], "second_cons": "The reliance on a single judge model (GPT-4) for evaluating the model's answers might introduce bias and limit the objectivity of the results.", "second_pros": "The inclusion of an interpretability study using attention visualization offers valuable insights into the internal workings of the models and provides a deeper understanding of their reasoning capabilities.", "summary": "This experiment section rigorously evaluates various multimodal large language models (MLLMs) on chart-related tasks using three benchmark categories. The results demonstrate that fine-tuning with the REACHQA dataset significantly improves not only performance on chart-related benchmarks but also general mathematical reasoning benchmarks, showcasing the transferability of learned skills.  A key finding highlights the benefit of combining recognition and reasoning datasets for training.  An interpretability study further enhances our understanding of model behavior."}}, {"page_end_idx": 10, "page_start_idx": 8, "section_number": 5, "section_title": "DISCUSSION", "details": {"details": "This section delves into the impact of expert rationales on model reasoning abilities, exploring the interplay between recognition and reasoning skills.  An experiment comparing three open-source datasets (ChartBench, ChartAst, ChartGemma) reveals that models trained on datasets incorporating expert rationales, like REACHQA, significantly outperform those trained on datasets lacking this element.  The analysis highlights the importance of including both recognition and reasoning-oriented data in training, with a balanced approach yielding the best results.  An investigation into the impact of data ratios (recognition vs. reasoning) shows that a balanced ratio is crucial for achieving the best performance across various benchmarks, while disproportionate emphasis on reasoning data may even lead to performance degradation.  Finally, the study examines the generalization abilities of models trained on specialized data (chart-related tasks) to general-purpose multimodal benchmarks, showing that specialized training can indeed benefit general reasoning abilities.  This section concludes by emphasizing the crucial role of attention mechanism in effectively extracting relevant visual information and supporting step-by-step reasoning.", "first_cons": "The study acknowledges a limitation in the controllability of current text-to-image models, impacting the precise control over details needed for creating high-quality synthetic data. This may affect the generalizability of the results and the suitability of the generated data for various applications. ", "first_pros": "The research demonstrates the effectiveness of distilling expert rationales into models, which significantly improves reasoning abilities on chart-related tasks, resulting in performance boosts exceeding 30% on average. This demonstrates the power of incorporating high-quality, nuanced training data.", "keypoints": ["Models trained on datasets with expert rationales (like REACHQA) significantly outperform those trained on datasets without such rationales.", "A balanced ratio of recognition and reasoning data in training is crucial for optimal performance; disproportionate emphasis on reasoning data may be detrimental.", "Models trained on specialized chart-related tasks demonstrate improved performance on general-purpose multimodal reasoning benchmarks, highlighting the transferability of learned skills."], "second_cons": "While the multimodal validation helps enhance the quality of the synthesized data, occasional inaccuracies might still exist in the generated charts and Q&A pairs.  This raises concerns regarding the robustness of the results and the need for stronger quality assurance methods for broader applications.", "second_pros": "The study provides valuable insights into the interaction between recognition and reasoning abilities, illustrating that a well-balanced approach to training data is crucial for achieving superior multimodal reasoning performance.  This suggests a need for future research to explore more sophisticated methods for generating training data.", "summary": "This section investigates the impact of expert rationales and balanced training data on model performance in visual reasoning tasks.  Experiments show that incorporating expert rationales and a balanced ratio of recognition and reasoning data improves reasoning abilities on chart-related tasks and generalizes to multimodal benchmarks, highlighting the importance of well-designed training data for better model performance. However, limitations remain in controlling the precision of synthetic data generation and ensuring complete data accuracy."}}]