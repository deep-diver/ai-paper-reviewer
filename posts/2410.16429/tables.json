[{"figure_path": "2410.16429/tables/table_13_0.md", "caption": "Table 1: LLM parameters for DSP Experiment", "description": "This table presents the hyperparameters used for the Large Language Model (LLM) during the Draft-Sketch-Proof (DSP) experiment.  It lists three parameters: Max tokens (set to 2048), Top P (set to 0.95), and Temperature (set to 0.8). These parameters control various aspects of the LLM's text generation process, influencing the length and randomness of the generated text.", "section": "5 Evaluation"}, {"figure_path": "2410.16429/tables/table_14_0.md", "caption": "Table 2: DSP's proof success rate (in %) using the Pantograph interface on the MiniF2F formal theorem proving benchmark. We used GPT-40 (labeled 40) and ol-preview (labeled o1) for the DSP experiments.", "description": "This table presents the results of a Draft-Sketch-Proof (DSP) experiment conducted using the Pantograph system. It compares the performance of two language models, GPT-40 and GPT-01-preview, in proving theorems from the MiniF2F benchmark.  The table shows the success rate (percentage of theorems successfully proven), the average number of hammer tactic invocations (attempts to solve goals using automated tactics), and the average runtime for both the validation and test sets of the benchmark. Results are broken down by the language model used (GPT-40 or GPT-01-preview) and the number of sketches (draft proofs) generated by the model (1 or 3 for GPT-40; only 1 for GPT-01-preview).", "section": "5 Evaluation"}]