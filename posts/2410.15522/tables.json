[{"figure_path": "2410.15522/tables/table_4_0.md", "caption": "Table 3: Performance drop from RewardBench (English) to M-REWARDBENCH across all categories for the top ten models in M-REWARDBENCH. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO ().", "description": "Table 3 presents the performance drop observed when comparing the results of ten top-performing reward models on the English-centric RewardBench benchmark against their performance on the multilingual M-REWARDBENCH benchmark.  The table displays the performance drop (in percentage points) for each model across four categories: Chat, Chat-Hard, Safety, and Reasoning.  Negative values indicate a decrease in performance from RewardBench to M-REWARDBENCH, highlighting the multilingual performance gap. Different model types (Classifier, Generative, and Implicit RMs trained with DPO) are visually distinguished with icons.", "section": "5 Results"}, {"figure_path": "2410.15522/tables/table_6_0.md", "caption": "Table 10: Performance of all reward models in the translation task. We source the translation evaluation set from MAPLE (Zhu et al., 2024), where we created EASY and HARD subsets. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO ().", "description": "Table 10 presents the performance of various reward models on the translation task, a new component of the M-REWARDBENCH benchmark. The dataset was derived from MAPLE (Zhu et al., 2024), which is a human preference dataset for machine translation, and includes two subsets, TRANSLATION-EASY and TRANSLATION-HARD, each with different difficulty levels. The table shows the average performance across all four translation directions (de\u2192en, en\u2192de, zh\u2192en, en\u2192zh) and lists both the average score and individual direction scores for each model in the EASY and HARD subsets. The models are categorized according to type (Classifier-based, Generative, and Implicit RMs trained using DPO).", "section": "5.2 Translation Task"}, {"figure_path": "2410.15522/tables/table_13_0.md", "caption": "Table 5: State-of-the-art models evaluated for M-REWARDBENCH.", "description": "This table lists 25 state-of-the-art reward models evaluated in the M-REWARDBENCH study.  For each model, it provides the provider (e.g., OpenAI, Cohere, Google), the model size in billions of parameters, and a reference to the publication where the model is described.  The models encompass various types, including generative, classifier, and implicit reward models trained using different methods.  The table is intended to provide a comprehensive overview of the models used in the benchmark, showing the diversity of approaches to reward modeling currently available.", "section": "4 Experiment Details"}, {"figure_path": "2410.15522/tables/table_13_1.md", "caption": "Table 6: The 23 languages in M-REWARDBENCH and their linguistic information. Script, language family, and resource availability are based on Aryabumi et al. (2024). Resource classes are from Joshi et al. (2020).", "description": "Table 6 presents a comprehensive overview of the 23 languages included in the M-REWARDBENCH dataset.  For each language, it provides its three-letter ISO 639-3 code,  the script used, the language family it belongs to, its resource availability level (categorized as High or Mid, based on Joshi et al., 2020), and its resource class (a numerical ranking indicating the abundance of resources for that language).  This table facilitates a deeper understanding of the linguistic diversity within M-REWARDBENCH and how the availability of resources may impact the performance of reward models across different languages.", "section": "4 Experiment Details"}, {"figure_path": "2410.15522/tables/table_14_0.md", "caption": "Table 2: Top ten reward models on M-REWARDBENCH. We evaluate several reward model types: Classifier RMs (), Generative RMs (), and Implicit RMs trained using DPO (). Full results can be found in Table 9.", "description": "This table presents the top ten reward models evaluated on the M-REWARDBENCH benchmark, ranked by their average scores across 23 languages.  The table lists the models' names, their average performance scores, and their performance broken down by language (Avg, Var, ar, cs, de, el, es, fa, fr, he, hi, id, it, jp, kr, nl, pl, pt, ro, ru, tr, uk, vi, zh). It also indicates the type of reward model used for each, distinguishing between Classifier RMs, Generative RMs, and Implicit RMs trained using DPO.  The full results for all 24 evaluated models are available in Table 9 of the paper.", "section": "5 Results"}, {"figure_path": "2410.15522/tables/table_15_0.md", "caption": "Table 9: All reward models evaluated on M-REWARDBENCH. We evaluate several reward model types: Classifier RMs (), Generative RMs (), and Implicit RMs trained using DPO ().", "description": "Table 9 presents the complete performance results of 23 reward models across various tasks and 23 languages within the M-REWARDBENCH benchmark.  The models are categorized into Classifier, Generative, and Implicit RMs based on their training methodology, and their performance is represented by a numerical score averaged across the different languages, showing the average score with standard deviation. Individual language-specific scores are also provided (with abbreviations for each language) allowing granular comparison. The table highlights the performance differences between model types and across various languages. ", "section": "5 Results"}, {"figure_path": "2410.15522/tables/table_16_0.md", "caption": "Table 10: Performance of all reward models in the translation task. We source the translation evaluation set from MAPLE (Zhu et al., 2024), where we created EASY and HARD subsets. Icons represent different model types: Classifier-based RMs (), Generative RMs (), and Implicit RMs trained using DPO ().", "description": "This table presents the performance of 23 reward models on a translation task, a new addition to the M-REWARDBENCH benchmark.  The models are evaluated on two subsets: TRANSLATION-EASY and TRANSLATION-HARD.  Each subset contains translations from four language pairs (de\u2192en, en\u2192de, zh\u2192en, en\u2192zh) and their scores reflect the models' ability to identify the higher-quality translation in each pair. The table lists the average performance across all language pairs along with the performance for each language pair and subset, distinguishing between three types of reward models: Classifier RMs, Generative RMs, and Implicit RMs (trained using DPO).", "section": "5.2 Translation Task"}]