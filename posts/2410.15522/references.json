{"references": [{" publication_date": "2017", "fullname_first_author": "Paul F Christiano", "paper_title": "Deep reinforcement learning from human preferences", "reason": "This paper is foundational in establishing the concept of using human preferences to train reward models, a core element of modern large language model alignment. Its introduction of preference learning as a mechanism for guiding the behavior of complex systems like LLMs is highly influential and directly relevant to the core topic of the paper.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly influential in the field of large language model alignment, as it details the successful use of reinforcement learning from human feedback (RLHF) to improve instruction-following in language models.  This is directly relevant to the current paper's discussion of reward models and their role in improving LLM alignment and behavior.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yuntao Bai", "paper_title": "Constitutional AI: Harmlessness from AI feedback", "reason": "This work introduces a novel approach to aligning language models with human values by using AI feedback to guide the training process.  This methodology is highly relevant to the current paper's focus on reward models and their alignment with human preferences, offering a different perspective on achieving responsible AI.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "RewardBench: Evaluating reward models for language modeling", "reason": "This paper presents RewardBench, a benchmark for evaluating reward models that is directly compared to the proposed multilingual benchmark M-REWARDBENCH in this paper. Understanding its strengths and limitations is key to evaluating the novelty and impact of the proposed work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Viraat Aryabumi", "paper_title": "Aya 23: Open weight releases to further multilingual progress", "reason": "This work introduces a new family of large language models, Aya 23, that is specifically developed and evaluated for multilingual performance. This is relevant to the present work because it demonstrates that multilingual capabilities are a significant research focus and directly ties to the challenges of creating effective multilingual reward models.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "John Dang", "paper_title": "RLHF can speak many languages: Unlocking multilingual preference optimization for LLMs", "reason": "This paper investigates the use of reinforcement learning from human feedback (RLHF) in multilingual settings.  It is directly relevant to the current paper as it highlights a similar interest in improving multilingual model performance. This highlights the relevance of the research problem and the significance of the proposed solution.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Aakanksha", "paper_title": "The multilingual alignment prism: Aligning global and local preferences to reduce harm", "reason": "This work explores the complexities of aligning LLMs with human values across different languages and cultures, directly addressing one of the key motivations behind this paper. This makes the paper significant in demonstrating the interest in the field and providing additional context.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen Technical Report", "reason": "This technical report provides details on the Qwen language model, one of the models evaluated in the current study. This is important for understanding the broader context of models used for evaluation and potential model-specific limitations or advantages.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This paper describes a dataset for evaluating mathematical reasoning capabilities in language models which is a relevant subtask within the multilingual benchmark M-REWARDBENCH used for evaluation.  It is directly relevant to the benchmark construction and tasks being evaluated.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Marta R Costa-juss\u00e0", "paper_title": "No language left behind: Scaling human-centered machine translation", "reason": "This paper presents NLLB, a multilingual machine translation model used for translating the dataset for evaluating the reward models. The quality of translation directly impacts the performance of RMs, thus, this paper is critical in understanding the impact of translation on the results.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces the LLaMa 3 family of language models, several of which are used in the current study's evaluations.  Understanding the capabilities and characteristics of these models is crucial for interpreting the results and comparing the performance of different reward models.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Hamish Ivison", "paper_title": "Unpacking DPO and PPO: Disentangling best practices for learning from preference feedback", "reason": "This paper provides a detailed analysis of Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO), both relevant to reward model training, allowing for a deeper understanding of the technical aspects of reward model design and training, which improves analysis and conclusion.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper introduces the Mistral 7B language model, which is used in the evaluation. Understanding its architecture and performance characteristics is crucial for interpreting the results and evaluating the effectiveness of the reward models.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Viet Dac Lai", "paper_title": "Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback", "reason": "This paper demonstrates a successful approach to aligning LLMs with human preferences in multiple languages, showcasing practical implementation in the multilingual alignment space. This approach shares the focus of this paper and helps the understanding of the context.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Nathan Lambert", "paper_title": "Rewardbench: Evaluating reward models for language modeling", "reason": "This work introduces RewardBench, which serves as a baseline for comparing the proposed multilingual benchmark (M-REWARDBENCH).  This comparison underscores the novel aspects and potential contributions of the multilingual benchmark.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Wen Lai", "paper_title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback", "reason": "This paper focuses on enhancing the multilingual capabilities of LLMs which directly relates to the goal of this paper. This provides a crucial context and comparison for the proposed work on evaluating multilingual reward models.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Junlong Li", "paper_title": "Generative judge for evaluating alignment", "reason": "This paper explores generative models for evaluating the alignment of LLMs, an approach that is relevant to the current paper's investigation of different reward model types. This contextual information improves the understanding of the paper.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Xuechen Li", "paper_title": "AlpacaEval: An Automatic Evaluator of Instruction-following Models", "reason": "This paper presents AlpacaEval, a dataset used in M-REWARDBENCH for evaluating chat capabilities of LLMs.  Understanding this dataset is vital to comprehending the design, construction, and scope of the proposed multilingual benchmark.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Rohan Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "reason": "This work introduces the Stanford Alpaca model, which is related to the models used in this study. This context helps to understand the models evaluated and their performance in relation to the benchmark dataset.", "section_number": 5}]}