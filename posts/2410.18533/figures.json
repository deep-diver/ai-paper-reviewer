[{"figure_path": "2410.18533/figures/figures_5_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "This figure presents a comparison of various Long-Context Models (LCMs) across three key aspects. Subfigure (a) shows the performance of different LCMs on real-world long-context tasks, highlighting their relative capabilities. Subfigure (b) focuses on a synthetic retrieval task, specifically evaluating the retrieval score (measuring contextual understanding) and recall score (assessing generation quality) of each model. Finally, subfigure (c) visualizes the amount of long-context training data used for each LCM, providing context to their performance differences.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18533/figures/figures_17_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "This figure presents a performance comparison of various Long-Context Models (LCMs) across different aspects.  Subfigure (a) shows the performance of several LCMs on real-world long-context tasks, measured using a combination of metrics. Subfigure (b) provides retrieval and recall scores for these models on a specific synthetic retrieval task, highlighting their long-context understanding and generation capabilities. Finally, subfigure (c) illustrates the amount of long-context training data used for each LCM, showing that while some models used significantly more data, others achieve competitive results with comparatively less data.", "section": "INTRODUCTION"}, {"figure_path": "2410.18533/figures/figures_18_0.png", "caption": "Figure 1: (a) Performance of LCMs on real-world long-context tasks; (b) Retrieval score (long-context understanding ability) and recall score (generation ability) of LCMs on the synthetic retrieval long-context task (multi-value NIAH); (c) Long-context (pre-)training data size for each LCM.", "description": "This figure presents a performance comparison of various Long-Context Models (LCMs) across different aspects.  Subfigure (a) shows the performance of several LCMs, including open-source models like Llama and closed-source models like GPT-4, on real-world long-context tasks, measured by a composite score. Subfigure (b) focuses on the retrieval and recall scores of these LCMs on a synthetic retrieval task (multi-value NIAH), illustrating their abilities in understanding and generating responses within long contexts, respectively. Lastly, subfigure (c) provides a comparison of the amount of training data used for each LCM, differentiating between long-context foundation models and those based on Llama or Llama-2, highlighting the relationship between training data and performance.", "section": "Introduction"}]