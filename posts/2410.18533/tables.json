[{"figure_path": "2410.18533/tables/table_7_0.md", "caption": "Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "The table presents the performance evaluation results of various Language Models (LCMs) on the LongBench benchmark.  It's divided into three sections: SCMs (short-context models) with their context window scaled up eight times, LCMs (long-context models) with their original context window, and a comparison of different training methods on LCMs.  For each model, performance is measured across five tasks: S-Doc QA, M-Doc QA, Summarization, Few-shot, and Synthetic.  The average score across all tasks is also provided.  The table shows baseline performance and the improvements achieved by applying different context scaling techniques (YaRN, RandPOS) and the LOGO training strategy.  The training methods compared include instruction tuning (applying loss to the full or partial sequence) and LOGO.  Results highlight that LOGO consistently improves the performance of both SCMs and LCMs across all tasks.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_13_0.md", "caption": "Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "The table presents the evaluation results of different long context models (LCMs) on the LongBench benchmark. It compares the performance of various models across six categories of long-context tasks: Single Document QA (S-Doc QA), Multi-Document QA (M-Doc QA), Summarization (Summ), Few-shot, Synthetic, and Code.  The models are categorized into Short-context Models (SCMs) and Long-context Models (LCMs) and are evaluated with and without additional training methods like YaRN, RandPOS, and LOGO.  The average scores across all categories are also provided, enabling a comprehensive comparison of the effectiveness of different context scaling and alignment techniques.  Training-free methods are marked with a \u2020 symbol.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}, {"figure_path": "2410.18533/tables/table_20_0.md", "caption": "Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "The table presents the evaluation results of various Long Context Models (LCMs) and Short Context Models (SCMs) on the LongBench benchmark.  It compares the performance of different models across six tasks: Single-Document QA (S-Doc QA), Multi-Document QA (M-Doc QA), Summarization (Summ), Few-shot learning, Synthetic, and Average.  The models are grouped into SCMs (short context models, with context window expanded using various methods), and LCMs (long context models).  For SCMs, the table shows results with context windows expanded to 8 times their original size using different techniques (YaRN, RandPOS, LOGO). For LCMs, the table compares performance using instruction tuning (full sequence or partial prediction loss) and LOGO. The table allows for comparison of the various context scaling and alignment strategies across different model architectures and their impact on different task types.", "section": "4.2 Performance on Long-Context Tasks"}, {"figure_path": "2410.18533/tables/table_20_1.md", "caption": "Table 1: Evaluation results on LongBench benchmark, where \u2020 denotes training-free method.", "description": "The table presents the evaluation results of different long-context models on the LongBench benchmark.  It compares the performance of various models across six categories of tasks: Single-Document QA (S-Doc QA), Multi-Document QA (M-Doc QA), Summarization (Summ), Few-shot, Synthetic, and Code.  The results are shown for both short-context models (SCMs) and long-context models (LCMs), with and without the LOGO training strategy, and other related methods such as YaRN, RandPOS and instruction tuning.  The average score across all tasks is also provided for each model to facilitate comparison and analysis of the performance improvements achieved by each model.", "section": "4.2 PERFORMANCE ON LONG-CONTEXT TASKS"}]