[{"figure_path": "2410.18647/tables/table_9_0.md", "caption": "Table 1: Success rate across all tasks. We report the average success rate and standard deviation across 8 unseen environments. The performance in each environment is detailed in Table 12.", "description": "Table 1 presents the success rates of the policies trained across 32 environment-object pairs for each of four manipulation tasks: Pour Water, Mouse Arrangement, Fold Towels, and Unplug Charger.  The table shows the average success rate and standard deviation across eight unseen environments for each task.  The success rates indicate the percentage of successful task completions, with success defined by specific scoring criteria detailed in Appendix D of the paper. The data suggests high success rates (around 90%) across all tasks, highlighting the effectiveness of the proposed data collection strategy.", "section": "5 VERIFICATION OF DATA COLLECTION STRATEGY"}, {"figure_path": "2410.18647/tables/table_10_0.md", "caption": "Model related experiments on Pour Water. The entries marked in gray are the same, which specify the default settings: the visual encoder is a fully fine-tuned ViT-L/14 model pretrained with DINOv2, while the action diffusion model employs a base-size 1D CNN U-Net.", "description": "This table presents results of experiments on the Pour Water task, exploring the impact of different model choices and training strategies on performance. It shows the normalized scores obtained by using different visual encoders (LfS ViT-L/14, frozen DINOv2, LoRA DINOv2, DINOv2 ViT-S/14, DINOv2 ViT-B/14, DINOv2 ViT-L/14) and different diffusion model sizes (small U-Net, base U-Net, large U-Net). The results highlight the importance of using a fully fine-tuned DINOv2 pretrained ViT-L/14 visual encoder and that scaling up the visual encoder improves performance, while scaling the action diffusion model does not significantly affect it.", "section": "Model Size and Training Strategy: Beyond Data Scaling"}, {"figure_path": "2410.18647/tables/table_10_1.md", "caption": "Model related experiments on Pour Water. The entries marked in gray are the same, which specify the default settings: the visual encoder is a fully fine-tuned ViT-L/14 model pre-trained with DINOv2, while the action diffusion model employs a base-size 1D CNN U-Net.", "description": "This table presents the results of experiments conducted on the Pour Water task to investigate the impact of model size and training strategies on policy performance. It shows the normalized scores achieved by different model configurations, varying the visual encoder size (from small to large) and the training strategy for the visual encoder (pre-training and full fine-tuning, and LORA). The results reveal that scaling the visual encoder consistently boosts performance, while scaling the action diffusion model does not yield a significant improvement, and the pre-training and full fine-tuning of the visual encoder are crucial for good performance.", "section": "Model Size and Training Strategy: Beyond Data Scaling"}, {"figure_path": "2410.18647/tables/table_23_0.md", "caption": "Table 3: A default set of hyper-parameters.", "description": "This table lists the hyperparameters used in the Diffusion Policy model for the robotic manipulation tasks.  It includes parameters related to image and proprioception observation horizons, action horizon, observation resolution, environment frequency, optimizer, learning rates for the action diffusion model and visual encoder, learning rate schedule, batch size, inference denoising iterations, temporal ensemble steps, and temporal ensemble adaptation rate.  The values specified are the defaults used, though some are task-specific (image observation horizon, proprioception observation horizon).", "section": "C POLICY TRAINING"}, {"figure_path": "2410.18647/tables/table_32_0.md", "caption": "Table 4: Object generalization on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 2.", "description": "Table 4 presents raw test scores for the Pour Water task before normalization. The data is organized into rows representing the number of objects used for training (from 1 to 32) and columns representing different percentages of the total demonstration data used for training (from 3.125% to 100%). Each cell in the table shows the average raw score obtained in the experiment with the specified number of objects and data usage.  These raw scores are later normalized by dividing by 9 to produce the results shown in Figure 2 of the paper.", "section": "G.3 RAW TEST SCORES"}, {"figure_path": "2410.18647/tables/table_32_1.md", "caption": "Environment generalization on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 3.", "description": "This table presents the raw test scores for the \"Pour Water\" task before normalization in an environment generalization experiment.  It shows the average normalized scores for different numbers of training environments (1, 2, 4, 8, 16, 32) and various percentages of demonstration data used for training (3.125%, 6.25%, 12.5%, 25%, 50%, 100%).  Each cell represents the average score across multiple trials. Normalizing these raw scores by dividing them by 9 produces the data shown graphically in Figure 3 of the paper.", "section": "G.3 RAW TEST SCORES"}, {"figure_path": "2410.18647/tables/table_32_2.md", "caption": "Generlization across environments and objects on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 4.", "description": "This table presents the raw test scores before normalization for the Pour Water task, specifically focusing on generalization across environments and objects.  The table shows the raw scores achieved by policies trained with different numbers of training environment-object pairs and various fractions of demonstrations. The rows represent the number of training environment-object pairs, while the columns represent different fractions of demonstrations used for training, ranging from 3.125% to 100%. The values in the table represent the raw scores, which are subsequently normalized by dividing by 9 to generate the normalized scores shown in Figure 4 of the paper. ", "section": "G.3 RAW TEST SCORES"}, {"figure_path": "2410.18647/tables/table_32_3.md", "caption": "Number of demonstrations on Pour Water. Normalizing these scores by dividing them by 9 yields the results shown in Fig. 7.", "description": "This table presents the raw test scores for the Pour Water task before normalization.  It shows the raw scores (out of 9) for different numbers of demonstrations (64, 100, 200, 400, 800, 1600, 3200, and 6400).  These scores are then normalized by dividing by 9 to produce the results displayed graphically in Figure 7 of the paper. The table thus provides the underlying data used to generate the plot showing how performance changes with different demonstration numbers, keeping other factors such as the number of training objects and environments constant.", "section": "G.3 RAW TEST SCORES"}, {"figure_path": "2410.18647/tables/table_33_0.md", "caption": "Object generalization on Mouse Arrangement. Normalizing these scores by dividing them by 6 yields the results shown in Fig. 2.", "description": "This table presents the raw test scores for object generalization in the Mouse Arrangement task before normalization.  It shows the raw scores for different fractions of demonstrations used (3.125%, 6.25%, 12.5%, 25%, 50%, and 100%) and varying numbers of training objects (1, 2, 4, 8, 16, and 32). These raw scores, when normalized by dividing them by 6, result in the data points used to generate Figure 2 in the paper, illustrating the relationship between the number of training objects and the policy's ability to generalize to unseen objects.", "section": "G.3 RAW TEST SCORES"}, {"figure_path": "2410.18647/tables/table_33_1.md", "caption": "Environment generalization on Mouse Arrangement. Normalizing these scores by dividing them by 6 yields the results shown in Fig. 3.", "description": "This table presents the raw test scores for the Mouse Arrangement task before normalization. The table shows the average normalized scores for the Mouse Arrangement task using different numbers of training environments (1, 2, 4, 8, 16, and 32). Each row represents a different number of training environments, and each column represents a different percentage of the full dataset used for training (3.125%, 6.25%, 12.5%, 25%, 50%, and 100%).  The scores are not normalized; normalization by dividing by 6 produces the results shown in Figure 3 of the paper.", "section": "G.3 RAW TEST SCORES"}, {"figure_path": "2410.18647/tables/table_33_2.md", "caption": "Generlization across environments and objects. Each curve corresponds to a different fraction of demonstrations used, with normalized scores shown as a function of the number of training environment-object pairs.", "description": "The table shows the raw test scores before normalization for the object generalization experiment on Pour Water.  It presents the average normalized scores for different fractions of demonstrations (3.125%, 6.25%, 12.5%, 25%, 50%, and 100%) used during training, across varying numbers of training environment-object pairs (1, 2, 4, 8, 16, and 32).  These scores are later normalized by dividing them by 9 to produce the results shown in Figure 4 of the paper, illustrating the relationship between the number of environment-object pairs and the generalization performance.", "section": "4.1 RESULTS AND QUALITATIVE ANALYSIS"}, {"figure_path": "2410.18647/tables/table_33_3.md", "caption": "Number of demonstrations on Mouse Arrangement. Normalizing these scores by dividing them by 6 yields the results shown in Fig. 7.", "description": "This table presents the raw test scores for the Mouse Arrangement task before normalization.  It shows the average score achieved across 8 unseen testing objects for different numbers of demonstrations (64, 100, 200, 400, 800, 1600, 3200, and 6400).  The scores are not normalized; normalization by dividing by 6 produces the results shown in Figure 7 of the paper. The table is organized by the number of demonstrations used for training, allowing for analysis of how the success rate increases with the amount of training data.", "section": "G.3 RAW TEST SCORES"}, {"figure_path": "2410.18647/tables/table_34_0.md", "caption": "Success rate across all tasks. For each task, we report the success rate in each evaluation environment.", "description": "Table 12 presents the success rates of the policies trained across 32 environment-object pairs for each task.  The table shows the success rate for each of the eight unseen environments in which each policy was evaluated, for each of the four tasks (Pour Water, Mouse Arrangement, Fold Towels, and Unplug Charger). The mean success rate across all eight environments is also provided for each task.  Detailed criteria for task success are described in Appendix D of the paper.", "section": "5 VERIFICATION OF DATA COLLECTION STRATEGY"}]