[{"figure_path": "2410.17883/figures/figures_4_0.png", "caption": "Figure 1: Illustration of AcT. A separate encoding of each UI element into a vector  e<sub>t,i</sub> by using pretrained embedding models. The embeddings are then fed into the sequence of a transformer X<sub>t</sub> along with the previous timesteps in that episode. The prediction of the transformer is decoded to produce the next action which consists of a<sub>type</sub> and a<sub>spec</sub>", "description": "The figure illustrates the Action Transformer (AcT) architecture.  It shows how AcT processes a smartphone's UI elements. First, each UI element (o<sub>t,i</sub>) is encoded into a vector (e<sub>t,i</sub>) using a combination of CLIP for image encoding and BERT for text encoding, along with one-hot encoding of attributes. These embeddings, along with positional embeddings and the previous timestep's action, are fed into a transformer (X<sub>t</sub>). The transformer's output is then decoded to predict the next action (a<sub>t</sub>), which consists of an action type (a<sub>type</sub>) and action specifications (a<sub>spec</sub>).", "section": "3.1 MODEL INPUTS"}, {"figure_path": "2410.17883/figures/figures_5_0.png", "caption": "Figure 2: The architecture of LiMAC. The history of observations-actions {ot, at-1, Ot-1..} and goal g are processed to vector x and passed to AcT. The image observation omg with the bounding boxes and the goal g are passed as inputs to the VLM. The VLM is only called if an action that requires text completion is selected, based on the action type output of AcT. The action is finally selected based on the protocol described in Section 3.", "description": "The figure illustrates the architecture of LiMAC, a lightweight multi-modal app control framework.  The process begins with inputting a goal (g) and a history of observations and actions.  This input is processed by an Action Transformer (AcT), which predicts the action type. If the action requires text (e.g., inputting text or opening an app), the goal, current observation (ot), and action type are passed to a Vision Language Model (VLM) to generate the necessary text. Otherwise, AcT directly determines the action.  The output of AcT and/or the VLM is a final action, which may include specifications such as a click target. AcT is further detailed in Figure 1.", "section": "3 THE LIGHTWEIGHT MULTI-MODAL APP CONTROL FRAMEWORK"}, {"figure_path": "2410.17883/figures/figures_16_0.png", "caption": "The architecture of LiMAC. The history of observations-actions {ot, at\u22121, ot\u22121..} and goal g are processed to vector x and passed to AcT. The image observation omg with the bounding boxes and the goal g are passed as inputs to the VLM. The VLM is only called if an action that requires text completion is selected, based on the action type output of AcT. The action is finally selected based on the protocol described in Section 3.", "description": "The figure illustrates the LiMAC architecture, showing the flow of information processing.  The input consists of a user goal (g) and a sequence of past observations (ot, at\u22121, ot\u22121...), representing the phone's state and past actions.  This combined input is processed by the Action Transformer (AcT), which predicts the type of action needed. If this action requires text (e.g., 'input-text', 'open-app'), the goal, observation and action type are passed to a Vision-Language Model (VLM) for text generation to complete the action specification. Otherwise, AcT directly determines the action specification. The final action is selected according to a protocol described in Section 3. The diagram visually shows the smartphone screen, input processing via AcT and VLM modules, and final action selection.", "section": "3 THE LIGHTWEIGHT MULTI-MODAL APP CONTROL FRAMEWORK"}, {"figure_path": "2410.17883/figures/figures_16_1.png", "caption": "Figure 5: Relaxed input-text in yellow (timestep 4) and overall successful episode. Timestep 4 is considered correct under our relaxed input-text textual component because it is simply the singular form of the correct text, leading to a Jaccard index greater than 0.5 and presumably the same search results. The episode terminates successfully, with all timesteps being considered correct under our evaluation metrics.", "description": "This figure shows a screenshot sequence of a successful episode from the AndroidControl dataset. The goal was to find a three-seater sofa on the Industrybuying app. The figure highlights the agent's actions (in green boxes) at each timestep.  The first action was correctly opening the Industrybuying app. Then the agent waited, clicked on the search bar, typed \"3 seater sofa\" (the relaxed accuracy allows for this slightly different phrasing), and then clicked on a relevant search result. Finally, the episode concludes, illustrating a series of successful actions leading to task completion.", "section": "E CASE STUDIES"}]