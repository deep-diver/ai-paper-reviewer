{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational to the field of large language models (LLMs), introducing the concept of few-shot learning and significantly advancing the capabilities of LLMs.  Its impact is immense, as it directly relates to the advancements in LLMs that form the basis for the current paper's work on multimodal LLMs for video understanding.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper significantly advanced the capabilities of LLMs by introducing the concept of training LLMs to follow instructions with human feedback, thereby improving the quality and alignment of LLM outputs. This is directly relevant to the current paper's use of LLMs and their ability to accurately process and understand videos.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "CLIP is a seminal paper in the field of vision-language models, introducing a method for training visual models using natural language supervision. This paper is highly influential and many subsequent works in this area build upon its foundations and are used in the current paper's related work section, and is therefore a crucial element in understanding the context of the work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiaohua Zhai", "paper_title": "Sigmoid loss for language image pre-training", "reason": "SigLIP introduced a novel loss function for training vision-language models, significantly enhancing their scalability and performance. This paper is particularly relevant to the current work as SigLIP's vision encoder is used as part of LongVU's architecture. The improved scalability enables the handling of large-scale datasets, crucial for training effective video-language models.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "Flamingo introduced a novel architecture that efficiently integrates vision and language, enabling few-shot learning capabilities.  This paper significantly advanced the capabilities of vision-language models and is cited in the paper's related work, providing relevant context for the proposed model, specifically in the discussion of advanced vision-language models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Junnan Li", "paper_title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models", "reason": "BLIP-2 represents a significant advancement in vision-language models, demonstrating improved performance and efficiency. The paper's architecture and techniques are relevant to the current research and are discussed in the related work section, providing important background for the proposed approach.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Deyao Zhu", "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models", "reason": "MiniGPT-4 is a highly influential paper that demonstrates the power of integrating LLMs with vision models. The paper's approach to integrating vision and language is particularly relevant to the current research, as LongVU also aims to effectively integrate visual and textual information. The improved understanding capabilities are crucial to the paper's work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Hao Liu", "paper_title": "LLaVA-next: Improved reasoning, OCR, and world knowledge", "reason": "LLaVA-Next significantly improved the capabilities of vision-language models. Its advancements in reasoning and world knowledge are highly relevant to the current paper's work on long-video understanding, as it showcases the state-of-the-art in the field and the improvements that the current paper aims to build upon.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "KunChang Li", "paper_title": "Videochat: Chat-centric video understanding", "reason": "This work is directly relevant to the current research, as it addresses the challenge of video understanding using chat-centric methods.  The paper's approach is discussed in the related work, and the current research proposes an improvement to address limitations in handling long videos efficiently.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Zesen Cheng", "paper_title": "Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms", "reason": "VideoLLaMA 2 introduced advancements in spatial-temporal modeling and audio understanding in video LLMs, providing relevant context for the current research.  The improvements it achieved in these areas contribute to the broader context of LongVU's capabilities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Bo Li", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "reason": "LLaVA-OneVision is a highly relevant work that utilizes a large dataset for training and achieves state-of-the-art results in visual understanding. This paper directly influences the current research, which aims to build upon these advancements by proposing a more efficient approach for long videos.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Peng Jin", "paper_title": "Chat-univi: Unified visual representation empowers large language models with image and video understanding", "reason": "Chat-UniVi is an important paper in the field that proposes a unified visual representation for empowering large language models with image and video understanding. Its approach, while different from LongVU, is relevant to the field and is discussed in the related work, providing context for the current work.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Muhammad Maaz", "paper_title": "Video-chatgpt: Towards detailed video understanding via large vision and language models", "reason": "VideoChatGPT is a highly relevant work that employs large vision and language models for video understanding.  The paper's approach is discussed in the related work, and the current research builds upon this work by addressing its limitations with long videos. The focus on detail and the use of large models provides important context for the current work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kirolos Ataallah", "paper_title": "Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens", "reason": "This paper is highly relevant to the current research, as it explores the use of multimodal LLMs for video understanding. The paper's approach to interleaving visual and textual tokens provides important context for the current work, highlighting the advancements in multimodal LLMs for video understanding.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Daniel Bolya", "paper_title": "Token merging: Your vit but faster", "reason": "This paper is relevant to the work on video token compression, as it proposes a method for merging tokens to improve efficiency.  This is an important technique for handling long videos, and the current research uses similar techniques, making this a highly relevant reference.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Joonmyung Choi", "paper_title": "Vid-tldr: Training free token merging for light-weight video transformer", "reason": "This paper proposes a method for training light-weight video transformers using free token merging, an important technique for efficient processing of long videos. This is directly relevant to the current work, which uses similar techniques for compression.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Maxime Oquab", "paper_title": "Dinov2: Learning robust visual features without supervision", "reason": "DINOv2 is a crucial component of LongVU's architecture.  The method leverages DINOv2 for temporal reduction.  This paper's contribution to self-supervised visual feature extraction is directly applicable to the core methodology of LongVU, making it essential to the understanding of the contributions of the work.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Chaoyou Fu", "paper_title": "Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis", "reason": "VideoMME is a benchmark dataset used to evaluate LongVU's performance. Its use is critical for the empirical validation of the model.  The choice of benchmarks directly impacts the conclusions drawn about the model's performance and the value of its contributions to the field.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Junjie Zhou", "paper_title": "Mlvu: A comprehensive benchmark for multi-task long video understanding", "reason": "MLVU is another benchmark used to evaluate LongVU's performance, focusing on long video understanding capabilities. The choice of benchmarks is essential for demonstrating the effectiveness of LongVU in handling long-video tasks, making this reference a crucial element in the empirical validation of the paper.", "section_number": 4}]}