[{"figure_path": "2410.17434/tables/table_6_0.md", "caption": "Results on comprehensive video understanding benchmarks", "description": "The table presents a comparison of various video language models' performance across four video understanding benchmarks: EgoSchema, MVBench, MLVU, and VideoMME.  The models are evaluated based on their size (in billions of parameters), context length, the number of frames used as input (either a fixed number or 1 frame per second), and their accuracy scores on each benchmark.  Proprietary models (GPT-4V and GPT-4-o) are compared against several open-source video LLMs, including Video-LLaVA, LLaMA-VID, Chat-UniVi, ShareGPT4Video, LLaVA-NeXT-Video, VideoLLaMA2, LongVA, VideoChat2, and LLaVA-OneVision. The table highlights LongVU's performance, showing its superior accuracy across all benchmarks, particularly in the longer video subsets of VideoMME and MLVU.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_7_0.md", "caption": "Table 2 Results of small-size video language models across video understanding benchmarks.", "description": "This table presents the performance of several small-size video language models on various video understanding benchmarks.  The models compared include InternVL2 (InternLM2-1.8B), VideoChat2 (Phi-3-mini-4B), Phi-3.5-vision-instruct (Phi-3-mini-4B), and LongVU (Ours) (Llama3.2-3B). The benchmarks include EgoSchema, MVBench, VideoMME Overall, VideoMME Long, and MLVU.  The table shows the accuracy scores achieved by each model on each benchmark, allowing for a comparison of performance across different models and task types. LongVU, the model presented in the paper, is shown to be competitive with larger models.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/tables/table_9_0.md", "caption": "Table 3 Ablation studies of number of tokens per frame, different context lengths, and our spatiotemporal compression components.", "description": "Table 3 presents an ablation study comparing different configurations of the LongVU model on three video understanding benchmarks: EgoSchema, VideoMME, and MLVU.  It shows the impact of varying the number of tokens per frame (64 or 144), the context length (8k or 16k), and the inclusion of key components of the LongVU model: DINO-based temporal reduction, cross-modal query-guided selection, and spatial token compression (STC).  The table demonstrates the performance gains achieved by each component and shows how the optimal settings of the model depend on the combination of these factors in order to process hour-long videos effectively.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/tables/table_9_1.md", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a comparison of various video language models on four video understanding benchmarks: EgoSchema, MVBench, VideoMME, and MLVU.  The table shows the model size, context length, number of frames used, and performance metrics (measured in accuracy) for each benchmark.  It includes both proprietary models (GPT-4-V, GPT-4-0) and open-source video LLMs (Video-LLaVA, LLAMA-VID, etc.), with LongVU presented as the proposed method,  showing that LongVU outperforms existing models across all the benchmarks.", "section": "4 Experiments"}, {"figure_path": "2410.17434/tables/table_9_2.md", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a comparison of various video language models across four video understanding benchmarks: EgoSchema, MVBench, VideoMME, and MLVU.  The table lists each model's performance (measured as accuracy) across different video lengths, from short clips to long videos.  Several proprietary models (GPT4-V, GPT4-0) and several open-source video LLMs (Video-LLaVA, LLAMA-VID, Chat-UniVi, ShareGPT4Video, LLaVA-NeXT-Video, VideoLLaMA2, LongVA, VideoChat2, LLaVA-OneVision) are included in the comparison, with the authors' proposed LongVU model also shown for comparison. The table displays the model size, context length, the number of video frames used, and the accuracy scores obtained on each benchmark. ", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_15_0.md", "caption": "Table 6 Training data statistics.", "description": "This table presents the training data statistics used in the paper. It shows the modality (Image-Text or Video-Text), the task (Single-Image, Captioning, Classification, VQA, or Instruction), the number of samples used for training, and the dataset used for each task and modality.  The datasets listed include LLaVA-OneVision, TextVR, MovieChat, YouCook2, Kinetics-710, NEXTQA, CLEVRER, EgoQA, TGIF, WebVidQA, DiDeMo, and ShareGPT4Video.", "section": "4 Experiments"}, {"figure_path": "2410.17434/tables/table_15_1.md", "caption": "Results on comprehensive video understanding benchmarks", "description": "Table 1 presents a comparison of various video language models' performance across four video understanding benchmarks: EgoSchema, MVBench, VideoMME, and MLVU.  The table lists each model's size (in billions of parameters), the sampling frequency of frames used (frames per second or fps), the context length, the number of frames processed, and the accuracy achieved on each benchmark.  Benchmarks are further broken down into overall, short, medium, and long video durations to assess performance differences depending on video length.  Proprietary and open-source models are included for a comprehensive comparison.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_16_0.md", "caption": "Table 8 Ablation study on with or without FPE.", "description": "This ablation study table investigates the impact of adding frame-level positional encoding (FPE) on the model's performance. Three model variations are compared: one with only DINO and query, one with DINO, query, and spatiotemporal compression (STC), and a third adding FPE to the second model.  The results are evaluated across three video understanding benchmarks: EgoSchema, VideoMME, and MLVU, using a context length of 8k tokens. The number of tokens is dynamically adjusted in the models with STC.  The table shows that adding FPE does not significantly improve performance and is not included in the default setting.", "section": "4.5 Ablation Studies"}, {"figure_path": "2410.17434/tables/table_16_1.md", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "This table presents the results of various video understanding models on four benchmarks: EgoSchema, MVBench, VideoMME, and MLVU.  The table lists each model's performance, including its size (in billions of parameters), context length, frames per second (fps), and scores on each benchmark.  The models are categorized into proprietary models and open-source video LLMs.  LongVU, the proposed model in the paper, is shown in the bottom row and compared against other methods on the same metrics.", "section": "4.2 Benchmarks and metrics"}, {"figure_path": "2410.17434/tables/table_17_0.md", "caption": "Table 1 Results on comprehensive video understanding benchmarks", "description": "This table presents the results of various video understanding models on four different benchmarks: EgoSchema, MVBench, VideoMME, and MLVU.  The table compares the performance of proprietary models (GPT4-V and GPT4-0) and several open-source video LLMs, including Video-LLaVA, LLAMA-VID, Chat-UniVi, ShareGPT4Video, LLaVA-NeXT-Video, VideoLLaMA2, LongVA, VideoChat2, and LLaVA-OneVision. The models are evaluated based on their performance across overall, long, medium, and short video durations.  The table shows the model's size (in billions of parameters), the context length used during training, the frame rate or number of frames used, and the accuracy scores obtained for each benchmark.  Finally, it also lists the results of the proposed model, LongVU, showing its superior performance compared to the other models across all benchmarks.", "section": "4.2 Benchmarks and metrics"}]