[{"figure_path": "2410.17434/charts/charts_2_0.png", "caption": "Effectiveness of our LongVU over commonly-used uniform sampling and dense sampling. Uniform sampling overlooks critical frames due to its sparse nature. Dense sampling may surpass the maximum context length, leading to truncation of tokens from targeted frames. In contrast, our method can adaptively conduct spatiotemporal compression, accommodating long video sequences while preserving more visual details.", "description": "The chart illustrates three different sampling methods for processing long videos: uniform sampling, dense sampling, and the proposed LongVU method. Uniform sampling selects frames at even intervals, resulting in a sparse representation that misses crucial information. Dense sampling includes all frames, leading to truncation due to exceeding the maximum context length.  In contrast, LongVU adapts to the video content, selectively compressing spatiotemporal information to effectively manage the context length while retaining important visual details. The chart uses a visual analogy of film frames to represent this, clearly demonstrating the advantages of LongVU over the other methods in handling long videos while preserving accuracy. The final result for each approach is also shown, with LongVU correctly answering the question posed.", "section": "1 Introduction"}, {"figure_path": "2410.17434/charts/charts_10_0.png", "caption": "Figure 4 We randomly sample hundreds of videos to demonstrate the frames/tokens level reduction rate. (a) The number of frames before/after temporal reduction based on DINOv2 features (Section 3.1). (b) The number of tokens before/after spatial token compression (Section 3.3).", "description": "This figure is composed of two sub-figures (a) and (b). Sub-figure (a) presents a bar chart showing the number of frames before and after temporal reduction using DINOv2 features for videos of varying durations (in seconds).  Sub-figure (b) displays another bar chart illustrating the number of tokens before and after spatial token compression, with the x-axis representing the number of frames remaining after the DINOv2 temporal reduction step. Both charts visually represent the effectiveness of the spatiotemporal compression techniques used in the LongVU model, showing a significant reduction in both frames and tokens after applying the respective compression methods.", "section": "4.6 Spatiotemporal Compression Analysis"}, {"figure_path": "2410.17434/charts/charts_10_1.png", "caption": "Figure 7 Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame.", "description": "This heatmap visualizes the results of a Needle-In-A-Video-Haystack experiment, comparing three different model configurations. The x-axis represents the number of frames in the video, while the y-axis indicates the depth (percentage) at which a needle frame (a randomly selected image from a VQA dataset) was inserted. The heatmap's color intensity represents the accuracy score, showing how well each model configuration identifies the needle frame within the video. The configurations tested are the proposed LongVU model (Ours), a version without spatial token compression (Ours w/o STC), and a version without both spatial token compression and query-guided frame selection (Ours w/o STC w/o Query).  The chart clearly illustrates how the proposed adaptive token compression scheme improves the score for locating the needle frame, especially when the number of frames is higher.", "section": "4.6 Spatiotemporal Compression Analysis"}, {"figure_path": "2410.17434/charts/charts_10_2.png", "caption": "Figure 7 Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame.", "description": "This heatmap visualizes the performance of the LongVU model on the Needle-In-A-Haystack task across various video lengths and depths. The x-axis represents the number of frames in the video, ranging from 0.2K to 3.0K. The y-axis represents the depth percentage of the needle frame's position within the video, from 0% to 100%. The color intensity in each cell represents the VQA accuracy score, ranging from 0 (dark red) to 1 (light green).  The chart compares three scenarios: LongVU without STC and without query (a), LongVU without STC (b), and the default LongVU model (c). The results show that the default LongVU model consistently outperforms the other configurations by achieving higher accuracy scores across all video lengths and needle frame depths, demonstrating the effectiveness of both spatial token compression and query-guided frame selection.", "section": "4.6 Spatiotemporal Compression Analysis"}, {"figure_path": "2410.17434/charts/charts_17_0.png", "caption": "Figure 7 Needle-In-A-Video-Haystack results. Our spatiotemporal adaptive token compression scheme improves the score for locating the needle frame.", "description": "This figure presents a heatmap visualization of the Needle-In-A-Video-Haystack experiment results. It compares three different methods: (a) Ours w/o STC w/o Query, (b) Ours w/o STC, and (c) Ours (default).  Each heatmap shows the relationship between the depth percentage (y-axis, representing how far into the video the needle frame was placed) and the number of frames (x-axis). The color intensity represents the score achieved in locating the needle frame, with warmer colors indicating higher scores. The figure demonstrates that the default method with spatiotemporal adaptive token compression achieves the best performance, especially as the number of frames and the depth percentage increase. The performance is significantly better than the other two methods that do not use adaptive token compression.", "section": "4.6 Spatiotemporal Compression Analysis"}]