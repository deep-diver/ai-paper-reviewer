[{"figure_path": "2410.17434/figures/figures_4_0.png", "caption": "Architecture of LongVU. Given a densely sampled video frames, we first utilize DINOv2 (Oquab et al., 2023) prior to remove redundant frames, and fuse the remaining frame features from both SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023), described in Section 3.1. Then we selectively reduce visual tokens via cross-modal query, detailed in Section 3.2. Finally, as demonstrated in Section 3.3, we conduct spatial token compression based on temporal dependencies to further meet the context length of LLMs.", "description": "This figure illustrates the architecture of the LongVU model. It starts with densely sampled video frames which are first processed by DINOv2 to remove redundant frames.  The remaining frames' features are then fused using SigLIP and DINOv2. Next, a cross-modal query mechanism selectively reduces visual tokens, preserving key frames at full resolution while applying spatial pooling to others.  Finally, a spatial token compression step, based on temporal dependencies, further reduces the number of tokens to fit within the context length of LLMs. The entire process is depicted as a flow chart showing the input video, the stages of processing, and the final output fed into the large language model.  A user query is also shown as input to the model.", "section": "Method"}, {"figure_path": "2410.17434/figures/figures_8_0.png", "caption": "Figure 3 Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.", "description": "This figure shows four examples of LongVU's video understanding capabilities. (a) demonstrates spatial-temporal orientation awareness by identifying the direction of a yellow sphere's movement. (b) showcases detailed video description by summarizing a scene featuring two animated characters in a fantastical setting. (c) illustrates action counting by determining the number of times a specific action (cleaning a toilet) appears. (d) highlights hour-long video understanding by answering a question about a tropical beach scene in a long video. Each example includes a screenshot of the video and the corresponding question and answer.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/figures/figures_8_1.png", "caption": "Figure 3 Examples for various video understanding capabilities of LongVU model. We showcase that our LongVU is able to complete different types of video understanding tasks.", "description": "The figure displays four examples of LongVU's video understanding capabilities. (a) shows LongVU correctly identifying the direction of movement of a yellow sphere in a video. (b) provides a detailed description of a video featuring two animated characters in a fantastical setting. (c) demonstrates action counting by identifying the number of times a specific action (cleaning a toilet) appears. Finally, (d) showcases hour-long video understanding by accurately answering a question about a tropical beach shown in a long video. These examples demonstrate the model's ability to perform various video understanding tasks.", "section": "4.4 Video Understanding"}, {"figure_path": "2410.17434/figures/figures_8_2.png", "caption": "Figure 1 Effectiveness of our LongVU over commonly-used uniform sampling and dense sampling. Uniform sampling overlooks critical frames due to its sparse nature. Dense sampling may surpass the maximum context length, leading to truncation of tokens from targeted frames. In contrast, our method can adaptively conduct spatiotemporal compression, accommodating long video sequences while preserving more visual details.", "description": "The figure illustrates the effectiveness of the proposed LongVU method compared to uniform and dense sampling techniques for processing long videos.  It shows that uniform sampling misses important frames, while dense sampling leads to exceeding the context length and truncation of information. LongVU, in contrast, adaptively compresses the video, preserving visual details while staying within the context length.", "section": "1 Introduction"}, {"figure_path": "2410.17434/figures/figures_16_0.png", "caption": "Figure 6. Similarity comparison between SigLIP (Zhai et al., 2023) and DINOv2 (Oquab et al., 2023) features. The similarity is calculated between the first frame and the remainings. DINO concentrating on vision centric task effectively capture subtle frame differences compared with SigLIP (Zhai et al., 2023) which is aligned on semantic space.", "description": "This figure shows a comparison of feature similarity between SigLIP and DINOv2.  The top part displays a sequence of six video frames showing a gymnast performing on a pommel horse. Below, two heatmaps represent the similarity scores calculated using SigLIP and DINOv2 respectively. Each cell in the heatmaps corresponds to the cosine similarity between the features of the first frame and the features of each subsequent frame. The heatmaps visually illustrate that DINOv2 captures subtle differences in frame features more effectively than SigLIP, highlighting DINOv2's superior ability to capture low-level visual details compared to SigLIP's focus on semantic alignment.", "section": "C DINOv2 v.s. SigLIP"}]