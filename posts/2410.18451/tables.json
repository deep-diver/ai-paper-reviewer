[{"figure_path": "2410.18451/tables/table_5_0.md", "caption": "Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling.", "description": "This table presents the statistics of the Skywork Reward Preference 80K dataset, which is a collection of preference pairs used for reward modeling. The table lists seven datasets included in the collection: HelpSteer2, OffsetBias, WildGuardMix, Magpie Ultra, Magpie Pro (Llama 3), Magpie Pro (Llama 3.1), and Magpie Air.  For each dataset, it provides the number of pairs, average number of turns, average number of tokens in the prompt and response, the source of the completion (whether generated by humans or various LLMs), and the annotator (either human or an automated system like ArmoRM). The table also shows the total number of pairs and average statistics across all datasets.", "section": "3.1. Dataset Mixture"}, {"figure_path": "2410.18451/tables/table_6_0.md", "caption": "Table 1 | Statistics of the Skywork Reward Preference 80K dataset for reward modeling.", "description": "This table presents the statistics of the Skywork Reward Preference 80K dataset used for reward modeling.  It lists seven datasets that were combined to create this dataset, giving the number of pairs, the average number of tokens in the prompt and response, the average number of turns, the source of the completion (human or LLM(s)), and the annotator (human or LLM). The datasets include HelpSteer2, OffsetBias, WildGuardMix, and four Magpie datasets (Ultra, Pro (Llama 3.1), Pro (Llama 3), and Air).", "section": "3.1. Dataset Mixture"}, {"figure_path": "2410.18451/tables/table_11_0.md", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "This table presents a performance comparison of various reward models on the RewardBench benchmark. It compares the performance of different reward models across four categories: Chat, Chat Hard, Safety, and Reasoning. The table includes three groups of models: top-performing models from the RewardBench leaderboard, models trained on the RLHFlow dataset (Preference 700K), and models trained on the Skywork Reward Preference dataset (Preference 80K).  For each model, the table shows its type (Generative, Custom, or Discriminative), average score, and scores in each of the four categories. The highest score in each category is bolded. The table highlights that the Skywork-Reward models, despite being trained on a smaller dataset, achieve state-of-the-art performance on RewardBench, significantly outperforming many existing models.", "section": "4.2. Experimental Results"}, {"figure_path": "2410.18451/tables/table_12_0.md", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "This table presents a performance comparison of various reward models on the RewardBench benchmark. It compares different models, including the top-performing models from the leaderboard (first block), models trained on the larger Preference 700K and smaller Preference 378K datasets (second block), and the authors' Skywork-Reward models trained on their curated Preference 80K dataset (third block).  The models are evaluated across four categories: Average Score, Chat, Chat Hard, Safety, and Reasoning, with the highest score in each category bolded. The table highlights the superior performance of the authors' Skywork-Reward models, particularly Skywork-Reward-Gemma-2-27B, which achieves state-of-the-art results.  The superscript * indicates that the results for some models on the leaderboard haven't yet been officially verified.", "section": "4.2. Experimental Results"}, {"figure_path": "2410.18451/tables/table_13_0.md", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "This table presents a performance comparison of various reward models on the RewardBench benchmark.  It's structured in three blocks. The first shows leading models from the RewardBench leaderboard (though some results haven't been officially verified), the second shows results for Llama 3.1-8B and Gemma-2-27B models trained on larger datasets (Preference 700K and Preference 378K), and the third showcases the performance of the authors' Skywork-Reward models (trained on their smaller, curated Skywork Reward Preference 80K dataset).  For each model, the table lists its type (e.g., Generative, Discriminative, Custom), average score across all benchmark tasks, and scores for four specific subtasks: Chat, Chat Hard, Safety, and Reasoning. The highest score achieved in each column is shown in bold, highlighting the top-performing models in each category.", "section": "4.2. Experimental Results"}, {"figure_path": "2410.18451/tables/table_13_1.md", "caption": "Table 2 | Performance comparison of different reward models on RewardBench. The first block of the table includes the top reward models on the RewardBench leaderboard. The superscript in this block indicates that the results have not been officially verified. The second block of the table corresponds to Llama-3.1-8B and Gemma-2-27B (both instruct version) trained on Preference 700K and Preference 378K data, respectively. The final block of the table showcases the performance of our Skywork-Reward model series, which are trained on the Skywork Reward Preference 80K dataset. Notably, Skywork-Reward-Gemma-2-27B achieves state-of-the-art performance, outperforming several competitive models on RewardBench. The highest performance in each column is masked as bold.", "description": "This table presents a performance comparison of various reward models on the RewardBench benchmark.  It's organized into three sections: the first shows top-performing models from the RewardBench leaderboard (with an asterisk indicating results not yet officially verified); the second compares Llama-3.1-8B and Gemma-2-27B models trained on different datasets (Preference 700K and Preference 378K); and the third shows the performance of the Skywork-Reward model series (trained on the Skywork Reward Preference 80K dataset).  The table provides the average score across all RewardBench tasks and broken down by four categories (Chat, Chat Hard, Safety, Reasoning).  The highest-performing model in each category is highlighted in bold.", "section": "4.2. Experimental Results"}]