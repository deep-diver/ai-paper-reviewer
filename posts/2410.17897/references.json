{"references": [{" publication_date": "2017", "fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, the foundation upon which this paper builds.  Understanding the strengths and limitations of the original Transformer is crucial for assessing the significance of the proposed improvements for mitigating attention concentration and improving efficiency.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Jacob Devlin", "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "reason": "BERT is a highly influential Transformer-based model that demonstrated the power of pre-training in NLP.  The success of BERT and similar models showcases the importance of Transformer architectures and motivates the research to improve their scalability and efficiency.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This study highlights the capabilities of large language models and further emphasizes the importance of scaling up transformer models for improved performance. The challenges and opportunities in scaling Transformers are directly relevant to this paper's focus on mitigating attention concentration.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper established scaling laws for neural language models, showing how increasing model size and training data correlates with improved performance. This provides context for the pursuit of larger and deeper Transformer models, which are also the subject of this paper's investigation into improving efficiency.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Daquan Zhou", "paper_title": "Deepvit: Towards deeper vision transformer", "reason": "This study investigates the challenges of training deeper Vision Transformers and provides insights into why increasing the depth of a Transformer model does not always result in improved performance. Overcoming the performance limitations of deep Transformers directly motivates this paper's efforts to improve training and efficiency.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Han Shi", "paper_title": "Revisiting over-smoothing in bert from the perspective of graph", "reason": "This paper analyzes over-smoothing in BERT and highlights the attention smoothing mechanism as a contributor. The detailed analysis of over-smoothing is crucial for understanding and addressing the core problem tackled in this paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Tam Nguyen", "paper_title": "Mitigating over-smoothing in transformers via regularized nonlocal functionals", "reason": "This work proposes a method to alleviate over-smoothing in transformers and provides additional context for the problem this paper addresses.  The approaches taken to address over-smoothing are highly relevant to the solution proposed in this paper.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Jackson Petty", "paper_title": "The impact of depth on compositional generalization in transformer language models", "reason": "This study investigates the theoretical benefits of increasing depth in Transformers, particularly the impact on compositional generalization. While this paper addresses practical challenges in training deeper models, the theoretical insights from this work contextualize the motivation and significance of the proposed solutions.", "section_number": 1}, {" publication_date": "2016", "fullname_first_author": "Kaiming He", "paper_title": "Deep residual learning for image recognition", "reason": "ResNet introduced residual connections, a crucial technique for training very deep networks.  The success of ResNet's residual connections highlights the importance of efficient information flow and motivates similar approaches in improving Transformer architectures.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Gao Huang", "paper_title": "Deep networks with stochastic depth", "reason": "Stochastic Depth is a regularization technique for training very deep networks that randomly drops layers during training to mitigate overfitting.  This approach is relevant to the paper's efforts to improve the efficiency and effectiveness of training very deep Transformer models.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Gao Huang", "paper_title": "Densely connected convolutional networks", "reason": "DenseNet, through its dense connections, enables efficient information flow in deep networks by allowing all layers to have access to the features of previous layers. This concept of efficient information flow inspired similar ideas for addressing the over-smoothing problem in Transformer networks.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Noam Shazeer", "paper_title": "Fast transformer decoding: One write-head is all you need", "reason": "This paper explores efficient transformer decoding techniques which are directly relevant to addressing the challenges of KV cache compression in large-scale transformer deployment.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Joshua Ainslie", "paper_title": "Gqa: Training generalized multi-query transformer models from multi-head checkpoints", "reason": "Multi-Query Attention (MQA) is a technique for improving the efficiency of attention mechanisms in Transformers and directly addresses the KV cache compression challenge.  Understanding MQA and similar approaches is relevant to the paper's proposed efficient methods.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "William Brandon", "paper_title": "Reducing transformer key-value cache size with cross-layer attention", "reason": "This work focuses on reducing the KV cache size in Transformers, a problem directly addressed by the paper.  The methods and insights in this paper offer relevant context for the paper's proposed solutions.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Albert Q Jiang", "paper_title": "Mistral 7b", "reason": "This paper describes the Mistral 7B language model, a large language model which addresses the challenges of scaling up language models. This is relevant because scaling Transformer models is a key challenge that the paper is trying to improve upon.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Matteo Pagliardini", "paper_title": "Enhancing information flow in transformers via depth weighted averaging", "reason": "This paper focuses on improving information flow in Transformer models which is highly relevant to the problem addressed in the current paper.  The techniques and insights used in this paper for addressing information flow are directly relevant to this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuhong Li", "paper_title": "SnapKV: Llm knows what you are looking for before generation", "reason": "SnapKV is a post-training method focusing on KV cache compression.  This paper aims at improving the inference efficiency of Transformers, which is directly related to the challenges of handling large KV caches, a problem this paper addresses.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Coleman Hooper", "paper_title": "Kvquant: Towards 10 million context length llm inference with kv cache quantization", "reason": "This paper explores the use of quantization to reduce the size of KV caches, a crucial aspect for efficient Transformer inference, especially for long sequences.  It addresses the same problem as the paper's proposed solutions.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper shows the current progress in large language models.  It provides a context for understanding the challenges of scaling up Transformer models and shows that existing approaches such as this paper are necessary to resolve these challenges.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Tianwen Wei", "paper_title": "Skywork: A more open bilingual foundation model", "reason": "This paper provides context on recent advancements in large language models which this paper is trying to resolve the issues with.", "section_number": 4}]}