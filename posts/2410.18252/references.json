{"references": [{" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is foundational to the field of RLHF and establishes the dominant paradigm of synchronous online on-policy RLHF, which this paper seeks to improve upon.  Understanding the limitations of this established method is crucial to evaluating the proposed alternative.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Rafael Rafailov", "paper_title": "Direct Preference Optimization: Your Language Model is Secretly a Reward Model", "reason": "This paper introduces Direct Preference Optimization (DPO), a highly efficient offline RLHF method that serves as a critical comparison point to the online methods explored in the current work. Its efficiency and performance are central to the discussion of the trade-offs involved in the proposed asynchronous approach.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "John Schulman", "paper_title": "Trust Region Policy Optimization", "reason": "Proximal Policy Optimization (PPO) is a widely used algorithm in RLHF.  This paper's introduction of the algorithm itself and its subsequent use in RLHF research provide a critical context for understanding the comparative analysis of RLHF algorithms presented in this paper.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Arash Ahmadian", "paper_title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs", "reason": "This paper is particularly relevant as it offers an alternative approach to PPO for RLHF. This alternative algorithm, REINFORCE Leave-One-Out (RLOO), is used as a comparison to assess the robustness of various RLHF methods to off-policy data.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Daniele Calandriello", "paper_title": "Human Alignment of Large Language Models through Online Preference Optimisation", "reason": "This paper introduces Online DPO and finds strong performance. This serves as a focal point for comparing different loss functions and their robustness to off-policy data, helping to justify the choice of Online DPO in the proposed asynchronous RLHF method.", "section_number": 2}, {" publication_date": "2016", "fullname_first_author": "Volodymyr Mnih", "paper_title": "Asynchronous Methods for Deep Reinforcement Learning", "reason": "This work lays the theoretical foundation for asynchronous methods in deep reinforcement learning, which is critical in the context of asynchronous off-policy RLHF presented in this paper.  It details the potential gains and the challenges inherent in applying asynchronous learning to reinforcement learning tasks.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Shengyi Huang", "paper_title": "Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform", "reason": "This is highly relevant as the authors utilize the Cleanba framework, which is designed for efficient asynchronous training, in their proposed asynchronous RLHF method.  The framework allows the separation of generation and training tasks crucial for asynchronous training.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient Memory Management for Large Language Model Serving with PagedAttention", "reason": "This paper introduces the highly efficient generation library vllm, which the authors utilize for accelerating sample generation in their asynchronous off-policy RLHF approach.  The library\u2019s performance is key to the efficiency gains reported.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Leo Gao", "paper_title": "Scaling Laws for Reward Model Overoptimization", "reason": "The authors build upon previous work, leveraging the reward model from Huang et al (2024) as ground truth and following their setup for creating a controlled environment. Understanding scaling laws for reward models is crucial to interpreting the performance trade-offs observed in their experiments.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "reason": "This paper describes FlashAttention, a more efficient mechanism for handling attention in LLMs.  While not directly used in the implementation, understanding advancements in efficient LLM training and inference is crucial for placing the asynchronous RLHF method in context.", "section_number": 3}, {" publication_date": "2024", "fullname_first_author": "Yunhao Tang", "paper_title": "Understanding the performance gap between online and offline alignment algorithms", "reason": "This paper directly addresses the limitations of offline methods in RLHF and highlights the need for online methods, a central argument of this paper.  It contributes to the motivation for exploring more efficient online methods such as the proposed asynchronous off-policy approach.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Yunhao Tang", "paper_title": "Generalized Preference Optimization: A Unified Approach to Offline Alignment", "reason": "This paper contributes to a broader discussion of RLHF optimization techniques, providing a comparison point against the online DPO method employed in this paper. It gives a deeper understanding of the design choices behind the proposed method.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Fahim Tajwar", "paper_title": "Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data", "reason": "This paper underscores the significance of on-policy data in RLHF, even if suboptimal.  This is particularly relevant to the current paper which explores off-policy learning, clarifying the trade-offs between on-policy performance and the potential speed improvements offered by off-policy approaches.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Shusheng Xu", "paper_title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study", "reason": "This paper offers a systematic comparison of different optimization strategies (PPO, DPO) within RLHF, providing valuable context and benchmarks that are directly relevant to the current paper. Its comparative approach helps understand the choices made in the proposed method.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Nazneen Rajani", "paper_title": "Hugging Face No Robots", "reason": "This paper introduces the No Robots dataset used in the large-scale experiment. The dataset is relevant for validating the effectiveness of the asynchronous off-policy method in a real-world application and understanding its performance under various conditions.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Meta AI Llama Team", "paper_title": "The Llama 3 Herd of Models", "reason": "The Llama 3 model is used in the large-scale experiment, providing a critical context and benchmark. Its characteristics influence both the practical challenges and the potential benefits of asynchronous training which are discussed in this work.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Zhiyu Mei", "paper_title": "ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation", "reason": "This paper introduces a different strategy for optimizing RLHF, providing an alternative approach to the one presented in this paper. Its inclusion demonstrates a broader range of solutions currently being explored in the field.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Aliz\u00e9e Pace", "paper_title": "West-of-N: Synthetic Preference Generation for Improved Reward Modeling", "reason": "This paper explores the use of synthetic data for reward model training.  This is relevant to the current work, especially regarding the creation of a synthetic preference dataset which involves similar considerations.  The comparison helps to understand the challenges in using synthetic data and the potential effects on training.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Lunjun Zhang", "paper_title": "Generative verifiers: Reward modeling as next-token prediction", "reason": "This paper offers a valuable comparison point for how to improve reward model training and is relevant to the current work's discussion about the limitations of current reward modeling techniques. The study helps to contextualize the choices made in this work's reward modeling approach.", "section_number": 6}]}