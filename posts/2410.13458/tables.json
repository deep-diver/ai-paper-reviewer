[{"figure_path": "2410.13458/tables/table_2_0.md", "caption": "Comparison of MEDINST to several datasets in biomedical field.", "description": "The table compares MEDINST with three other biomedical datasets: SUP-NATINST, BoX, and BLURB.  It provides a qualitative comparison across several features, indicating whether each dataset includes task instructions, is a multi-task dataset, provides examples, and is publicly available. It also presents quantitative comparisons by listing the number of tasks, number of instructions, number of annotated task types, and average task definition length (in words) for MEDINST and SUP-NATINST.  The BoX and BLURB datasets have some of these fields marked as unavailable ('x' or '-').", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_4_1.md", "caption": "Comparison of MEDINST to several datasets in biomedical field.", "description": "The table compares MEDINST with four other datasets in the biomedical field across several characteristics: whether task instructions and multi-task datasets are present, whether examples are provided and public availability.  It then presents the number of tasks, instructions, annotated task types, and average task definition length (in words) for each dataset.  This allows for a quantitative comparison of the size and scope of MEDINST relative to existing biomedical datasets.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_8_1.md", "caption": "Table 3: Test results of various models on MEDINST32. \u2020 indicates that the training sets of LLaMA3-MI includes the corresponding training sets of the datasets used by MEDINST32, whereas other models have not seen the MEDINST32 dataset. \u2193 represents that a lower score is better, while for other metrics, a higher score is better. The best and second-best results for each row are highlighted in bold and underlined, respectively. For the baselines, we use a few-shot prompt, providing two examples in the instruction. For the fine-tuned models, we use a zero-shot prompt.", "description": "Table 3 presents the performance evaluation results of seven different LLMs on the MEDINST32 benchmark. The models include three baselines (LLaMA3, BioMistral, MMedL3-EnIns), a state-of-the-art model (GPT-40), and three instruction fine-tuned LLMs on MEDINST (LLaMA3-MI32, MMedL3-MI32, LLaMA3-MI). The table shows the performance of each model on 163 tasks across various categories, using metrics like Label F1, Entity F1, Exact Match, MSE, and Rouge-L, chosen based on the task type.  The table also highlights the best and second-best results for each task. A key aspect is the comparison of zero-shot and few-shot performance on unseen tasks, showing the effectiveness of instruction fine-tuning with MEDINST for improved generalization.", "section": "4.2 Results"}, {"figure_path": "2410.13458/tables/table_13_0.md", "caption": "Table 1: Comparison of MEDINST to several datasets in biomedical field.", "description": "The table compares MEDINST with four other datasets in the biomedical field across several key characteristics. These characteristics include whether the datasets contain task instructions and multiple tasks, whether they provide examples, their public availability, the number of tasks and instructions they contain, and the average length of task definitions.  MEDINST is shown to be significantly larger and more comprehensive than the others.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_14_0.md", "caption": "Table 1: Comparison of MEDINST to several datasets in biomedical field.", "description": "The table compares MEDINST with four other datasets in the biomedical field across several features.  These features include whether task instructions and multi-task datasets are present, the availability of examples, whether the dataset is publicly available, the number of tasks, the number of instructions, the number of annotated task types, and the average length of task definitions in words.  The table highlights that MEDINST is unique in having task instructions and multi-task datasets, and it has the largest number of tasks and instructions.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_15_0.md", "caption": "Table 1: Comparison of MEDINST to several datasets in biomedical field.", "description": "The table compares MEDINST to several other datasets in the biomedical field across various characteristics.  These characteristics include whether the dataset includes task instructions and multi-task datasets, the presence of examples, its public availability, the number of tasks and instructions, the number of annotated task types, and the average length of task definitions (in words).  This comparison highlights MEDINST's superior comprehensiveness in terms of its scale and scope.", "section": "1 Introduction"}, {"figure_path": "2410.13458/tables/table_16_0.md", "caption": "Table 3: Test results of various models on MEDINST32. \u2020 indicates that the training sets of LLaMA3-MI includes the corresponding training sets of the datasets used by MEDINST32, whereas other models have not seen the MEDINST32 dataset. \u2193 represents that a lower score is better, while for other metrics, a higher score is better. The best and second-best results for each row are highlighted in bold and underlined, respectively. For the baselines, we use a few-shot prompt, providing two examples in the instruction. For the fine-tuned models, we use a zero-shot prompt.", "description": "Table 3 presents the performance of several LLMs (LLaMA3, BioMistral, MMedL3-EnIns, GPT-40, LLaMA3-MI32, MMedL3-MI32, and LLaMA3-MI) on the MEDINST32 benchmark, which comprises 32 biomedical tasks.  The table shows the performance of each model on each task using various metrics relevant to the specific task type (e.g., Label-F1, Entity-F1, EM, Rouge-L, MSE). The results indicate the performance of zero-shot and few-shot models, comparing the performance of models fine-tuned on different datasets (MEDINST32, MEDINST, and English medical instructions).  The table highlights the impact of the MEDINST dataset on model generalization ability and compares performance across different models and evaluation settings.", "section": "4.2 Results"}, {"figure_path": "2410.13458/tables/table_16_1.md", "caption": "Table 10: TRANSL task: ParaMed results.", "description": "The table presents the performance of various models on the ParaMed dataset for the translation task.  The models evaluated include LLaMA3, BioMistral, MMEDL3-EnIns, GPT-40, LLaMA3-MI32 (ours), MMEDL3-MI32 (ours), and LLaMA3-MI (ours).  The performance is measured using two metrics: BERTScore and METEOR Score.  The table shows the scores for each model, allowing for comparison of their performance on this specific translation task.  The 'ours' designation indicates models developed by the authors of the paper.", "section": "4.2 Results"}, {"figure_path": "2410.13458/tables/table_17_0.md", "caption": "Table 11: Dataset collection.", "description": "This table presents the dataset collection details for the MEDINST dataset. It lists each dataset used, specifying the task category (QA, TE, NER, NED, RE, COREF, STS, EE, TRANSL, TEXTPAIRCLASS, SUM), and the number of training, development, and test samples for each.  The table is extensive, covering numerous datasets across diverse biomedical NLP tasks.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}, {"figure_path": "2410.13458/tables/table_18_0.md", "caption": "Table 11: Dataset collection.", "description": "This table lists all datasets employed in the MEDINST dataset.  It details the task each dataset is associated with (e.g., NER, QA, RE), along with the number of training, development, and test samples for each dataset.  Because one dataset might be used for multiple tasks, suffixes are appended to dataset names to distinguish their use in different tasks. For instance, BC5CDR appears in the NER, NED, and RE tasks.  For the primary task (NER), the original dataset name is used, while suffixes are added for the other two tasks to denote dataset usage.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}, {"figure_path": "2410.13458/tables/table_20_0.md", "caption": "Table 11: Dataset collection.", "description": "Table 11 presents a comprehensive list of the datasets included in the MEDINST dataset collection.  It details the task category (e.g., NER, QA, RE, etc.) to which each dataset contributes, along with the number of training, development, and testing samples available for each.  The table is organized by task category and then lists individual datasets.  Some datasets are used across multiple tasks, reflected by their appearance in multiple rows. The total number of samples within each dataset varies widely reflecting the different sizes of the original datasets included in MEDINST.", "section": "3 MEDINST: Meta Dataset of Biomedical Instructions"}]