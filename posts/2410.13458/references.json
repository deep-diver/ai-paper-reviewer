{"references": [{" publication_date": "2024", "fullname_first_author": "Wenhan Han", "paper_title": "MedINST: Meta Dataset of Biomedical Instructions", "reason": "This is the main paper introducing the MEDINST dataset, a novel meta-dataset of biomedical instructions, which is the central focus and contribution of this work.  The paper details the dataset's creation, composition, and usage for instruction tuning of LLMs in the biomedical domain.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "GPT-4", "reason": "GPT-4 is a leading LLM, and its mention highlights the impressive advancements in LLMs.  It contextualizes the current state-of-the-art and underlines the motivation for adapting LLMs to specific tasks, including in the biomedical field.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Hugo Touvron", "paper_title": "LLaMA: Open and Efficient Foundation Language Models", "reason": "LLaMA-3 is another prominent LLM, its inclusion reinforces the point about significant progress in LLMs and provides a concrete example of a model used for comparison and benchmarking in the experimental section.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7B", "reason": "This LLM further strengthens the argument about rapid advancement in LLM capabilities and illustrates the trend of adapting LLMs to specific tasks efficiently, making it relevant to the paper's focus on instruction finetuning.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language Models are Few-Shot Learners", "reason": "This paper is foundational in demonstrating that LLMs can achieve strong performance in a wide range of tasks with only a few examples, paving the way for the instruction tuning approach employed in the current paper and thus highly relevant to its methodology.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Jinhyuk Lee", "paper_title": "BioBERT: A pre-trained biomedical language representation model for biomedical text mining", "reason": "BioBERT is a well-known biomedical LLM and highlighting this model helps to show the evolution of LLMs in the biomedical field. Its limitations regarding generalization to unseen tasks, which are addressed by the MEDINST dataset, are discussed.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Yizhong Wang", "paper_title": "Super-NATURAL INSTRUCTIONS", "reason": "This paper presents the SUPER-NATURAL INSTRUCTIONS dataset, which is a relevant benchmark in the field of instruction tuning and LLM generalization.  It helps to contextualize the scale and complexity of MEDINST in comparison to other established datasets.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "reason": "This paper is highly influential in introducing instruction tuning as a method for training and adapting LLMs, and it is pivotal for understanding the methodology employed in creating and evaluating the MEDINST dataset.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Shayne Longpre", "paper_title": "The Flan Collection: Designing Data and Methods for Effective Instruction Tuning", "reason": "FLAN is another important dataset for instruction tuning. Its mention helps to show the broader context of instruction tuning research and contrasts with the paper's focus on biomedical datasets.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jason Wei", "paper_title": "Finetuned Language Models Are Zero-Shot Learners", "reason": "This paper's demonstration of zero-shot learning capabilities in finetuned LLMs is highly relevant to MEDINST's objective of enabling LLMs to generalize to unseen tasks with little to no further fine-tuning.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Monica Agrawal", "paper_title": "Large Language Models are Few-Shot Clinical Information Extractors", "reason": "This paper demonstrates the application of LLMs to clinical information extraction, highlighting their potential in a biomedical domain.  It is useful for understanding and showcasing the existing related work and the motivation to move towards a more comprehensive dataset like MEDINST.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Tianyu Han", "paper_title": "MedAlpaca - An Open-Source Collection of Medical Conversational AI Models and Training Data", "reason": "MedAlpaca is a relevant biomedical dataset and including it shows the context of the work and identifies the limitations of existing datasets, particularly their size and diversity of tasks, which the paper addresses.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yunxiang Li", "paper_title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge", "reason": "This paper is relevant because it shows an attempt to adapt LLMs for medical applications. However, limitations in data size and task diversity motivate this work. Its inclusion highlights the limitations of existing models and the need for MEDINST.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Rohan Taori", "paper_title": "Stanford alpaca: An instruction-following llama model", "reason": "Alpaca is a significant effort in instruction tuning. Its mention further helps to contextualize the approach and shows the existing state-of-the-art in adapting LLMs to various tasks. The paper uses this methodology for biomedical tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yanis Labrak", "paper_title": "BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains", "reason": "BioMistral is directly compared to the proposed LLM in this paper. It serves to benchmark and highlight improvements in using instruction tuning with a larger, more diverse dataset.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Jason Alan Fries", "paper_title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing", "reason": "BigBIO is a framework for building and evaluating biomedical NLP datasets, and this inclusion in the 'Related Work' section helps to contextualize the effort in constructing a standardized dataset for instruction tuning.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Yu Gu", "paper_title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing", "reason": "BLURB is a biomedical benchmark dataset that is compared to the proposed dataset.  Including this highlights the contribution of the dataset in relation to existing benchmarks in terms of size and diversity.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Yifan Peng", "paper_title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMO on Ten Benchmarking Datasets", "reason": "This paper examines the application of transfer learning in the biomedical domain, highlighting the increasing interest and effort in adapting pre-trained language models for biomedical tasks, thus setting the stage for this paper's contribution.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding", "reason": "MMLU is a large-scale benchmark used for comparing the performance of LLMs across multiple tasks and is relevant to the paper's experiments that measure performance on MMLU-Medicine to demonstrate generalization capabilities.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Pengfei Liu", "paper_title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "reason": "This survey paper provides a comprehensive overview of prompting methods for LLMs, which helps to contextualize and understand the techniques used in this paper for adapting LLMs to biomedical tasks using instruction tuning.", "section_number": 4}]}