[{"figure_path": "2410.13458/charts/charts_7_0.png", "caption": "Figure 3: Training sample and model parameter scale analysis.", "description": "The bar chart displays the average Rouge-L scores across various tasks (QA, TE, NER, TXTCLASS, NED, RE, COREF, SUM, EE, TRANSL) for four different model training scenarios: MMedL3-EnIns, MMedL3-MI32-5K, MMedL3-MI32-50K, and MMedL3-MI32-100K.  Each scenario represents a different size of training data used to fine-tune the models, showcasing the performance impact of varying training sample size.  The chart allows comparison of model performance across multiple tasks with different levels of sample size for training.", "section": "4.3 Ablation Analysis"}, {"figure_path": "2410.13458/charts/charts_7_1.png", "caption": "Figure 3: Training sample and model parameter scale analysis.", "description": "The bar chart presents an ablation study analyzing the impact of training data size and model parameters on the performance of fine-tuned LLMs across various tasks. It displays the average Rouge-L scores for different tasks (QA, TE, NER, TXTCLASS, NED, RE, COREF, SUM, EE, TRANSL) achieved by two models: Phi3-4B-MI32-50K (4 billion parameter model trained with 50K samples) and Phi3-14B-MI32-50K (14 billion parameter model trained with 50K samples). The chart helps to understand how model performance varies depending on the scale of training data and model parameters, indicating the trade-offs between model size, data requirements, and performance across multiple tasks in the biomedical domain.", "section": "4.3 Ablation Analysis"}]