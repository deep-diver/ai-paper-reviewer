[{"figure_path": "2410.18693/tables/table_6_0.md", "caption": "Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "The table presents the main results of the ScaleQuest model on four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  It compares the performance of several models, including Mistral-7B, Llama3-8B, DeepSeekMath-7B, and Qwen2-Math-7B, each fine-tuned with different datasets created using various methods, including ScaleQuest and other publicly available methods like MetaMath, MMIQC, and DART-Math. The table shows the zero-shot pass@1 accuracy for each model and dataset combination, highlighting the performance improvements achieved by ScaleQuest.  It also indicates the synthesis models used for creating the baselines' datasets.", "section": "3.2 Main Results"}, {"figure_path": "2410.18693/tables/table_9_0.md", "caption": "Table 1: Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "The table presents the main results of the ScaleQuest method on four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  It compares the performance of various models, including Mistral-7B, Llama3-8B, DeepSeekMath-7B, and Qwen2-Math-7B, each fine-tuned with different datasets (including ScaleQuest and other existing datasets like MetaMath, MMIQC, DART-Math, and NuminaMath).  The models are grouped by base model type (general base model, math-specialized base model) and the dataset used for fine-tuning is indicated, allowing a comparison between models trained with the proposed ScaleQuest data synthesis method and other methods.  The average performance across all four benchmarks is given for each model, and bold numbers highlight the best performance for each base model.", "section": "3.2 Main Results"}, {"figure_path": "2410.18693/tables/table_9_1.md", "caption": "Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "The table presents the main results of four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  It compares the performance of various models, specifically focusing on those fine-tuned using the ScaleQuest dataset and other existing datasets created using different synthesis methods (e.g., GPT-4, GPT-4-Turbo, DeepSeekMath, Qwen2-Math).  The table shows zero-shot pass@1 accuracy for each model on each benchmark, highlighting the improvements achieved by using the ScaleQuest dataset.  It includes general base models and math-specialized base models, allowing for a comparison of performance across different model architectures and training datasets.", "section": "3.2 Main Results"}, {"figure_path": "2410.18693/tables/table_10_0.md", "caption": "Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "The table presents the main results of the ScaleQuest model and various baseline models on four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  For each benchmark, the table shows the average performance of different models, categorized by their base model (General Base Model or Math-Specialized Base Model).  The models' performances are reported as a percentage, reflecting their zero-shot accuracy. The table also specifies the data synthesis model used for each baseline, indicating which model was used to create the training data. Notably, the ScaleQuest model is compared against various other data synthesis methods, highlighting its superior performance across different benchmarks and base models.", "section": "3.2 Main Results"}, {"figure_path": "2410.18693/tables/table_11_0.md", "caption": "Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "The table presents the performance of various LLMs on four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  Models are grouped by their base model (Mistral-7B, Llama3-8B, DeepSeekMath-7B, and Qwen2-Math-7B) and results are shown for both general and math-specialized models.  Each model's performance is indicated, along with the synthesis model used to generate the training data (e.g., GPT-4, GPT-4-Turbo, DeepSeekMath-7B-RL).  The \"ScaleQuest\" method, using a smaller open-source model for question synthesis, is compared against several baselines using various publicly available datasets. The average score across all four benchmarks is provided for each model.", "section": "3.2 Main Results"}, {"figure_path": "2410.18693/tables/table_16_0.md", "caption": "Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "The table presents the main results of the ScaleQuest method on four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  It compares the performance of various models, including Mistral-7B, Llama3-8B, DeepSeekMath-7B, and Qwen2-Math-7B, both with and without fine-tuning using the ScaleQuest dataset. The table shows the zero-shot pass@1 accuracy for each model on each benchmark, highlighting the improvements achieved by using the ScaleQuest dataset.  It also indicates the synthesis models used to create the baseline datasets.  The best score within each base model is bolded.", "section": "3.2 Main Results"}, {"figure_path": "2410.18693/tables/table_19_0.md", "caption": "Table 1: Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "This table presents the main results of four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  It compares the performance of various models, primarily focusing on those fine-tuned using the ScaleQuest dataset against those using other data synthesis methods. The table lists the model name, the synthesis model used to generate the training data, and the resulting accuracy scores for each benchmark.  It highlights the best performance within each base model family.  The baselines include various models using different data synthesis approaches, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math.", "section": "3.2 Main Results"}, {"figure_path": "2410.18693/tables/table_19_1.md", "caption": "Main results on four mathematical reasoning benchmarks. Bold means the best score within the respective base model. The baselines use different synthesis models, such as GPT-4, GPT-4-Turbo, GPT-40, DeepSeekMath, and Qwen2-Math. If multiple models are used, only the latest released one is marked. More details concerning these datasets are shown in Figure 5.", "description": "This table presents the main results of the ScaleQuest method on four mathematical reasoning benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  It compares the performance of various models, including those using different data synthesis methods (e.g., GPT-4, GPT-4-Turbo, DeepSeekMath, Qwen2-Math) and the ScaleQuest method, across these four benchmarks.  The table shows zero-shot pass@1 accuracy and highlights the best-performing model for each base model.  The average performance across all benchmarks is also provided. The table details different base models (general and math-specialized) and the synthesis models used for comparison.  The results demonstrate the significant performance improvements achieved by the ScaleQuest method compared to existing methods.", "section": "3.2 Main Results"}]