[{"figure_path": "2410.18693/charts/charts_1_0.png", "caption": "Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.", "description": "The chart is a dual chart visualizing the performance of various LLMs on the MATH benchmark.  The left chart is a bar chart comparing the accuracy of different LLMs, including both proprietary models (GPT-4, GPT-4-Turbo, Claude 3.5 Sonnet) and open-source models (Qwen2-Math-7B, Llama3, Mistral), with and without fine-tuning using the ScaleQuest dataset.  The right chart is a line graph showing the performance improvement of Llama3-8B when fine-tuned using different publicly available datasets versus the ScaleQuest dataset, illustrating the effectiveness of ScaleQuest in improving model accuracy. The chart highlights the significant performance gains achieved by fine-tuning open-source LLMs with the ScaleQuest dataset, surpassing even proprietary models in some cases.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18693/charts/charts_1_1.png", "caption": "Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.", "description": "The chart displays the performance of various mathematical reasoning models, including proprietary models (GPT-4-Turbo, Claude-3.5 Sonnet) and open-source models (Qwen2-Math, Llama, Mistral) on the MATH benchmark.  The left side shows the accuracy of different models, highlighting the significant improvement achieved by fine-tuning the Qwen2-Math-7B-Base model using the ScaleQuest dataset. The right side presents a comparison of the performance gains obtained by fine-tuning the Llama3-8B model with different publicly available datasets, showcasing ScaleQuest's superior performance in improving accuracy.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18693/charts/charts_1_2.png", "caption": "Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.", "description": "This figure presents two bar charts and two line charts visualizing the performance of various language models on mathematical reasoning tasks. The left chart shows the performance of different models on the MATH benchmark, highlighting the improvement achieved by the proposed ScaleQuest method. The right chart illustrates how fine-tuning Llama3-8B on various datasets affects its performance, indicating the effectiveness of the ScaleQuest dataset compared to publicly available datasets. The line charts in the figure show the improvement in performance on the Olympiad Bench benchmark with increasing training data size, for different methods.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18693/charts/charts_3_0.png", "caption": "Figure 3: The difficulty distribution of two real-world datasets and two synthetic datasets. The difficulty score is calculated based solely on the problem part.", "description": "This chart is a combined histogram showing the distribution of difficulty scores for four datasets: GSM8K, MATH, Qwen2-QFT-GSM8K, and Qwen2-QFT-MATH.  The x-axis represents the difficulty score, ranging from 0.0 to 1.0, and the y-axis represents the density. The top panel displays the distributions for the real-world datasets GSM8K and MATH, showing that MATH has a higher proportion of difficult problems (scores above 0.6). The bottom panel shows the distributions for synthetic datasets generated using Qwen2-QFT models trained on GSM8K and MATH. These distributions indicate the ability of the QFT process to generate varied question difficulty levels, though they show different distributions than their respective source datasets.", "section": "2.2 QUESTION FINE-TUNING (QFT)"}, {"figure_path": "2410.18693/charts/charts_4_0.png", "caption": "Figure 4: The solvability and difficulty of the raw questions generated by the QFT model and the optimized ones.", "description": "This bar chart displays the solvability and difficulty ratios of questions before and after optimization using two different optimization models: Qwen2-Math-7B-Ins and GPT-40-mini.  The solvability ratio shows the percentage of questions deemed solvable, while the difficulty ratio presents the distribution of question difficulties (easy, medium, hard) as determined by GPT-40-mini.  The chart reveals that GPT-40-mini is more effective in enhancing both solvability and difficulty compared to Qwen2-Math-7B-Ins.  Specifically, GPT-40-mini significantly improved the solvability and difficulty ratios across all difficulty levels compared to Qwen2-Math-7B-Ins, particularly for harder questions.", "section": "2.3 QUESTION PREFERENCE OPTIMIZATION (QPO)"}, {"figure_path": "2410.18693/charts/charts_8_0.png", "caption": "Figure 5: A comparison of the synthetic dataset generated by the raw instruct model, the model after QFT, the model after QPO, and the final dataset after applying reward filtering. The evaluation covers question solvability, difficulty, and instruction tuning effectiveness on Llama3-8B.", "description": "This grouped bar chart presents a comparison of the synthetic dataset generated using four different methods: raw instruct model, after Question Fine-Tuning (QFT), after Question Preference Optimization (QPO), and finally after reward filtering.  The chart displays the percentage for each method across three evaluation metrics: solvability, difficulty, and accuracy on the Llama3-8B model across four benchmarks: GSM8K, MATH, College Math, and Olympiad Bench.  It visually demonstrates the improvements in solvability and difficulty achieved through each processing stage, culminating in the enhanced performance after reward filtering, as indicated by the accuracy on each benchmark.", "section": "3.3 ABLATION STUDY"}, {"figure_path": "2410.18693/charts/charts_15_0.png", "caption": "Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.", "description": "This figure contains two bar charts. The left chart presents the performance of various models on the MATH benchmark, highlighting the improvement achieved by incorporating the ScaleQuest dataset.  The right chart shows the performance gains of Llama3-8B when fine-tuned on different publicly available datasets, comparing these gains to those obtained using the ScaleQuest dataset.  Both charts illustrate the significant performance improvements enabled by the ScaleQuest dataset compared to existing methods and datasets.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18693/charts/charts_15_1.png", "caption": "Figure 1: Left: Results of different models on MATH, where -ScaleQuest denotes ours. Right: Results of Llama3-8B fine-tuned on publicly available datasets constructed by different methods.", "description": "This figure is composed of two subfigures. The left subfigure is a bar chart comparing the performance of various models, including proprietary models (GPT-4, GPT-4-Turbo, Claude 3.5 Sonnet) and open-source models (Qwen2-Math-7B-Instruct, Llama 3.1-70B-Instruct, Qwen2-72B-Instruct, Gemini 1.5 Pro, Qwen2-72B-Numina-Math), on the MATH benchmark.  A key finding is that the Qwen2-Math-7B-ScaleQuest model, fine-tuned using the ScaleQuest dataset, outperforms even some of the proprietary models. The right subfigure is a line chart illustrating the performance of Llama3-8B when fine-tuned on different publicly available datasets (MetaMath, DART-Math, NuminaMath) and ScaleQuest, showing the improvement in accuracy as the size of the training data increases.  Both charts demonstrate the effectiveness of the ScaleQuest dataset in improving the reasoning capability of LLMs.", "section": "1 INTRODUCTION"}]