[{"figure_path": "2410.13816/figures/figures_1_0.png", "caption": "Figure 1: (V-GPS) We introduce Value-Guided Policy Steering (V-GPS), a novel approach that improves the performance of pre-trained generalist robotic policies by re-ranking their actions at deployment time based on a value function learned via offline RL. The same single V-GPS value function can be combined with any off-the-shelf generalist policy in a plug-and-play manner, without the need to fine-tune or access the policy's weights, improving downstream performance across multiple robotic platforms.", "description": "This figure illustrates the Value-Guided Policy Steering (V-GPS) method. It shows three stages: 1. Action Proposal, where a generalist robot policy proposes multiple actions (a1, a2,...ak) given the current observation (s) and a language instruction (l). 2. Re-Rank with Q-value, where these actions are re-ranked using a learned value function Qe(s,a,l), assigning higher scores to actions predicted to be better. 3. Execute High-Ranking Action, where the highest-ranking action is selected and executed by the robot. The value function is trained offline using reinforcement learning, and the V-GPS framework combines it with any generalist policy without needing fine-tuning or access to the policy's weights.", "section": "Test-time Policy Steering via Value Guidance"}, {"figure_path": "2410.13816/figures/figures_6_0.png", "caption": "Figure 3: (Experimental setup) We evaluate our method on 12 tasks in total. In the real-world WidowX robot platform, we study 6 tasks across 3 different scenes. In the SIMPLER simulated evaluation suite, we study 4 tasks on the WidowX platform and 2 tasks on the Google Robot.", "description": "This figure shows the experimental setup used to evaluate the proposed V-GPS method. It illustrates 12 different manipulation tasks performed across two robotic platforms: a real-world WidowX robot and a simulated SIMPLER environment. The real-world experiments involve 6 tasks distributed across three distinct scenes, each with its own set of objects and layouts.  The SIMPLER experiments also encompass 6 tasks but utilize two distinct robot models: the WidowX and Google robots. The tasks are visually represented with images for both real-world and simulated scenarios, showing the robot arms, environments and object placements.", "section": "6.1 Experimental Scenarios and Comparisons"}, {"figure_path": "2410.13816/figures/figures_7_0.png", "caption": "Figure 2: (Failures of Octo) Octo policy encounters failures such as imprecise grasping (first row), dropping the object prematurely (second row), and holding onto the object for too long (third row).", "description": "This figure shows three examples of failures of the Octo policy on three different tasks: \"put pepper in pot\", \"put mushroom on cloth\", and \"put sushi in pot\". Each row presents a sequence of images showing the robot's actions during a failed attempt. The first row shows failures due to imprecise grasping, where the robot fails to securely grasp the pepper. The second row demonstrates failures due to prematurely timed attempts to complete the task, with the mushroom being dropped before it reaches the target location. The third row displays an example of the robot holding onto the sushi for too long, causing it to be dropped outside of the target container.", "section": "Analysis: Failure Modes of Generalist Policies"}, {"figure_path": "2410.13816/figures/figures_15_0.png", "caption": "Figure 5: (Model Architecture.) Our value function uses a ResNet-34 image encoder with FiLM language conditioning.", "description": "This figure illustrates the architecture of the language-conditioned Q-function, Qe(s, a, l), used in the V-GPS method.  The architecture comprises three main components: a ResNet-34 image encoder which processes visual input (camera observation s), a frozen MUSE encoder which processes the language instruction (l), and FiLM layers that combine these visual and language features.  The combined features then pass through multiple MLP layers to output the Q-value, a scalar representing the expected future reward for taking action a in state s given instruction l.", "section": "5.1 Training: Learning Value Function via Offline RL Pre-training"}]