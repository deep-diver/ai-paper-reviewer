[{"page_end_idx": 2, "page_start_idx": 1, "section_number": 1, "section_title": "Introduction", "details": {"details": "The success of Large Language Models (LLMs) is heavily reliant on the availability of high-quality, extensive pre-training corpora.  While open-source datasets like The Pile (825GB) and Common Crawl have significantly advanced LLM development, the demand for pre-training data has now surpassed 10 trillion tokens, highlighting the need for both increased scale and improved data quality.  Current research trends emphasize both scaling data and improving its quality, with a shift from rule-based filtering to model-driven approaches in data curation. However, a significant gap exists in the availability of high-quality open-source datasets for Chinese language models, with existing datasets limited in scale and suffering from suboptimal data quality.  This lack of high-quality Chinese data presents a major barrier to the development of high-performing Chinese LLMs, making the creation of a robust, high-quality dataset a crucial area for research and development.", "first_cons": "The section primarily focuses on the problem of insufficient high-quality data for training Chinese LLMs, without offering concrete solutions within the introduction itself.", "first_pros": "The introduction effectively highlights the critical importance of high-quality data in LLM development, and emphasizes the significant gap in the availability of such data for Chinese language models.", "keypoints": ["The success of LLMs is directly tied to the availability of high-quality pre-training data.", "Demand for pre-training data has exceeded 10 trillion tokens.", "Existing open-source datasets, while helpful, are limited in scale (e.g., The Pile at 825GB) and quality.", "High-quality Chinese datasets are significantly underrepresented, hindering development of Chinese LLMs.", "Improving quality classification for Chinese web data is a major challenge requiring further research and development. "], "second_cons": "The section lacks specific examples of existing Chinese datasets and their shortcomings beyond general statements about limitations in scale and quality. More concrete examples would strengthen the argument.", "second_pros": "The introduction provides a clear and concise overview of the current landscape of LLM pre-training data, effectively establishing the context and motivation for the research presented in the paper.", "summary": "The success of Large Language Models (LLMs) depends heavily on the quality and quantity of training data. While progress has been made with open-source datasets in English, a significant gap exists in the availability of high-quality, large-scale Chinese datasets.  This deficit hampers the development of robust Chinese LLMs, highlighting the need for improved data curation techniques and the creation of new, high-quality datasets."}}, {"page_end_idx": 3, "page_start_idx": 2, "section_number": 2, "section_title": "Pipeline", "details": {"details": "The data processing pipeline consists of two main stages: Fundamental Processing and High-Quality Processing.  Fundamental Processing involves four steps: safety filtering (removing unsafe content), text extraction and cleaning, document-level de-duplication (using global MinHash to remove near-duplicates), and heuristic and basic quality filtering (using a classifier based on ChineseWebText to remove low-quality content).  This stage results in the CCI3.0 dataset.  The second stage, High-Quality Processing, refines the CCI3.0 dataset further.  This is done by distilling the quality scoring ability of the Qwen2-72B-Instruct model into a smaller 0.5B parameter model. This smaller model is trained on 140,000 high-quality samples (with a testing set of 14,000 samples) to create a quality classifier. The classifier then filters the CCI3.0 dataset, resulting in the final high-quality CCI3.0-HQ dataset.  This two-stage process is designed to significantly improve data quality for pre-training large language models, enhancing data integrity and ultimately LLM performance.  The High-Quality Processing uses model-driven approaches rather than earlier rule-based methods.", "first_cons": "The reliance on a large language model (Qwen2-72B-Instruct) in the High-Quality Processing stage might limit reproducibility for researchers who don't have access to such powerful models.  The cost of training and deploying the 0.5B quality classifier is also a consideration.", "first_pros": "The two-stage pipeline offers a systematic and robust approach to data cleaning, significantly improving the quality of the resulting dataset compared to simpler, single-stage methods.", "keypoints": ["Two-stage pipeline: Fundamental Processing followed by High-Quality Processing", "Fundamental Processing includes safety filtering, text extraction, deduplication, and basic quality filtering", "High-Quality Processing uses a 0.5B parameter model trained on 140k samples to filter CCI3.0, resulting in CCI3.0-HQ", "Model-driven approach for high-quality filtering using Qwen2-72B-Instruct", "Improved data quality leading to better LLM performance"], "second_cons": "The description lacks detail on the specific heuristics and methods used in both the Fundamental and High-Quality Processing stages.  More transparency on these methods would aid in reproducibility and allow for critical evaluation.", "second_pros": "The pipeline combines traditional data cleaning techniques with modern, model-driven approaches, showing a pragmatic and effective strategy for creating a high-quality dataset.  The use of the smaller 0.5B model for classification is particularly noteworthy, demonstrating an efficient approach to large-scale data filtering.", "summary": "This section details a two-stage data processing pipeline for creating a high-quality Chinese pre-training dataset (CCI3.0-HQ). The first stage, Fundamental Processing, uses standard methods to clean and filter the raw data. The second, High-Quality Processing, leverages a smaller model trained on high-quality samples to further refine the data, resulting in a significantly improved dataset for LLM training."}}, {"page_end_idx": 6, "page_start_idx": 3, "section_number": 3, "section_title": "Experiments", "details": {"details": "The experiment section evaluates the effectiveness of the CCI3.0-HQ dataset in pre-training large language models (LLMs).  Two main experiments were conducted: a mixed dataset experiment (60% English, 10% code, 30% Chinese) and a Chinese-only dataset experiment.  The Qwen2-0.5B model was trained from scratch on 100 billion tokens using different dataset compositions.  Results demonstrate that CCI3.0-HQ significantly outperforms competing datasets (SkyPile and Wanjuan-v1) across various evaluation metrics (ARC-C, ARC-E, HellaSwag, Winograd, MMLU, OpenbookQA, PIQA, SIQA, CEval, CMMLU). In the Chinese-only experiment, CCI3.0-HQ shows a considerable advantage, particularly in ARC-C (0.235), ARC-E (0.388), and CEval (0.331). The section also compares two high-quality annotation methods (FineWeb-edu and DCLM), concluding that FineWeb-edu is superior for Chinese corpora, and evaluates the performance of different quality classifiers, with CCI3.0-HQ classifier achieving superior F1-score (0.73) compared to others (FineWeb-edu, IndustryCorpus2, and ChineseWebText).  The analysis includes examination of the model's performance at various intermediate training checkpoints, showing consistent superiority of CCI3.0-HQ.", "first_cons": "The experiment section primarily focuses on the performance comparison of the CCI3.0-HQ dataset against existing datasets and lacks a deep analysis of the dataset itself.  The description of the datasets used is relatively brief and lacks many important specifics, such as the detailed quality filters employed.", "first_pros": "The experiment section provides a comprehensive evaluation of the CCI3.0-HQ dataset, using multiple benchmark datasets and metrics. The results clearly demonstrate the significant advantage of CCI3.0-HQ in pre-training LLMs, providing strong empirical evidence for its superior quality.", "keypoints": ["CCI3.0-HQ significantly outperforms SkyPile and Wanjuan-v1 across multiple metrics in both mixed and Chinese-only experiments.", "In the Chinese-only experiment, CCI3.0-HQ achieves notably higher scores in key metrics like ARC-C (0.235), ARC-E (0.388), and CEval (0.331).", "FineWeb-edu method is shown to be superior to DCLM for defining high-quality samples in Chinese language datasets.", "The CCI3.0-HQ classifier achieves the highest F1-score (0.73) among compared classifiers, indicating its effectiveness in identifying high-quality data.", "Performance improvements from CCI3.0-HQ are observed consistently at various training checkpoints, highlighting its sustained advantage throughout the training process"], "second_cons": "The experiments primarily use zero-shot settings, which might not fully capture the capabilities of LLMs trained on the dataset. Further research is needed to examine the model's performance in other settings, like few-shot or fine-tuning scenarios.", "second_pros": "The methodology of using both mixed and Chinese-only datasets allows for a more nuanced evaluation of CCI3.0-HQ's capabilities in different language contexts.   The inclusion of intermediate checkpoint analysis provides valuable insights into the training dynamics and the dataset's impact on model learning at various stages.", "summary": "The experiments section rigorously evaluates the CCI3.0-HQ dataset's effectiveness in LLM pre-training, comparing its performance against other datasets and annotation methods through two key experiments and multiple metrics. Results consistently demonstrate that CCI3.0-HQ, particularly the Chinese-specific experiment, significantly outperforms competitors, highlighting the effectiveness of its quality filtering and selection process. The analysis also validates the FineWeb-edu method for sample annotation and the superiority of the CCI3.0-HQ classifier in identifying high-quality data."}}]