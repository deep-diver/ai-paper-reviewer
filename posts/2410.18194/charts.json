[{"figure_path": "2410.18194/charts/charts_2_0.png", "caption": "Code Generation: ZIP-FIT accelerates cross-entropy loss reduction, even in code-specialized models like CodeGemma-2B. The plots show cross-entropy test loss versus the number of training tokens for Gemma2-2B (top row) and CodeGemma-2B (bottom row) across different token selection sizes. ZIP-FIT (blue) consistently reduces loss faster than DSIR (green) and D4 (red), achieving up to 85.11% speed improvement at lower token counts. These results demonstrate ZIP-FIT's efficiency in data selection for fine-tuning models on code-geneation tasks.", "description": "This chart presents a comparison of cross-entropy test loss reduction speeds for different language models (Gemma2-2B and CodeGemma-2B) using three data selection methods: ZIP-FIT, DSIR, and D4.  The x-axis represents the number of training tokens (in millions), and the y-axis shows the cross-entropy test loss.  Six subplots are displayed, three for each language model with varying training token sizes (800k, 930k, 1.6M for Gemma2-2B and 770k, 842k, 1M for CodeGemma-2B). Each subplot shows curves for the three data selection methods, demonstrating that ZIP-FIT consistently achieves lower cross-entropy loss faster than DSIR and D4 across all models and token counts. The percentage improvement of ZIP-FIT over DSIR in reaching the lowest cross-entropy loss is also indicated on each subplot, highlighting the efficiency gains of ZIP-FIT.", "section": "5.2 CODE GENERATION"}, {"figure_path": "2410.18194/charts/charts_3_0.png", "caption": "Figure 3: Higher ZIP-FIT alignment correlates with lower cross-entropy loss. The relationship between ZIP-FIT alignment and cross-entropy (CE) loss for (a) GPT-2 trained on 22k tokens (R2 = 0.90, p = 0.001) and (b) Mistral7B trained on 22k tokens (R2 = 0.75, p = 0.025). Each point represents a dataset, with its position reflecting the dataset's ZIP-FIT alignment score against the ProofNet test set and the resulting CE loss. The dashed red line indicates the linear regression fit, while the dashed grey line shows the pretrained CE loss. Higher alignment scores correspond to lower CE losses, demonstrating that training on better aligned data yields better performance.", "description": "This chart displays the correlation between ZIP-FIT alignment scores and cross-entropy loss for two language models, GPT-2 and Mistral7B, each trained on 22,000 tokens.  The x-axis represents the ZIP-FIT alignment score of various datasets against the ProofNet test set, while the y-axis shows the corresponding cross-entropy test loss. Each data point represents a dataset. A dashed red line shows the linear regression fit for each model, illustrating a strong negative correlation (R-squared of 0.90 for GPT-2 and 0.75 for Mistral7B, both statistically significant), indicating that higher ZIP-FIT alignment scores (better alignment with the target task) are associated with lower cross-entropy losses (better model performance). A dashed gray line represents the pre-trained cross-entropy loss for comparison.", "section": "3 HIGHER ALIGNMENT INTERVENTIONALLY ACHIEVES BETTER MODEL PERFORMANCE"}, {"figure_path": "2410.18194/charts/charts_3_1.png", "caption": "Figure 3: Higher ZIP-FIT alignment correlates with lower cross-entropy loss. The relationship between ZIP-FIT alignment and cross-entropy (CE) loss for (a) GPT-2 trained on 22k tokens (R2 = 0.90, p = 0.001) and (b) Mistral7B trained on 22k tokens (R2 = 0.75, p = 0.025). Each point represents a dataset, with its position reflecting the dataset's ZIP-FIT alignment score against the ProofNet test set and the resulting CE loss. The dashed red line indicates the linear regression fit, while the dashed grey line shows the pretrained CE loss. Higher alignment scores correspond to lower CE losses, demonstrating that training on better aligned data yields better performance.", "description": "This figure displays two scatter plots illustrating the correlation between ZIP-FIT alignment scores and cross-entropy loss for two different language models: GPT-2 and Mistral7B, each trained on 22,000 tokens.  The x-axis in both plots represents the ZIP-FIT alignment score of various datasets against the ProofNet test set, indicating how well-aligned each dataset is to the target task. The y-axis shows the corresponding cross-entropy loss achieved after training the respective model on each dataset. A dashed red line depicts the linear regression fit for each model, showcasing a strong negative correlation: higher alignment scores (better alignment to the target task) are associated with lower cross-entropy loss (better model performance). A dashed gray line represents the pretrained cross-entropy loss for each model, providing a baseline for comparison. The plot labels indicate the specific datasets used, and the R-squared and p-values demonstrate the statistical significance of the negative correlations.", "section": "3 HIGHER ALIGNMENT INTERVENTIONALLY ACHIEVES BETTER MODEL PERFORMANCE"}, {"figure_path": "2410.18194/charts/charts_5_0.png", "caption": "Figure 4: Highly aligned data lowers cross-entropy loss more efficiently. The x-axis shows the number of training tokens, and the y-axis represents the cross-entropy (CE) test loss. Different curves correspond to datasets filtered by different alignment scores, indicating their relevance to the target domain. The most aligned data reduce Test CE loss significantly faster than less aligned data. The left panel depicts results using GPT-2, and the right panel uses Mistral7B, demonstrating that using highly aligned data not only accelerates training but also achieves better model performance, validating the effectiveness of ZIP-FIT for data selection in fine-tuning.", "description": "This figure displays two line charts, one for GPT-2 and one for Mistral7B, illustrating the relationship between the number of training tokens and the cross-entropy test loss for datasets with varying alignment scores. Each chart shows multiple lines, each representing a different dataset filtered by a specific alignment threshold (Gzip Alignment).  The lines show that datasets with higher alignment scores (closer to 0.3) achieve a lower cross-entropy test loss more quickly than datasets with lower alignment scores (closer to 0.0).  The charts demonstrate that using highly aligned data leads to more efficient training and better model performance.", "section": "Higher Alignment Interventionally Achieves Better Model Performance"}, {"figure_path": "2410.18194/charts/charts_6_0.png", "caption": "Figure 5: AutoFormalization: ZIP-FIT consistently achieves lower test loss more quickly than D4 and DSIR, demonstrating its efficiency in data selection. The plots show cross-entropy test loss versus the number of training tokens for three models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) across different token selection sizes. ZIP-FIT (blue line) consistently outperforms both DSIR (green line) and D4 (red line) across all model and token size configurations, highlighting its ability to process data more efficiently. The percentage labels in each plot indicate the relative speedup of ZIP-FIT over DSIR in reaching the lowest cross-entropy loss, reinforcing the method's scalability and adaptability for domain-specific fine-tuning.", "description": "This chart displays the cross-entropy test loss against the number of training tokens for three different language models (InterLM-Math-Plus-1.8B, Gemma2-2B, and Mistral7B) using three different data selection methods (ZIP-FIT, DSIR, and D4).  The chart is divided into two rows, each representing a specific token selection size, with three columns, one for each language model. Each column shows a line graph plotting the cross-entropy test loss for the three data selection methods. The ZIP-FIT method consistently shows a lower test loss and faster convergence than DSIR and D4, demonstrating its greater efficiency in data selection. Percentage labels are included to quantify the speedup achieved by ZIP-FIT in comparison to DSIR.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_8_0.png", "caption": "Figure 6: Selective data filtering with ZIP-FIT allows us to achieve better cross-entropy test loss faster than training on all the data, resulting in improved performance and efficiency. The x-axis represents the number of training tokens, while the y-axis shows the cross-entropy test loss. The curves represent models fine-tuned (FT) on datasets filtered by varying alignment thresholds (>0.1, >0.2, >0.3). The dashed line indicates the baseline performance of the pretrained Mistral7B model. Training on data filtered with higher alignment thresholds leads to superior performance, demonstrating the effectiveness of removing misaligned data in fine-tuning.", "description": "This chart displays the relationship between the number of training tokens and the cross-entropy test loss for the Mistral7B model, trained on datasets filtered by different ZIP-FIT alignment thresholds: >0.1, >0.2, and >0.3.  The x-axis shows the number of training tokens, while the y-axis represents the cross-entropy test loss. Three lines depict the performance of the model trained on datasets with different alignment scores, showing a clear trend where higher alignment thresholds (more aligned data) lead to lower cross-entropy loss and faster convergence. A dashed horizontal line indicates the baseline cross-entropy loss of the pretrained Mistral7B model.  A color bar on the right side of the chart shows a gradient corresponding to the ZIP-FIT alignment scores.", "section": "6 IMPACT OF DATA MISALIGNMENT ON MODEL PERFORMANCE"}, {"figure_path": "2410.18194/charts/charts_17_0.png", "caption": "InternLM-Math-Plus-1.8B - Top 49k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the InternLM-Math-Plus-1.8B model, utilizing 49,000 tokens.  Four lines represent different data selection methods:  a dotted grey line shows the pre-trained model's loss, a green line represents DSIR, a blue line represents ZIP-FIT, and a red line represents D4.  ZIP-FIT consistently shows a lower cross-entropy test loss than DSIR and D4 with the note that it is 50.0% faster.", "section": "C ADDITIONAL EXPERIMENTAL RESULTS: DATA SELECTION FOR EFFICIENT FINE-TUNING USING ZIP-FIT"}, {"figure_path": "2410.18194/charts/charts_17_1.png", "caption": "InternLM-Math-Plus-1.8B - Top 81k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the InternLM-Math-Plus-1.8B model with 81k tokens.  Four lines represent different data selection methods: pre-trained model (dotted grey), DSIR (green), ZIP-FIT (blue), and D4 (red).  ZIP-FIT demonstrates faster convergence, achieving a lower cross-entropy loss than other methods,  reaching its lowest point approximately 50% faster than DSIR. All lines initially show a steep decline in cross-entropy loss, which levels off as the number of training tokens increases.  The pre-trained model's loss is shown as a flat horizontal line representing the baseline performance.", "section": "C ADDITIONAL EXPERIMENTAL RESULTS: DATA SELECTION FOR EFFICIENT FINE-TUNING USING ZIP-FIT"}, {"figure_path": "2410.18194/charts/charts_17_2.png", "caption": "Gemma2-2B - Top 42k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the Gemma2-2B model with 42k tokens.  It compares four different data selection methods: a pre-trained Gemma2-2B model (baseline), DSIR, ZIP-FIT, and D4.  The x-axis represents the number of training tokens, and the y-axis shows the cross-entropy test loss.  The lines for each method show how the loss decreases as the number of training tokens increases.  ZIP-FIT shows a faster convergence rate than the other methods, reaching a lower loss with fewer training tokens.  Specifically, the chart highlights that ZIP-FIT is 50% faster than DSIR in reducing the loss.", "section": "Additional Experimental Results: Data Selection for Efficient Fine-Tuning Using ZIP-FIT"}, {"figure_path": "2410.18194/charts/charts_17_3.png", "caption": "Mistral7B - Top 45k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the Mistral7B model.  Four lines represent different data selection methods: pre-trained Mistral7B (baseline), DSIR, ZIP-FIT, and D4.  The chart shows that ZIP-FIT achieves a lower cross-entropy test loss than DSIR and D4, and significantly faster convergence to a lower loss, as indicated by the steeper slope of the ZIP-FIT line. Specifically, ZIP-FIT is reported as 50% faster than DSIR in reaching a similar level of cross-entropy loss.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_4.png", "caption": "Gemma2-2B - Top 77k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the Gemma2-2B model during fine-tuning.  Four lines represent different data selection methods: a pre-trained Gemma2-2B baseline (grey, dashed), DSIR (green), ZIP-FIT (blue), and D4 (red).  The chart shows that ZIP-FIT consistently achieves lower cross-entropy loss than DSIR and D4 across all token counts, indicating more efficient fine-tuning.  Specifically, ZIP-FIT reaches a comparable loss to DSIR with 42.9% fewer tokens, highlighting its superiority in data selection for model training.", "section": "5.2 Code Generation"}, {"figure_path": "2410.18194/charts/charts_17_5.png", "caption": "Mistral7B - Top 83k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the Mistral7B model, comparing four data selection methods: ZIP-FIT, DSIR, D4, and a pre-trained baseline.  The x-axis represents the number of training tokens, and the y-axis shows the cross-entropy test loss.  All four methods start at approximately the same cross-entropy test loss at 0k tokens.  The ZIP-FIT method consistently demonstrates the lowest cross-entropy test loss across all token counts.  Importantly, ZIP-FIT achieves this lowest loss substantially faster (42.9% faster than DSIR) than the other data selection methods, showing a steeper decline in cross-entropy as training tokens increase.  DSIR and D4 show a slower decline, with D4 initially performing comparably to ZIP-FIT but eventually levelling off at a significantly higher cross-entropy test loss.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_17_6.png", "caption": "InternLM-Math-Plus-1.8B - Top 111k Tokens", "description": "This chart displays the cross-entropy test loss versus the number of training tokens for the InternLM-Math-Plus-1.8B model with 111k tokens.  Four lines represent different data selection methods: Pre-trained InternLM-Math-Plus-1.8B (a dotted horizontal line indicating the baseline performance before fine-tuning), DSIR (green), ZIP-FIT (blue), and D4 (red).  ZIP-FIT consistently achieves a lower cross-entropy loss than DSIR and D4 at a faster rate, demonstrating its efficiency in data selection.  The chart highlights that ZIP-FIT reaches a significantly lower loss than other methods, with a speed improvement of 64.3% noted in comparison to DSIR.", "section": "ADDITIONAL EXPERIMENTAL RESULTS: DATA SELECTION FOR EFFICIENT FINE-TUNING USING ZIP-FIT"}, {"figure_path": "2410.18194/charts/charts_17_7.png", "caption": "Gemma2-2B - Top 106k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the Gemma2-2B model.  Four lines represent different data selection methods: the pre-trained Gemma2-2B model (dotted grey line) serves as the baseline, while DSIR (green), ZIP-FIT (blue), and D4 (red) show the performance after fine-tuning with data selected by each respective method.  ZIP-FIT demonstrates a significantly faster decrease in cross-entropy loss compared to the other methods, achieving the lowest loss at 100k tokens; specifically, it's 22.2% faster than DSIR.  D4 shows the highest loss overall.", "section": "5.2 CODE GENERATION"}, {"figure_path": "2410.18194/charts/charts_17_8.png", "caption": "Mistral7B - Top 115k Tokens", "description": "The chart displays the cross-entropy test loss versus the number of training tokens for the Mistral7B model.  Four lines represent different data selection methods:  Pre-trained Mistral7B (a horizontal dashed line showing the initial loss before fine-tuning), DSIR (green), ZIP-FIT (blue), and D4 (red).  The chart illustrates that ZIP-FIT achieves a lower cross-entropy test loss significantly faster than DSIR and D4, reaching the lowest loss 60% faster than DSIR, as indicated by the purple text. The x-axis represents the number of training tokens used, and the y-axis represents the cross-entropy test loss.", "section": "5.1 Autoformalization"}, {"figure_path": "2410.18194/charts/charts_18_0.png", "caption": "Figure 8: ZIP-FIP demonstrates lower cross-entropy and lower run time during data selection than competing DSIR and D4 methods. ZIP-FIT is cheaper, faster, and better performing. The run times do no include fine-tuning time, since it's a constant offset across all models. D4's data selection (not shown) takes 5hs because it uses an embedding model (opt-125m Zhang et al. (2022)), the same one as the original paper Tirumala et al. (2023).", "description": "The scatter plot displays the relationship between data selection time (in seconds) and cross-entropy test loss for different models and data selection methods (ZIP-FIT and DSIR).  The x-axis represents the time taken for data selection, and the y-axis represents the cross-entropy test loss achieved after fine-tuning.  Each point in the chart represents a specific model (Gemma2-2B and CodeGemma-2B), data selection method (ZIP-FIT or DSIR), and the number of training tokens used (800k, 930k, or 1M). The chart visually demonstrates that ZIP-FIT consistently achieves a lower cross-entropy test loss in less time compared to DSIR across various model and data size configurations.", "section": "D DATA SELECTION PROFILING (RUN TIMES)"}]