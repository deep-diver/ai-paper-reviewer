{"references": [{" publication_date": "2020", "fullname_first_author": "Tom B. Brown", "paper_title": "Language models are few-shot learners", "reason": "This paper is foundational in the field of large language models (LLMs), introducing the concept of few-shot learning, which is directly relevant to the data selection problem addressed in the current paper.  The ability of LLMs to learn effectively from a small number of examples underlines the importance of selecting high-quality data, a central theme of the current work.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Suchin Gururangan", "paper_title": "Don't stop pretraining: Adapt language models to domains and tasks", "reason": "This paper addresses the challenge of adapting pre-trained language models to specific domains and tasks, directly aligning with the goal of the current paper. It highlights the limitations of relying solely on general-purpose pre-training, emphasizing the need for domain-specific fine-tuning and data selection techniques.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Jordan Hoffmann", "paper_title": "Training compute-optimal large language models", "reason": "This paper focuses on optimizing the computational efficiency of training large language models, which is highly relevant to the current work's focus on efficient data selection.  By improving training efficiency through data selection, ZIP-FIT contributes to the overall goal of reducing the computational resources needed for training LLMs.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Amro Abbas", "paper_title": "Semdedup: Data-efficient learning at web-scale through semantic deduplication", "reason": "SemDeDup directly addresses data efficiency by removing semantically similar examples, a relevant concern when dealing with large datasets.  This paper's focus on deduplication complements the current paper's goal of selecting the most relevant data for improved training efficiency.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Kushal Tirumala", "paper_title": "D4: Improving llm pretraining via document de-duplication and diversification", "reason": "D4, like SemDeDup, focuses on enhancing the efficiency of LLM training. By removing redundant and diversifying the data, D4's approach addresses similar challenges to those tackled by ZIP-FIT, with both aiming to improve model performance through targeted data selection.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Sang Michael Xie", "paper_title": "Data selection via importance resampling", "reason": "DSIR directly addresses the problem of data selection, providing a baseline method against which ZIP-FIT's performance is compared.  The authors' detailed comparison with DSIR highlights ZIP-FIT's superior efficiency and performance improvements.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Gr\u00e9goire Del\u00e9tang", "paper_title": "Language modeling is compression", "reason": "This paper establishes a connection between language modeling and compression, providing a theoretical foundation for ZIP-FIT's use of gzip compression as a metric for data alignment. This theoretical link supports the rationale behind ZIP-FIT's algorithm and strengthens its methodological foundation.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhiying Jiang", "paper_title": "\u201clow-resource", "reason": "This paper explores the challenges of text classification with limited data resources. Its focus on low-resource scenarios directly aligns with the motivation of ZIP-FIT, which aims to improve model training efficiency by selecting only the most relevant data points.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Leonardo De Moura", "paper_title": "The lean theorem prover (system description)", "reason": "This paper describes the Lean theorem prover, a formal mathematical language used in the Autoformalization task.  Understanding the intricacies of Lean is crucial for evaluating the effectiveness of data selection in this specific domain, as used in the experimental section of the current paper.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Alec Radford", "paper_title": "Language models are unsupervised multitask learners", "reason": "The GPT-2 model, used in the experiments of this paper, is based on this foundational paper on language models.  Understanding GPT-2's capabilities and its strengths and weaknesses is crucial for interpreting the results.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "Mistral-7B is one of the models used in the experimental evaluation, demonstrating the generalizability of the ZIP-FIT method across various model architectures.  The paper describing Mistral-7B is important for providing the necessary context and details of the model used in the experiments.", "section_number": 3}, {" publication_date": "2023", "fullname_first_author": "Jacob Austin", "paper_title": "Program synthesis with large language models", "reason": "The concept of program synthesis, a crucial aspect of code generation, is addressed in this paper, providing background and context for evaluating the effectiveness of ZIP-FIT in the code generation task.  The connection between program synthesis and LLM training underscores the relevance of this citation.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Gemma Team", "paper_title": "Gemma 2: Improving open language models at a practical size", "reason": "Gemma2-2B is one of the models used in the experiments, its performance showcasing the effectiveness of ZIP-FIT in different models. Understanding Gemma2-2B's architecture and its capabilities is vital to interpreting the empirical findings.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Pablo Villalobos", "paper_title": "Will we run out of data? limits of Ilm scaling based on human-generated data", "reason": "This paper discusses the limitations of scaling LLMs based solely on human-generated data. The paper directly connects to the core problem of the current paper by highlighting the scarcity of high-quality data and the need for efficient data selection methods.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Joshua Kazdan", "paper_title": "Collapse or thrive? perils and promises of synthetic data in a self-generating world", "reason": "This paper explores the challenges and potential benefits of using synthetic data for training LLMs, a related area of research. Exploring synthetic data generation is suggested as future work in the current paper, hence the relevance of this citation.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Brando Miranda", "paper_title": "Beyond scale: The diversity coefficient as a data quality metric for variability in natural language data", "reason": "This paper introduces the concept of data diversity as a crucial metric for data quality. The current paper implicitly addresses the importance of data diversity, but this citation provides additional context and a theoretical framework for understanding this dimension of data quality.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Sang Michael Xie", "paper_title": "Doremi: Optimizing data mixtures speeds up language model pretraining", "reason": "DoReMi, another data selection method, is discussed and contrasted with ZIP-FIT.  By comparing ZIP-FIT to DoReMi, the authors highlight the specific advantages of their proposed approach in terms of efficiency and task-specificity.", "section_number": 7}, {" publication_date": "2024", "fullname_first_author": "Alexander Wettig", "paper_title": "Qurating: Selecting high-quality data for training language models", "reason": "This paper focuses on selecting high-quality data for LLM training, which is the same primary goal of ZIP-FIT. The comparison with this work further substantiates the novelty and contributions of the ZIP-FIT method by highlighting the differences in approach and performance.", "section_number": 7}, {" publication_date": "2014", "fullname_first_author": "Steven T. Piantadosi", "paper_title": "Zipf's word frequency law in natural language: a critical review and future directions", "reason": "Zipf's law, mentioned in the paper's name rationale, provides a theoretical framework for understanding the relationship between word frequency and rank in natural language.  This provides linguistic grounding for the observation of scaling behavior that the ZIP-FIT method exhibits.", "section_number": 9}]}