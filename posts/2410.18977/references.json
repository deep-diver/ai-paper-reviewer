{"references": [{" publication_date": "2022", "fullname_first_author": "Guy Tevet", "paper_title": "Human motion diffusion model", "reason": "This paper introduces a novel diffusion-based model for human motion generation, which is highly relevant to the current research's focus on improving the quality and diversity of generated motions. The use of diffusion models has become increasingly popular in recent years, and this paper's contributions are directly applicable to the task of fine-grained motion editing.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Chuan Guo", "paper_title": "Generating diverse and natural 3d human motions from text", "reason": "This paper addresses the challenge of generating diverse and natural human motions from text descriptions, a core problem in the current research's domain. Its success in generating high-quality motions serves as a strong baseline for comparison and establishes the importance of addressing diversity in motion generation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Damai Dai", "paper_title": "Motionlcm: Real-time controllable motion generation via latent consistency model", "reason": "This paper presents a method for generating controllable and high-quality human motions, which directly relates to the objective of the current work, which aims to enable fine-grained control over the motion generation process during editing. The real-time aspect is also valuable in the context of interactive motion editing applications.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Nikos Athanasiou", "paper_title": "MotionFix: Text-driven 3d human motion editing", "reason": "This work tackles the problem of text-driven human motion editing. Its focus on enabling fine-grained edits to motion sequences, directly addresses one of the main limitations highlighted in the current research's introduction, namely the need for more effective and interactive motion editing capabilities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Rishabh Dabral", "paper_title": "Mofusion: A framework for denoising-diffusion-based motion synthesis", "reason": "This paper explores the use of diffusion models for motion synthesis.  Diffusion-based models have shown significant promise in motion generation, and understanding this framework provides a strong foundation for the current research's efforts in creating an improved attention-based diffusion model for training-free motion editing.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This foundational paper introduces the Transformer architecture and its attention mechanism, which are crucial to the current research's development of the MotionCLR model. The Transformer architecture and its attention mechanism provide the foundation for many modern sequence-to-sequence models, and understanding this work is essential for appreciating the advancements in MotionCLR.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Hyemin Ahn", "paper_title": "Text2action: Generative adversarial synthesis from language to action", "reason": "This paper presents an early approach to generating human actions from text descriptions. Its exploration of this problem area provides valuable context for the current research, which aims to address the limitations of existing methods by introducing a novel attention-based model.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Kfir Aberman", "paper_title": "Skeleton-aware networks for deep motion retargeting", "reason": "This work provides a strong foundation in motion retargeting techniques which is relevant to the current research's goal of providing versatile motion editing capabilities.  Understanding the challenges and approaches used in motion retargeting is beneficial for designing effective methods for motion editing.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Chuan Guo", "paper_title": "Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts", "reason": "This paper addresses the challenge of generating human motions and texts reciprocally. The reciprocal generation aspect provides valuable context for the current research, which aims to improve the correspondence between text and motion and to use this correspondence for interactive motion editing.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kehong Gong", "paper_title": "Tm2d: Bimodality driven 3d dance generation via music-text integration", "reason": "This paper tackles the problem of multi-modal motion generation by integrating music and text, providing valuable insight into how different modalities can be integrated to control motion generation. The integration of multiple modalities is relevant to the current research which aims to manipulate attention maps to control the editing process.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wenxun Dai", "paper_title": "Motionlcm: Real-time controllable motion generation via latent consistency model", "reason": "This paper is highly relevant to the current research due to its focus on real-time controllable motion generation. The real-time generation aspect is crucial for interactive applications, and the techniques used in this paper could potentially be adapted for improving the speed and efficiency of the MotionCLR editing process.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Nikos Athanasiou", "paper_title": "MotionFix: Text-driven 3d human motion editing", "reason": "This work focuses on text-driven human motion editing. The ability to control and edit specific aspects of the motion based on text commands is highly relevant to the research presented in this paper, and understanding the techniques and challenges in this area is essential for improving the fine-grained editing capabilities.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Purvi Goel", "paper_title": "Iterative motion editing with natural language", "reason": "This paper is highly relevant to this paper's focus on interactive motion editing capabilities. The use of natural language for motion editing is a key area of research, and understanding the challenges and successes in this area is essential for improving the user experience and controllability of motion editing tools.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Mingyuan Zhang", "paper_title": "Remodiffuse: Retrieval-augmented motion diffusion model", "reason": "This paper introduces a novel approach to motion generation that combines diffusion models with retrieval-based methods. The use of retrieval methods is highly relevant to the current research's focus on improving the efficiency and effectiveness of the motion editing process, by enabling rapid access to relevant motion data.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Chongyang Zhong", "paper_title": "Attt2m: Text-driven human motion generation with multi-perspective attention mechanism", "reason": "This paper explores the use of multi-perspective attention mechanisms for human motion generation.  The use of multiple perspectives for attention is relevant to the current research's development of the MotionCLR model, which also employs self-attention and cross-attention mechanisms for improving the controllability of the motion generation process.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Matthias Plappert", "paper_title": "Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks", "reason": "This paper explores the use of recurrent neural networks for mapping between human whole-body motion and natural language.  The exploration of this problem area is highly relevant to the current research's focus on improving the mapping between text and motion for more effective and interactive motion editing.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Shunlin Lu", "paper_title": "Humantomato: Text-aligned whole-body motion generation", "reason": "This work presents a model that addresses the challenge of text-aligned human motion generation. The emphasis on generating text-aligned motions is highly relevant to the current research's focus on improving the correspondence between text and motion data for more effective and interactive motion editing.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Guy Tevet", "paper_title": "Motionclip: Exposing human motion generation to clip space", "reason": "This paper introduces a method for exposing human motion generation to CLIP space, providing a novel way to integrate text information into motion generation.  This approach is highly relevant to the current research's goal of improving the correspondence between text and motion for more effective and interactive motion editing.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Mingyuan Zhang", "paper_title": "Motiondiffuse: Text-driven human motion generation with diffusion model", "reason": "This paper introduces a diffusion-based model for text-driven human motion generation.  The use of diffusion models has become increasingly popular in recent years, and this paper's contributions are directly applicable to the task of fine-grained motion editing.  Understanding the challenges and successes in diffusion-based motion generation is essential for improving the MotionCLR model.", "section_number": 2}, {" publication_date": "2015", "fullname_first_author": "Olaf Ronneberger", "paper_title": "U-net: Convolutional networks for biomedical image segmentation", "reason": "This paper introduces the U-Net architecture, which is a fundamental building block for many modern image segmentation tasks and has also been applied to motion generation.  The U-Net architecture provides a strong foundation for the current research's development of the MotionCLR model, which is also a U-Net-like architecture, composed of CLR blocks.", "section_number": 3}]}