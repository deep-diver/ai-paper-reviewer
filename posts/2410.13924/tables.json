[{"figure_path": "2410.13924/tables/table_1_0.md", "caption": "Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories.", "description": "This table presents the number of training, validation, and test samples for several datasets used in the paper's experiments, including S3DIS, ScanNet, ScanNet200, ScanNet++, Structured3D, and the ARKit LabelMaker dataset.  For each dataset, it shows the number of samples used for training, validation, and testing, along with the number of real-world scenes and the number of labels used. The ARKit LabelMaker dataset stands out as the largest real-world dataset with dense semantic annotations (4471 training, 548 validation).", "section": "4.2. Datasets and Metrics for Evaluation"}, {"figure_path": "2410.13924/tables/table_5_0.md", "caption": "Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories.", "description": "This table presents the number of training, validation, and test samples for various datasets used in the paper's experiments, including S3DIS, ScanNet, ScanNet200, ScanNet++, ARKit LabelMaker, and Structured3D.  It also indicates whether the data is real (V) or synthetic (X) and the number of semantic labels used in each dataset.  The table highlights the significantly larger size of the ARKit LabelMaker dataset, emphasizing its contribution of providing automatically generated dense semantic annotations compared to previous datasets. The table serves to quantify and compare the scale of different datasets used for training and evaluating 3D semantic segmentation models.", "section": "4.2. Datasets and Metrics for Evaluation"}, {"figure_path": "2410.13924/tables/table_6_0.md", "caption": "Table 2. Semantic Segmentation Scores on ScanNet20. We compare different training strategies for two top-performing models (PointTransformerv3 [36] and MinkowskiNet [7]) on the ScanNet20 dataset. We can show for both models adding ALS200 through pre-training and co-training improves the performance for both models. With PonderV2 [42] and Mix3D [20], we compare large-scale pretraining to two other training strategies. We can show that large-scale pre-training is superior to both, extensive data augmentation (Mix3D) and self-supervised pre-training (PonderV2).", "description": "This table presents the results of semantic segmentation on the ScanNet20 dataset using two different models: Point Transformer V3 and MinkowskiNet.  It compares several training approaches: vanilla training (using only ScanNet data), pre-training on the ALS200 dataset (automatically generated labels from ARKitScenes) followed by fine-tuning on ScanNet20, co-training with ALS200 and ScanNet200, self-supervised pre-training using PonderV2, and extensive data augmentation using Mix3D.  The table shows the validation and test mean Intersection over Union (mIoU) scores for each training method and model, demonstrating that large-scale pre-training with automatically generated labels significantly outperforms other approaches.", "section": "4. Results"}, {"figure_path": "2410.13924/tables/table_6_1.md", "caption": "Table 3. Semantic Segmentation Scores on ScanNet200 [29].", "description": "This table presents the results of semantic segmentation experiments on the ScanNet200 dataset.  It compares the performance of different training strategies for two leading models: MinkowskiNet [7] and PointTransformerV3 [36].  The strategies evaluated include vanilla training on ScanNet200, fine-tuning models pretrained on the ALS200 and ALC datasets (generated by the authors' method), and a co-training approach combining ALS200 with ScanNet200.  For PointTransformerV3, results are also shown for a joint training approach using ScanNet200, S3DIS, and Structured3D, along with the authors' approach incorporating their ALC dataset.  The metrics used are validation and test mean Intersection over Union (mIoU).", "section": "4.2. Datasets and Metrics for Evaluation"}, {"figure_path": "2410.13924/tables/table_6_2.md", "caption": "Table 4. Semantic Segmentation Scores on ScanNet++ [39]. We evaluated the efficacy of our ALC dataset on the ScanNet++ benchmark using both pre-training and joint training methods. \u2020: this number comes from Wu et al.", "description": "This table presents the results of semantic segmentation experiments on the ScanNet++ benchmark dataset.  It compares the performance of several methods, including vanilla training, pre-training with the ARKitScenes LabelMaker dataset (ALC200), and joint training that incorporates ALC200 with other datasets (ScanNet200, ScanNet++, Structure3D). The table shows the validation and test mIoU (mean Intersection over Union) scores and the top-1/3 mIoU, indicating the performance across different training strategies and datasets.  The number of data points used for each training scenario is also specified.  The results demonstrate the impact of pre-training with the ARKitScenes LabelMaker dataset on improving the semantic segmentation performance on the ScanNet++ benchmark.", "section": "4. Results"}, {"figure_path": "2410.13924/tables/table_8_0.md", "caption": "Table B1. ScanNet200 validation and test mIoU for head, common and tail classes. For MinkowskiNet, ARKit LabelMaker pre-trained network shows significant improvement on head and common classes. For PTv3, we see improvements across all three splits.", "description": "This table presents a performance comparison of different training strategies on the ScanNet200 dataset, focusing on head, common, and tail classes.  It compares the performance of the vanilla MinkowskiNet and Point Transformer V3 (PTv3) models against versions pre-trained on the ARKit LabelMaker dataset (ALS200) and a co-training approach combining ALS200 with ScanNet200.  The results are shown in terms of mean Intersection over Union (mIoU) for both validation and test sets, illustrating the improvements achieved by leveraging the ARKit LabelMaker data for pre-training, particularly noticeable in head and common classes for MinkowskiNet and across all classes for PTv3.", "section": "B. Head, common and tail split mIoU scores for ScanNet200"}, {"figure_path": "2410.13924/tables/table_9_0.md", "caption": "Table 1. The size of dataset that is used for training and evaluation in this work. We provide by far the largest real-world labeled training dataset compared to existing real-world datasets. We provide automatically generated dense semantic annotations for 4471 training trajectories and 548 validation trajectories.", "description": "This table presents the sizes of various datasets used for training and evaluation in the paper, including the number of training, validation, and test samples, as well as the number of real-world and labeled samples.  It compares the ARKit LabelMaker dataset (the authors' contribution) to other established datasets such as S3DIS, ScanNet, ScanNet200, ScanNet++, and Structured3D, highlighting that ARKit LabelMaker offers a significantly larger number of labeled samples than any of the other datasets listed.", "section": "4.2. Datasets and Metrics for Evaluation"}]