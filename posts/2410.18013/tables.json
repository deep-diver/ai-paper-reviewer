[{"figure_path": "2410.18013/tables/table_7_0.md", "caption": "Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "The table presents a quantitative comparison of different models' performance on the GenEval benchmark, focusing on several attributes such as single object recognition, two-object recognition, counting, color distinction, positional accuracy and color attribution.  The models compared include various versions of DALL-E, PixArt, SDXL, and SD3-Medium. For each model, the table shows the mean score across all GenEval categories and individual scores for each of the six attributes. The results demonstrate that RankDPO consistently improves the overall performance and particularly excels in \"two objects\", \"counting\", and \"color attribution\" categories for both SDXL and SD3-Medium models.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_8_0.md", "caption": "Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs.", "description": "This table presents a quantitative comparison of different models' performance on the DPG-Bench benchmark, which evaluates visual quality and prompt alignment using long and detailed prompts.  The benchmark employs three metrics: DSG score (Cho et al., 2024), VQAScore (Lin et al., 2024), and Q-Align score (Wu et al., 2024a). DSG and VQAScore assess prompt following using VQA models, while Q-Align evaluates visual quality with multimodal LLMs. The table lists the scores achieved by several models, including SDXL, DPO-SDXL (Diffusion-DPO trained on Pick-a-Picv2), MaPO-SDXL, SPO-SDXL, and the proposed RankDPO model, for both SDXL and SD3-Medium.  The results showcase RankDPO's superior performance across all three metrics compared to other approaches, highlighting its effectiveness in improving both prompt alignment and visual quality.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_8_1.md", "caption": "Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs.", "description": "This table presents a quantitative comparison of different models on the DPG-Bench benchmark, focusing on prompt alignment and visual quality.  It shows the scores achieved by several models including SDXL, DPO-SDXL, MaPO-SDXL, SPO-SDXL, and the proposed RankDPO, both for SDXL and SD3-Medium. The evaluation metrics used are DSG score, VQA score, and Q-Align score, representing different aspects of prompt following and visual quality.  The table illustrates the improvements obtained by RankDPO compared to other methods and baselines.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_8_2.md", "caption": "Table 3: Quantitative results on DPG-Bench. DSG (Cho et al., 2024) and VQAScore (Lin et al., 2024) measure prompt following using VQA models while Q-Align (Wu et al., 2024a) measures visual quality using multimodal LLMs.", "description": "This table presents a quantitative comparison of different models on the DPG-Bench benchmark, focusing on prompt alignment and visual quality.  It shows the performance of several models, including SDXL (baseline), SDXL after applying Direct Preference Optimization (DPO),  SDXL fine-tuned with MaPO and SPO, and the proposed RankDPO method applied to both SDXL and SD3-Medium.  The results for each model are displayed using three metrics: DSG score (measuring prompt alignment), VQAScore (prompt alignment), and Q-Align score (assessing visual quality).  The table highlights the superior performance of RankDPO on both SDXL and SD3-Medium, achieving state-of-the-art results in prompt alignment and visual quality.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_15_0.md", "caption": "Table 6: Comparison of T2I-Compbench Dataset with DPG-Bench, including model attributes, training time, and inference time increases.", "description": "This table compares the performance of three different models\u2014SDXL, ELLA (SDXL), and RankDPO (SDXL)\u2014on the T2I-Compbench and DPG-Bench datasets.  For each model, it presents scores across five image quality attributes: Color, Shape, Texture, Spatial, and Non-Spatial.  It also lists the DPG-Bench score (a measure of overall image quality), the training time (in A100 GPU days), the size of the training dataset (in millions of parameters), and whether or not the inference time was comparable across these different methods.  The table shows that RankDPO achieves comparable performance to ELLA with significantly less training time and a smaller dataset, offering a more efficient approach to training.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_15_1.md", "caption": "Table 7: Comparing features of our proposal against baselines that aim to improve T2I model quality post-training. ELLA* also replaces the CLIP text-encoders with T5-XL text-encoder and a 470M parameter adapter applied at each timestep, thereby increasing the inference cost.", "description": "This table compares the proposed RankDPO method with other baselines for improving text-to-image (T2I) model quality.  It presents a comparison of the training data size (in millions of images), training time (in A100 GPU days), inference cost (indicated as equal or excessive compared to the baseline), and the resulting DPG-Bench score.  The table shows that RankDPO achieves a comparable or better DPG-Bench score to other methods while using significantly less training data and significantly less training time. The table also notes that using ELLA* results in a much higher inference cost due to use of an additional adapter and larger text encoder.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_16_0.md", "caption": "Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "This table presents a quantitative comparison of different models' performance on the GenEval benchmark.  It shows the mean performance scores across several attribute categories (Single, Two, Counting, Colors, Position, Color Attribution) for various models including SD v2.1, PixArt-a, PixArt-\u03a3, DALL-E 2, DALL-E 3, SDXL, and SD3-Medium.  For each model, both baseline results and results after applying the proposed RankDPO method are reported, enabling direct comparison of performance improvements achieved by RankDPO.  The table highlights improvements in multiple categories, particularly in \"two objects\", \"counting\", and \"color attribution\", indicating the effectiveness of RankDPO in enhancing the text-to-image generation models' performance.", "section": "4.1 Comparison Results"}, {"figure_path": "2410.18013/tables/table_18_0.md", "caption": "Table 1: Quantitative Results on GenEval. RankDPO improves results on most categories, notably \"two objects\", \"counting\", and \"color attribution\" for SDXL and SD3-Medium.", "description": "Table 1 presents a quantitative comparison of different models on the GenEval benchmark, focusing on their performance across various attributes such as single object generation, two object generation, counting, color, position and color attribution.  The table lists seven models: SD v2.1, PixArt-a, PixArt-\u03a3, DALL-E 2, DALL-E 3, SDXL, and SDXL (Ours), with the last being the model enhanced using the proposed RankDPO method. For each model, the mean performance and individual scores for each attribute are provided.  These scores likely represent improvements over a baseline, although that baseline is not explicitly defined within the table itself. The table highlights that RankDPO consistently improves performance across most categories of the GenEval benchmark.", "section": "4.1 Comparison Results"}]