[{"figure_path": "2410.18013/figures/figures_5_0.png", "caption": "Figure 2: Overview of our two novel components: (A) Syn-Pic and (B) RankDPO. Left illustrates the pipeline to generate a synthetically ranked preference dataset. It starts by collecting prompts and generating images using the same prompt for different T2I models. Next, we calculate the overall preference score using Reward models (e.g., PickScore, ImageReward). Finally, we rank these images in the decreasing order of preference scores. Right: Given true preference rankings for generated images per prompt, we first obtain predicted ranking by current model checkpoint using scores si (see Eq. 5). In this instance, although the predicted ranking is inverse of the true rankings, the ranks (1, 4) obtains a larger penalty than the ranks (2, 3). This penalty is added to our ranking loss through DCG weights (see Eq. 6). Thus, by optimizing \u03b8 with Ranking Loss (see Eq. 7), the updated model addresses the incorrect rankings (1,4). This procedure is repeated over the training process, where the rankings induced by the model aligns with the labelled preferences.", "description": "The figure is divided into two parts (A) and (B), illustrating the two main contributions of the paper: Syn-Pic dataset and RankDPO optimization method. Part (A) shows the process of creating Syn-Pic: prompts are inputted into multiple text-to-image (T2I) models, generating multiple images per prompt; these are then scored using multiple reward models, and finally ranked according to the aggregate scores. Part (B) shows the RankDPO training process: given the target ranking from Syn-Pic, the model's predicted rankings (from scores) are compared to the target.  A ranking loss is calculated using Discounted Cumulative Gain (DCG) weighting to penalize discrepancies, iteratively refining the model until its predicted rankings align with the ground truth rankings from Syn-Pic.", "section": "3 METHOD"}, {"figure_path": "2410.18013/figures/figures_9_0.png", "caption": "Figure 4: Comparison among different preference optimization methods and RankDPO for SDXL. The results illustrate that we generate images with better prompt alignment and aesthetic quality.", "description": "The figure is a qualitative comparison of images generated by different preference optimization methods for the SDXL model.  It shows side-by-side comparisons of images generated from the base SDXL model and models fine-tuned using DPO, MaPO, SPO, and the authors' proposed RankDPO method. Each row represents a different optimization method, and each column represents a different prompt. The prompts are varied and include descriptions involving animals, objects, and scenes. The comparison illustrates that the RankDPO method produces images with improved prompt alignment and aesthetic quality compared to the other methods.", "section": "4 Experiments"}, {"figure_path": "2410.18013/figures/figures_17_0.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure presents a qualitative comparison of image generation results from different models for four prompts.  For each prompt, it shows outputs from SDXL and SD3, followed by outputs from the proposed method.  The results illustrate improvements in prompt following and visual quality obtained by the authors' approach compared to the baselines (SDXL and SD3). The prompts used are diverse and include descriptions involving multiple elements and styles, providing a comprehensive assessment of each method's capabilities.", "section": "Introduction"}, {"figure_path": "2410.18013/figures/figures_19_0.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure displays a comparative analysis of text-to-image generation results using different models. It showcases the outputs from SDXL and SD3 models alongside the improved results achieved using the authors' proposed approach.  Each row presents a specific text prompt, followed by a comparison of the generated images: the original model's output is shown on the left, while the results generated after incorporating the new approach are displayed on the right. The improvements in prompt following and visual quality are evident across different text prompts and model types.", "section": "1 Introduction"}, {"figure_path": "2410.18013/figures/figures_19_1.png", "caption": "Figure 1: Our approach, trained on a synthetic preference dataset with a ranking objective in the preference optimization, improves prompt following and visual quality for SDXL (Podell et al., 2023) and SD3-Medium (Esser et al., 2024), without requiring any manual annotations.", "description": "The figure displays a comparison of image generation results between two state-of-the-art text-to-image models, SDXL and SD3-Medium, and the same models after being fine-tuned using the authors' proposed method.  For each model, two sets of generated images are shown: one produced by the original model and one produced by the fine-tuned model, each corresponding to a given text prompt. The prompts are provided under each image set. The images demonstrate how the authors' technique improves adherence to the prompt instructions and enhances the visual quality of generated images, as the fine-tuned models produce images that better match the description provided in the prompt.", "section": "Introduction"}]