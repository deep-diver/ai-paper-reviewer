[{"figure_path": "2410.18538/figures/figures_1_0.png", "caption": "Figure 1: SMITE. Using only one or few segmentation references with fine granularity (left), our method learns to segment different unseen videos respecting the segmentation references.", "description": "The figure illustrates the SMITE (Segment Me In Time) pipeline.  On the left, it shows the input: a few RGB images of a subject (horses in this case) with their corresponding fine-grained segment annotations.  These annotated images serve as references for the model. An arrow indicates the process of the model learning from these few examples. On the right, the output is demonstrated:  multiple unseen videos of horses, with the model successfully segmenting the horses in each frame, mirroring the granularity and segmentations defined in the reference images. This highlights the ability of SMITE to generalize from a few annotated images to segment various unseen videos.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18538/figures/figures_4_0.png", "caption": "Figure 2: SMITE pipeline. During inference (a), we invert a given video into a noisy latent by iteratively adding noise. We then use an inflated U-Net denoiser (b) along with the trained text embedding as input to denoise the segments. A tracking module ensures that the generated segments are spatially and temporally consistent via spatio-temporal guidance. The video latent z\u0142 is updated by a tracking energy Etrack (c) that makes the segments temporally consistent and also a low-frequency regularizer (d) Ereg which guides the model towards better spatial consistency.", "description": "The figure illustrates the SMITE pipeline for video segmentation.  It begins with an input video (a) that's converted into a noisy latent representation through iterative noise addition. This noisy latent is then processed by an inflated U-Net denoiser (b), guided by a trained text embedding, to generate initial segmentations. A crucial tracking module (c) refines these segmentations to ensure both spatial and temporal consistency, incorporating a tracking energy (Etrack) to maintain temporal coherence. Simultaneously, a low-frequency regularizer (d) (Ereg) is applied to enhance spatial consistency by comparing the predicted segments against a reference segment using Discrete Cosine Transform (DCT) and a low-pass filter. The final output is a temporally and spatially consistent segmentation of the input video.", "section": "4 METHOD"}, {"figure_path": "2410.18538/figures/figures_6_0.png", "caption": "Figure 4: Segment tracking module ensures that segments are consistent across time. It uses co-tracker to track each point of the object's segment (here it is nose) and then finds point correspondence of this segment (denoted by blue dots) across timesteps. When the tracked point is of a different class (e.g,. face) then it is recovered by using temporal voting. The misclassified pixel is then replaced by the average of the neighbouring pixels of adjacent frames. This results are temporally consistent segments without visible flickers.", "description": "This figure illustrates the segment tracking module of the SMITE pipeline.  The left side shows a sequence of frames from a video, with segmentations before and after applying the tracking module. The top row demonstrates initial segmentations with inconsistencies, while the bottom row displays the refined segmentations resulting from the tracking module.  The right side provides a detailed schematic of the segment tracking and voting process. It depicts how a co-tracker follows individual points within a segment (in this example, the nose) across frames.  When a tracked point is misclassified (e.g., labeled as a face instead of a nose), a majority voting scheme based on neighboring pixel probabilities corrects the misclassification, ensuring temporal consistency and minimizing flickering artifacts in the final segmentation.", "section": "4.3 TEMPORAL CONSISTENCY"}, {"figure_path": "2410.18538/figures/figures_6_1.png", "caption": "Figure 3: Best viewed in Adobe Acrobat.", "description": "This figure shows a visual comparison of video segmentation results using three different approaches.  The first column (a) shows frame-by-frame segmentation, resulting in noticeable inconsistencies between frames. The second column (b) demonstrates results without employing a tracking method and low-pass regularizer, leading to some flickering in the output.  The third column (c) presents the output of the proposed SMITE method without low-pass regularization, exhibiting some inconsistencies, while the final column (d) displays the final results of the SMITE method, revealing temporally consistent and flicker-free segmentation.", "section": "4 METHOD"}, {"figure_path": "2410.18538/figures/figures_7_0.png", "caption": "Figure 5: SMITE-50 Dataset sample.", "description": "The figure displays sample images from the SMITE-50 dataset, showcasing the three main categories used in the study: horses, human faces, and cars.  Each category has example images, illustrating the diversity of poses, lighting conditions, and object appearances within each class.  The images are presented to demonstrate the variety of visual characteristics and challenges present in the dataset used for evaluating the SMITE method.  This visual representation is meant to show the dataset's complexity which includes various challenging conditions for video segmentation.", "section": "5 RESULTS AND EXPERIMENTS"}, {"figure_path": "2410.18538/figures/figures_8_0.png", "caption": "Figure 6: Visual comparisons with other methods demonstrate that SMITE maintains better motion consistency of segments and delivers cleaner, more accurate segmentations. Both GSAM2 and Baseline-I struggle to accurately capture the horse\u2019s mane, and GSAM2 misses one leg (Left), whereas our method yields more precise results. Additionally, both alternative techniques create artifacts around the chin (Right), while SMITE produces a cleaner segmentation.", "description": "This figure presents a visual comparison of video segmentation results obtained using four different methods: the proposed SMITE method and three baseline methods (Baseline-1, GSAM2, and RGB Video). The comparison is shown for two video clips, one featuring a horse and the other featuring a human face. For each video clip, the figure displays the original RGB video frames alongside the segmentation results obtained using each of the four methods. The segmentation results are color-coded to represent different segments, which helps to visualize the quality of each method\u2019s ability to identify and segment objects. The results clearly showcase that SMITE produces more precise, consistent, and artifact-free segmentations compared to the baseline methods, particularly in capturing finer details and handling object motion.", "section": "5 RESULTS AND EXPERIMENTS"}, {"figure_path": "2410.18538/figures/figures_10_0.png", "caption": "Figure 7: Additional results. We visualize the generalization capability of SMITE model (trained on the reference images) in various challenging poses, shape, and even in cut-shapes.", "description": "This figure displays four rows, each illustrating the generalization capability of the SMITE model for video segmentation.  Each row showcases a different object category: Faces, Penguin, Cars, and Pineapple.  For each category, on the left side are shown a few reference images with their corresponding segment annotations. On the right, a sequence of video frames from a different unseen video, with the same object class and segment granularity is shown, demonstrating how the SMITE method successfully generalizes segmentations to these new videos. The color-coded segmentations highlight the consistency achieved across the varied poses, scales, and even in cut-shapes of the object in different video frames.", "section": "5 RESULTS AND EXPERIMENTS"}, {"figure_path": "2410.18538/figures/figures_10_1.png", "caption": "Figure 8: Segmentation results in challenging scenarios . SMITE accurately segments out the objects under occlusion (\"ice-cream\") or camouflage (\"turtle\") highlighting the robustness of our segmentation technique.", "description": "This figure presents two sets of video frames and their corresponding segmentations generated by the SMITE model. The top row shows four frames from a video depicting a woman eating ice cream, where the ice cream cone is partially obscured by her hand and face, representing an occlusion scenario.  The bottom row displays the SMITE-generated segmentation masks for the same frames, demonstrating the accurate segmentation of the ice cream even in the presence of occlusion. The second pair showcases a similar comparison, but this time with four frames of a video of turtles swimming underwater, where the turtles blend with the background's colors and textures.  Again, the bottom row shows that the SMITE model accurately segments the turtles, even when the objects are camouflaged within their surroundings, showcasing the method's robustness in challenging conditions.", "section": "5 RESULTS AND EXPERIMENTS"}]