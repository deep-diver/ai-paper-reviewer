{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper is foundational for the field of multi-modal learning, introducing a method for training visual models using natural language supervision.  This is directly relevant to WAFFLE, which leverages multi-modal learning for UI-to-HTML code generation.  The concepts and techniques presented in this paper are crucial to understanding and building upon the advancements in multi-modal large language models that underly the WAFFLE method.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Lauren\u00e7on", "paper_title": "Unlocking the conversion of web screenshots into html code with the websight dataset", "reason": "This paper introduces the WebSight dataset, which is crucial for WAFFLE's training.  The WebSight dataset provides a large collection of webpages and their corresponding HTML code and images.  WAFFLE leverages this dataset to train and evaluate its UI-to-HTML code generation model.  Therefore, this is a highly relevant paper which directly affects the methodology used in the WAFFLE method.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Chenglei Si", "paper_title": "Design2Code: How far are we from automating front-end engineering?", "reason": "This paper introduces Design2Code, a benchmark dataset used for evaluating the performance of WAFFLE. Design2Code contains real-world website screenshots and their corresponding HTML code, providing a realistic evaluation scenario. The choice of this dataset is critical in demonstrating the effectiveness of WAFFLE's approach on real-world scenarios.", "section_number": 3}, {" publication_date": "2022", "fullname_first_author": "Junnan Li", "paper_title": "BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation", "reason": "This work is highly influential in the area of vision and language models, proposing a unified model for understanding and generating visual and textual information. This approach is closely aligned with WAFFLE, which also aims to bridge the gap between visual UI designs and textual HTML code. The methodologies are highly relevant to the WAFFLE model.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "reason": "Flamingo is a significant advancement in visual language models, showcasing strong few-shot learning capabilities.  This is relevant to WAFFLE because WAFFLE also aims to improve the ability of LLMs to understand and generate code from limited visual data.  The few-shot learning capabilities of Flamingo are particularly relevant to the structure-aware attention mechanism in WAFFLE.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This paper introduces LLAMA, a family of large language models that have shown considerable progress in code generation.  The efficiency and architecture of LLAMA are relevant to WAFFLE's methodology because they provide a strong foundation upon which WAFFLE's improvements are implemented.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Raymond Li", "paper_title": "Starcoder: may the source be with you!", "reason": "This paper introduces Starcoder, a large language model with a strong focus on code generation.  Starcoder's architecture and training methods are directly relevant to the training process of WAFFLE, providing insight into the capabilities of large language models in the context of code generation. The architecture and training methods discussed are applicable to the methodologies presented within the WAFFLE method.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Lei Chai", "paper_title": "Pyramid attention for source code summarization", "reason": "Pyramid attention is a novel attention mechanism that addresses the challenges of handling long-range dependencies in source code. This is relevant to the structure-aware attention mechanism in WAFFLE, which aims to efficiently process the hierarchical structure of HTML code.  The attention mechanism discussed is relevant to the WAFFLE method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Lin Chen", "paper_title": "Sharegpt4v: Improving large multi-modal models with better captions", "reason": "This paper demonstrates the effectiveness of captioning in improving multi-modal models.  This is relevant to WAFFLE, as WAFFLE also aims to bridge the gap between visual and textual information in UI design images and HTML code.  The specific approach to captioning is highly relevant to the concepts implemented in the WAFFLE approach.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Tianyu Gao", "paper_title": "SimCSE: Simple contrastive learning of sentence embeddings", "reason": "This paper introduces SimCSE, a simple contrastive learning method for sentence embeddings.  Contrastive learning is a key component of WAFFLE, which uses it to align the model's understanding of visual UI and textual HTML code.  The methodology discussed in this paper is highly relevant to the WAFFLE method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Wenliang Dai", "paper_title": "Instructblip: Towards general-purpose vision-language models with instruction tuning", "reason": "This work is relevant to WAFFLE because it explores general-purpose vision-language models with instruction tuning.  WAFFLE aims to develop a model that is also generalizable to various scenarios, but more specifically focused on UI-to-HTML conversion. Instruction tuning is directly relevant to the WAFFLE methodology.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Daniel Fried", "paper_title": "Incoder: A generative model for code infilling and synthesis", "reason": "Incoder is a generative model for code infilling and synthesis.  This is relevant to WAFFLE because WAFFLE aims to generate code from UI designs. While Incoder focuses on different programming languages, the fundamental concept of generating code from input data is highly relevant to WAFFLE.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Albert Q. Jiang", "paper_title": "Mistral 7b", "reason": "This work introduces Mistral 7B, a large language model.  The architecture and capabilities of Mistral 7B are relevant to understanding the context of large language models and the challenges faced in training and deploying large language models. This is directly related to the foundation upon which the WAFFLE method is built.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Anton Lozhkov", "paper_title": "Obelics: An open web-scale filtered dataset of interleaved image-text documents", "reason": "This work introduces Obelics, a large-scale dataset of interleaved image-text documents, which is relevant to WAFFLE due to the multi-modal nature of the problem.  Obelics showcases the potential of combining textual and visual information and can be used as a resource for future improvements.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "Ilya Loshchilov", "paper_title": "Decoupled weight decay regularization", "reason": "This paper introduces a decoupled weight decay regularization method.  WAFFLE uses AdamW optimizer, which incorporates weight decay. Therefore, the method discussed is highly relevant to the optimization techniques implemented in the WAFFLE method.", "section_number": 3}, {" publication_date": "2017", "fullname_first_author": "Ashish Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduces the transformer architecture, which is fundamental to the design of many large language models, including those used in WAFFLE.  The transformer architecture and attention mechanisms are core components that heavily influence the performance and capabilities of the WAFFLE method.", "section_number": 5}, {" publication_date": "2016", "fullname_first_author": "Zichao Yang", "paper_title": "Hierarchical attention networks for document classification", "reason": "This paper introduces hierarchical attention networks, a type of attention mechanism suitable for handling hierarchical data structures.  This is directly relevant to WAFFLE's structure-aware attention mechanism which is specifically designed for the hierarchical nature of HTML code.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Jinze Bai", "paper_title": "Qwen-vl: A frontier large vision-language model with versatile abilities", "reason": "This paper introduces Qwen-VL, a large vision-language model, which is relevant to the context of WAFFLE because it explores multi-modal models with versatile abilities, similar to WAFFLE's aim of developing a model capable of handling diverse aspects of UI-to-HTML conversion. Qwen-VL's abilities, especially in visual tasks, are directly relevant to the visual aspects of the WAFFLE method.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Lukas Blecher", "paper_title": "Nougat: Neural optical understanding for academic documents", "reason": "While this paper focuses on academic documents, it is relevant to WAFFLE because it highlights the importance of understanding the visual layout of documents for accurate extraction of information. This relates to WAFFLE's need to extract structural information from UI images for accurate code generation.", "section_number": 1}]}