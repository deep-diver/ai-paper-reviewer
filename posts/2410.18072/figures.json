[{"figure_path": "2410.18072/figures/figures_2_0.png", "caption": "Figure 1: Overview of the hierarchical capabilities of the Predictive Models. Models at higher stages demonstrate more advanced capabilities. We take the initial step in evaluating Predictive Generative Models up to the S3 stage, known as World Simulators, by introducing a parallel evaluation framework, WorldSimBench. WorldSimBench assesses the models both Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, focusing on video generation and action transformation across three critical embodied scenarios.", "description": "The figure presents a hierarchical classification of predictive models based on their embodiment level, ranging from S0 (text prediction) to S3 (actionable video generation).  It introduces WorldSimBench, a dual evaluation framework for World Simulators (S3 models) comprising Explicit Perceptual Evaluation (assessing visual quality) and Implicit Manipulative Evaluation (assessing video-action consistency in embodied tasks).  Three embodied scenarios (Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation) are highlighted, illustrating the evaluation process and key elements considered in each scenario.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/figures/figures_5_0.png", "caption": "Figure 2: Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation. We use a large collection of video captions from the internet and our predefined embodied evaluation dimensions. These are expanded using GPT and manually verified to create a corresponding Task Instruction Prompt List for data generation and evaluation. (Bottom) HF-Embodied Dataset Generation. Massive internet-sourced embodied videos with captions are used to train data generation models. Fine-grained Human Feedback Annotation is then applied to the embodied videos according to the corresponding Task Instruction Prompt List, covering multiple embodied dimensions.", "description": "This figure illustrates the Explicit Perceptual Evaluation process. The top half shows the Instruction Prompt Generation, which starts with massive video captions and predefined embodied evaluation dimensions. These are expanded using GPT and manually checked to create a Task Instruction Prompt List for both data generation and evaluation. The bottom half details the HF-Embodied Dataset Generation process, involving the use of massive internet-sourced videos with captions to train data generation models.  Finally, fine-grained human feedback is applied to these videos based on the Task Instruction Prompt List, encompassing various embodied dimensions, to create the HF-Embodied dataset.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/figures/figures_7_0.png", "caption": "Overview of Implicit Manipulative Evaluation. Embodied tasks in different scenarios are decomposed into executable sub-tasks. The video generation model generates corresponding predicted videos based on the current instructions and real-time observations. Using a pre-trained IDM or a goal-based policy, the agent executes the generated sequence of actions. After a fixed timestep, the predicted video is refreshed by sampling again from the video generation model, and this process repeats. Finally, the success rates of various embodied tasks are obtained through monitors in the simulation environment.", "description": "The figure illustrates the Implicit Manipulative Evaluation process. It begins with a high-level task decomposition into executable sub-tasks, which are then used by the video generation model to predict videos based on current instructions and environment observations.  A video-action mapping component, using either an inverse dynamic model or a goal-based policy, translates the predicted video into a sequence of actions for an agent operating within a simulated environment.  After a specific time interval, the process repeats with updated observations and video predictions, generating a closed-loop evaluation process. The task's success rate is monitored and used to evaluate the world simulator model's effectiveness.", "section": "4.2 Implicit Manipulative Evaluation"}, {"figure_path": "2410.18072/figures/figures_22_0.png", "caption": "Figure 7: Rollout of Open-Ended Embodied Environment in Implicit Manipulative Evaluation.", "description": "This figure shows a sequence of images from the Minecraft environment, illustrating the rollout process of the Implicit Manipulative Evaluation in the Open-Ended Embodied Environment scenario. Each image captures a step in the execution of a task, which involves collecting wood in a forest. The images show the agent's actions, such as moving through the forest, approaching trees, and harvesting logs. The instruction of the task is shown in the lower left corner of each image.", "section": "4.2 IMPLICIT MANIPULATIVE EVALUATION"}, {"figure_path": "2410.18072/figures/figures_24_0.png", "caption": "Figure 8: Rollout of Autonomous Driving in Implicit Manipulative Evaluation.", "description": "This figure shows a sequence of frames from a video generated by a video generation model for autonomous driving. Each frame displays a first-person view of the driving scene, with the corresponding instruction shown in the lower-left corner.  The generated video was used as input to a pre-trained video-to-action model that controls an agent in the CARLA simulator. The frames highlight various aspects of the driving task, including navigation through different road types (straight roads, turns, intersections), changing weather conditions, and varying amounts of traffic and other on-road obstacles. The figure illustrates the performance of the system throughout the task, providing a visual example of how the model makes real-time decisions and takes actions in a dynamic environment.", "section": "4.2 Implicit Manipulative Evaluation"}, {"figure_path": "2410.18072/figures/figures_26_0.png", "caption": "Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation. We use a large collection of video captions from the internet and our predefined embodied evaluation dimensions. These are expanded using GPT and manually verified to create a corresponding Task Instruction Prompt List for data generation and evaluation. (Bottom) HF-Embodied Dataset Generation. Massive internet-sourced embodied videos with captions are used to train data generation models. Fine-grained Human Feedback Annotation is then applied to the embodied videos according to the corresponding Task Instruction Prompt List, covering multiple embodied dimensions.", "description": "The figure illustrates the process of Explicit Perceptual Evaluation, which involves two main stages: Instruction Prompt Generation and HF-Embodied Dataset Generation.  The top part shows how diverse task instruction prompts are created using video captions and GPT, organized by evaluation dimension and embodied scenario. These prompts are used to generate videos using various models. The bottom part depicts the generation of the HF-Embodied Dataset, where internet-sourced videos are annotated with fine-grained human feedback across multiple embodied dimensions (such as trajectory, velocity, perspectivity and interaction), based on the previously generated instruction prompts. This process provides a comprehensive dataset for training a Human Preference Evaluator to assess the visual quality of generated videos.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}]