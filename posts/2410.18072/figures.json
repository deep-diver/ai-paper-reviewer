[{"figure_path": "2410.18072/figures/figures_2_0.png", "caption": "Figure 1: Overview of the hierarchical capabilities of the Predictive Models. Models at higher stages demonstrate more advanced capabilities. We take the initial step in evaluating Predictive Generative Models up to the S3 stage, known as World Simulators, by introducing a parallel evaluation framework, WorldSimBench. WorldSimBench assesses the models both Explicit Perceptual Evaluation and Implicit Manipulative Evaluation, focusing on video generation and action transformation across three critical embodied scenarios.", "description": "This figure illustrates a hierarchical classification of predictive models based on their capabilities and level of embodiment, ranging from S0 (predicting text) to S3 (generating actionable videos).  It introduces WorldSimBench, a dual evaluation framework for World Simulators (S3 models), comprising Explicit Perceptual Evaluation (assessing visual quality) and Implicit Manipulative Evaluation (evaluating video-action consistency in embodied tasks). Three representative embodied scenarios \u2013 Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation \u2013 are depicted, each with key evaluation elements and corresponding benchmarks.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/figures/figures_5_0.png", "caption": "Figure 2: Overview of Explicit Perceptual Evaluation. (Top) Instruction Prompt Generation. We use a large collection of video captions from the internet and our predefined embodied evaluation dimensions. These are expanded using GPT and manually verified to create a corresponding Task Instruction Prompt List for data generation and evaluation. (Bottom) HF-Embodied Dataset Generation. Massive internet-sourced embodied videos with captions are used to train data generation models. Fine-grained Human Feedback Annotation is then applied to the embodied videos according to the corresponding Task Instruction Prompt List, covering multiple embodied dimensions.", "description": "This figure illustrates the process of Explicit Perceptual Evaluation in the WorldSimBench framework.  The top half shows the generation of instruction prompts, starting with a large collection of video captions and embodied evaluation dimensions. These are expanded using GPT and manually checked to create a Task Instruction Prompt List. The bottom half details the HF-Embodied Dataset generation: internet-sourced embodied videos with captions are used to train data generation models, and fine-grained human feedback annotation is applied to the videos based on the instruction prompts, covering multiple embodied dimensions such as velocity, perspectivity, interaction, and trajectory.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/figures/figures_7_0.png", "caption": "Figure 3: Overview of Implicit Manipulative Evaluation. Embodied tasks in different scenarios are decomposed into executable sub-tasks. The video generation model generates corresponding predicted videos based on the current instructions and real-time observations. Using a pre-trained IDM or a goal-based policy, the agent executes the generated sequence of actions. After a fixed timestep, the predicted video is refreshed by sampling again from the video generation model, and this process repeats. Finally, the success rates of various embodied tasks are obtained through monitors in the simulation environment.", "description": "The figure illustrates the Implicit Manipulative Evaluation process. It shows how embodied tasks across three scenarios (Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation) are broken down into sub-tasks. A video generation model produces videos based on current instructions and observations. These videos are then processed by a video-to-action mapping module (either an Inverse Dynamic Model or a goal-based policy) to generate actions for an agent within a simulated environment. After a set time, the process repeats with updated observations and a new video prediction. The success of the overall task is monitored by evaluating agent performance within the simulation.", "section": "4.2 IMPLICIT MANIPULATIVE EVALUATION"}, {"figure_path": "2410.18072/figures/figures_22_0.png", "caption": "Figure 7: Rollout of Open-Ended Embodied Environment in Implicit Manipulative Evaluation.", "description": "This figure shows a sequence of eight images illustrating the rollout of an Open-Ended Embodied Environment task within the Implicit Manipulative Evaluation framework. Each image represents a frame from a video generated by a video generation model, showing the agent's actions in a Minecraft environment. The top row displays the agent's actions as it moves through a birch forest, seemingly searching for wood. The bottom row focuses on the process of collecting wood, showing the agent's hand approaching, breaking, and collecting wooden logs. The textual instruction \"Collect wood in the forest\" is visible in the lower left corner of each frame, guiding the agent's actions.", "section": "4.2 IMPLICIT MANIPULATIVE EVALUATION"}, {"figure_path": "2410.18072/figures/figures_24_0.png", "caption": "Figure 8: Rollout of Autonomous Driving in Implicit Manipulative Evaluation.", "description": "This figure shows a series of frames from the Autonomous Driving simulation environment.  Each frame shows a first-person driving perspective of the agent navigating a road, with various environmental features like trees, buildings, and other vehicles.  The frames illustrate an agent following a series of instructions (provided in the text labels within each frame) to reach a destination, demonstrating the real-time closed-loop interaction between the video generation model, a video-to-action policy, and the simulator.", "section": "4.2 Implicit Manipulative Evaluation"}, {"figure_path": "2410.18072/figures/figures_26_0.png", "caption": "Figure 9: Rollout of Robot Manipulation in Implicit Manipulative Evaluation.", "description": "This figure shows a sequence of images depicting the execution of a robot manipulation task in a simulated environment. The top row illustrates the robot arm completing the task \"Press the button to turn on the led light\", while the bottom row displays the execution of the task \"Go push the blue block right\". Each row presents a sequence of frames showing the robot arm's movements and interactions with the objects in the scene.  The images demonstrate the robot arm's progress in completing the tasks, showcasing its ability to successfully manipulate the objects according to instructions. The figure provides a visual representation of the Implicit Manipulative Evaluation process in the Robot Manipulation scenario.", "section": "F Implicit Manipulative Evaluation-RM"}]