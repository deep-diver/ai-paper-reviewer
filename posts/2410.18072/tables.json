[{"figure_path": "2410.18072/tables/table_3_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares various existing benchmarks for evaluating predictive models, categorizing them based on input modality (text or text and images), output modality (text, image, or video), the underlying method (LLM, MLLM, IGM, or VGM), the stage of predictive model capability (So-S3), the type of interaction with the environment (task-level or action-level), and the evaluation strategy used (human judgement, multi-choice, GPT judgement, feature similarity, or human preference evaluator with embodied metrics).  It highlights the differences in how these benchmarks assess predictive models and provides context for the introduction of WorldSimBench, a new benchmark designed to address the limitations of existing methods in evaluating highly embodied predictive models.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_6_0.md", "caption": "Table 2: Hierarchical Evaluation Dimension. The dimensions are categorized into three main aspects: Visual Quality for evaluating the overall quality, Condition Consistency for evaluating the alignment to the input instruction, and Embodiment for evaluating embodied related factors like physical rules.", "description": "This table presents a hierarchical breakdown of evaluation dimensions for assessing World Simulators across three embodied scenarios: Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation.  It categorizes the evaluation criteria into three main aspects: Visual Quality (assessing aesthetics and foreground/background consistency); Condition Consistency (measuring instruction and scenario alignment); and Embodiment (evaluating aspects specific to each scenario such as velocity, trajectory, embodied interaction, perspectivity, key elements, and safety).  Each scenario has a unique set of dimensions within these three aspects.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/tables/table_8_0.md", "caption": "Table 3: The overall performance comparison between Human Preference Evaluator and GPT-40. HPE indicates Human Preference Evaluator. HPE@Lavie means that HPE is trained on videos except those generated by Lavie. The validation is conducted on videos generated by Laive under zero-shot setting.", "description": "This table presents a performance comparison between a Human Preference Evaluator (HPE) and GPT-40 across three embodied scenarios: Open-Ended Embodied Environment (OE), Autonomous Driving (AD), and Robot Manipulation (RM).  For each scenario, it shows the performance of GPT-40 and the HPE using accuracy (Acc) for OE and Pearson linear correlation coefficient (PLCC) for AD and RM.  It also includes results for GPT-40 and HPE when trained on videos excluding those generated by Lavie (HPE@Lavie, GPT-40@Lavie), indicating zero-shot performance evaluations on videos generated by Lavie.", "section": "5 EXPERIMENTS"}, {"figure_path": "2410.18072/tables/table_17_0.md", "caption": "Table 4: Analysis of HF-Embodied Dataset. Samples scored higher than 3 in AD and RM are considered positive.", "description": "This table presents a quantitative analysis of the HF-Embodied Dataset used in the study.  It breaks down the dataset by embodied scenario (Open-Ended Embodied Environment, Autonomous Driving, and Robot Manipulation), providing the number of instructions, videos, dimensions, actions, positive samples (scores higher than 3 for AD and RM), and negative samples for each scenario. This data offers insights into the dataset's composition and the distribution of scores across different evaluation dimensions.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/tables/table_17_1.md", "caption": "Table 5: Training Frames of Generation Models.", "description": "This table presents the number of frames used for training short and long videos for eight different video generation models.  The models are: Open-Sora-Plan, Lavie, ModelScope, OpenSora, AnimateDiff, DynamicCrafter, and EasyAnimate. For each model, the table shows the number of frames used to train short videos and the number of frames used to train long videos.", "section": "B Detaild Implementation of Explicit Perceptual Evaluation"}, {"figure_path": "2410.18072/tables/table_18_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares various existing Predictive Model benchmarks across several key features.  It lists each benchmark's input and output modalities, the type of model it is based on, the stage of predictive model capabilities (So-S3) it evaluates, whether it involves interactive environments, and its evaluation strategy.  The table also specifies whether the interaction with the environment occurs at the task level or action level, and the type of embodied metric (if any) that is used.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_19_0.md", "caption": "Table 7: Evaluation results in OE. The abbreviations are listed in Tab. 2.", "description": "Table 7 presents the evaluation results of seven video generation models across seven dimensions (BC, FC, IA, SA, VC, TJ, EI) in the Open-Ended Embodied Environment scenario of the WorldSimBench.  Each dimension represents a specific aspect of video quality and alignment with the instructions, such as background consistency (BC), foreground consistency (FC), instruction alignment (IA), scenario alignment (SA), velocity (VC), trajectory (TJ), and embodied interaction (EI). The overall score is the average across all seven dimensions, providing a comprehensive evaluation of each model's performance in the OE scenario.  The table is used to compare the performance of different video generation models on the different dimensions of video quality.", "section": "C Detailed Result of Explicit Perceptual Evaluation"}, {"figure_path": "2410.18072/tables/table_20_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares eight existing predictive model benchmarks across several key features.  These features include the input and output modalities used (e.g., text, images, video), the stage of predictive modeling capability they assess (So-S2), whether they involve interactive environments, their evaluation strategies (e.g., human judgment, feature similarity), and the type of agent interaction (Task-level or Action-level). The table also includes the keywords associated with each benchmark, highlighting the specific aspects of predictive models that are evaluated.  Finally, it contrasts these existing benchmarks with the newly proposed WorldSimBench, indicating its focus on Action-Level Interaction within a highly embodied context (S3) and its use of a dual evaluation framework.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_20_1.md", "caption": "Table 9: Evaluation results in RM. The abbreviations are listed in Tab. 2.", "description": "Table 9 presents a performance comparison of seven video generation models across eight evaluation metrics within the Robot Manipulation scenario.  Each metric assesses a different aspect of video quality, including aesthetics (AE), background consistency (BC), foreground consistency (FC), instruction alignment (IA), perspectivity (PV), trajectory quality (TJ), embodied interaction (EI), and an overall score. The scores are on a scale of 1-5, with higher values indicating better performance.  The table provides a quantitative overview of each model's strengths and weaknesses, allowing for a detailed comparative analysis of their capabilities in generating videos appropriate for robot manipulation tasks.", "section": "4.2 IMPLICIT MANIPULATIVE EVALUATION"}, {"figure_path": "2410.18072/tables/table_21_0.md", "caption": "Table 1: Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares several existing predictive model benchmarks across various features.  These features include the input and output modalities used (e.g., text, images, video), the stage of predictive model capability (So-S2), whether the benchmark involves interactive environments, the type of interaction (task-level or action-level), and the evaluation strategy used (e.g., human judgment, feature similarity).  It also shows the relevant keywords associated with each benchmark.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_22_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares various existing Predictive Model benchmarks across several criteria.  These include the input and output modalities used by the models (e.g., text, images, video), the stage of Predictive Model capabilities that they evaluate (So-S2), whether they involve interactive environments, the evaluation strategy employed (human judgment, feature similarity, etc.), and the type of interaction (task-level or action-level).  The table provides a summary of existing benchmarks to highlight the limitations of current approaches to evaluating highly embodied Predictive Models and to showcase how WorldSimBench addresses these shortcomings.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_24_0.md", "caption": "Table 3: The overall performance comparison between Human Preference Evaluator and GPT-40. HPE indicates Human Preference Evaluator. HPE@Lavie means that HPE is trained on videos except those generated by Lavie. The validation is conducted on videos generated by Laive under zero-shot setting.", "description": "This table presents a comparison of the overall performance of the Human Preference Evaluator (HPE) and GPT-40 across three embodied scenarios: Open-Ended Embodied Environment (OE), Autonomous Driving (AD), and Robot Manipulation (RM).  The table shows the performance of both GPT-40 and the HPE, as well as versions of the HPE trained without data from the Lavie model (HPE@OpenSora, HPE@Lavie) and GPT-40 trained without data from the OpenSora model (GPT-40@Lavie).  For each scenario and model, the performance is reported using different metrics: accuracy (Acc) for OE, and Pearson Linear Correlation Coefficient (PLCC) for AD and RM.  The results demonstrate the superior performance of the HPE compared to GPT-40 in all scenarios and settings.", "section": "5 EXPERIMENTS"}, {"figure_path": "2410.18072/tables/table_25_0.md", "caption": "Table 1: Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares several existing predictive model benchmarks across various features.  The benchmarks are compared based on their input and output modalities, the type of methods they use, the stage of predictive model they evaluate (S0-S2), whether they involve interactive environments, their evaluation strategy, and the type of interaction (task-level or action-level). The table also lists the key elements and embodied metric used in each benchmark.  Finally, it includes WorldSimBench, highlighting its use of actionable video as output, action-level interaction, and a dual evaluation strategy combining explicit perceptual and implicit manipulative evaluations.", "section": "1 INTRODUCTION"}]