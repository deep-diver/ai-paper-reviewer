[{"figure_path": "2410.18072/tables/table_3_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "The table compares various existing benchmarks for evaluating predictive models, categorizing them based on input modality (text, text and images), output modality (text, image, video, actionable video), the underlying method (LLM, MLLM, IGM, VGM), the stage of prediction capability (So to S3), the type of interaction with the environment (task-level or action-level), and the evaluation strategy used (human judgment, multi-choice, GPT judgment, feature similarity, or human preference evaluator with embodied metrics).  It highlights the limitations of existing benchmarks in evaluating highly embodied predictive models and positions WorldSimBench as a more comprehensive evaluation framework for World Simulators.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_6_0.md", "caption": "Table 2: Hierarchical Evaluation Dimension. The dimensions are categorized into three main aspects: Visual Quality for evaluating the overall quality, Condition Consistency for evaluating the alignment to the input instruction, and Embodiment for evaluating embodied related factors like physical rules.", "description": "This table presents a hierarchical evaluation dimension checklist for evaluating video generation models across three embodied scenarios: Open-Ended Embodied Environment (OE), Autonomous Driving (AD), and Robot Manipulation (RM).  It categorizes evaluation dimensions into three main aspects: Visual Quality (evaluating overall video quality), Condition Consistency (evaluating alignment with input instructions), and Embodiment (evaluating embodied factors like physical rules). For each scenario, specific dimensions are listed under each aspect; for example, in OE, Visual Quality includes Background Consistency and Foreground Consistency, while Embodiment includes Velocity, Trajectory, and Embodied Interaction.  The table provides a framework for a comprehensive evaluation of video generation models, considering visual fidelity and alignment with the task instructions, as well as the physical consistency and embodiment of the generated content.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/tables/table_8_0.md", "caption": "Table 3: The overall performance comparison between Human Preference Evaluator and GPT-40. HPE indicates Human Preference Evaluator. HPE@Lavie means that HPE is trained on videos except those generated by Lavie. The validation is conducted on videos generated by Laive under zero-shot setting.", "description": "This table presents a comparison of the overall performance of GPT-40 and the Human Preference Evaluator (HPE) across three embodied scenarios: Open-Ended Embodied Environment (OE), Autonomous Driving (AD), and Robot Manipulation (RM).  For each scenario, it shows the performance metrics for both GPT-40 and HPE, as well as HPE trained on data excluding videos generated by Lavie (HPE@Lavie) and GPT-40 trained on videos generated by Lavie(GPT-40@Lavie). The performance is evaluated using Accuracy (Acc) for OE and Pearson Linear Correlation Coefficient (PLCC) for AD and RM.  The table highlights the superior performance of the HPE compared to GPT-40, both when trained on all data and when tested on videos generated by Lavie under a zero-shot setting.", "section": "5 Experiments"}, {"figure_path": "2410.18072/tables/table_17_0.md", "caption": "Table 4: Analysis of HF-Embodied Dataset. Samples scored higher than 3 in AD and RM are considered positive.", "description": "This table presents a quantitative analysis of the HF-Embodied Dataset, which is a video assessment dataset based on fine-grained human feedback.  It shows the number of instructions, videos, dimensions, and actions used for each of the three embodied scenarios evaluated: Open-Ended Embodied Environment (OE), Autonomous Driving (AD), and Robot Manipulation (RM).  For AD and RM, it further breaks down the number of samples that were rated positively (higher than 3) versus negatively by human annotators.  This data provides insights into the scale and characteristics of the dataset used in the Explicit Perceptual Evaluation of World Simulators.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/tables/table_17_1.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "The table compares several existing Predictive Model benchmarks across various aspects, including the input and output modalities used (text, images, video), the stage of the predictive model (S0-S3, representing increasing levels of embodiment), the type of environment interaction (task-level or action-level), and the evaluation strategy employed (human judgment, feature similarity, or embodied metrics).  The benchmarks are categorized by the type of predictive model they evaluate, ranging from those producing text predictions to those generating videos suitable for action control. It highlights the limitations of previous benchmarks in evaluating highly embodied models from an embodied perspective, providing context for the introduction of the WorldSimBench.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_18_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares various existing Predictive Model benchmarks across several key features.  These features include the input and output modalities used by the models (text, images, video), the stage of predictive model capability (So to S3, reflecting increasing levels of embodiment), whether the benchmark involves interactive environments during prediction, and the evaluation strategy employed (human judgment, multi-choice, GPT judgment, feature similarity, or an embodied metric). The table provides a comprehensive overview of existing methods to evaluate different types of predictive models, highlighting their strengths and limitations in evaluating embodied AI agents.", "section": "Table 1: Comparisons between existing Predictive Model benchmarks"}, {"figure_path": "2410.18072/tables/table_19_0.md", "caption": "Table 7: Evaluation results in OE. The abbreviations are listed in Tab. 2.", "description": "Table 7 presents the evaluation results for seven video generation models in the Open-Ended Embodied Environment scenario of the WorldSimBench framework.  The table shows the average scores for each model across seven evaluation dimensions: Background Consistency (BC), Foreground Consistency (FC), Instruction Alignment (IA), Scenario Alignment (SA), Velocity (VC), Trajectory (TJ), and Embodied Interaction (EI). The overall average score for each model is also included, providing a comprehensive summary of their performance in this scenario. The abbreviations used are listed in Table 2 of the paper.", "section": "C Detailed Result of Explicit Perceptual Evaluation"}, {"figure_path": "2410.18072/tables/table_20_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares several existing Predictive Model benchmarks across various features.  Benchmarks are categorized by the input and output modalities they use (e.g., Text, Image, Video), the methodologies employed (e.g., LLM, MLLM, IGM, VGM), the stage of Predictive Model they evaluate (So-S2), whether they involve an interactive environment, and their evaluation strategy (e.g., human judgment, feature similarity, embodied metrics).  The table highlights the differences in the level of interaction, primarily distinguishing between task-level interactions (each task interacts once) and action-level interactions (frequent interactions for control).  WorldSimBench is included for comparison, showcasing its unique Action-Level interaction and Human Preference Evaluator-based evaluation strategy.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_20_1.md", "caption": "Table 9: Evaluation results in RM. The abbreviations are listed in Tab. 2.", "description": "This table presents the evaluation results for seven video generation models across seven evaluation dimensions (AE, BC, FC, IA, PV, TJ, EI) in the Robot Manipulation scenario.  Each dimension assesses a specific aspect of video quality and alignment with task instructions; for instance, AE refers to aesthetics, BC to background consistency, FC to foreground consistency, IA to instruction alignment, PV to perspectivity, TJ to trajectory, and EI to embodied interaction. The 'Overall' column provides the average score across all seven dimensions for each model, indicating its overall performance in the Robot Manipulation task.", "section": "4.1 EXPLICIT PERCEPTUAL EVALUATION"}, {"figure_path": "2410.18072/tables/table_21_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares various existing Predictive Model benchmarks across several key features.  These features include the input and output modalities of the models (text, images, video), the stage of Predictive Model capability they evaluate (So-S2), the type of interaction with the environment (task-level or action-level), and the employed evaluation strategies (human judgment, feature similarity, or other methods).  The table also includes a row for WorldSimBench, highlighting its novel approach and the Action-Level Interaction via human preference evaluation.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_22_0.md", "caption": "Table 1: Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares various existing benchmarks for evaluating predictive models.  It shows the input and output modalities of each benchmark, the type of model it is based on, its stage in a hierarchical classification of model capabilities (So-S3), whether it involves interactive simulation environments, its evaluation strategy (e.g., human judgement, feature similarity), and the type of agent interaction (task-level or action-level). The table highlights the differences in approach and scope between previous benchmarks and the newly proposed WorldSimBench, emphasizing WorldSimBench's focus on evaluating highly embodied predictive models (S3) using both visual and action-level metrics.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18072/tables/table_24_0.md", "caption": "Table 3: The overall performance comparison between Human Preference Evaluator and GPT-40. HPE indicates Human Preference Evaluator. HPE@Lavie means that HPE is trained on videos except those generated by Lavie. The validation is conducted on videos generated by Laive under zero-shot setting.", "description": "This table presents a comparison of the overall performance between the Human Preference Evaluator (HPE) and GPT-40 across three embodied scenarios: Open-Ended Embodied Environment (OE), Autonomous Driving (AD), and Robot Manipulation (RM).  For each scenario, it shows the performance of GPT-40, HPE trained on all data, HPE trained without data from Lavie, and HPE evaluated on Lavie's data.  The metrics used for comparison are Accuracy (Acc) for OE and Pearson Linear Correlation Coefficient (PLCC) for AD and RM.  The table highlights that HPE consistently outperforms GPT-40 across all scenarios and evaluation settings.", "section": "5 EXPERIMENTS"}, {"figure_path": "2410.18072/tables/table_25_0.md", "caption": "Comparisons between existing Predictive Model benchmarks. Interactive Environment refers to the interaction with the simulation environment during the prediction phase. Task-Level Interaction denotes that each task interacts once, whereas Action-Level Interaction represents the frequency of interactions that occur through the generation of actions for control purposes.", "description": "This table compares several existing Predictive Model benchmarks across various aspects.  It shows the input and output modalities used by each benchmark (text, images, videos), the type of method employed (LLM, MLLM, IGM, VGM), the stage of Predictive Model capabilities each assesses (S0-S2), the type of interaction with the environment (Task-Level or Action-Level), and finally the specific evaluation strategy used (human judgment, multi-choice, GPT judgment, feature similarity).  The table also includes WorldSimBench for comparison, highlighting its unique focus on Action-Level Interaction and Human Preference Evaluator in evaluating World Simulators.", "section": "1 INTRODUCTION"}]