[{"figure_path": "2410.18975/tables/table_9_0.md", "caption": "Comparison of UNBOUNDED and other methods for maintaining environment consistency and character consistency. UNBOUNDED achieves the best performance in maintaining consistency, while maintaining comparable semantic alignment with the text prompt. Best scores are in bold.", "description": "The table compares UNBOUNDED against three other methods (IP-Adapter, IP-Adapter-Instruct, and Story Diffusion) across three metrics related to image generation: environment consistency (measured using CLIP-I, DINO, and DreamSim), character consistency (also measured using CLIP-I, DINO, and DreamSim), and semantic alignment with the text prompt (measured using CLIP-T).  For each method, the table shows the scores achieved for each of these three metrics.  The best scores for each metric are highlighted in bold, demonstrating UNBOUNDED's superior performance in maintaining both environment and character consistency while achieving comparable semantic alignment.", "section": "4.1 Evaluation Benchmarks"}, {"figure_path": "2410.18975/tables/table_9_1.md", "caption": "Table 1: Comparison of UNBOUNDED and other methods for maintaining environment consistency and character consistency. UNBOUNDED achieves the best performance in maintaining consistency, while maintaining comparable semantic alignment with the text prompt. Best scores are in bold.", "description": "Table 1 presents a comparison of UNBOUNDED and three other methods (IP-Adapter, IP-Adapter-Instruct, Story Diffusion) for image generation, focusing on environment consistency, character consistency, and semantic alignment.  For each method, it provides quantitative scores based on CLIP-I, DINO, and DreamSim metrics for both environment and character consistency, and CLIP-T for semantic alignment.  Higher scores for CLIP-I and DINO indicate better consistency, while lower scores for DreamSim indicate better consistency.  Higher scores for CLIP-T represent better semantic alignment.  The table also includes qualitative examples of image generation for each method, illustrating the visual differences in environment and character consistency.  UNBOUNDED demonstrates superior performance across all three evaluation aspects.", "section": "4.1 EVALUATION BENCHMARKS"}, {"figure_path": "2410.18975/tables/table_10_0.md", "caption": "Comparison of UNBOUNDED and other methods for maintaining environment consistency and character consistency. UNBOUNDED achieves the best performance in maintaining consistency, while maintaining comparable semantic alignment with the text prompt. Best scores are in bold.", "description": "The table compares UNBOUNDED with three other methods (IP-Adapter, IP-Adapter-Instruct, and Story Diffusion) across three evaluation metrics: environment consistency, character consistency, and semantic alignment.  Environment consistency is measured using CLIP-I, DINO, and DreamSim scores; character consistency uses the same metrics with a different suffix; and semantic alignment is assessed with CLIP-T. Each method's performance on each metric is represented by three scores, with the best scores for each metric bolded.  The table also includes visual examples illustrating the different methods' performance on environment and character consistency for a variety of character and environment types.", "section": "4.1 Evaluation Benchmarks"}, {"figure_path": "2410.18975/tables/table_11_0.md", "caption": "Table 3: Comparison of UNBOUNDED and different LLMs on serving as game engines for open-ended interactions and integrated game mechanics. We use GPT-4 to provide pairwise scores between our model and other LLMs.", "description": "This table compares the performance of UNBOUNDED's game engine against several other large language models (LLMs) across various aspects.  The comparison uses GPT-4 to provide pairwise scores, evaluating the models' ability to handle state updates, environment relevance, story coherence, and instruction following.  The table shows that UNBOUNDED (ours) outperforms other smaller LLMs (Gemma-2B, Gemma-7B, Llama3.2-3B), even exceeding the performance of a larger LLM (Ours-1k), and achieves comparable performance to GPT-4 across all aspects. The table also includes a comparison using a smaller training dataset for UNBOUNDED, suggesting that performance is scalable with data size.", "section": "5.3 EFFECTIVENESS OF DISTILLING SPECIALIZED LARGE LANGUAGE MODEL"}]