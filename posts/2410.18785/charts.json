[{"figure_path": "2410.18785/charts/charts_5_0.png", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model's abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "This chart displays the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K).  The x-axis represents the number of edits performed, shown on a logarithmic scale, while the y-axis shows the performance score for each method on each benchmark. The chart reveals that PMET and MEND methods are most effective in preserving model abilities across all tasks with increasing edits.  In contrast, the KN method shows a significant performance drop with a relatively small number of edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/charts/charts_6_0.png", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "The chart displays the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K).  The x-axis represents the number of edits performed, shown on a logarithmic scale, while the y-axis represents the score achieved on each benchmark.  Each line in the graph corresponds to a different benchmark task. The chart shows that PMET and MEND generally maintain model performance across all tasks better than other methods, while KN demonstrates a significant performance drop with a small number of edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/charts/charts_6_1.png", "caption": "Figure 4: Quantitative results of exploring the impact of model scale on edited language models. We perform editing with different-size models in Pythia model families with ROME (the first row) and MEMIT (the second row), and then these models are evaluated across diverse benchmarks.", "description": "This chart displays the quantitative results of an experiment exploring the impact of model scale on the performance of edited language models.  It uses two different editing methods (ROME and MEMIT) on several language models from the Pythia family with varying parameter scales (160M to 12B). The models' performance is then evaluated across four different benchmarks (MMLU, GSM8K, BBH, CSQA) showing the score for each benchmark in relation to the number of edits performed on each model. This allows for comparison of editing methods across models of varying sizes.", "section": "4.3 RQ3: Do the General Abilities of the Edited Model Differ on Model Scales?"}, {"figure_path": "2410.18785/charts/charts_7_0.png", "caption": "Figure 5: Evaluation of different kinds of general capabilities of edited language models. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks.", "description": "The chart displays the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) across four different benchmark tasks (MMLU, GSM8K, BBH, and CSQA) on the Llama2-7B base language model. Each line represents a distinct editing method, showing its performance score on each task as the number of edits increases. The chart illustrates how different methods affect different aspects of the model's capabilities at various numbers of edits. The performance change on different tasks under various edits is shown by the chart, indicating the general capabilities of the model under various edits.", "section": "4.4 RQ4: How Does Editing Affects Different Aspects of a Model\u2019s Capabilities?"}, {"figure_path": "2410.18785/charts/charts_8_0.png", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "The figure displays performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K).  Each sub-chart shows the performance score on a specific benchmark as the number of edits increases from 10 to 1000. The results indicate that PMET and MEND methods effectively preserve the model\u2019s abilities across all tasks, while the KN method shows a drastic drop in performance even after fewer than ten edits. The remaining methods show varying degrees of performance degradation as the number of edits increases.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/charts/charts_21_0.png", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "The chart displays the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K).  Each line represents a different editing method, and the x-axis shows the number of edits performed. The y-axis represents the score achieved on each benchmark task, indicating performance. The chart demonstrates that PMET and MEND generally maintain model performance across all tasks with increased edits, while KN experiences a significant performance drop with only a few edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}, {"figure_path": "2410.18785/charts/charts_21_1.png", "caption": "Figure 2: Performance trends of evaluating edited Llama2-7B base model across different benchmarks using six editing methods. Results reveal that PMET and MEND can effectively preserve the model\u2019s abilities across all tasks. While KN drastically drops even less than ten edits.", "description": "The chart displays the performance trends of six different model editing methods (ROME, MEMIT, PMET, GRACE, MEND, and KN) on the Llama2-7B base model across five benchmark tasks (CSQA, MMLU, BBH, GSM8K). Each line in the chart represents a single editing method, and the x-axis shows the number of edits. The y-axis represents the score achieved on each task, indicating the model\u2019s performance after a certain number of edits. The chart demonstrates that PMET and MEND generally preserve the model\u2019s abilities across all tasks, while KN shows a significant performance drop after fewer than 10 edits.  The other methods (ROME and GRACE) show varying degrees of performance degradation with increasing numbers of edits.", "section": "4.1 RQ1: Impact of the Number of Edits"}]