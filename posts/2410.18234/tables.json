[{"figure_path": "2410.18234/tables/table_9_0.md", "caption": "Table 1: Block efficiency achieved in the Dolly task for different number of draft models.", "description": "This table presents the block efficiency results for the Dolly task using the proposed importance sampling (IS) scheme and two baseline methods, SpecInfer and SpecTr, with varying numbers of draft models (K).  The block efficiency metric represents the average number of tokens accepted per use of the draft model.  The results show that the IS scheme consistently outperforms the baselines across all tested numbers of draft models (K = 2, 3, 4, 5, 6). The numbers are averages over four randomly chosen seeds and are presented with their standard deviation.", "section": "5.1 IDENTICAL DRAFT MODELS"}, {"figure_path": "2410.18234/tables/table_10_0.md", "caption": "Table 2: Effect of LP Truncation and Alphabet Truncation", "description": "This table presents the impact of applying LP truncation and alphabet truncation techniques on the model's performance.  It shows the block efficiency and the percentage improvement in token rate relative to the single-draft baseline across different truncation parameters. Specifically, it varies the alphabet truncation size (2\u2300) from 10 to 50 and the LP-truncation threshold (s) from 5 to 15, demonstrating how these parameters affect block efficiency and token rate.  The results indicate that increasing the vocabulary size (up to 40) leads to improvements in both metrics, while the token rate is not highly sensitive to LP-truncation threshold.", "section": "5.1 IDENTICAL DRAFT MODELS"}, {"figure_path": "2410.18234/tables/table_37_0.md", "caption": "Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts.", "description": "The table compares the average acceptance probability across different tasks (XSum and Dolly) for varying numbers of draft models (K=2, 4, 8).  It presents the results for four different methods: the theoretically optimal acceptance probability (shown in gray), Importance Sampling (IS), SpecTr, and SpecInfer.  The acceptance probability is a measure of how closely the draft model distribution matches the target model distribution, with higher values indicating better performance. The table shows that, across all tasks and numbers of drafts, the Importance Sampling method consistently achieves a higher acceptance probability than SpecTr and SpecInfer, getting closest to the optimal results.", "section": "H.1 DRAFT ACCEPTANCE PROBABILITY"}, {"figure_path": "2410.18234/tables/table_37_1.md", "caption": "Table 4: Block Efficiency achieved in the Dolly Task with top-k sampling", "description": "Table 4 presents a comparison of block efficiency achieved by different multi-draft speculative sampling methods on the Dolly task, using top-k sampling with k=10 and k=5, and a temperature of 1.0 for both draft and target models.  The table compares the block efficiency and percentage loss (relative to the IS method) for the IS (Importance Sampling), SpecTr, and SpecInfer methods, with results shown for both 2 and 3 draft models. The IS scheme consistently achieves a higher block efficiency, although the gains are relatively small, explained by the fact that baseline schemes already exhibit acceptance probabilities close to the theoretical limit.", "section": "5.1 IDENTICAL DRAFT MODELS"}, {"figure_path": "2410.18234/tables/table_38_0.md", "caption": "Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts.", "description": "Table 3 presents a comparison of the average acceptance probability across three different tasks (XSum, Dolly, and WMT) for various numbers of drafts (K=2, 4, 8) and different decoding methods (optimal, IS, SpecTr, and SpecInfer).  The optimal acceptance probability is calculated theoretically.  The table shows that the IS method consistently outperforms SpecTr and SpecInfer across all tasks and numbers of drafts, and that the acceptance probability generally increases as the number of drafts increases.", "section": "Additional Experimental Results"}, {"figure_path": "2410.18234/tables/table_38_1.md", "caption": "Comparison of average acceptance probability across different tasks for K = 2, 4, 8 drafts.", "description": "Table 3 presents a comparison of average acceptance probabilities across different tasks (XSum, Dolly, WMT) and varying numbers of drafts (K=2, 4, 8).  For each task and draft count, the table shows the acceptance probabilities obtained using the optimal scheme (in gray), the proposed importance sampling (IS) scheme, and two baseline methods: SpecTr and SpecInfer.  The results demonstrate that the acceptance probability increases with the number of drafts and that the proposed IS scheme consistently outperforms the baseline methods.", "section": "H.1 DRAFT ACCEPTANCE PROBABILITY"}, {"figure_path": "2410.18234/tables/table_38_2.md", "caption": "Table 7: ROUGE-L scores on the XSum task across various decoders and sampling temperatures.", "description": "This table presents ROUGE-L scores for the XSum dataset across different decoding methods and sampling temperatures.  The decoding methods compared include the proposed Importance Sampling (IS) scheme, SpecInfer, SpecTr, and a single-draft baseline.  The temperature of the first draft model is held constant at 1.2, while the temperature of the second draft model is varied (1.2, 1.6, 2.0, and 2.4), allowing for a comparison of performance under different temperature settings. The table shows that the IS method generally achieves higher ROUGE-L scores than the other methods.", "section": "5.2 NON-IDENTICAL DRAFT MODELS"}, {"figure_path": "2410.18234/tables/table_38_3.md", "caption": "Table 8: BLEU scores on the WMT dataset across various decoders and sampling temperatures.", "description": "This table presents BLEU scores achieved on the WMT dataset using different decoding methods.  The scores are compared across varying sampling temperatures for the draft models (Draft 1 and Draft 2), showing the performance of the Importance Sampling (IS) scheme against single-draft speculative decoding and two other multi-draft baselines, SpecInfer and SpecTr.  The temperature of the first draft model is fixed at 1.2, while the temperature of the second draft model is varied (1.2, 1.6, 2.0, 2.4).  A final column shows the results for single draft decoding for comparison. The table highlights the performance of each decoding method under different draft model temperature settings.", "section": "5.2 NON-IDENTICAL DRAFT MODELS"}]