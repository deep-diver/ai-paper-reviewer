[{"figure_path": "2410.17242/figures/figures_2_0.png", "caption": "Figure 1: LVSM supports feed-forward novel view synthesis from sparse posed image inputs (even from a single view) on both objects and scenes. LVSM achieves significant quality improvements compared with the previous SOTA method, i.e., GS-LRM (Zhang et al., 2024). (Please zoom in for more details.)", "description": "The figure showcases the Large View Synthesis Model's (LVSM) novel view synthesis capabilities on both scene-level and object-level data.  The top row presents scene-level examples, comparing input images, results from the GS-LRM (state-of-the-art baseline), the LVSM's output, and the ground truth. The bottom row demonstrates the same comparison for object-level data.  Both rows visually highlight LVSM's ability to generate high-quality novel views from sparse inputs, surpassing the GS-LRM in terms of visual fidelity and detail, even when using a single input image.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17242/figures/figures_4_0.png", "caption": "LVSM model architecture. LVSM first patchifies the posed input images into tokens. The target view to be synthesized is represented by its Pl\u00fccker ray embeddings and is also tokenized. The input view and target tokens are sent to a full transformer-based model to predict the tokens that are used to regress the target view pixels. We study two LVSM transformer architectures, as a Decoder-only architecture (left) and a Encoder-Decoder architecture (right).", "description": "This figure illustrates the two main architectures of the Large View Synthesis Model (LVSM): an encoder-decoder architecture and a decoder-only architecture.  Both architectures begin by processing input images and target view Pl\u00fccker rays into tokens.  In the encoder-decoder architecture, an encoder transformer maps the input image tokens into a fixed number of 1D latent tokens, which act as an intermediate scene representation. These latent tokens, along with the target view tokens, are then fed into a decoder transformer to generate the final image tokens. These tokens are then processed to create the synthesized target view.  The decoder-only architecture bypasses the intermediate scene representation, directly mapping the input and target view tokens through a decoder-only transformer to produce the output image. Both pathways use linear and unpatchify layers to create the final output.", "section": "3.1 OVERVIEW"}, {"figure_path": "2410.17242/figures/figures_7_0.png", "caption": "Figure 3: Object-level visual comparison at 512 resolution. Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3D's Triplane-LRM (Li et al., 2023) and GS-LRM (Res-512) (Zhang et al., 2024) . Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example).", "description": "The figure is a visual comparison of object-level novel view synthesis results at 512 resolution.  It shows five columns: the first shows four sparse input images; subsequent columns show the novel view synthesis results generated by Triplane-LRM, GS-LRM, the encoder-decoder version of LVSM, and the decoder-only version of LVSM, respectively; the last column contains the ground truth.  Each row presents a different object, showcasing the results on various object shapes and materials, highlighting the superiority of LVSM in handling complex geometry, fine details, and reducing artifacts compared to the baseline methods.", "section": "4.3 EVALUATION AGAINST BASELINES"}, {"figure_path": "2410.17242/figures/figures_8_0.png", "caption": "Scene-level visual comparison. We evaluate our encoder-decoder and decoder-only models on scene-level view synthesis, comparing them against the prior leading baseline methods, namely pixelSplat (Charatan et al., 2024), MVSplat (Chen et al., 2024), and GS-LRM (Zhang et al., 2024). Our methods exhibit fewer texture and geometric artifacts, generate more accurate and realistic specular reflections, and are closer to the ground truth images.", "description": "The figure presents a visual comparison of scene-level view synthesis results.  It shows input images and the results generated by pixelSplat, MVSplat, GS-LRM, LVSM's encoder-decoder model, and LVSM's decoder-only model, alongside the ground truth images for five different scenes. Each scene contains several novel views generated by each model, allowing for a direct comparison of their performance. The comparison highlights the superior visual quality of LVSM's models, particularly in terms of reducing texture and geometric artifacts, producing more accurate specular reflections, and achieving greater realism compared to the baseline methods.", "section": "4 Experiments"}, {"figure_path": "2410.17242/figures/figures_16_0.png", "caption": "Figure 3: Object-level visual comparison at 512 resolution. Given 4 sparse input posed images (leftmost column), we compare our high-res object-level novel-view rendering results with two baselines: Instant3D\u2019s Triplane-LRM (Li et al., 2023) and GS-LRM (Res-512) (Zhang et al., 2024) . Both our Encoder-Decoder and Decoder-Only models exhibit fewer floaters (first example) and fewer blurry artifacts (second example), compared to the baselines. Our Decoder-Only model effectively handles complex geometry, including small holes (third example) and thin structures (fourth example). Additionally, it preserves the details of high-frequency texture (last example).", "description": "This figure shows a qualitative comparison of novel view synthesis results on object-level datasets at 512 resolution.  The leftmost column displays the four sparse input images used for each object.  Subsequent columns present the novel views generated by Triplane-LRM, GS-LRM, LVSM\u2019s encoder-decoder model, and LVSM\u2019s decoder-only model, respectively.  The final column shows the ground truth images.  The figure highlights that both LVSM models produce superior results compared to the baselines, demonstrating fewer artifacts (e.g., floaters, blurry areas) and better preservation of fine details, especially in complex geometries and high-frequency textures.  The decoder-only model exhibits particularly impressive results.", "section": "4.3 EVALUATION AGAINST BASELINES"}]