[{"figure_path": "2410.17242/tables/table_7_0.md", "caption": "Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution.", "description": "This table presents a quantitative comparison of the proposed LVSM model with various baseline methods on both object-level and scene-level view synthesis tasks.  The object-level results are shown on the left and the scene-level results on the right.  For object-level evaluation, PSNR, SSIM, and LPIPS metrics are reported for the ABO and GSO datasets at resolutions of 256 and 512 pixels.  Different model variants of LVSM (encoder-decoder and decoder-only) are compared against several state-of-the-art methods, such as Triplane-LRM and GS-LRM.  The scene-level evaluation uses the RealEstate10K dataset at a resolution of 256 pixels, and the same metrics are used for comparison. The table highlights the superior performance of the proposed decoder-only LVSM in comparison to other baselines, achieving significant gains in PSNR across all datasets and resolutions.", "section": "4.3 EVALUATION AGAINST BASELINES"}, {"figure_path": "2410.17242/tables/table_9_0.md", "caption": "Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution.", "description": "This table presents a quantitative comparison of object-level and scene-level view synthesis results.  The left half shows object-level results, comparing the proposed LVSM's encoder-decoder and decoder-only models (at both 256 and 512 resolutions) against baselines such as Triplane-LRM and GS-LRM on the ABO and GSO datasets, using PSNR, SSIM, and LPIPS metrics. The right half displays scene-level results on the RealEstate10K dataset (256 resolution), comparing the LVSM models with baselines such as pixelNeRF, GPNR, Du et al., pixelSplat, MVSplat, and GS-LRM, again using PSNR, SSIM, and LPIPS metrics.  The table highlights the superior performance of the LVSM decoder-only model, particularly at higher resolutions.", "section": "4.3 EVALUATION AGAINST BASELINES"}, {"figure_path": "2410.17242/tables/table_9_1.md", "caption": "Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution.", "description": "Table 1 presents a quantitative comparison of object-level and scene-level view synthesis results. The left side shows object-level results at resolutions of 256 and 512, comparing the proposed LVSM's encoder-decoder and decoder-only models against baselines like Triplane-LRM and GS-LRM.  Metrics include PSNR, SSIM, and LPIPS. The right side displays scene-level results at a resolution of 256, comparing the LVSM models against other baselines such as pixelNeRF, GPNR, Du et al., pixelSplat, MVSplat, and GS-LRM, using the same metrics.", "section": "4.3 EVALUATION AGAINST BASELINES"}, {"figure_path": "2410.17242/tables/table_17_0.md", "caption": "Quantitative comparisons on object-level (left) and scene-level (right) view synthesis. For the object-level comparison, we matched the baseline settings with GS-LRM (Zhang et al., 2024) in both input and rendering under both resolution of 256 (Res-256) and resolution of 512 (Res-512). For the scene-level comparison, we use the same validation dataset used by pixelSplat (Charatan et al., 2024), which has 256 resolution.", "description": "This table presents a quantitative comparison of the proposed LVSM model's performance against several baseline methods on both object-level and scene-level view synthesis tasks.  The object-level results are shown on the left, comparing PSNR, SSIM, and LPIPS metrics across different models at two resolutions (256 and 512). The scene-level results are presented on the right, using PSNR, SSIM, and LPIPS metrics at a single resolution of 256.  Baselines include Triplane-LRM, GS-LRM, PixelNeRF, GPNR, Du et al., pixelSplat, and MVSplat.  The table showcases the superior performance of both the encoder-decoder and decoder-only variants of LVSM, particularly the decoder-only model which significantly outperforms the baselines in terms of PSNR.", "section": "4.3 EVALUATION AGAINST BASELINES"}]