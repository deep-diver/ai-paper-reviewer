[{"figure_path": "2410.17779/tables/table_6_0.md", "caption": "EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table 1 presents a comprehensive comparison of various vision-language models on the ScienceQA dataset.  It categorizes models into zero/few-shot methods, full training methods, and parameter-efficient fine-tuning (PEFT) methods using LLAMA and LLAMA2. For each method, it lists the number of trainable parameters, the context modality used (text, image, or both), the grade level of the questions, and the average accuracy achieved across different science subjects (Natural Science, Social Science, Language Science).  The table provides a detailed breakdown of performance across various grade levels and subject categories.  The results highlight the superiority of the proposed ADEM-VL model in terms of accuracy and efficiency.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_7_0.md", "caption": "EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "The table presents a comprehensive comparison of various vision-language (VL) models on the ScienceQA dataset, categorized into zero-/few-shot methods, full training methods, and parameter-efficient fine-tuning (PEFT) methods using LLaMA and LLaMA2.  For each method, it lists the number of trainable parameters, the language model used (LLaMA or LLaMA2), and the average accuracy across different subjects (Natural Science, Social Science, Language Science), context modalities (text-only, image-text, no context), and grade levels (1-6, 7-12).  This allows for a detailed comparison of performance and efficiency across various approaches to VL model tuning.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_7_1.md", "caption": "EVALUATION RESULTS ON COCO CAPTION USING THE KARPATHY TEST SPLIT WITH LLAMA-13B AS THE LANGUAGE MODEL. #T. = TRAINABLE PARAMETERS. *PEFT METHODS.", "description": "The table presents a quantitative comparison of different vision-language models on the COCO Caption dataset using the Karpathy test split.  The models are evaluated based on their performance using two metrics: BLEU-4 and CIDEr. The table includes both full training methods and parameter-efficient fine-tuning (PEFT) methods, with the number of trainable parameters (#T.) reported for each.  The results show the BLEU-4 and CIDEr scores achieved by each model.  The table highlights that ADEM-VL achieves comparable performance to larger, fully trained models while using significantly fewer trainable parameters.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_8_0.md", "caption": "EVALUATION RESULTS ON THE MME BENCHMARK WITH LLAMA-13B AS THE LANGUAGE MODEL. MME-C AND MME-P MEASURE THE PERCEPTION AND COGNITION ABILITIES OF THE MODEL, RESPECTIVELY. EXTRA TOKENS REFER TO THE NUMBER OF ADDITIONAL TOKENS PROCESSED BY THE LLM BEYOND THE STANDARD TEXT TOKENS. #T. = TRAINABLE PARAMETERS. *PEFT METHODS.", "description": "Table III presents the quantitative results of various vision-language models on the MME benchmark, focusing on the LLaMA-13B language model.  The table compares full training methods (LLaVA) with several parameter-efficient fine-tuning (PEFT) methods, including Prompt-Aware Adapter, MiniGPT-4, LayerNorm, and ADEM-VL. For each model, it lists the number of trainable parameters (#Trainable param), the number of additional tokens processed beyond the standard text tokens (#Extra tokens), and the scores obtained for the perception (MME-P) and cognition (MME-C) aspects of the MME benchmark. The table highlights ADEM-VL's competitive performance with a significantly reduced number of trainable parameters and only one extra token, emphasizing its efficiency compared to other methods.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_8_1.md", "caption": "Comparison among different VL models on more image understanding tasks. * Baseline results evaluated through our implementation using the official checkpoint.", "description": "This table compares the performance of various Vision-Language (VL) models on several image understanding benchmarks.  It includes results for full training methods (LLaVA, mPLUG-Owl2, InternLM-XComposer2, MoE-LLaVA) and parameter-efficient fine-tuning (PEFT) methods (MiniGPT-4, LaVIN, ADEM-VL). The metrics used are VQAv2, GQA, MMB, and MMMU, which measure different aspects of image understanding capabilities. The table shows the number of trainable parameters and the underlying Language Model (LLM) used for each method, with the results showing ADEM-VL achieves comparable performance with fewer trainable parameters, especially when compared to full training methods.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_0.md", "caption": "TABLE V\nTRAINING AND INFERENCE SPEED OF DIFFERENT APPROACHES. MEMORY-SAVING OR SPEED-UP APPROACHES SUCH AS CHECKPOINTING AND\nFLA SHATTENTION ARE NOT ADOPTED. FLOPS ARE ESTIMATED FOR GENERATING A SINGLE NEW TOKEN WITH A TEXT SEQUENCE LENGTH OF 256.\nEXPERIMENTS ON COCO CAPTIONING AND INSTRUCTION-FOLLOWING WERE NOT IMPLEMENTED IN THE ORIGINAL PAPERS OF LLAVA-LORA AND\nMEMVP, SO THE OVERALL TRAINING TIME FOR THESE TASKS IS UNAVAILABLE.", "description": "This table presents a comparison of the training and inference speed of several different vision-language models.  The models are compared across three metrics: the number of trainable parameters (#Param), the number of FLOPs required to generate a single token, and the time taken for training and inference per batch. The table includes both the total training time and the time per batch for the ScienceQA dataset but only provides time per batch for COCO Caption and instruction-following tasks as the original papers for LLaVA-LoRA and MemVP did not report these overall training times.  The models compared are LLaVA-LoRA, LaVIN, MemVP, and ADEM-VL, with variations in model size (7B and 13B parameters).", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_1.md", "caption": "EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table 1 presents a comprehensive comparison of various vision-language (VL) models on the ScienceQA dataset.  The table is organized to showcase the performance of different approaches, categorized as zero/few-shot methods, full training methods, and parameter-efficient fine-tuning (PEFT) methods using LLAMA and LLAMA2. For each model, the table lists the number of trainable parameters (#Param), the context modality used (TXT, IMG, NO), the grade levels (G1-6, G7-12), and the average accuracy achieved across different subject areas (NAT, SOC, LAN).  The table highlights the superior performance of ADEM-VL compared to existing methods, particularly its higher accuracy and efficiency.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_2.md", "caption": "EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table 1 presents a comparison of various methods for solving the ScienceQA dataset, categorized into zero/few-shot methods, full training methods, and parameter-efficient fine-tuning (PEFT) methods using LLAMA and LLAMA2.  For each method, the table lists the number of trainable parameters, the context modality used (text, image, or both), the grade level of the questions, and the average accuracy achieved across different subjects (Natural Science, Social Science, Language Science) and grade levels.  The table highlights the superior performance of the proposed ADEM-VL method compared to other approaches, particularly in achieving high accuracy with fewer parameters.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_9_3.md", "caption": "EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table 1 presents a comparison of various vision-language (VL) models on the ScienceQA dataset, categorized by zero/few-shot methods, full training methods, and parameter-efficient fine-tuning (PEFT) methods using LLAMA and LLAMA2.  For each model, the table shows the number of trainable parameters, context modality (text only, image only, image and text, or no context), grade level (1-6 or 7-12), and average accuracy across different subjects (natural science, social science, language science)  The results highlight the performance of different models and the impact of PEFT approaches in improving accuracy while reducing the number of trainable parameters.  Several baselines are included for comparison, including results from human evaluations.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_10_0.md", "caption": "EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table 1 presents a comparison of various vision-language (VL) models on the ScienceQA dataset, categorized by method type (zero-/few-shot, full training, parameter-efficient fine-tuning (PEFT)), model size (in number of parameters), and context modality (text-only, image-only, text and image, no context).  For each model, the table provides average accuracy scores broken down by subject (natural, social, and language sciences), grade level (1-6 and 7-12), and overall average accuracy.  This allows for a comparison of performance across different approaches and model scales, highlighting the effectiveness of different methods in achieving high accuracy on the ScienceQA task.", "section": "IV. EXPERIMENT"}, {"figure_path": "2410.17779/tables/table_10_1.md", "caption": "EVALUATION RESULTS ON SCIENCEQA TEST SET. NAT = NATURAL SCIENCE, SOC = SOCIAL SCIENCE, LAN = LANGUAGE SCIENCE, TXT = TEXT CONTEXT, IMG = IMAGE CONTEXT, NO = NO CONTEXT, G1-6 = GRADES 1-6, G7-12 = GRADES 7-12.", "description": "Table I presents a quantitative comparison of various vision-language (VL) models on the ScienceQA dataset, categorized by the method used (zero/few-shot, full training, or parameter-efficient fine-tuning (PEFT)), the size of the language model (LLM) used (in terms of the number of parameters), and the context modality considered (text only, image only, or both).  The table shows average accuracy scores broken down by subject matter (natural science, social science, language science), text context, image context, no context, and grade levels (1-6 and 7-12). This allows for detailed comparisons of performance across different approaches and configurations, highlighting the effectiveness of different methods in handling the task of science question answering.", "section": "IV. EXPERIMENT"}]