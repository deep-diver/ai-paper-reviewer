{"references": [{" publication_date": "2021", "fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "reason": "This paper introduces a benchmark dataset (MATH) for evaluating mathematical problem-solving abilities in large language models (LLMs). It's highly relevant to this study because it directly addresses the core issue of LLMs' performance in mathematical tasks, providing a standardized method to assess and compare the capabilities of various LLMs.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "reason": "This work introduces the GSM8K dataset, a benchmark for evaluating LLMs' performance in solving mathematical word problems.  Its relevance to this study lies in its focus on a challenging aspect of mathematical reasoning, which aligns with the research's examination of LLMs' limitations in arithmetic.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Nayoung Lee", "paper_title": "Teaching arithmetic to small transformers", "reason": "This paper presents a method for training LLMs to perform arithmetic calculations using a technique called 'Scratchpad'.  It's significant to this study because it directly addresses the issue of improving LLMs' arithmetic capabilities, which is a core theme of the current research.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Zhen Yang", "paper_title": "GPT can solve mathematical problems without a calculator", "reason": "This paper demonstrates the ability of LLMs to solve multi-digit multiplication problems.  Its significance to this study is that it establishes a benchmark for the complexity level of arithmetic problems that are considered challenging for LLMs, setting the context for examining why such difficulties arise.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yuntian Deng", "paper_title": "Implicit chain of thought reasoning via knowledge distillation", "reason": "This research shows how to improve LLMs' performance in mathematical tasks by using an internalized chain-of-thought (CoT) reasoning process.  This is relevant to the current study because CoT methods are closely related to the authors' investigation into how LLMs handle the decomposition of complex problems into subgroups.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yuntian Deng", "paper_title": "From explicit cot to implicit cot: Learning to internalize cot step by step", "reason": "This paper expands on previous work by exploring the internalization of chain-of-thought (CoT) reasoning in LLMs for arithmetic tasks. This connects to the current study's investigation of LLMs' symbolic learning by examining the way they decompose complex tasks into smaller, manageable steps.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Alessandro Stolfo", "paper_title": "A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis", "reason": "This study uses causal mediation analysis (CMA) to understand how different components of LLMs contribute to their arithmetic reasoning. This is highly relevant to the current study's focus on exploring the internal mechanisms underlying LLMs' performance in arithmetic.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Michael Hanna", "paper_title": "How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model", "reason": "This work investigates the interpretability of LLMs' mathematical reasoning, specifically focusing on the greater-than comparison. This is pertinent to the current study because it shares the focus on understanding the underlying mechanisms of how LLMs approach mathematical problems.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Sasha Boguraev", "paper_title": "Models can and should embrace the communicative nature of human-generated math", "reason": "This paper highlights the importance of considering the communicative nature of human-generated mathematical expressions when assessing LLMs' abilities in arithmetic. This insight supports the current study by emphasizing the importance of understanding the symbolic representations used in arithmetic, rather than simply focusing on computational aspects.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Abhimanyu Dubey", "paper_title": "The llama 3 herd of models", "reason": "This paper introduces Llama-3-8B, one of the LLMs used in the current study. Understanding the architecture and training data for Llama-3-8B is crucial for interpreting the results obtained in this research.", "section_number": 5}, {" publication_date": "2024", "fullname_first_author": "OpenAI", "paper_title": "GPT-4 technical report", "reason": "This report provides detailed information about the GPT-4 model, which is a leading LLM mentioned in this study. The knowledge of GPT-4's capabilities and architecture is relevant for understanding the context of the research and the limitations of current LLMs in arithmetic.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Gemma Team", "paper_title": "Gemma 2: Improving open language models at a practical size", "reason": "This paper introduces Gemma-2-2B, one of the LLMs used in the current study. It's crucial for understanding the model's characteristics to interpret the study's results and ensure reproducibility.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Aitor Lewkowycz", "paper_title": "Solving quantitative reasoning problems with language models", "reason": "This paper explores techniques for improving the performance of language models on quantitative reasoning tasks. This study is directly relevant to the current research's investigation into methods for improving LLMs' abilities in mathematical tasks.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Sowmya S. Sundaram", "paper_title": "Why are NLP models fumbling at elementary math?", "reason": "This survey paper provides a valuable overview of existing research on LLMs and mathematical reasoning. It's highly relevant to the current study because it sets the stage for the research by identifying existing limitations and gaps in the field.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Siyuan Guo", "paper_title": "Learning beyond pattern matching? Assaying mathematical understanding in LLMs", "reason": "This paper explores the mathematical understanding capabilities of LLMs. It is directly related to this study because both studies investigate the underlying mechanisms of LLMs' performance in mathematical tasks.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yilun Zhao", "paper_title": "Financemath: Knowledge-intensive math reasoning in finance domains", "reason": "This paper focuses on financial mathematical reasoning, a specific application of mathematical skills.  The study's relevance to the current research is its emphasis on the application of LLMs to solve complex mathematical tasks, which is directly pertinent to the current study's goal of understanding the mechanisms behind LLMs' mathematical performance.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Yilun Zhao", "paper_title": "Docmath-eval: Evaluating math reasoning capabilities of LLMs in understanding long and specialized documents", "reason": "This work provides an evaluation framework for assessing LLMs' math reasoning abilities within long and specialized documents. This relates to the current study by emphasizing that LLMs' capabilities in mathematical reasoning are context-dependent, something the current study also explores through different subgroups of arithmetic problems.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Xiang Yue", "paper_title": "Mammoth: Building math generalist models through hybrid instruction tuning", "reason": "This paper presents Mammoth, a model designed to improve LLMs' general mathematical reasoning capabilities. Its relevance to the current study comes from the common goal of enhancing LLMs' mathematical skills. The techniques used in Mammoth can inform future research on enhancing LLMs' arithmetic capabilities.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Jason Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "reason": "This paper introduces chain-of-thought (CoT) prompting, a technique that has shown significant improvement in LLMs' ability to solve complex reasoning problems. The current study directly relates to this research as it explores the effect of symbolic learning which is facilitated by CoT prompting.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Wei Zhang", "paper_title": "Interpreting and improving large language models in arithmetic calculation", "reason": "This study investigates the mechanisms behind LLMs' arithmetic calculation abilities using interpretability techniques. It shares the focus on understanding how LLMs perform arithmetic and complements this study by exploring the use of different interpretability methods for a better understanding of LLM performance.", "section_number": 2}]}