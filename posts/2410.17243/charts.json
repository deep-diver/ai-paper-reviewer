[{"figure_path": "2410.17243/charts/charts_1_0.png", "caption": "Figure 1: GPU memory usage comparison between Inf-CL and previous methods (CLIP, Open-CLIP). The dashed line marks the common GPU memory limit. Memory costs exceeding the bottleneck of 80G A800 are estimated by curve fitting. Left: With 8\u00d7A800, CLIP and OpenCLIP's memory consumption increases quadratically, while Inf-CL achieves linear growth, reducing memory costs by 78\u00d7 at a batch size of 256k. Right: At a batch size of 1024k, even with 128 GPUs, previous methods exceed memory limits, whereas Inf-CL reduces memory demand by 281\u00d7.", "description": "This figure compares the GPU memory usage of three contrastive learning methods: CLIP, OpenCLIP, and Inf-CL (the proposed method).  The left panel shows memory usage on 8 GPUs (8 x A800) as batch size increases from 64k to 1024k.  CLIP and OpenCLIP demonstrate a rapid quadratic increase in memory usage, quickly surpassing the 80GB A800 memory limit. Inf-CL shows a nearly linear increase, significantly outperforming the other methods.  At a batch size of 256k, Inf-CL reduces memory costs by 78 times compared to CLIP and OpenCLIP. The right panel illustrates memory usage at a fixed large batch size (1024k) as the number of GPUs increases from 1 to 128. While CLIP and OpenCLIP still exceed the memory limit even with 128 GPUs, Inf-CL maintains a manageable memory footprint with a reduction of 281 times compared to the other methods.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.17243/charts/charts_8_0.png", "caption": "Figure 4: Training Speed of ViT-L/14 CLIP on 8\u00d7A800 for Varying Batch Sizes. The left figure shows the time per iteration step, while the right displays the time per epoch. Loss calculation contributes minimally to the total iteration time, making Inf-CL's iteration time comparable to previous methods. Furthermore, the iteration time of Inf-CL scales linearly with batch size, leading to a stable training duration of approximately 59 hours per epoch.", "description": "This chart displays the training speed of the ViT-L/14 CLIP model on 8 A800 GPUs across various batch sizes.  The left-hand side shows the time taken per iteration step, while the right-hand side shows the total time per epoch. The chart compares the training speed of CLIP, OpenCLIP, and Inf-CL (the proposed method).  It reveals that Inf-CL's iteration time scales linearly with batch size, resulting in a consistent training duration of around 59 hours per epoch, despite the increased batch sizes, and that it is comparable in speed to previous approaches (CLIP and OpenCLIP). Out of memory (OOM) errors occur for CLIP and OpenCLIP at higher batch sizes, which Inf-CL avoids.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/charts/charts_15_0.png", "caption": "Figure 5: Performance of ViT-B/32 across Varying Batch Sizes. Except batch size, other experiment settings are consistent. In Figure, the most suitable batch size is increasing with data scale.", "description": "The chart displays the relationship between batch size and accuracy delta for three different datasets (CC3M, CC12M, and Laion400M) using the ViT-B/32 model.  The x-axis represents the batch size (in thousands), and the y-axis represents the change in accuracy (delta).  Each dataset is represented by a different colored line, showing how accuracy changes as batch size increases.  The chart shows that for each dataset, there is a peak accuracy at a specific batch size, after which accuracy decreases as the batch size continues to grow. The optimal batch size is different for each dataset and increases as the scale of the dataset increases. The chart also highlights that the peak point for each dataset occurs at a larger batch size as the size of the dataset increases.", "section": "4 EXPERIMENTS"}]