{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces CLIP, a foundational model in contrastive learning and a key benchmark for image-text multimodal learning. Its significance stems from the introduction of a novel contrastive training approach and the resulting model's remarkable zero-shot transfer capabilities, which have propelled advancements in various applications.  Understanding CLIP is paramount to understanding the present work's context, innovations, and performance.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Ting Chen", "paper_title": "A simple framework for contrastive learning of visual representations", "reason": "This paper introduces a simple yet highly influential framework for contrastive learning, setting a foundation for many subsequent works. Its clarity and effectiveness in learning visual representations from unlabeled data have made it a benchmark and a source of inspiration for many researchers in the field.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "This paper introduces MoCo, a highly influential method in contrastive learning. MoCo's key contribution is a memory bank implementation enabling efficient contrastive learning with significantly larger batch sizes compared to previous methods.  This is directly relevant to the current paper's focus on overcoming the memory limitations of contrastive learning at large batch sizes.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Huaishao Luo", "paper_title": "Clip4clip: Fast image-text models through multi-modal reinforced training", "reason": "This paper builds upon the foundation of CLIP and introduces an approach to adapt CLIP for efficient video-text multimodal learning. This is directly relevant to the present work's context, as it underscores the widespread adoption of CLIP as a foundation model and the challenges of scaling such methods to larger datasets and batch sizes.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Liang Wang", "paper_title": "Text embeddings by weakly-supervised contrastive pre-training", "reason": "This paper explores contrastive learning in the context of dense text retrieval. It highlights the use of contrastive techniques for learning effective text embeddings, which is relevant to the present work's exploration of contrastive methods in a different application domain. This demonstrates that understanding how to improve the effectiveness of contrastive learning is important across domains.", "section_number": 1}, {" publication_date": "2021", "fullname_first_author": "Gabriel Ilharco", "paper_title": "Openclip, July 2021", "reason": "OpenCLIP is a significant contribution to the field of contrastive learning, focusing on making CLIP training more efficient and accessible. The paper\u2019s open-source nature and focus on improving efficiency make it important as a key baseline against which the present work's advancements are compared. The methods employed in OpenCLIP offer useful insight for the current work.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Yihao Chen", "paper_title": "Disco-CLIP: A distributed contrastive loss for memory efficient clip training", "reason": "This paper directly addresses the memory limitations of contrastive loss by using a distributed training approach. It serves as a strong baseline for comparison with the present work's method. The strategies employed in DisCo-CLIP offers useful insights for comparison with the proposed method.", "section_number": 1}, {" publication_date": "2006", "fullname_first_author": "Raia Hadsell", "paper_title": "Dimensionality reduction by learning an invariant mapping", "reason": "This seminal paper lays the groundwork for contrastive learning, introducing the core concept of learning an embedding space where similar data points are close and dissimilar points are far apart. It forms a foundational basis for understanding the broader approach and motivations of the current work. The concept has seen wide adaptation in subsequent contrastive approaches.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Aaron van den Oord", "paper_title": "Representation learning with contrastive predictive coding", "reason": "This paper introduces contrastive predictive coding, a significant approach in representation learning that builds upon the principles of contrastive learning.  It provides a theoretical foundation and context for understanding the broader principles and motivations behind the current research effort. The paper's concept has significant influence in many subsequent contrastive learning approaches.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Luyu Gao", "paper_title": "Scaling deep contrastive learning batch size under memory limited setup", "reason": "This paper directly addresses the memory limitations of contrastive learning when scaling batch size. It proposes solutions to mitigate memory constraints and improve training efficiency, setting the stage for the present work\u2019s exploration of this challenge.  It highlights the existing work's limitations, thus creating a compelling narrative for the proposed research.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Gabriel Ilharco", "paper_title": "Openclip, July 2021", "reason": "This paper introduces OpenCLIP, a significant contribution to the field of contrastive learning, focusing on making CLIP training more efficient and accessible. Its open-source nature and focus on improving efficiency make it important as a key baseline against which the present work's advancements are compared. Understanding the methodology used in OpenCLIP provides useful insight into the current work.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "This paper introduces FlashAttention, a memory-efficient attention mechanism crucial for processing long sequences in transformer models. Its relevance lies in addressing the memory efficiency challenges that are also critical for scaling contrastive learning, showcasing the importance of addressing memory efficiency at multiple levels of deep learning models.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hao Liu", "paper_title": "Ring attention with blockwise transformers for near-infinite context", "reason": "This paper introduces a memory-efficient attention mechanism that enables processing of extremely long sequences in transformer models. This is relevant to the present work as it tackles memory efficiency issues that are also pertinent in contrastive learning, but from a different perspective.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Paulius Micikevicius", "paper_title": "Mixed precision training", "reason": "This paper introduces mixed-precision training, a technique to reduce memory consumption and speed up training by using both single-precision (FP32) and half-precision (FP16) floating-point numbers. This technique is directly relevant to the present work as it helps to reduce the memory usage in training deep learning models. Many large models utilize this technique.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Noam Shazeer", "paper_title": "Adafactor: Adaptive learning rates with sublinear memory cost", "reason": "This paper introduces Adafactor, an optimizer that reduces memory consumption by employing adaptive learning rates.  The ability to efficiently train large-scale models with reduced memory footprints makes Adafactor directly relevant to the challenges addressed by the present work. The paper's concept has significant influence on many subsequent works.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Luyu Gao", "paper_title": "Scaling deep contrastive learning batch size under memory limited setup", "reason": "This paper directly addresses the memory limitations of contrastive learning when scaling the batch size. It proposes solutions to mitigate memory constraints and improve training efficiency, and thus is highly relevant to the current work's exploration of this critical challenge. It also highlights the existing work\u2019s limitations, creating a compelling narrative for the proposed research.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This work empirically explores the scaling laws of neural language models. Although focused on language models, its findings are relevant to the current paper\u2019s exploration of scaling laws for contrastive learning models. Understanding how model performance and computational costs scale with model size and data size has direct implications for contrastive learning.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Ting Chen", "paper_title": "Big self-supervised models are strong semi-supervised learners", "reason": "This paper explores the relationship between model size and performance in self-supervised and semi-supervised learning scenarios. This is relevant to the present work\u2019s focus on scaling contrastive learning models, as understanding the relationship between model scale and performance is vital for determining optimal training strategies.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Chao Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "reason": "This paper focuses on improving the efficiency of visual and vision-language representation learning, an area closely related to contrastive learning. The techniques employed in scaling up the learning process can inform the design of memory-efficient methods in contrastive learning models.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Shen Li", "paper_title": "Pytorch distributed: Experiences on accelerating data parallel training", "reason": "This paper examines techniques to accelerate data-parallel training in PyTorch.  It discusses practical strategies and optimization techniques useful for training large models, which is directly relevant to the challenge of training contrastive learning models with large batch sizes. The techniques described can inform the design of memory-efficient training approaches in the current work.", "section_number": 5}]}