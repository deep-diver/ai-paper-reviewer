{"references": [{" publication_date": "2021", "fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "reason": "This paper introduces CLIP, a foundational model for contrastive learning, which significantly impacted the field and motivated the current research to address its limitations in scalability.  It is highly cited and widely recognized as a key contribution to the field, making it a highly important reference for this paper.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Ting Chen", "paper_title": "A simple framework for contrastive learning of visual representations", "reason": "This paper is a seminal work in contrastive learning, introducing a simple yet effective framework that significantly advanced the field. It is highly influential, heavily cited, and provides the foundational understanding of contrastive learning that the current work builds upon.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Kaiming He", "paper_title": "Momentum contrast for unsupervised visual representation learning", "reason": "This paper introduced MoCo, another highly influential method in contrastive learning.  The development of MoCo and its impact on the field warrants its inclusion as an important reference.  It's a key method in the field and provides context for the current research.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Huaishao Luo", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "reason": "This paper focuses on scaling visual and vision-language representation learning, which is directly relevant to the challenges and solutions explored in the current work. Its focus on scalability and the methods used are directly relevant to the current paper\u2019s main contribution.", "section_number": 1}, {" publication_date": "2022", "fullname_first_author": "Liang Wang", "paper_title": "Text embeddings by weakly-supervised contrastive pre-training", "reason": "This work focuses on contrastive learning for text embeddings, which is a closely related area to the current paper's focus on image-text contrastive learning. The methods and challenges presented are directly relevant to the current paper\u2019s main contribution.", "section_number": 1}, {" publication_date": "2018", "fullname_first_author": "Aaron van den Oord", "paper_title": "Representation learning with contrastive predictive coding", "reason": "This paper provides a foundational understanding of contrastive learning and its applications, serving as an important basis for many subsequent works in the field, including the current research.  It is highly influential and provided important theoretical contributions.", "section_number": 2}, {" publication_date": "2006", "fullname_first_author": "Raia Hadsell", "paper_title": "Dimensionality reduction by learning an invariant mapping", "reason": "This paper is a foundational work in contrastive learning that established many of the core concepts and methods used in the field. It significantly influenced many subsequent works, providing a critical historical perspective.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Elad Hoffer", "paper_title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks", "reason": "This paper addresses the challenges of large-batch training, which are directly relevant to the current work's focus on increasing batch size in contrastive learning. It provides valuable insights into the impact of large batch sizes and training optimization.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Gabriel Ilharco", "paper_title": "Openclip", "reason": "This paper introduces OpenCLIP, a memory-efficient implementation of contrastive loss, which serves as a key baseline and comparison point for the current work. The challenges and tradeoffs in memory-efficient contrastive learning are directly relevant.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Yihao Chen", "paper_title": "Disco-CLIP: A distributed contrastive loss for memory efficient clip training", "reason": "This paper tackles memory efficiency in contrastive learning, making it directly comparable to the current research. The approach and results of this work are critical for benchmarking and comparing the proposed method.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Jared Kaplan", "paper_title": "Scaling laws for neural language models", "reason": "This paper provides insights into scaling laws for neural networks, which are relevant to the current research's focus on scaling batch size in contrastive learning. It offers a broader perspective on the challenges and possibilities of scaling large models.", "section_number": 2}, {" publication_date": "2022", "fullname_first_author": "Tri Dao", "paper_title": "Flashattention: Fast and memory-efficient exact attention with io-awareness", "reason": "This paper addresses memory efficiency in attention mechanisms, a component relevant to contrastive learning methods.  The techniques discussed provide context for the current research, specifically on the memory optimization techniques used in Inf-CL.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "Hao Liu", "paper_title": "Ring attention with blockwise transformers for near-infinite context", "reason": "This paper demonstrates methods for achieving near-infinite context in attention mechanisms, which is relevant to the current research's goal of near-infinite batch size scaling in contrastive learning.  The techniques explored provide inspiration and comparison points.", "section_number": 2}, {" publication_date": "2021", "fullname_first_author": "Chao Jia", "paper_title": "Scaling up visual and vision-language representation learning with noisy text supervision", "reason": "This work directly tackles the challenges of scaling vision-language models, which is directly relevant to the current research's aim to improve the scalability of contrastive learning methods.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Shen Li", "paper_title": "Pytorch distributed: Experiences on accelerating data parallel training", "reason": "This paper provides insights into distributed training, which is directly relevant to the current research's focus on scaling contrastive learning across multiple GPUs. The optimization strategies and challenges discussed are closely aligned with the current work.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Paulius Micikevicius", "paper_title": "Mixed precision training", "reason": "This paper discusses mixed-precision training, a technique to improve efficiency in deep learning training, which is directly relevant to the current work's experimental setup.  The choice of mixed precision training warrants this important reference.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Noam Shazeer", "paper_title": "Adafactor: Adaptive learning rates with sublinear memory cost", "reason": "This paper introduces Adafactor, an optimizer used in the experiments of the current paper. Adafactor's efficiency and characteristics are directly relevant to the experimental results and methodology. It is directly used in the experimental setup and results.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Christoph Schuhmann", "paper_title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs", "reason": "This paper introduces the Laion-400M dataset, which is used in the experiments of the current paper.  The properties and characteristics of this dataset are essential to understanding the experimental results and limitations.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Ting Chen", "paper_title": "Big self-supervised models are strong semi-supervised learners", "reason": "This paper explores the benefits of large-scale self-supervised learning and shows the relation between the scale of the model and the performance.  The findings and analysis are directly relevant to the current research which is trying to increase the scale of contrastive learning.", "section_number": 5}, {" publication_date": "2021", "fullname_first_author": "Gabriel Ilharco", "paper_title": "Openclip", "reason": "This paper introduces OpenCLIP, an alternative implementation of contrastive learning, that is often compared to the proposed Inf-CL.  Direct comparison is done between Inf-CL and OpenCLIP, making it a highly important reference.", "section_number": 5}]}