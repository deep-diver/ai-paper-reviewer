[{"figure_path": "2410.17243/tables/table_6_0.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "This table presents the peak memory cost during training for different models (CLIP, OpenCLIP, and Inf-CL), hardware configurations (8x and 32x A800 GPUs), and batch sizes (32k, 64k, 128k, 256k, and 1024k).  The memory cost is broken down into the loss calculation and the overall peak memory usage.  It demonstrates how Inf-CL significantly reduces the memory consumption for loss calculations compared to CLIP and OpenCLIP, especially as the batch size increases, making training with much larger batch sizes feasible.  The table also shows the impact of using a data offload strategy with Inf-CL, further reducing memory usage.  'X' indicates that the memory limit was exceeded for the given hardware and batch size, preventing training.", "section": "Experiments"}, {"figure_path": "2410.17243/tables/table_6_1.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "The table compares the peak memory consumption (in GB) of three different contrastive loss methods (CLIP, OpenCLIP, and Inf-CL) across varying batch sizes (32k, 64k, 128k, 256k, and 1024k) using 8 and 32 A800 GPUs.  It shows the memory cost for both the loss calculation itself and the overall peak memory usage, including model and data.  The results highlight the significant reduction in memory achieved by Inf-CL, especially as the batch size increases. The table also uses \"X\" to indicate scenarios where the memory requirements exceed the GPU's capacity, rendering training infeasible for the baseline methods. A modified version of Inf-CL, indicated by Inf-CL*, incorporates a data offload strategy for further memory reduction.", "section": "Experiments"}, {"figure_path": "2410.17243/tables/table_7_0.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes", "description": "Table 1 presents the peak GPU memory usage during training for various batch sizes (32k, 64k, 128k, 256k, and 1024k) using 8 and 32 A800 GPUs.  It compares three methods: CLIP (vanilla implementation), OpenCLIP (local loss), and Inf-CL (the proposed method).  The table shows the memory cost (in GB) broken down into loss and peak memory for each method and batch size. For larger batch sizes, the memory usage of CLIP and OpenCLIP exceeds the capacity of the hardware, denoted by \"X\", while Inf-CL shows significantly lower memory consumption.  Inf-CL* indicates the results using a data offload strategy to further reduce memory usage. The experiment uses the ViT-L/14 model architecture and the AdamW optimizer, leveraging data parallelism and automatic mixed precision.", "section": "Experiments"}, {"figure_path": "2410.17243/tables/table_8_0.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes.", "description": "Table 1 presents the peak memory consumption during training for different models (CLIP, OpenCLIP, and Inf-CL) across varying batch sizes (32k, 64k, 128k, 256k, and 1024k) and hardware configurations (8x A800 and 32x A800).  The memory cost is broken down into the loss memory cost (the memory required to store the similarity matrix during contrastive loss calculation) and the total peak memory cost (the total memory consumption, which includes model memory).  'X' indicates that the memory limits of the hardware are exceeded, indicating that training was not feasible under those conditions. The table also shows results of Inf-CL with data offloading (*) which further optimizes memory usage by transferring a small data batch from CPU to GPU during accumulation.", "section": "Experiments"}, {"figure_path": "2410.17243/tables/table_9_0.md", "caption": "Table 3: Performance Verification. The training strategies is consistent with Table 2. We choose ViT-B/16 as the model architecture and adopt LiT strategy like Table 4. We evaluate zero-shot top-1 classification accuracy on several data sets, e.g., ImageNet-Validation Deng et al. (2009), ImageNet-v2 (Recht et al., 2019), ObjectNet (Barbu et al., 2019) and ImageNet-OOD (Hendrycks et al., 2021). We also evaluate zero-shot image-text top-1 retrieval accuracy on MSCOCO (Chen et al., 2015).", "description": "Table 3 presents a performance comparison of different methods on various datasets, including ImageNet validation, ImageNet v2, ObjectNet, ImageNet out-of-distribution (OOD), and MS COCO.  The methods compared are a vanilla implementation of contrastive loss, OpenCLIP, and Inf-CL, each evaluated with different batch sizes (64k, 256k, 1024k).  The table displays zero-shot top-1 classification accuracy for ImageNet datasets and zero-shot image-text top-1 retrieval accuracy (R@1) for MS COCO.  The results demonstrate the performance of Inf-CL, showing comparable or improved performance compared to other methods, especially at larger batch sizes.", "section": "4.3 Performance Analysis"}, {"figure_path": "2410.17243/tables/table_9_1.md", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "This table presents a comparison of peak memory costs during training for different models (CLIP, OpenCLIP, and Inf-CL) under varying batch sizes and hardware configurations (8x or 32x A800 GPUs). It shows the memory usage for both the loss calculation and the overall training process.  The table highlights the significant reduction in memory consumption achieved by Inf-CL compared to the baseline methods (CLIP and OpenCLIP), especially at larger batch sizes.  It also demonstrates that Inf-CL can train with significantly larger batch sizes than the baselines due to its superior memory efficiency, including a variant that incorporates data offloading for further optimization. The 'X' entries indicate that memory limits were exceeded for the specified batch size and GPU configuration, preventing training with baseline methods.", "section": "4 EXPERIMENTS"}, {"figure_path": "2410.17243/tables/table_14_0.md", "caption": "Table 1: Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "This table presents the peak memory cost during training for different models (CLIP, OpenCLIP, and Inf-CL) using various batch sizes (32k, 64k, 128k, 256k, and 1024k) and hardware configurations (8x A800 and 32x A800 GPUs).  It compares the memory usage of the vanilla contrastive loss implementation (CLIP), a memory-efficient local loss approach (OpenCLIP), and the proposed Inf-CL method. The table shows the memory cost for both the loss calculation and the overall peak memory usage, highlighting the significant reduction in memory usage achieved by Inf-CL, especially at larger batch sizes.  The 'X' entries indicate that the baseline methods exceeded the GPU memory limit, making training infeasible. Inf-CL* shows memory usage with the data offload strategy enabled.  This table demonstrates Inf-CL's ability to handle significantly larger batch sizes than competing methods.", "section": "4 Experiments"}, {"figure_path": "2410.17243/tables/table_14_1.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128.  * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "This table presents a comparison of peak memory usage during training for different contrastive loss methods (CLIP, OpenCLIP, and Inf-CL) across varying batch sizes (32k, 64k, 128k, 256k, and 1024k) and hardware configurations (8 and 32 A800 GPUs).  The table shows the peak memory usage in gigabytes (GB) for both the loss calculation and the overall model.  It highlights the significant memory savings achieved by Inf-CL, especially at larger batch sizes, where CLIP and OpenCLIP exceed the available GPU memory.  The table also includes a variant of Inf-CL (*Inf-CL*) that incorporates a data offload strategy to further reduce memory usage at very large batch sizes.", "section": "Experiments"}]