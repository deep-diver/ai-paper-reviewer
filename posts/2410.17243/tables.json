[{"figure_path": "2410.17243/tables/table_7_0.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128. * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "This table presents the peak memory cost during training for different models (CLIP, OpenCLIP, and Inf-CL) across varying batch sizes (32k, 64k, 128k, 256k, and 1024k) and hardware configurations (8 and 32 A800 GPUs).  It shows the memory usage for both the loss calculation and the overall model.  The results highlight that Inf-CL significantly reduces memory consumption compared to the baseline methods, particularly at larger batch sizes, enabling training with batch sizes that would otherwise exceed the GPU memory limits. The table also includes a row for Inf-CL*, showing the memory cost when an additional data offload strategy is utilized, reducing the memory footprint even further.", "section": "Experiments"}, {"figure_path": "2410.17243/tables/table_8_0.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes. Experiments utilize Data Parallelism with Automatic Mixed Precision for efficient distributed training. The baselines include the Vanilla loss (CLIP) and Local loss (OpenCLIP). To minimize memory consumption, Gradient Cache is adopted, with an accumulation batch size of 128.  * indicates the use of the data offload strategy, which reduces memory usage by transferring only a small data batch from CPU to GPU during each accumulation step. X denotes cases where the baseline exceeds the hardware memory limit for a given batch size, making training infeasible. Memory cost is evaluated using the ViT-L/14 architecture and the AdamW optimizer.", "description": "This table presents a comparison of peak memory usage during training for different contrastive loss methods (CLIP, OpenCLIP, and Inf-CL) across varying batch sizes (32k, 64k, 128k, 256k, and 1024k) and hardware configurations (8 and 32 A800 GPUs).  It shows the memory cost (in GB) for both the loss calculation and the overall peak memory usage.  The table highlights Inf-CL's significantly lower memory footprint compared to CLIP and OpenCLIP, particularly at larger batch sizes.  It also demonstrates that Inf-CL can handle batch sizes beyond the capacity of the other methods, even with a relatively small number of GPUs, and that a data offload strategy further improves memory efficiency for Inf-CL.", "section": "Experiments"}, {"figure_path": "2410.17243/tables/table_14_1.md", "caption": "Training Memory Cost Across Different Hardware and Batch Sizes.", "description": "The table presents a comparison of peak memory usage during training for three different contrastive loss methods: CLIP (vanilla implementation), OpenCLIP (local loss), and Inf-CL (the proposed method). It shows the peak memory cost in gigabytes (GB) for each method across varying batch sizes (32k, 64k, 128k, 256k, and 1024k) and hardware configurations (8xA800 and 32xA800).  The results highlight the significant reduction in memory consumption achieved by Inf-CL compared to CLIP and OpenCLIP, especially at larger batch sizes, demonstrating the effectiveness of the proposed tile-based computation strategy in mitigating the memory barrier in contrastive learning.  An asterisk (*) indicates use of a data offload strategy to further reduce memory costs. 'X' denotes cases where memory limits were exceeded making training infeasible.", "section": "Experiments"}]