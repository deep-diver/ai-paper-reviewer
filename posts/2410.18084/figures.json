[{"figure_path": "2410.18084/figures/figures_1_0.png", "caption": "Figure 1: Dynamic LiDAR scene generation from DynamicCity. We introduce a new LiDAR generation model that generates diverse 4D scenes of large spatial scales (80 \u00d7 80 \u00d7 6.4 meter\u00b3) and long sequential modeling (up to 128 frames), enabling a diverse set of downstream applications. For more examples, kindly refer to our Project Page: https://dynamic-city.github.io.", "description": "This figure illustrates the DynamicCity framework for LiDAR scene generation.  It displays examples of various LiDAR scene generation tasks, including command-driven scene generation (top left, showing a sequence of scenes generated by following driving commands), dynamic object generation (top right, generating objects such as cars and pedestrians within a scene), trajectory-guided generation (bottom left, generating scenes that follow given trajectories), dynamic scene inpainting (bottom middle, completing missing parts of a scene), and layout-conditioned generation (bottom right, generating scenes based on a predefined layout). Each task demonstrates the model's ability to generate large-scale (80x80x6.4 meters) and temporally long (up to 128 frames) sequences of realistic 4D LiDAR scenes.", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18084/figures/figures_4_0.png", "caption": "Pipeline of dynamic LiDAR scene generation. Our DynamicCity framework consists of two key procedures: (a) Encoding HexPlane with an VAE architecture (cf. Sec. 4.1), and (b) 4D Scene Generation with HexPlane DiT (cf. Sec. 4.2).", "description": "This figure illustrates the DynamicCity framework's pipeline for dynamic LiDAR scene generation. It is divided into two main stages. The first stage (a) focuses on encoding the 4D LiDAR scene into a compact HexPlane representation using a Variational Autoencoder (VAE). This involves a 3D backbone for feature extraction, a projection module to compress the features into six 2D feature maps, and a decoding process to reconstruct the 4D scene. The second stage (b) utilizes a Diffusion Transformer (DiT) to generate the HexPlane, which is then decoded to produce the final 4D LiDAR scene. This stage incorporates different conditions such as trajectory, command, layout, and image, to guide the DiT-based generation.  The diagram visually represents the flow of data and processing steps in both stages, highlighting the key components and their interactions.", "section": "4 OUR APPROACH"}, {"figure_path": "2410.18084/figures/figures_5_0.png", "caption": "Figure 3: VAE for Encoding 4D LIDAR Scenes. We use HexPlane H as the 4D representation.  fo and go are convolution-based networks with downsampling and upsampling operations, respectively. h(.) denotes the projection network based on transformer modules.", "description": "This figure illustrates the Variational Autoencoder (VAE) model used in DynamicCity for encoding 4D LiDAR scenes into a compact HexPlane representation. The process begins with a 4D LiDAR scene (Q) which is fed into a 3D convolutional feature extractor (f\u03b8) to generate a feature volume sequence (Xtxyz).  A novel Projection Module, composed of multiple projection networks (h(.) based on transformers), then compresses this sequence into six 2D feature maps that constitute the HexPlane (H). These maps represent spatial and spatiotemporal features. An Expansion & Squeeze Strategy (ESS) is then used to decode the HexPlane back into the original feature volume sequence, and a convolutional network (g\u03c6) upsamples this to reconstruct the original 4D scene (Q'). The loss function for training the VAE incorporates cross-entropy, Lov\u00e1sz-softmax, and Kullback-Leibler divergence losses.", "section": "4.1 VAE FOR 4D LIDAR SCENES"}, {"figure_path": "2410.18084/figures/figures_6_0.png", "caption": "Figure 2: Pipeline of dynamic LiDAR scene generation. Our DynamicCity framework consists of two key procedures: (a) Encoding HexPlane with an VAE architecture (cf. Sec. 4.1), and (b) 4D Scene Generation with HexPlane DiT (cf. Sec. 4.2).", "description": "This figure illustrates the two-stage pipeline of DynamicCity for dynamic LiDAR scene generation.  The top half (a) shows the VAE (Variational Autoencoder) architecture used for encoding the 4D LiDAR scene into a compact HexPlane representation. This involves a 3D backbone for feature extraction, a Projection Module to compress the features into six 2D feature maps, and a decoding process to reconstruct the 3D feature volumes. The bottom half (b) details the HexPlane Diffusion model, which uses Diffusion Transformers (DiT) to generate new HexPlanes.  This process involves a denoising operation, a DiT network, and a decoder to generate a final 4D LiDAR scene. Various conditions (e.g., trajectory, layout) can be injected into the diffusion process to control different aspects of the generation.", "section": "4 OUR APPROACH"}, {"figure_path": "2410.18084/figures/figures_6_1.png", "caption": "Figure 5: Condition Injection for DiT", "description": "This figure illustrates how different conditions are injected into the Diffusion Transformer (DiT) model for conditional generation.  It shows two distinct conditioning paths: one for numeric conditions (trajectory, command, timestamp) which uses Multi-layer Perceptrons (MLPs) for embedding and then combines them additively with the input tokens; and another for image conditions (HexPlane, Layout) which uses a patchifying process and combines them via cross-attention mechanisms before being added to the input tokens.  Both paths ultimately modulate the DiT's input tokens through conditioning before being processed through the model's layers.", "section": "4.2 Diffusion Transformer for HexPlane"}, {"figure_path": "2410.18084/figures/figures_8_0.png", "caption": "Figure 6: Dynamic Scene Generation Results. We provide unconditional generation scenes from the 1st, 8th, and 16th frames on Occ3D-Waymo (Left) and CarlaSC (Right), respectively. Kindly refer to the Appendix for complete sequential scenes and longer temporal modeling examples.", "description": "This figure displays the results of unconditional 4D LiDAR scene generation using the DynamicCity model.  It shows sample scenes from the Occ3D-Waymo and CarlaSC datasets at three different time steps (T=1, T=8, and T=16). Each scene is presented as a top-down bird's-eye view, color-coded to show different semantic categories (building, vehicle, pedestrian, road, sidewalk, barrier, ground, vegetation, other, and pole). The figure demonstrates the model's ability to generate diverse and realistic large-scale dynamic scenes.  The caption directs the reader to the appendix for more complete sequential scenes and examples with longer temporal modeling.", "section": "5.2 4D SCENE RECONSTRUCTION & GENERATION"}, {"figure_path": "2410.18084/figures/figures_9_0.png", "caption": "Figure 7: Dynamic Scene Generation Applications. We demonstrate the capability of our model on a diverse set of downstream tasks. We show the 1st, 8th, and 16th frames for simplicity. Kindly refer to the Appendix for complete sequential scenes and longer temporal modeling examples.", "description": "This figure showcases the results of DynamicCity applied to various downstream tasks, demonstrating its versatility. It presents qualitative examples of 4D LiDAR scene generation under different conditions, including command-driven generation (Turn Left, Forward, Turn Right), dynamic object inpainting (Before/After), layout-conditioned scene generation, and trajectory-guided generation. Each task is illustrated with three scenes representing the 1st, 8th and 16th frames of a longer sequence (refer to the appendix for complete sequences). The legend indicates the semantic categories included in the visualization (Build, Vehicle, Ped, Road, Sidewalk, Barrier, Ground, Veg, Other, Pole).", "section": "4.3 Downstream Applications"}, {"figure_path": "2410.18084/figures/figures_20_0.png", "caption": "Figure 8: Unconditional Dynamic Scene Generation Results. We provide qualitative examples of a total of 16 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details.", "description": "This figure shows 16 consecutive frames of an unconditonally generated dynamic LiDAR scene from the DynamicCity model. The scene depicts a road intersection with various objects such as buildings, vehicles, pedestrians, and vegetation.  The frames are arranged in a grid, progressing from left to right and top to bottom, illustrating the temporal evolution of the scene.  Each frame is color-coded according to semantic class, highlighting the different object types and their movements throughout the sequence. The visual progression shows natural changes in the scene, implying DynamicCity's ability to generate realistic and temporally consistent dynamic LiDAR scenes.", "section": "C Additional Qualitative Results"}, {"figure_path": "2410.18084/figures/figures_21_0.png", "caption": "Figure 8: Unconditional Dynamic Scene Generation Results. We provide qualitative examples of a total of 16 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details.", "description": "The figure shows 16 consecutive frames of an unconditonally generated dynamic LiDAR scene from the DynamicCity model. Each frame is a bird's-eye view of a road scene with various objects, including buildings, vehicles, pedestrians, roads, sidewalks, barriers, ground, vegetation, and poles. The color-coding distinguishes between these categories. The frames demonstrate the model's capacity to generate temporally consistent and realistic dynamic scenes, showcasing the movement of vehicles and other objects over time.", "section": "C Additional Qualitative Results"}, {"figure_path": "2410.18084/figures/figures_22_0.png", "caption": "Figure 10: HexPlane-Guided Generation Results. We provide qualitative examples of a total of 64 consectutive frames generated by DynamicCity on the Occ3D-Waymo (Tian et al., 2023) dataset. Best viewed in colors and zoomed-in for additional details.", "description": "This figure shows 64 consecutive frames generated by the DynamicCity model, demonstrating its ability to generate long sequences via HexPlane-based conditional generation.  The frames depict a T-intersection scene, displaying a variety of semantic elements (buildings, vehicles, pedestrians, roads, etc.) and showcasing the dynamic movement of objects across multiple time steps. The consistent scene layout and object behavior across these frames highlight the model's ability to maintain temporal coherence over long sequences.", "section": "C Additional Qualitative Results"}, {"figure_path": "2410.18084/figures/figures_23_0.png", "caption": "Figure 1: Dynamic LiDAR scene generation from DynamicCity. We introduce a new LiDAR generation model that generates diverse 4D scenes of large spatial scales (80 \u00d7 80 \u00d7 6.4 meter\u00b3) and long sequential modeling (up to 128 frames), enabling a diverse set of downstream applications. For more examples, kindly refer to our Project Page: https://dynamic-city.github.io.", "description": "The figure illustrates the DynamicCity framework for large-scale LiDAR generation. It shows examples of command-driven scene generation, where the LiDAR scenes change according to commands like \"forward\", \"turn left\", etc., demonstrating the generation of long sequences. Dynamic object generation, which includes the addition and movement of objects within scenes, is also illustrated. Other aspects of the framework, such as trajectory-guided generation, dynamic scene inpainting, and layout-conditioned generation, are shown, highlighting the versatility of DynamicCity. The figure demonstrates the model's capability to generate diverse, high-quality 4D LiDAR scenes of large spatial scales and long sequential modeling. ", "section": "1 INTRODUCTION"}, {"figure_path": "2410.18084/figures/figures_24_0.png", "caption": "Figure 12: Command-Guided Scene Generation Results. We provide qualitative examples of a total of 16 consectutive frames generated under the command RIGHT by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details.", "description": "This figure shows 16 frames of a scene generated by the DynamicCity model under the \"Turn Right\" command. Each frame is a bird's-eye view of a road scene, color-coded to show different semantic categories (building, vehicle, pedestrian, road, sidewalk, barrier, ground, vegetation, other, pole). The sequence demonstrates the model's ability to generate a coherent and realistic-looking scene with consistent changes in the environment and vehicle positions as the vehicle turns right.", "section": "C Additional Qualitative Results"}, {"figure_path": "2410.18084/figures/figures_25_0.png", "caption": "Figure 13: Trajectory-Guided Scene Generation Results. We provide qualitative examples of a total of 16 consectutive frames generated by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details.", "description": "This figure displays 16 consecutive frames generated by the DynamicCity model on the CarlaSC dataset, demonstrating trajectory-guided scene generation. Each frame shows a bird's-eye view of a road scene segmented into various categories such as buildings, vehicles, pedestrians, roads, sidewalks, barriers, ground, vegetation, other objects, and poles, represented by different colors. The figure illustrates how the model generates a realistic and dynamic scene that follows a specified trajectory, with the ego-vehicle and other objects smoothly moving along the defined path over time. The change in the scene from frame to frame reflects a coherent and natural flow of motion.", "section": "C Additional Qualitative Results"}, {"figure_path": "2410.18084/figures/figures_26_0.png", "caption": "Figure 14: Dynamic Inpainting Results. We provide qualitative examples of a total of 16 consecutive frames generated by DynamicCity on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details.", "description": "This figure displays the results of dynamic inpainting performed by the DynamicCity model on the CarlaSC dataset.  It showcases 16 consecutive frames, each presented as a before-and-after pair, demonstrating the model's ability to seamlessly fill in masked regions within the 4D LiDAR scenes. The inpainting is done in a way that maintains consistency with the surrounding areas, both spatially and temporally. Each frame shows various elements including buildings, vehicles, pedestrians, roads, and other objects. The consistent visual quality and smooth transitions between frames highlight the method\u2019s effectiveness in maintaining the integrity and realism of the dynamic scene during the inpainting process.", "section": "C Additional Qualitative Results"}, {"figure_path": "2410.18084/figures/figures_27_0.png", "caption": "Figure 15: Comparisons of Dynamic Scene Generation. We provide qualitative examples of a total of 16 consecutive frames generated by OccSora (Wang et al., 2024) and our proposed DynamicCity framework on the CarlaSC (Wilson et al., 2022) dataset. Best viewed in colors and zoomed-in for additional details.", "description": "This figure presents a qualitative comparison of dynamic scene generation results between the proposed DynamicCity model and the OccSora method. It displays 16 consecutive frames generated by both models, for the same scene from the CarlaSC dataset. The frames are arranged in two columns, with DynamicCity's results on the right and OccSora's results on the left, each row representing a single point in time. Visual comparison reveals that DynamicCity's generated scene is more realistic and coherent in its object representation, layout, and motion compared to the scene from OccSora. Specifically, OccSora struggles with object placement, with a pedestrian placed in the middle of the road, unrealistic broken vehicles, and a lack of overall dynamism. DynamicCity demonstrates better semantic accuracy, improved dynamic object modeling, and more plausible scene composition.", "section": "C.6 Comparisons with OccSora"}]