{"references": [{" publication_date": "2024", "fullname_first_author": "Antonio Alliegro", "paper_title": "Polydiff: Generating 3d polygonal meshes with diffusion models", "reason": "This paper is highly relevant because it explores diffusion models for 3D object generation, a closely related area to LiDAR scene generation.  Understanding advancements in diffusion models for 3D objects is crucial for developing effective methods for LiDAR scene generation, which is the primary focus of the current paper.  The techniques and insights from this work could inform the design of new methods for the challenging task of 4D LiDAR scene generation.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Sherwin Bahmani", "paper_title": "Tc4d: Trajectory-conditioned text-to-4d generation", "reason": "This paper focuses on trajectory-conditioned 4D generation, which is directly relevant to the current paper's aim of generating dynamic scenes.  The techniques and approaches used in this work are valuable for understanding how to incorporate temporal information and movement conditions into the generation process, crucial for generating realistic and dynamic 4D LiDAR scenes.", "section_number": 2}, {" publication_date": "2018", "fullname_first_author": "Maxim Berman", "paper_title": "The lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks", "reason": "This paper introduces the Lov\u00e1sz-softmax loss function, a key component of the loss function used in the current paper's VAE model for training. The Lov\u00e1sz-softmax loss is particularly suitable for tasks involving semantic segmentation, as it directly optimizes the intersection-over-union (IoU) metric, which is a crucial evaluation metric in the current work.  Its inclusion highlights the attention to detail and rigorous approach to evaluation in the model development.", "section_number": 4}, {" publication_date": "2023", "fullname_first_author": "Andreas Blattmann", "paper_title": "Align your latents: High-resolution video synthesis with latent diffusion models", "reason": "This paper is relevant because it deals with high-resolution video synthesis using latent diffusion models.  Since 4D LiDAR scene generation is closely related to video generation (representing temporal sequences), understanding the advancements in high-resolution video synthesis can help in designing better 4D LiDAR generation methods. This paper's insights into the use of latent diffusion models for high-resolution video generation are valuable for improving the quality and resolution of the 4D LiDAR scenes generated by the current method.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Lucas Caccia", "paper_title": "Deep generative modeling of lidar data", "reason": "This paper is highly relevant because it tackles LiDAR scene generation, providing a baseline against which to compare the current research's advancements.  It explores generative models for LiDAR data, which is central to the current paper's focus on generating 4D LiDAR scenes.  Understanding previous approaches to LiDAR scene generation allows for a clearer understanding of the novel contributions and improvements made by the DynamicCity framework.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Holger Caesar", "paper_title": "nuscenes: A multimodal dataset for autonomous driving", "reason": "The nuScenes dataset is one of the primary datasets used in the current paper for training and evaluation. This paper's detailed description of the nuScenes dataset, including its modalities and annotations, is essential for understanding the data used in the experiments and interpreting the results.  The paper's contribution to the autonomous driving community by providing a rich and comprehensive dataset is significant for evaluating and benchmarking the performance of LiDAR scene generation models.", "section_number": 5}, {" publication_date": "2023", "fullname_first_author": "Ang Cao", "paper_title": "Hexplane: A fast representation for dynamic scenes", "reason": "This paper introduces HexPlane, the core 4D representation used by the DynamicCity framework.  HexPlane is crucial to the efficiency and effectiveness of DynamicCity, allowing for compact encoding of dynamic 4D LiDAR scenes. Understanding the design, implementation, and properties of HexPlane is critical to understanding the overall approach presented by this paper.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Eric R Chan", "paper_title": "Efficient geometry-aware 3d generative adversarial networks", "reason": "This paper is relevant due to its focus on 3D generative adversarial networks (GANs). While the current paper utilizes a different approach (VAEs and diffusion models), understanding the advancements in GAN-based methods for 3D scene generation provides valuable context. The techniques and challenges discussed in this paper, such as handling geometric details and relationships in 3D spaces, are relevant for the more complex task of generating high-quality, dynamic 4D LiDAR scenes.", "section_number": 2}, {" publication_date": "2019", "fullname_first_author": "Christopher Choy", "paper_title": "4d spatio-temporal convnets: Minkowski convolutional neural networks", "reason": "This paper explores Minkowski Convolutional Neural Networks, which are used in the 3D encoder of the current work for feature extraction from LiDAR data. Understanding how Minkowski networks effectively handle sparse and irregular point cloud data is crucial for comprehending the feature extraction process in DynamicCity, thereby contributing to the understanding of the model's performance.", "section_number": 5}, {" publication_date": "2022", "fullname_first_author": "Sara Fridovich-Keil", "paper_title": "K-planes: Explicit radiance fields in space, time, and appearance", "reason": "This paper introduces K-planes, which is related to HexPlanes used in the current work.  Both representations are structured methods for representing dynamic 3D scenes.  By understanding the similarities and differences between these representations, we can better appreciate the design choices and the novel aspects of HexPlanes for 4D LiDAR scene generation.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Jonathan Ho", "paper_title": "Classifier-free diffusion guidance", "reason": "This paper introduces Classifier-Free Guidance (CFG), a technique used in the DiT-based diffusion model of DynamicCity to improve the quality of conditional generation.  Understanding CFG and its implementation details is crucial to the success of conditional generation in DynamicCity, such as trajectory-guided, layout-conditioned scene generation, and inpainting, all of which are discussed in the current paper.", "section_number": 4}, {" publication_date": "2021", "fullname_first_author": "Siyuan Huang", "paper_title": "Spatio-temporal self-supervised representation learning for 3d point clouds", "reason": "This paper focuses on spatio-temporal representation learning for 3D point clouds, a critical aspect of 4D LiDAR scene generation.  The methods and techniques used in this work, particularly in handling temporal information in 3D point clouds, are relevant to the current paper's aim of generating dynamic 4D LiDAR scenes.   This paper's contributions are relevant to the challenges inherent in effectively capturing temporal dependencies in 4D LiDAR data.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Jumin Lee", "paper_title": "Semcity: Semantic scene generation with triplane diffusion", "reason": "SemCity is a recent method for generating large-scale 3D scenes that utilizes a triplane diffusion model.  This paper provides a direct comparison of approaches to large-scale scene generation.  Comparing the performance and capabilities of SemCity with those of DynamicCity highlights the key improvements and contributions of the proposed DynamicCity framework.  The qualitative comparison of outputs is particularly important in understanding the strengths and weaknesses of both approaches.", "section_number": 5}, {" publication_date": "2023a", "fullname_first_author": "Yuheng Liu", "paper_title": "Pyramid diffusion for fine 3d large scene generation", "reason": "This paper is highly relevant due to its focus on generating large-scale 3D scenes using a pyramid diffusion model.  The approaches discussed in this paper for efficient generation of large-scale 3D scenes are directly relevant to the task of 4D LiDAR scene generation. Understanding this related work provides valuable context for evaluating the scalability and efficiency of the DynamicCity framework. The work is significant for large scale 3D scene generation.", "section_number": 2}, {" publication_date": "2023b", "fullname_first_author": "Zhen Liu", "paper_title": "Meshdiffusion: Score-based generative 3d mesh modeling", "reason": "This paper explores score-based generative models for 3D mesh generation.  Although not directly related to LiDAR data, the use of score-based diffusion models, which is a core concept in DynamicCity, provides valuable context and informs the choice of architectural techniques used in this paper. The theoretical understanding of score-based models provides background for the diffusion process used by DynamicCity.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Kazuto Nakashima", "paper_title": "Lidar data synthesis with denoising diffusion probabilistic models", "reason": "This paper is directly relevant because it addresses LiDAR scene generation using diffusion models, which is a similar approach to that used in DynamicCity.  Comparing the methods and results presented in this paper to those of DynamicCity offers valuable insights into the advancements and improvements made by the proposed DynamicCity framework.  The focus on diffusion models for LiDAR synthesis is directly relevant to the current work.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Lucas Nunes", "paper_title": "Scaling diffusion models to real-world 3d lidar scene completion", "reason": "This paper addresses the challenges of scaling diffusion models to real-world 3D LiDAR scene completion, a topic closely related to the current paper.  By understanding the scaling challenges addressed by this work, we can gain insights into the scalability and robustness of the DynamicCity framework, which tackles the even more complex task of 4D LiDAR scene generation.  The practical considerations of scaling are directly relevant.", "section_number": 2}, {" publication_date": "2023", "fullname_first_author": "William Peebles", "paper_title": "Scalable diffusion models with transformers", "reason": "This paper introduces diffusion transformers (DiT), a key component of the DynamicCity framework. The DiT model is used for generating the HexPlane representation, which is then decoded into 4D LiDAR scenes. Understanding the architecture and capabilities of DiT is crucial to comprehending the core generation process in DynamicCity.  The use of transformers within the diffusion framework is important.", "section_number": 4}, {" publication_date": "2022", "fullname_first_author": "Robin Rombach", "paper_title": "High-resolution image synthesis with latent diffusion models", "reason": "This paper introduces high-resolution image synthesis using latent diffusion models.  This is highly relevant to the current paper because it addresses the generation of high-quality images, a crucial aspect of LiDAR scene generation.  By understanding the techniques for generating high-resolution images, we can gain insights into improving the quality and detail of the 4D LiDAR scenes generated by DynamicCity.", "section_number": 2}]}