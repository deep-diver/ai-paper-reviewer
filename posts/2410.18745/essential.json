{"reason": "This paper investigates why the effective context length of large language models (LLMs) falls short of their claimed context window size. The authors attribute this limitation to a left-skewed frequency distribution of relative positions in the LLM's pre-training data.  They propose STRING, a training-free method that shifts well-trained positions to enhance performance and demonstrate significant improvements in multiple LLMs on long-context benchmarks.  The findings highlight a critical limitation in current LLM designs and provide a potential solution.", "takeaways": ["The effective context length of LLMs is often significantly less than their training length due to a left-skewed frequency distribution of relative positional indices.", "STRING, a training-free method, shifts well-trained position embeddings to enhance performance within existing training lengths, improving long context capabilities without additional training.", "The proposed STRING method achieves state-of-the-art results on popular long-context benchmarks for open-source LLMs, surpassing even some commercial models in performance."], "tldr": "Large language models (LLMs) don't use their full context window due to a skewed distribution of positional information during training.  The authors introduce STRING, a training-free method that shifts position embeddings to improve performance on long context tasks.  STRING dramatically improves performance on open-source LLMs, even outperforming some commercial models."}