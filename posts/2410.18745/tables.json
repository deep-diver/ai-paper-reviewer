[{"figure_path": "2410.18745/tables/table_8_0.md", "caption": "Needle-in-a-Haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the results of the Needle-in-a-Haystack task (4 needles) for 7 different base models. The models are evaluated using various methods, including ReROPE, NTK, the original RoPE, Self-Extend, YaRN, DCA, and STRING.  The results are shown for different training context window sizes (Ltrain). Each model's performance is measured using its training length. The table is ordered by the average performance of the methods across all the models.", "section": "4.2 Main Results of STRING"}, {"figure_path": "2410.18745/tables/table_9_0.md", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the performance of seven baseline models on the Needle-in-a-Haystack task, a common benchmark for evaluating the effective context length of LLMs.  The table compares the performance of the original RoPE (Rotary Position Embedding) with several training-free extrapolation methods: NTK-Aware ROPE, YaRN, ReRoPE, Self-Extend, and DCA.  The results are shown for each model,  including its training context length (Ltrain) and the accuracy achieved on the Needle-in-a-Haystack task (using 500 test cases) for each method.  The table is organized by average performance, with the models and methods sorted from lowest average accuracy to highest.", "section": "4.2 Main Results of STRING"}, {"figure_path": "2410.18745/tables/table_9_1.md", "caption": "Table 3: Comparison of STRING with three leading commercial long-context models on InfiniteBench. Each model is evaluated using a maximum context length of 128K.", "description": "This table presents a comparison of the performance of STRING against three leading commercial long-context models (GPT-4, Claude2, and Kimi-chat) and the baseline RoPE method on the InfiniteBench benchmark.  The benchmark includes several tasks assessing a broad spectrum of practical scenarios including long-context question answering, multiple-choice QA, mathematical problem-solving, long dialogues, summarization, retrieval and code debugging. The table shows the average performance across all tasks for each model, indicating STRING's effectiveness in improving performance on open-source LLMs.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_19_0.md", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "This table presents the performance of seven base models on the Needle-in-a-Haystack task, a benchmark for evaluating the effective context length of LLMs.  Each model is evaluated using its training context length across various methods: ReROPE, NTK, original ROPE, Self-Extend, YaRN, DCA, and STRING. The results, expressed as percentages, indicate the success rate in retrieving at least two out of four inserted needles within the specified context. The table highlights the consistent superior performance of STRING across all models.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_20_0.md", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "This table presents the performance of seven baseline language models on the Needle-in-a-Haystack task, a benchmark for evaluating long-context understanding in LLMs. Each model is evaluated using its training context length, with 500 test cases. The table compares the performance of the original RoPE position embedding with several training-free extrapolation methods, namely ReROPE, NTK, Self-Extend, YaRN, DCA, and STRING, and reports the accuracy scores achieved by each method.  The models tested include TinyLlama-1.3B, TinyLlama-1.1B-3T, Llama-2-7B, Llama-3-8B, LWM-7B-base, Mistral-7B-base, and Llama-3.1-8B, with various training context lengths (2K, 4K, 8K, 32K, and 128K). The table is ordered by the average score across all models and methods.", "section": "4.2 Main Results of STRING"}, {"figure_path": "2410.18745/tables/table_21_0.md", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  Ltrain means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "Table 1 presents the performance of seven base models on the Needle-in-a-Haystack task, a benchmark evaluating the effective context length of large language models.  The table compares the original RoPE (Rotary Position Embedding) with several training-free extrapolation methods (ReRoPE, NTK-Aware ROPE, Self-Extend, YaRN, and DCA) and STRING (Shifted Rotray position embeddING).  For each model, results are shown for each method, indicating the accuracy achieved when using the model's training length. The models vary in their training context window size ( Ltrain), ranging from 2K to 128K. The table is organized with models listed in ascending order of average accuracy, highlighting STRING's consistent superior performance across all models.", "section": "4.2 MAIN RESULTS OF STRING"}, {"figure_path": "2410.18745/tables/table_22_0.md", "caption": "Table 1: Needle-in-a-haystack (4 needles) results of 7 base models across various methods (columns reordered from smallest to largest average) where  Ltrain  means the size of the training context window. All the models were tested using their training length. The number of test cases is 500.", "description": "This table presents the performance of seven base models on the Needle-in-a-Haystack task, a popular long-context benchmark.  The models are evaluated using their training length with seven different methods: ReROPE, NTK, original ROPE, Self-Extend, YaRN, DCA, and STRING. The table shows the performance of each method for each model, with the results ordered from the lowest average score to the highest average score across all models and methods.  The  Ltrain  column indicates the training context window size for each model. The average performance across all models is also provided for each method.", "section": "4.2 MAIN RESULTS OF STRING"}]