{"references": [{" publication_date": "2017", "fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "reason": "This paper introduced the Transformer architecture, which revolutionized NLP and forms the basis for many subsequent models.  It is foundational to the field and directly relevant to the paper's discussion of the limitations of Transformers regarding long sequences.", "section_number": 1}, {" publication_date": "2020", "fullname_first_author": "Tom B Brown", "paper_title": "Language models are few-shot learners", "reason": "This highly influential work demonstrated the remarkable capabilities of large language models and sparked widespread interest in few-shot learning, directly motivating research into more efficient long-context modeling techniques as discussed in the paper.", "section_number": 1}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This work introduced the LLaMA model, a significant advancement in efficient large language models. The paper uses LLaMA as a baseline for comparison, highlighting the continuing quest for efficient long-context processing techniques.", "section_number": 1}, {" publication_date": "2021b", "fullname_first_author": "Albert Gu", "paper_title": "Combining recurrent, convolutional, and continuous-time models with linear state space layers", "reason": "This paper is highly relevant because it introduces the concept of combining different model types (recurrent, convolutional, and continuous-time) with linear state space layers, which is crucial to the understanding of the Taipan model's hybrid approach.", "section_number": 1}, {" publication_date": "2024", "fullname_first_author": "Simran Arora", "paper_title": "Simple linear attention language models balance the recall-throughput tradeoff", "reason": "This paper presents a different approach to addressing the long-context problem by focusing on the recall-throughput tradeoff.  It directly addresses the limitations of Transformers and provides alternative perspectives on handling long-range dependencies, making it highly relevant to the goals of the introduced Taipan model.", "section_number": 1}, {" publication_date": "2017", "fullname_first_author": "A Vaswani", "paper_title": "Attention is all you need", "reason": "This foundational paper introduced the Transformer architecture and its self-attention mechanism.  Understanding this architecture is crucial to evaluating the trade-offs between different approaches presented in this paper.", "section_number": 2}, {" publication_date": "2020", "fullname_first_author": "Angelos Katharopoulos", "paper_title": "Transformers are RNNs: Fast autoregressive transformers with linear attention", "reason": "This paper explores linear attention, an efficient variant of self-attention. This is directly relevant to the paper's background on efficient attention mechanisms and their limitations. Understanding Linear Attention's strengths and weaknesses is crucial for assessing the motivation and design of Taipan.", "section_number": 2}, {" publication_date": "2024", "fullname_first_author": "Tri Dao", "paper_title": "Transformers are ssms: Generalized models and efficient algorithms through structured state space duality", "reason": "This paper is crucial to the understanding of the Mamba model, the basis for Taipan's design. It provides theoretical and practical insights into the relationship between Transformers and State Space Models (SSMs), setting the stage for the discussion of Taipan's novel hybrid approach.", "section_number": 2}, {" publication_date": "2017", "fullname_first_author": "Eric Jang", "paper_title": "Categorical reparameterization with gumbel-softmax", "reason": "This work presents the Gumbel-Softmax trick which is crucial for the differentiability of the gating network in Taipan's SALs, making it essential for the paper's methodology.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Iz Beltagy", "paper_title": "Longformer: The long-document transformer", "reason": "This paper presents a model that handles long documents efficiently. This is highly relevant to the paper's focus on long-context language modeling, providing a comparative context for evaluating Taipan's performance and capabilities.", "section_number": 3}, {" publication_date": "2020", "fullname_first_author": "Noam Shazeer", "paper_title": "Glu variants improve transformer", "reason": "This paper introduces SwiGLU, which is used in Taipan's architecture.  Understanding SwiGLU is important to fully understand the contributions and innovations of the Taipan model. ", "section_number": 3}, {" publication_date": "2021", "fullname_first_author": "Stephanie Lin", "paper_title": "Truthfulqa: Measuring how models mimic human falsehoods", "reason": "This dataset is used in the evaluation of the Taipan model. Understanding the nature and scope of this dataset is necessary for understanding the experimental results and their implications.", "section_number": 4}, {" publication_date": "2020", "fullname_first_author": "Rowan Zellers", "paper_title": "HellaSwag: Can a machine really finish your sentence?", "reason": "This dataset is used in the evaluation of the Taipan model, providing a benchmark against which to compare its zero-shot performance. The characteristics of this dataset are important for understanding the experimental results presented in the paper.", "section_number": 4}, {" publication_date": "2018", "fullname_first_author": "Pranav Rajpurkar", "paper_title": "Know what you don't know: Unanswerable questions for squad", "reason": "This dataset is used in the evaluation of the Taipan model and its ability to perform in-context retrieval. Understanding SQUAD is crucial for interpreting the experimental results regarding in-context recall performance.", "section_number": 4}, {" publication_date": "2024", "fullname_first_author": "Simran Arora", "paper_title": "Simple linear attention language models balance the recall-throughput tradeoff", "reason": "This paper focuses on recall-intensive tasks, which are also a central part of the Taipan evaluation. Understanding this research is crucial for evaluating the model's performance in similar tasks.", "section_number": 4}, {" publication_date": "2021a", "fullname_first_author": "Albert Gu", "paper_title": "Efficiently modeling long sequences with structured state spaces", "reason": "This paper introduces the original S4 model which is the foundation of many subsequent State Space Models (SSMs), including Mamba-2, a core component of Taipan.  Understanding the evolution of SSMs is key to grasping the context and innovation of Taipan's architecture.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Albert Gu", "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces", "reason": "Mamba-2, a variant of this model, is a central building block of the Taipan architecture.  This paper is crucial for understanding the design choices and rationale behind Taipan.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Hugo Touvron", "paper_title": "Llama: Open and efficient foundation language models", "reason": "This work introduced the LLaMA model which is used as a baseline for comparison in the paper's experiments. Understanding LLaMA is crucial for evaluating Taipan's performance.", "section_number": 6}, {" publication_date": "2024", "fullname_first_author": "Opher Lieber", "paper_title": "Jamba: A hybrid transformer-mamba language model", "reason": "Jamba is used as a baseline model for comparison with Taipan in the experimental section of the paper. Understanding Jamba's architecture and performance is essential for a complete evaluation of Taipan's contributions.", "section_number": 6}, {" publication_date": "2023", "fullname_first_author": "Eric Nguyen", "paper_title": "Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution", "reason": "This paper addresses the challenge of long-range sequence modeling, a problem also addressed by Taipan. This comparison provides context for evaluating Taipan's efficiency and effectiveness.", "section_number": 6}]}