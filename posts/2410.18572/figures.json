[{"figure_path": "2410.18572/figures/figures_4_0.png", "caption": "Figure 2: An overview of the Taipan architecture.", "description": "The figure provides a visual representation of the Taipan architecture, illustrating its hybrid structure that combines Mamba-2 layers with Selective Attention Layers (SALs). The input token embeddings (x) are initially processed through a Mamba-2 layer, producing hidden representations (h). These representations are then passed through a SAL, which consists of a gating network that determines which tokens require further processing. Selected tokens (indicated by the  h symbol) undergo feature refinement and are then enhanced using a softmax attention module, before being passed to another Mamba-2 layer, followed by a SwiGLU layer. The entire process is repeated multiple times.", "section": "3 TAIPAN MODEL"}]