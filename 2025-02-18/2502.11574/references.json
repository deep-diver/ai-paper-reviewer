{"references": [{"fullname_first_author": "Antoine Roux Albert Q. Jiang", "paper_title": "Mixtral of experts", "publication_date": "2024-01-01", "reason": "This paper introduces Mixtral, a large language model, which is one of the models evaluated in the study, providing valuable data on its mathematical reasoning capabilities."}, {"fullname_first_author": "Konstantin Chernyshev", "paper_title": "U-math: A university-level benchmark for evaluating mathematical skills in LLMs", "publication_date": "2024-12-01", "reason": "This paper presents U-MATH, a dataset of university-level mathematical problems, which is a related work that provides a comparison point for the dataset used in the current study."}, {"fullname_first_author": "Karl Cobbe", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-01-01", "reason": "This paper is cited as one of the first studies exploring the mathematical capabilities of large language models (LLMs), establishing a foundation for later research in the area."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring mathematical problem solving with the math dataset", "publication_date": "2021-01-01", "reason": "The MATH dataset, introduced in this paper, is a widely used benchmark for evaluating mathematical reasoning abilities of LLMs, making it a significant reference for the current work."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-01-01", "reason": "This paper discusses RLHF (Reinforcement Learning from Human Feedback), a crucial training technique used in many LLMs including those analyzed in the current study, thus explaining the reasoning capabilities of those models."}]}