[{"figure_path": "https://arxiv.org/html/2502.08441/extracted/6201951/figs/toy_example.png", "caption": "Figure 1: Toy example of a hidden state vector h\u210ehitalic_h (shown in blue) and three embedding vectors eisubscript\ud835\udc52\ud835\udc56e_{i}italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (shown in red) in H=2\ud835\udc3b2H=2italic_H = 2 dimensions. The gray vectors represent the embedding update vectors, for the SGD (dark) and the Adam (light) optimizer. The update vector of the true token is aligned with h\u210ehitalic_h, while the others point in the opposite direction, see Eq.\u00a0(5). Note that the sum of embedding update vectors vanishes for SGD, while this is not necessarily the case for Adam, cf.\u00a0Eqs.\u00a0(11) and (16).", "description": "Figure 1 illustrates the core concept of the paper regarding the anisotropy problem in embedding vectors.  It uses a simplified 2D example to visually represent the embedding update vectors for both SGD and Adam optimizers. A hidden state vector (blue) interacts with three embedding vectors (red), one of which corresponds to the correct token.  The gray arrows depict the update vectors for each embedding vector, with SGD producing update vectors that sum to zero (demonstrating a balanced update), and Adam having update vectors that do not sum to zero (implying an unbalanced update and thus the creation of anisotropy). This difference in update vector summation between the optimizers helps to explain why Adam contributes to the anisotropy problem that is the main focus of the paper.", "section": "On the Root Cause of Anisotropic Embeddings"}, {"figure_path": "https://arxiv.org/html/2502.08441/extracted/6201951/figs/experimental_results_E_p_125M_20B.png", "caption": "Figure 4: Experimental results for \ud835\udd3c\u2062[v^i]\ud835\udd3cdelimited-[]subscript^\ud835\udc63\ud835\udc56\\mathbb{E}\\left[\\widehat{v}_{i}\\right]blackboard_E [ over^ start_ARG italic_v end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] (vertical axis) vs. p~isubscript~\ud835\udc5d\ud835\udc56\\widetilde{p}_{i}over~ start_ARG italic_p end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (horizontal axis) for N=125\u2062M\ud835\udc41125MN=125\\rm Mitalic_N = 125 roman_M and D=D\u2032=20\u2062B\ud835\udc37superscript\ud835\udc37\u203220BD=D^{\\prime}=20\\rm Bitalic_D = italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT = 20 roman_B. The blue line shows the linear fit with R2=0.91superscript\ud835\udc4520.91R^{2}=0.91italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.91.", "description": "This figure displays the relationship between the expected value of the embedding update vector's squared magnitude (\ud835\udd3c[v^i]) and the unigram probability (p~i) for a specific model size (N=125M) and dataset size (D=20B).  Each point represents an embedding vector, and its vertical position indicates the expected value of the squared magnitude of its update vector, while its horizontal position shows its corresponding unigram probability. The blue line is a linear fit through the data points, showing a strong positive correlation (R\u00b2=0.91). This visualization helps to demonstrate that embeddings of more frequent words (higher p~i) tend to have smaller updates (lower \ud835\udd3c[v^i]), and vice versa, which is crucial for understanding the effects of Adam optimization on anisotropic embeddings.", "section": "D Magnitude of the Second Moment in Adam"}, {"figure_path": "https://arxiv.org/html/2502.08441/extracted/6201951/figs/experimental_results_A.png", "caption": "Figure 5: Experimental results for the linear fit parameter A\ud835\udc34Aitalic_A as a function of N\ud835\udc41Nitalic_N and D\u2032superscript\ud835\udc37\u2032D^{\\prime}italic_D start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT.", "description": "This figure displays the results of linear fits performed to experimentally determine the proportionality constant A in the relationship E[v\u1d62] \u2248 A\u22c5p\u1d62, where E[v\u1d62] is the expectation value of the Adam optimizer's second moment for the i-th embedding vector and p\u1d62 is the unigram probability of the i-th token.  The graph shows how the fitted parameter A varies with model size (N) and the amount of training data (D').  Different colors represent different model sizes, demonstrating that the relationship between A and D' is dependent upon model size.  This helps to understand the influence of model size and training dataset size on the anisotropy issue. ", "section": "D.2 Experimental Confirmation"}, {"figure_path": "https://arxiv.org/html/2502.08441/extracted/6201951/figs/experiments_log.png", "caption": "Figure 6: Overview of the dataset (horizontal axis) and model sizes (vertical axis) involved in our small-scale (blue, green and orange circles) and large-scale (red squares) experiments. The dashed, black line shows N=D/20\ud835\udc41\ud835\udc3720N=D/20italic_N = italic_D / 20, which is approximately the compute-optimal trajectory according to hoffmann2022trainingcomputeoptimallargelanguage.", "description": "Figure 6 shows the relationship between dataset size and model size used in the experiments.  The horizontal axis represents the size of the dataset (D, in tokens), and the vertical axis represents the size of the model (N, in parameters). Small-scale experiments are represented by blue, green, and orange circles, while large-scale experiments are shown as red squares. The dashed black line illustrates the compute-optimal trajectory proposed by Hoffmann et al. (2022), suggesting an approximate ratio of dataset size to model size (D/N) of 20.  This line serves as a reference point to compare the dataset and model sizes used in the study.", "section": "4 Experiments"}]