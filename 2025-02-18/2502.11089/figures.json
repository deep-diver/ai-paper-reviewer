[{"figure_path": "https://arxiv.org/html/2502.11089/x1.png", "caption": "Figure 1: Comparison of performance and efficiency between Full Attention model and our NSA. Left: Despite being sparse, NSA surpasses Full Attention baseline on average across general benchmarks, long-context tasks, and reasoning evaluation. Right: For 64k-length sequence processing, NSA achieves substantial computational speedup compared to Full Attention in all stages: decoding, forward propagation, and backward propagation.", "description": "This figure compares the performance and efficiency of the proposed Native Sparse Attention (NSA) model against a full attention model. The left panel shows that despite its sparsity, NSA outperforms the full attention baseline on various benchmarks, including general tasks, long-context scenarios, and reasoning evaluations.  The right panel demonstrates the significant speedup achieved by NSA over full attention for processing sequences of length 64k. This speed improvement is observed across all stages of the model's lifecycle, namely decoding, forward propagation, and backward propagation.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.11089/x2.png", "caption": "Figure 2: Overview of NSA\u2019s architecture. Left: The framework processes input sequences through three parallel attention branches: For a given query, preceding keys and values are processed into compressed attention for coarse-grained patterns, selected attention for important token blocks, and sliding attention for local context. Right: Visualization of different attention patterns produced by each branch. Green areas indicate regions where attention scores need to be computed, while white areas represent regions that can be skipped.", "description": "This figure illustrates the Native Sparse Attention (NSA) architecture. The left panel shows the three main processing pathways: compressed attention (coarse-grained patterns), selected attention (important token blocks), and sliding attention (local context).  These pathways operate in parallel for each query. The right panel provides a visual representation of the attention patterns generated by each branch, showing where attention scores are computed (green) and skipped (white).  This highlights NSA's hierarchical approach to efficiently handling long sequences by prioritizing important information and contextual details while selectively omitting less critical parts.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.11089/x3.png", "caption": "Figure 3: Kernel design for NSA. The kernel loads queries by GQA groups (Grid Loop), fetches corresponding sparse KV blocks (Inner Loop), and performs attention computation on SRAM. Green blocks indicate data on SRAM, while blue indicates data on HBM.", "description": "This figure illustrates the kernel design of the Native Sparse Attention (NSA) mechanism. It highlights the three main stages: the Grid Loop, the Inner Loop, and the attention computation on SRAM.  The Grid Loop handles loading queries based on GQA groups. The Inner Loop focuses on sequentially loading the corresponding sparse key-value blocks into SRAM. Finally, the attention computation is performed efficiently within the SRAM. The color-coding helps differentiate data location: green blocks represent data residing in SRAM (fast access memory), whereas blue blocks denote data stored in HBM (high-bandwidth memory, slower access). This design prioritizes memory efficiency, minimizing data transfer between different memory levels and maximizing computational performance.", "section": "3.4. Kernel Design"}, {"figure_path": "https://arxiv.org/html/2502.11089/x4.png", "caption": "Figure 4: Pretraining loss comparison between Full Attention and our NSA on 27B-parameter model. Both models exhibit stable convergence, with NSA achieving lower loss values.", "description": "This figure displays the training loss curves for both the Full Attention model and the proposed NSA model, both trained on a 27B-parameter model.  The x-axis represents the training steps, and the y-axis represents the training loss. Both curves show a steady decrease in loss as training progresses, indicating stable convergence. Notably, the NSA model consistently achieves a lower training loss compared to the Full Attention model, suggesting improved training efficiency and possibly better generalization potential.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.11089/x5.png", "caption": "Figure 5: Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length. NSA achieves perfect accuracy through its hierarchical sparse attention design.", "description": "This figure displays the Needle-in-a-Haystack retrieval accuracy across different context positions within a 64k context length.  The y-axis represents the retrieval accuracy (percentage), and the x-axis shows the context position.  The heatmap visualization clearly demonstrates that NSA (Natively trainable Sparse Attention) achieves perfect (100%) accuracy across all positions. This highlights the effectiveness of NSA's hierarchical sparse attention mechanism in accurately retrieving information even with extremely long contexts.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.11089/x6.png", "caption": "Figure 6: Comparison of Triton-based NSA kernel with Triton-based FlashAttention-2 kernel. Our implementation significantly reduces latency across all context lengths, with the improvement becoming more pronounced as input length increases.", "description": "This figure compares the performance of the NSA (Natively trainable Sparse Attention) kernel with the FlashAttention-2 kernel, both implemented using Triton. The x-axis represents the input sequence length, and the y-axis shows the execution time in milliseconds for forward and backward propagation, as well as decoding. The results demonstrate that NSA significantly reduces latency across all sequence lengths tested, achieving a speedup of up to 9x for forward propagation and 6x for backward propagation at a sequence length of 64k. The speedup is more pronounced as the input sequence length increases.", "section": "5. Efficiency Analysis"}, {"figure_path": "https://arxiv.org/html/2502.11089/x7.png", "caption": "Figure 7: Compare training loss on a 3B-parameter model with Full Attention and different token selection strategies and. Our NSA achieves better performance.", "description": "This figure compares the training loss curves for a 3-billion parameter model trained with different sparse attention methods and Full Attention.  The x-axis shows training steps, and the y-axis shows the training loss. The curves show that a model using the proposed NSA (Natively Sparse Attention) method achieves significantly lower training loss compared to models using other token selection strategies and the baseline Full Attention method. This demonstrates the effectiveness of NSA in achieving efficient and accurate training.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2502.11089/x8.png", "caption": "Figure 8: Visualization of Attention Map on a Full Attention transformer. Light-colored regions indicate higher attention values. As shown in the figure, attention scores exhibit blockwise clustering distribution.", "description": "This figure visualizes the attention map generated by a full attention transformer model. The heatmap shows the attention weights between different tokens in the input sequence. Lighter colors represent higher attention weights, indicating stronger relationships between the corresponding tokens. The visualization reveals a notable pattern: the attention scores exhibit a blockwise clustering distribution. This means that tokens within the same block tend to have stronger attention weights among themselves compared to tokens outside the block. This blockwise clustering behavior suggests that attention is not uniformly distributed but rather concentrated in specific regions or blocks of tokens, which is a key observation that inspired the design of the NSA architecture.", "section": "6.2 Visualization"}]