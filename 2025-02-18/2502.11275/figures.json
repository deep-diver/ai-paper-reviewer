[{"figure_path": "https://arxiv.org/html/2502.11275/x1.png", "caption": "Figure 1: \\ourtakes a free ride on LLM resources (e.g., C4 and TuluV3\u00a0(Lambert et\u00a0al., 2024)) by formalizing next token prediction for duplicative spans as extraction in the BIO paradigm. During the inference, the prompts can be adjusted to different extractive tasks, making \\oura versatile IE model.", "description": "The figure illustrates how Cuckoo, an information extraction (IE) model, leverages the vast amount of data used to train large language models (LLMs). Instead of requiring separate, painstakingly annotated data for training, Cuckoo re-purposes the next-token prediction (NTP) task of LLMs.  Specifically, it identifies duplicate spans of text within the LLM's training data and frames the prediction of those spans as an extraction task. By assigning BIO tags (Begin, Inside, Outside) to these spans, Cuckoo efficiently creates a large, diverse training dataset.  This allows it to adapt to various IE tasks during inference simply by modifying the prompt. This makes Cuckoo a flexible and effective IE model that benefits from ongoing LLM advancements without requiring extra manual annotation.", "section": "3 Our Cuckoo"}, {"figure_path": "https://arxiv.org/html/2502.11275/x2.png", "caption": "Figure 2: Comparison of scale, cost, and diversity among different IE pre-training datasets. Our data collection for \\ouris free by converting LLM\u2019s learning resources, which forces the tagger to learn from diverse contexts. \\ourcan also evolve with the data collection for LLM\u2019s post-training.", "description": "Figure 2 illustrates a comparison of several information extraction (IE) pre-training datasets across three key aspects: scale (number of instances), cost (financial resources needed for data acquisition), and diversity (variety of contexts and data sources).  The figure highlights that the dataset used in the Cuckoo model stands out due to its massive scale (102.6M instances), zero cost (leveraging freely available LLM pre-training and post-training data), and high diversity (the model is trained on data from diverse sources, leading to improved adaptability).  Importantly, the Cuckoo dataset's unique characteristic is its ability to evolve with ongoing advancements in large language model (LLM) data preparation. Unlike traditional methods, the Cuckoo dataset requires no additional manual effort to adapt to improvements in LLM training pipelines.", "section": "3 Our Cuckoo"}, {"figure_path": "https://arxiv.org/html/2502.11275/x3.png", "caption": "Figure 3: The evolution of Cuckoo with LLM\u2019s post-training resources. Domain [\u03bc\u22122\u2062\u03c3,\u03bc+2\u2062\u03c3]\ud835\udf072\ud835\udf0e\ud835\udf072\ud835\udf0e[\\mu-2\\sigma,\\mu+2\\sigma][ italic_\u03bc - 2 italic_\u03c3 , italic_\u03bc + 2 italic_\u03c3 ] is annotated under each evaluation dimension.", "description": "This figure illustrates how the performance of the Cuckoo model improves as it is trained on increasingly larger and more diverse datasets from LLMs' post-training.  The x-axis represents different dimensions of evaluation for information extraction (IE) tasks, while the y-axis shows the performance scores. The shaded region for each dimension represents the range from two standard deviations below the mean (\u03bc-2\u03c3) to two standard deviations above the mean (\u03bc+2\u03c3).  The figure visually demonstrates Cuckoo's ability to evolve its performance in various aspects of IE by leveraging improvements in LLM training data, highlighting its 'free rider' advantage.", "section": "3 Our Cuckoo"}, {"figure_path": "https://arxiv.org/html/2502.11275/x4.png", "caption": "Figure 4: In-context tagging ability emerges in Cuckoo but not in IE models pre-trained by other resources.", "description": "Figure 4 investigates the in-context learning capabilities of Cuckoo and other IE models. In-context learning refers to a model's ability to adapt to a new task using only a few examples provided within the input context. The figure compares Cuckoo's performance on two datasets (CONLL2003 and SQUAD) with and without in-context examples against the performance of other IE models, such as NuNER and MRQA. It demonstrates that Cuckoo exhibits in-context learning ability, improving its performance with the addition of in-context examples. Conversely, other IE models pre-trained using different resources do not show a similar ability. This highlights Cuckoo's unique capacity to adapt and learn in context, a characteristic often associated with large language models.", "section": "5.2 Emergence of In-context Tagging"}, {"figure_path": "https://arxiv.org/html/2502.11275/x5.png", "caption": "Figure 5: The data scaling trend of \\ouron the early 4.14.14.14.1M C4 instances and the massive 100100100100M instances.", "description": "This figure displays the impact of increasing the size of the training dataset on the performance of the Cuckoo model.  Two separate scaling experiments are shown: one starting with 4.1M instances from the C4 dataset, and another scaling up to 100M instances. The plots show how performance on three types of information extraction tasks (Basic IE, Query-based IE, and Instruction-following IE) varies with dataset size, demonstrating the effects of data scaling on Cuckoo's capabilities.", "section": "5.3 Data Scaling Trend"}, {"figure_path": "https://arxiv.org/html/2502.11275/x6.png", "caption": "Figure 6: The performance comparison between \\ourand LLMs on few-shot IE performance.", "description": "This figure compares the performance of Cuckoo and several LLMs on few-shot information extraction (IE) tasks.  It shows that Cuckoo, even without extensive fine-tuning, outperforms both open-source and closed-source LLMs across various IE tasks. The results highlight Cuckoo's superior efficiency and adaptability in learning from limited examples, demonstrating the effectiveness of its next token extraction (NTE) paradigm.", "section": "4.5 Instruction-following IE"}, {"figure_path": "https://arxiv.org/html/2502.11275/x7.png", "caption": "Figure 7: The scaling-up performance on adaptive supervision from CoNLL2003 of pre-trained IE models.", "description": "This figure demonstrates how the performance of different pre-trained information extraction (IE) models changes as the amount of adaptive supervision data increases.  Adaptive supervision refers to fine-tuning the model on a small set of examples for a specific task. The experiment uses the CoNLL2003 dataset and evaluates performance using the F1 score. The results show that as the number of training examples increases, the performance of all models improves. However, Cuckoo and Rainbow Cuckoo demonstrate significantly better scaling-up performance than NuNER, highlighting the effectiveness of the Cuckoo's pre-training approach.", "section": "4 Experiments"}]