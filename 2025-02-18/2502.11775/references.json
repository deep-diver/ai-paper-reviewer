{"references": [{"fullname_first_author": "Cobbe, K.", "paper_title": "Training verifiers to solve math word problems", "publication_date": "2021-10-14", "reason": "This paper introduces a novel approach to training large language models for improved reasoning abilities by using verifiers, which is a key concept in the development of video-SALMONN-01's reasoning capabilities."}, {"fullname_first_author": "Uesato, J.", "paper_title": "Solving math word problems with process- and outcome-based feedback", "publication_date": "2022-11-14", "reason": "This work introduces process reward models (PRMs), a core component of video-SALMONN-01's training strategy, for enhancing the LLM's reasoning abilities by providing feedback at each reasoning step."}, {"fullname_first_author": "Wang, P.", "paper_title": "Math-Shepherd: Verify and reinforce LLMs step-by-step without human annotations", "publication_date": "2024-00-00", "reason": "This paper presents a method for improving LLMs' reasoning capabilities without the need for manual annotations, which is highly relevant to video-SALMONN-01's goal of efficient reasoning optimization."}, {"fullname_first_author": "Zhang, R.", "paper_title": "Direct Preference Optimization of Video Large Multimodal Models from Language Model Reward", "publication_date": "2024-04-01", "reason": "This paper proposes direct preference optimization (DPO), a core component of video-SALMONN-01, providing a more efficient method for multimodal reward modeling than previous methods."}, {"fullname_first_author": "Li, B.", "paper_title": "LLaVA-OneVision: Easy visual task transfer", "publication_date": "2024-08-03", "reason": "This paper introduces LLaVA-OneVision, which serves as a strong baseline model against which video-SALMONN-01 is compared for performance evaluation."}]}