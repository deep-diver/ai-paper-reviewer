[{"figure_path": "https://arxiv.org/html/2502.13189/x1.png", "caption": "(a)", "description": "The figure shows the scaling law comparison between Mixture of Block Attention (MOBA) and full attention.  Specifically, it plots the language modeling (LM) loss on the validation set against the PFLOP/s-days, a measure of computational cost.  The plot shows that both MOBA and full attention exhibit similar scaling trends, demonstrating that MOBA achieves comparable scaling performance despite employing a sparse attention mechanism.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x2.png", "caption": "(b)", "description": "This figure shows the scaling law comparison between MOBA and full attention for the trailing LM loss on a validation set with a sequence length of 32K, focusing on the last 2K tokens. It visually represents the relationship between computational cost (PFLOP/s-days) and the trailing LM loss for both MOBA and full attention models of varying sizes.  The plot helps to understand how the loss changes as the model size and computational resources increase, providing insights into the efficiency and scalability of MOBA compared to the traditional full attention mechanism in the context of processing long sequences.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x3.png", "caption": "Figure 1: Illustration of mixture of block attention\u00a0(MoBA).\n(a) A running example of MoBA; (b) Integration of MoBA into Flash Attention.", "description": "Figure 1 illustrates the Mixture of Block Attention (MoBA) architecture. (a) shows a simplified example of MoBA in action, demonstrating how query tokens attend to only a subset of key-value blocks rather than the entire context, enabling efficient processing of long sequences.  A gating network dynamically selects the relevant blocks for each query.  (b) shows how MoBA integrates with FlashAttention, a high-performance attention mechanism, to further enhance computational efficiency.", "section": "2 Method"}, {"figure_path": "https://arxiv.org/html/2502.13189/x4.png", "caption": "(a)", "description": "This figure shows the scaling law comparison between MOBA and full attention for LM loss on the validation set with a sequence length of 8K.  The x-axis represents the computation (PFLOP/s-days), while the y-axis represents the LM loss. Two curves, one for MOBA and one for full attention, show how loss changes with increasing computational resources. The figure helps to demonstrate the scalability and efficiency of the MOBA model by comparing it to the full attention model.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x5.png", "caption": "(b)", "description": "This figure shows the scaling law comparison between MOBA and full attention for the trailing LM loss on the validation set. The sequence length is 32K, and only the last 2K tokens' loss is considered. The x-axis represents the training FLOPs in PFLOPs-days, and the y-axis represents the trailing LM loss.  The plot shows the loss curves for MOBA and full attention methods.  This helps to evaluate the long context capability of MOBA by looking at the loss of the final tokens in a sequence.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x6.png", "caption": "Figure 2: Efficiency of MoBA vs. full attention (implemented with Flash Attention). (a) 1M Model speedup evaluation: Computation time scaling of MoBA versus Flash Attention on 1M model with increasing sequence lengths (8K-1M). (b) Fixed Sparsity Ratio scaling: Computation time scaling comparison between MoBA and Flash Attention across increasing sequence lengths (8K-10M), maintaining a constant sparsity ratio of 95.31%percent95.3195.31\\%95.31 % (fixed 64 MoBA blocks with variance block size and fixed top-k=3).", "description": "Figure 2 presents a comparison of the computational efficiency of Mixture of Block Attention (MOBA) against standard full attention, both implemented using Flash Attention.  Subfigure (a) shows the speedup achieved by MOBA compared to full attention on a 1M parameter model across increasing sequence lengths (8K to 1M tokens).  Subfigure (b) demonstrates the scaling behavior while maintaining a consistent sparsity ratio of approximately 95.31% by adjusting block sizes and keeping the number of MoBA blocks fixed at 64 and top-k fixed at 3. This figure highlights MOBA's ability to achieve significant speedups over traditional full attention, particularly beneficial for longer sequences while controlling sparsity.", "section": "Implementation"}, {"figure_path": "https://arxiv.org/html/2502.13189/x7.png", "caption": "(a)", "description": "This figure shows the scaling law comparison between MOBA and full attention for the LM loss on the validation set with a sequence length of 8K.  The x-axis represents the compute (PFLOP/s-days), and the y-axis represents the LM loss.  The plot shows the loss curves for both MOBA and full attention models. The figure visually demonstrates that MOBA achieves a comparable performance to that of full attention, even with its sparse attention pattern.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x8.png", "caption": "(b)", "description": "This figure displays the scaling law comparison between Mixture of Block Attention (MOBA) and full attention, specifically focusing on the trailing Language Model (LM) loss on a validation set with a sequence length of 32K, considering only the last 1K tokens. It visually represents the relationship between the computational cost (PFLOP/s-days) and the trailing LM loss for both MOBA and full attention models of varying sizes. The graph helps assess the efficiency and long-context capabilities of MOBA by comparing how well each method handles long sequences and how the loss scales with increasing computational resources.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x9.png", "caption": "(c)", "description": "This figure displays the scaling law comparison between MOBA and full attention. Specifically, it shows the trailing LM loss on the validation set (sequence length = 32K, last 2K tokens). The fitted scaling law curve is also shown, illustrating the relationship between compute (in PFLOP/s-days) and LM loss. The plot allows one to observe how the loss changes for both the MOBA and Full Attention methods as the compute resources increase.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x10.png", "caption": "Figure 3: Scaling law comparison between MoBA and full attention. (a) LM loss on validation set (seqlen=8K); (b) trailing LM loss on validation set (seqlen=32K, last 1K tokens); (c) fitted scaling law curve.", "description": "This figure displays the results of scaling law experiments comparing MoBA (Mixture of Block Attention) and Full Attention mechanisms.  Subfigure (a) shows the Language Model (LM) loss on a validation set using sequences of length 8K. Subfigure (b) presents the trailing LM loss, focusing on the last 1K tokens of sequences with length 32K, which assesses the model's ability to accurately generate the concluding part of long sequences. Finally, subfigure (c) illustrates the fitted scaling law curve derived from the experimental data, showing the relationship between compute and model performance for both MoBA and Full Attention.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x11.png", "caption": "Figure 4: Fine-Grained Block Segmentation. The LM loss on validation set v.s. MoBA with different block granularity.", "description": "This figure displays the results of an ablation study on the impact of different block granularities in the Mixture of Block Attention (MoBA) model.  The experiment used a 1.5B parameter model with a 32K context length.  Various block sizes (8, 16, 32, 64, and 128) and corresponding top-k values (2, 4, 8, 16, and 32) were used to maintain a constant 75% sparsity.  The LM loss on the validation set is plotted against the different block granularities. This allows for an assessment of the model's performance with varying levels of granularity in block segmentation.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x12.png", "caption": "(a)", "description": "This figure shows the scaling law comparison between MOBA and full attention in terms of LM loss on the validation set with a sequence length of 8K.  The x-axis represents the compute (PFLOP/s-days), and the y-axis represents the LM loss. Two lines are plotted, one for MOBA and another for full attention. The figure demonstrates that MOBA achieves scaling performance comparable to full attention.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x13.png", "caption": "(b)", "description": "The figure shows the scaling law comparison between MOBA and full attention in terms of trailing LM loss (on validation set, sequence length = 32K, last 1K tokens).  It displays the relationship between the computational cost (PFLOP/s-days) and the trailing LM loss for models trained with MOBA and with full attention. The plot helps to visualize the efficiency gains achieved by using MOBA, especially when dealing with longer sequences. The scaling law curve shows that MoBA achieves comparable performance to full attention while using significantly less computation.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x14.png", "caption": "(c)", "description": "This figure displays the fitted scaling law curve, comparing the loss between MOBA and full attention models across different computational costs (PFLOP/s-days).  The x-axis represents the computational cost, while the y-axis shows the language model loss. The plot visualizes the relationship between computational resources and model performance for both MOBA and full attention.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x15.png", "caption": "Figure 5: Hybrid of MoBA and full attention. (a)\nposition-wise LM loss for MoBA, full attention, and MoBA/full hybrid training;\n(b) SFT LM loss w.r.t the number of full attention layers in layer-wise hybrid;\n(c) SFT trailing LM loss (seqlen=32K, last 2K) w.r.t the number of full attention layers in layer-wise hybrid.", "description": "Figure 5 presents a comparison of three different training approaches: using only MoBA, using only full attention, and a hybrid approach that combines both.  Panel (a) displays the position-wise LM loss across a sequence.  Panels (b) and (c) analyze the impact of progressively incorporating full attention layers during supervised fine-tuning (SFT).  Panel (b) shows the SFT LM loss while (c) shows the SFT trailing LM loss specifically focusing on the last 2K tokens of a 32K sequence, providing insights into the models' ability to generate long sequences effectively.", "section": "3.2 Hybrid of MoBA and Full Attention"}, {"figure_path": "https://arxiv.org/html/2502.13189/x16.png", "caption": "Figure 6: The continual pre-training and SFT recipes.", "description": "Figure 6 illustrates the training pipeline used for the continual pre-training and subsequent supervised fine-tuning (SFT) of language models.  The continual pre-training stage involves gradually increasing the context length (from 256K to 1M tokens) through a series of pre-training phases.  After each pre-training phase, supervised fine-tuning is performed with increasingly large context lengths (from 32K to 1M tokens).  This figure visually represents the steps involved in this iterative training process, clarifying the context window expansion at each stage and depicting the transitions between pre-training and SFT.", "section": "3.2 Hybrid of MoBA and Full Attention"}, {"figure_path": "https://arxiv.org/html/2502.13189/x17.png", "caption": "Figure 7: Performance of LLama-8B-1M-MoBA on the Needle in the Haystack benchmark\u00a0(upto 1M context length).", "description": "This figure displays the performance of the Llama-8B-1M-MoBA model on the Needle in a Haystack benchmark. The Needle in a Haystack benchmark tests a model's ability to find a specific piece of information within a large context.  The x-axis represents the context length (up to 1 million tokens), and the y-axis shows the performance score. The heatmap visualization makes it easy to see how the model's performance changes as the context length increases. This experiment demonstrates the model's ability to effectively process extremely long contexts.", "section": "3.3 Large Language Modeling Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.13189/x18.png", "caption": "(a) Scaling law (0-2k)", "description": "This figure displays the scaling law for the initial 2000 tokens.  It shows the relationship between the computational cost (PFLOP/s-days) and the loss achieved by models using MOBA (Mixture of Block Attention) versus full attention.  The x-axis represents the computational cost, and the y-axis represents the loss. The graph shows two curves, one for MOBA and one for the full attention approach, allowing comparison of the performance and efficiency of the two methods for this portion of the sequence.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x19.png", "caption": "(b) Scaling law (2-4k)", "description": "This figure shows the scaling law for positions 2000-4000.  It plots the PFLOP/s-days (floating point operations per second-days, a measure of computational cost) against the LM Loss (language model loss, a measure of model performance). Two lines are shown: one for the MOBA (Mixture of Block Attention) model and one for a full attention model. The lines illustrate the relationship between computational cost and model performance for different model sizes in this specific token range.  The slope of the lines indicates how the loss changes as computational cost increases.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x20.png", "caption": "(c) Scaling law (4-6k)", "description": "This figure shows the scaling law for positions 4000-6000, comparing the performance of MoBA and full attention in terms of LM loss and PFLOP/s-days. The x-axis represents the computational cost (PFLOP/s-days), and the y-axis represents the LM loss. The lines represent different model sizes, and the plot illustrates how the LM loss scales with computational resources for both MoBA and full attention.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x21.png", "caption": "(d) Scaling law (6-8k)", "description": "This figure shows the scaling law for positions 6000 to 8000.  It plots the PFLOP/s-days (performance) against the LM Loss (training loss) for both MoBA (Mixture of Block Attention) and Full Attention models.  The lines represent different model sizes, illustrating how training loss changes with compute and model size for each approach.  This helps determine the efficiency and scaling behavior of MoBA relative to traditional full attention mechanisms for long sequences.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x22.png", "caption": "(e) Scaling law (8-10k)", "description": "This figure shows the scaling law for positions 8000 to 10000.  It plots the PFLOP/s-days against the LM loss for both MOBA (Mixture of Block Attention) and Full Attention models.  The graph illustrates how the computational cost (PFLOP/s-days) scales with the language model loss (LM loss) for different model sizes in this specific range of token positions within a sequence. It helps to assess the computational efficiency of MOBA compared to traditional Full Attention in a particular region of the sequence being processed.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x23.png", "caption": "(f) Scaling law (10-12k)", "description": "This figure shows the scaling law for positions 10k-12k. It is part of a set of figures (Figure 8) demonstrating scaling laws for various ranges of token positions (0-16k and 16-32k) and comparing the performance of MOBA and full attention models across different computational costs.  The x-axis represents the computational cost (PFLOP/s-days), and the y-axis represents the LM loss.  The curves show the relationship between computational resources and model performance for both MOBA and the full-attention baseline.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x24.png", "caption": "(g) Scaling law (12-14k)", "description": "This figure is part of a scaling law experiment comparing MOBA and Full Attention. Specifically, it shows the relationship between PFLOP/s-days (a metric related to computational cost) and LM loss (a measure of model performance) for the token positions 12000-14000 in the training data.  It demonstrates how the model performance changes as computational resources are scaled, allowing assessment of both MOBA's efficiency and scalability compared to the baseline Full Attention mechanism.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x25.png", "caption": "(h) Scaling law (14-16k)", "description": "This figure shows the scaling law for positions 14,000 to 16,000.  It presents the relationship between compute (PFLOP/s-days) and LM loss for both the MOBA (Mixture of Block Attention) and Full Attention models. The plot helps visualize how the loss changes with varying computational resources, offering insights into the efficiency of MOBA compared to traditional full attention. The lines represent the results from different model sizes. ", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x26.png", "caption": "Figure 8: Scaling laws for positions 0-16k", "description": "This figure displays scaling laws for different positions within the context window, specifically from position 0 to 16k.  It shows the relationship between the computational cost (in PFLOP/s-days) and the loss of the language model. The plots are separated into subfigures (a) through (h), each covering a 2k-token range within the 0-16k span.  Each subfigure presents the scaling laws for both MOBA (Mixture of Block Attention) and full attention models.  By comparing the curves for MOBA and full attention across different parts of the context window, we can analyze how efficiently MOBA uses computational resources while maintaining performance comparable to full attention.", "section": "3.1 Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x27.png", "caption": "(i) Scaling law (16-18k)", "description": "This figure presents the scaling law for positions 16,000 to 18,000 in the context of a large language model. It showcases the relationship between the computational resources (PFLOP/s-days) and the resulting loss (LM Loss) using two different attention mechanisms: MoBA (Mixture of Block Attention) and Full Attention.  The graph allows for a visual comparison of the performance and efficiency of both mechanisms at this specific context window within the model.", "section": "Scaling Law Experiments and Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2502.13189/x28.png", "caption": "(j) Scaling law (18-20k)", "description": "This figure shows the scaling law for positions 18k to 20k in a language model.  It plots the performance (likely LM loss) of the model against the computational resources (PFLOP/s-days). Two lines are shown, one for the model using the proposed Mixture of Block Attention (MOBA) and another for the model using full attention. The graph illustrates how the loss changes with increasing computational resources for the specified position range within the text sequence.  This helps assess the efficiency and scalability of MOBA compared to the traditional full attention mechanism for long sequences.", "section": "Scaling Law Experiments and Ablation Studies"}]