[{"figure_path": "https://arxiv.org/html/2502.14397/x2.png", "caption": "Figure 1: PhotoDoodle can mimic the styles and techniques of human artists in creating photo doodles, adding decorative elements to photos while maintaining perfect consistency between the pre- and post-edit states.", "description": "Figure 1 showcases PhotoDoodle's ability to generate artistic photo edits.  It demonstrates the system's capacity to add decorative elements to photographs, mimicking the styles and techniques of human artists.  Crucially, the examples highlight PhotoDoodle's capability to seamlessly integrate these additions into the original photo, maintaining a consistent appearance between the before and after images. The edits range in style from adding cartoon elements and magical effects to incorporating hand-drawn lines and star decorations, illustrating the system's versatility.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2502.14397/x3.png", "caption": "Figure 2: The overall architecture and training prodigim of photodoodle. The ominiEditor and EditLora all follow the lora training prodigm. We use a high rank lora for pre-training the OmniEditor on a large-scale dataset for general-purpose editing and text-following capabilities, and a low rank lora for fine-tuning EditLoRA on a small set of paired stylized images to capture individual artists\u2019 specific styles and strategies for efficient customization. We encode the original image into a condition token and concatenate it with a noised latent token, controlling the generation outcome through MMAttention.", "description": "PhotoDoodle's architecture involves a two-stage training process. First, a high-rank LoRA (Low-Rank Adaptation) trains the OmniEditor on a large dataset for general image editing and text-to-image generation.  Second, a low-rank LoRA fine-tunes the EditLoRA on a smaller dataset of artist-specific photo doodles (before-and-after pairs). This captures individual artists' unique styles. During inference, the original image is encoded as a condition token and concatenated with a noisy latent token. MMAttention (Multi-Modal Attention) controls the generation based on these inputs.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.14397/x4.png", "caption": "Figure 3: The generated results of PhotoDoodle. PhotoDoodle can mimic the manner and style of artists creating photo doodles, enabling instruction-driven high-quality image editing.", "description": "Figure 3 showcases the results of PhotoDoodle, demonstrating its ability to generate high-quality photo doodles that seamlessly blend artistic styles with original images.  The examples show how PhotoDoodle can mimic diverse artistic techniques, including adding decorative elements, applying stylized modifications, and maintaining contextual coherence. The figure highlights PhotoDoodle's capacity for instruction-driven image editing, where specific instructions guide the generation of personalized photo doodles, resulting in images consistent with both the user's request and the artist's style.", "section": "4.2 Generation Results"}, {"figure_path": "https://arxiv.org/html/2502.14397/x5.png", "caption": "Figure 4:  Compared to baseline methods, PhotoDoodle demonstrates superior instruction following, image consistency, and editing effectiveness.", "description": "Figure 4 presents a qualitative comparison of PhotoDoodle against three baseline methods (InstructPix2Pix, MagicBrush, and SDEdit) in both universal and customized image editing tasks.  The top row showcases universal image editing, where the instruction is relatively simple and general. The bottom row depicts customized image editing with more complex and specific directives.  For each task and method, the input image, the result from each method, and the instruction are shown. The figure highlights PhotoDoodle's superior performance in accurately following instructions (instruction following), maintaining the original image's consistency (image consistency), and producing effective and high-quality edits (editing effectiveness).  PhotoDoodle produces results that are visually more pleasing and closely adhere to the instructions given, compared to the noticeably inferior results of the baselines.", "section": "4.2. Qualitative Evaluation"}, {"figure_path": "https://arxiv.org/html/2502.14397/x6.png", "caption": "Figure 5: Ablation study results. Without OmniEditor pre-training, training EditLoRA directly reduces the harmony between sketches and photos and weakens text-following capabilities; removing Position Encoding Cloning decreases output consistency and introduces unwanted background changes; using only pre-trained OmniEditor without EditLoRA significantly reduces the degree of stylization.", "description": "This ablation study analyzes the impact of different components of the PhotoDoodle model on its performance.  The figure shows that removing any key component significantly degrades the results.  Specifically:\n\n* **No OmniEditor Pre-training:** Training EditLoRA alone significantly reduces the harmony between sketches and photos and impairs the model's ability to follow text instructions accurately. This highlights the importance of the OmniEditor's general-purpose image editing capabilities as a foundation.\n* **No Position Encoding Cloning:**  Removing this mechanism reduces output consistency and introduces unwanted background modifications. This demonstrates the crucial role of maintaining background consistency during the editing process.\n* **Only Pre-trained OmniEditor (No EditLoRA):** Using only the pre-trained model without fine-tuning it with EditLoRA significantly reduces the degree of stylization. This shows that EditLoRA is essential for learning and applying the unique editing styles of individual artists.", "section": "4.3. Ablation Study"}]