[{"figure_path": "https://arxiv.org/html/2502.14776/x1.png", "caption": "Figure 1. The number of papers received annually by the arXiv website from 2010 to 2025, with data sourced from our arXiv database. The projected number of submissions for 2025 is anticipated to be five times greater than that of 2010.", "description": "This figure shows a line graph illustrating the significant growth in the number of research papers submitted to arXiv.org over time, from 2010 to 2025.  The data, sourced from the authors' own arXiv database, reveals an exponential increase in submissions, culminating in a projected number for 2025 that is five times higher than the number in 2010. This visually represents the substantial increase in research output and the challenges this presents for researchers seeking to keep up with the latest advancements.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2502.14776/x2.png", "caption": "Figure 2. Pipeline of SurveyX.", "description": "This figure details the pipeline of the SURVEYX system for automated survey generation.  It visually depicts the two main phases: the Preparation Phase and the Generation Phase.  The Preparation Phase involves acquiring and pre-processing relevant references using techniques like keyword expansion and the AttributeTree method. The Generation Phase then utilizes this processed information to generate the survey's outline and content, incorporating figures and tables, and finally refining the draft for improved quality and presentation.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.14776/x3.png", "caption": "Figure 3. An example of generating secondary outlines. LLMs first generate hints based on the attribute tree to guide the generating of the secondary outline. Then, by synthesizing all hints, LLMs identify the most suitable entry points to determine the segmentation strategy and generate the secondary outline.", "description": "This figure illustrates the process of generating secondary outlines using Large Language Models (LLMs).  The process starts with LLMs generating hints based on information extracted from the Attribute Tree (a pre-processing method described in the paper). These hints guide the LLM in creating the secondary outline. Subsequently, the LLM synthesizes all generated hints to identify the most appropriate entry points for creating a well-structured outline. This method helps ensure the resulting outline is comprehensive and well-organized.", "section": "3.2 Generation Phase"}, {"figure_path": "https://arxiv.org/html/2502.14776/x4.png", "caption": "Figure 4. Human evaluation results.", "description": "This figure presents a radar chart visualizing the results of human evaluation for the SURVEYX system and its baselines.  Each axis represents a key aspect of survey quality (Coverage, Structure, Relevance, Synthesis, and Critical Analysis). The chart compares the performance of SURVEYX against human-written surveys and two baseline automated methods, illustrating SURVEYX's strengths and weaknesses across multiple evaluation dimensions and highlighting its performance relative to both automated and human expert-level survey generation.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.14776/x5.png", "caption": "Figure 5. Content coverage prompt for evaluation.", "description": "This figure shows the prompt used for evaluating the \"content coverage\" aspect of automatically generated surveys.  The prompt instructs evaluators to rate the survey's comprehensiveness based on five levels: (1) very limited coverage, (2) some coverage but with significant omissions, (3) generally comprehensive but with a few missing points, (4) comprehensive but with only very minor omissions, and (5) completely comprehensive. This structured approach helps ensure consistent and reliable evaluation of the generated survey content.", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2502.14776/x6.png", "caption": "Figure 6. Content structure prompt for evaluation.", "description": "This figure presents the prompt used for evaluating the structure of the automatically generated surveys. The prompt guides evaluators to assess the logical organization and coherence of the generated survey's sections and subsections, considering aspects like clarity of connections, orderliness, and the presence of any redundancies or inconsistencies.  Scores from 1 to 5 are used to rate the survey's structure, with 1 representing a poorly structured survey and 5 signifying a highly organized and logically sound structure.", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2502.14776/x7.png", "caption": "Figure 7. Content relevance prompt for evaluation.", "description": "This figure shows the prompt used for evaluating the relevance of the content in a generated academic survey.  The prompt instructs the evaluator to score the survey's relevance to the given topic on a scale of 1 to 5, with detailed descriptions for each score level.  The descriptions range from completely off-topic (score 1) to entirely focused and relevant (score 5). This detailed rubric helps ensure consistent and objective evaluation of content relevance.", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2502.14776/x8.png", "caption": "Figure 8. Content synthesis prompt for evaluation.", "description": "This figure shows the prompt used for evaluating the 'synthesis' aspect of content quality in automatically generated surveys.  The prompt instructs evaluators to assess how well the survey integrates different studies, identifies patterns or contradictions, and builds a cohesive framework beyond simple summaries.  A 5-point Likert scale is provided, ranging from a purely descriptive summary (score 1) to a masterfully integrated analysis with novel perspectives (score 5).", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2502.14776/x9.png", "caption": "Figure 9. Content critical analysis prompt for evaluation.", "description": "This figure presents the prompt used for human evaluators to assess the \"critical analysis\" aspect of automatically generated surveys.  The prompt instructs evaluators to score the survey based on the depth of critique applied to existing studies, including identification of methodological limitations, theoretical inconsistencies, and research gaps. A 5-point Likert scale is provided, ranging from a score of 1 (no critical analysis) to 5 (rigorous critical analysis with identified research gaps and proposed future directions).", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2502.14776/x10.png", "caption": "Figure 10. Citation prompt for evaluation.", "description": "This figure shows the prompt used for evaluating the quality of citations in the automatically generated surveys.  The prompt presents evaluators with a claim and its purported source. The evaluators are then asked to judge if the claim faithfully reflects the information presented in the source. This helps assess whether the generated surveys accurately cite and represent the information from referenced sources.", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2502.14776/x11.png", "caption": "Figure 11. Judge relevance for reference paper.", "description": "This figure shows the prompt used for evaluating the relevance of a reference paper to a given topic. Evaluators are presented with a paper abstract and the topic, then asked to determine if the paper is relevant.  The prompt is designed to elicit a simple \"yes\" or \"no\" response, facilitating efficient evaluation.", "section": "4.1 Evaluation Metrics"}, {"figure_path": "https://arxiv.org/html/2502.14776/x12.png", "caption": "Figure 12. Prompts for Naive RAG.", "description": "This figure shows the prompt used for the Naive RAG (Retrieval Augmented Generation) baseline method in the paper's experiments.  The prompt provides the role of an academic reviewer and research synthesizer, outlining the background, goals, constraints, and desired output format (Markdown).  It includes instructions for analyzing abstracts, organizing information, and writing a coherent review paper that meets academic standards.  Specific instructions are given on how to handle formatting, citations, and originality. The provided abstracts are a placeholder that would be replaced with actual text during experimentation.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.14776/x13.png", "caption": "Figure 13. Method paper attribute tree.", "description": "This figure presents a hierarchical tree structure representing the key attributes and sub-attributes used to categorize and analyze method papers.  Each node in the tree signifies a crucial aspect of a method paper, providing a systematic framework for extracting key information and facilitating a comprehensive understanding. The top-level node is \"Method Paper,\" which branches down into sections like \"Background,\" \"Problem,\" \"Idea,\" \"Method,\" \"Experiments,\" \"Conclusion,\" \"Discussion,\" and \"Other Info.\"  Each of these sections contains further granular sub-attributes that allow for detailed extraction of specific information concerning each aspect. This detailed breakdown aids in the efficient pre-processing of research papers for automated survey generation.", "section": "3.1.2 References Pre-processing"}, {"figure_path": "https://arxiv.org/html/2502.14776/x14.png", "caption": "Figure 14. Benchmark paper attribute tree.", "description": "This figure presents a hierarchical tree structure that outlines the key attributes or components typically found in a benchmark paper.  It's designed to aid in the extraction of relevant information from benchmark papers, efficiently structuring the data for use in subsequent survey generation processes. The tree is broken down into several main categories (Background, Problem, Idea, Dataset, Metrics, Experiments, Conclusion, Discussion, and Other Info) with sub-categories under each to capture specific details like problem definition, dataset characteristics, and experimental methodology.", "section": "3.1.2 References Pre-processing"}, {"figure_path": "https://arxiv.org/html/2502.14776/x15.png", "caption": "Figure 15. Theory paper attribute tree.", "description": "This figure presents a hierarchical structure, or tree, that outlines the key attributes typically found in a theory paper.  It's a template to guide the extraction of essential information from theory papers for use in the SURVEYX system. The tree organizes information into categories like Background (problem context), Problem (definition and challenges), Idea (intuition, innovation), Theory (perspective, proof), Experiments (setup and process), Conclusion, Discussion (advantages, limitations, future work), and Other Info (additional details).  Each category has sub-attributes to ensure comprehensive data capture for accurate and thorough survey generation.", "section": "3.1.2 References Pre-processing"}, {"figure_path": "https://arxiv.org/html/2502.14776/x16.png", "caption": "Figure 16. Survey paper attribute tree.", "description": "This figure presents a hierarchical structure, or tree, outlining the key attributes that constitute a survey paper.  It's a template for organizing information when generating survey papers automatically. The tree includes sections such as background (purpose, scope), problem (definition, key obstacles), architecture (perspective, fields/stages), conclusion (comparisons, results), discussion (advantages, limitations, gaps, future work/trends), and other relevant details. This breakdown helps structure the writing process by providing a systematic framework for the information included in a survey paper.", "section": "3.1.2 References Pre-processing"}]