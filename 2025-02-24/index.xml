<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-02-24s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/</link><description>Recent content in 2025-02-24s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Fri, 21 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/index.xml" rel="self" type="application/rss+xml"/><item><title>Evaluating Multimodal Generative AI with Korean Educational Standards</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15422/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15422/</guid><description>KoNET: Evaluating multimodal AI in Korean with edu standards.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15422/cover.png"/></item><item><title>LightThinker: Thinking Step-by-Step Compression</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15589/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15589/</guid><description>LightThinker: LLMs dynamically compress intermediate steps, reducing memory &amp;amp; boosting reasoning efficiency without sacrificing accuracy.</description></item><item><title>One-step Diffusion Models with $f$-Divergence Distribution Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15681/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15681/</guid><description>f-distill: One-step diffusion models through f-divergence minimization, outperforming reverse-KL with better mode coverage and lower variance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15681/cover.png"/></item><item><title>The Relationship Between Reasoning and Performance in Large Language Models -- o3 (mini) Thinks Harder, Not Longer</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15631/</link><pubDate>Fri, 21 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15631/</guid><description>LLMs: 03-mini achieves superior accuracy without longer reasoning chains, suggesting &amp;rsquo;thinking harder&amp;rsquo; matters more than &amp;rsquo;thinking longer'.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15631/cover.png"/></item><item><title>CrossOver: 3D Scene Cross-Modal Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15011/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15011/</guid><description>CrossOver: Flexible scene-level cross-modal alignment via modality-agnostic embeddings, unlocking robust 3D scene understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15011/cover.png"/></item><item><title>InterFeedback: Unveiling Interactive Intelligence of Large Multimodal Models via Human Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15027/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15027/</guid><description>InterFeedback: LMMs need better human feedback to enhance AI assistants!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15027/cover.png"/></item><item><title>Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15086/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15086/</guid><description>LLMs fail to act safely when considering user-specific safety standards, which were made to be solved via new benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15086/cover.png"/></item><item><title>KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14949/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14949/</guid><description>KITAB-Bench: A new multi-domain Arabic OCR benchmark to bridge the performance gap with English OCR technologies.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14949/cover.png"/></item><item><title>LLM-Microscope: Uncovering the Hidden Role of Punctuation in Context Memory of Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15007/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15007/</guid><description>LLMs use punctuation in context memory, surprisingly boosting performance by using seemingly trivial tokens.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15007/cover.png"/></item><item><title>PhotoDoodle: Learning Artistic Image Editing from Few-Shot Pairwise Data</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14397/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14397/</guid><description>PhotoDoodle: Mimicking artistic image editing with personalized decorative elements through learning from few-shot pairwise data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14397/cover.png"/></item><item><title>ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14637/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14637/</guid><description>ReQFlow: Efficiently generate high-quality protein backbones with rectified quaternion flow, outperforming existing methods in speed and designability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14637/cover.png"/></item><item><title>StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14494/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14494/</guid><description>Current LLM evaluation benchmarks often overlook the structural dependencies in multi-turn dialogues, treating them as simple concatenations of single-turn interactions. This approach neglects user in&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14494/cover.png"/></item><item><title>SurveyX: Academic Survey Automation via Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14776/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14776/</guid><description>SURVEYX automates academic survey generation, enhancing content and citation quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14776/cover.png"/></item><item><title>UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15082/</link><pubDate>Thu, 20 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15082/</guid><description>UPCORE reduces unintended unlearning effects via coreset selection, balancing knowledge removal and utility preservation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.15082/cover.png"/></item><item><title>JL1-CD: A New Benchmark for Remote Sensing Change Detection and a Robust Multi-Teacher Knowledge Distillation Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.13407/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.13407/</guid><description>JL1-CD: New all-inclusive dataset &amp;amp; multi-teacher knowledge distillation framework for robust remote sensing change detection, achieving state-of-the-art results!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.13407/cover.png"/></item><item><title>SIFT: Grounding LLM Reasoning in Contexts via Stickers</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14922/</link><pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14922/</guid><description>SIFT: Grounds LLM reasoning with &amp;lsquo;Stickers&amp;rsquo; to highlight context and improve accuracy without extra training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14922/cover.png"/></item><item><title>MoBA: Mixture of Block Attention for Long-Context LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.13189/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.13189/</guid><description>MoBA: Mixture of Block Attention enables efficient long-context LLMs by dynamically selecting relevant blocks, improving performance without compromising efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.13189/cover.png"/></item><item><title>Think Inside the JSON: Reinforcement Strategy for Strict LLM Schema Adherence</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14905/</link><pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14905/</guid><description>ThinkJSON presents a reinforcement learning strategy to enforce strict schema adherence in LLM generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.14905/cover.png"/></item><item><title>MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.11663/</link><pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2502.11663/</guid><description>MaskGWM: Improves driving world models by using video mask reconstruction for better generalization.</description></item><item><title>WHAC: World-grounded Humans and Cameras</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2403.12959/</link><pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2403.12959/</guid><description>WHAC: Grounding humans and cameras together!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-02-24/2403.12959/cover.png"/></item></channel></rss>