[{"figure_path": "https://arxiv.org/html/2502.14494/x1.png", "caption": "Figure 1: The Structural Flow Taxonomy includes six fundamental structures, each used to describe the inter-turn relationships in multi-turn dialogues. It can be applied to analyze any dialogue and generate specific structural flows.", "description": "This figure presents a taxonomy of six fundamental inter-turn relationships in multi-turn dialogues.  These relationships are: Follow-up, Refinement, Recall, Summary, Expansion, and Unrelatedness. Each relationship is visually represented, showing how subsequent turns in a conversation relate to previous turns. The taxonomy provides a framework for analyzing existing dialogues and generating new dialogues with specified structural flows, allowing for more nuanced and controlled evaluation of multi-turn instruction following capabilities.", "section": "3 StructFlowBench"}, {"figure_path": "https://arxiv.org/html/2502.14494/x2.png", "caption": "Figure 2: The construction pipeline of StructFlowBench. First, tasks, topics, user types, and structural flow templates are defined. Then, dialogue data is generated in two steps: intermediate dialogue plans (i.e., the summarized prompts) are created from the structural flow, followed by generating complete dialogues from these plans. Finally, intra-turn constraints are extracted by GPT-4o, and structural constraints are added based on the structural flow information.", "description": "The figure illustrates the three-stage pipeline for constructing the StructFlowBench dataset.  First, parameters such as the task type, topic, user type, and structural flow template are defined. Second, a two-step dialogue generation process creates the dialogue data. This begins with generating intermediate dialogue plans (summarized prompts) from the structural flow templates, followed by generating complete dialogues from these plans using GPT-4. Finally, the generated dialogues undergo constraint extraction.  Intra-turn constraints are extracted automatically using GPT-4, while the structural constraints are added based on the predefined structural flow information.", "section": "3.3 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.14494/x3.png", "caption": "Figure 3: The comprehensive complex scenario evaluation heatmap of five multi-turn dialogue datasets.", "description": "This heatmap visualizes a comparative analysis of five multi-turn dialogue datasets across three key aspects: logical coherence, goal clarity, and transition naturalness.  Each dataset's performance is rated on a scale of 1 to 5 for each aspect, with 5 representing the highest score.  Additionally, a 'Confusion Factor' is calculated to indicate the proportion of dialogues in each dataset that scored 4 or higher, suggesting a higher likelihood of being mistaken for real human interactions.  This provides a comprehensive overview of the suitability of each dataset for evaluating complex conversational scenarios.", "section": "4.3 Further Analysis"}, {"figure_path": "https://arxiv.org/html/2502.14494/x4.png", "caption": "Figure 4: The radar chart of intra-turn-constraint-categorized performance (a) and task-categorized performance (b).", "description": "Figure 4 presents a comparative analysis of the performance of various LLMs across two different categorization schemes. Panel (a) shows the models' performance across various intra-turn constraints, such as content, style, format, keyword, and inverse constraints.  Each axis represents a constraint type, and the distance from the center indicates the model's proficiency in that constraint. Panel (b) displays the models' performance across seven different task categories, including fact-based questions, open-ended questions, practical writing, creative writing, professional writing, casual chat, and task-oriented role-playing.  This allows for a comprehensive view of the strengths and weaknesses of each model across both constraint types and task types.", "section": "4.2 Main Results"}, {"figure_path": "https://arxiv.org/html/2502.14494/x5.png", "caption": "Figure 5: Intermediate Dialogue Plan Generation Template", "description": "This template guides the generation of intermediate dialogue plans.  It structures the process by first requiring understanding of background knowledge and a dialogue structure template.  Then, it specifies setting a user purpose (topic and type) and defining the overall user goal.  Finally, it details generating summarized user prompts for each turn in the dialogue, ensuring they align naturally with the overall dialogue structure and user goals. The output format is specified to ensure consistency and usability.", "section": "3.3 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.14494/x6.png", "caption": "Figure 6: Complete Dialogue Generation Prompt Template", "description": "This figure displays the prompt template used to generate complete dialogues from the intermediate dialogue plans.  The prompt instructs the model to expand the summarized user prompts into realistic and detailed user prompts, incorporating various constraints from the \"Constraint Guideline.\" These constraints ensure the expanded prompts align with the summarized prompts, reflecting genuine user inquiries and adhering to specified user characteristics.  The output should follow a JSON format.  The template includes sections for background information and specific instructions to guide the model's response generation.", "section": "3.3 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.14494/x7.png", "caption": "Figure 7: Constraint Extraction Prompt Template", "description": "This figure details the prompt template used for extracting constraints from a multi-turn dialogue.  The prompt instructs an LLM to identify and categorize atomic constraint expressions from a given user prompt.  It defines atomic constraint expressions as the smallest units describing task requirements within instructions.  The prompt provides examples and a guideline specifying the types and definitions of atomic constraints (e.g., Inverse Constraint, Keyword/Element Constraint, Style Constraint).  The output format is JSON, structured to categorize each extracted constraint by type, content (expressed as a question), and an explanation justifying the type classification.", "section": "3.3 Data Construction Pipeline"}, {"figure_path": "https://arxiv.org/html/2502.14494/x8.png", "caption": "Figure 8: GPT-4o Evaluation Prompt Template", "description": "Figure 8 presents the prompt template utilized for the GPT-40 evaluation in the StructFlowBench. This template guides the GPT-40 model in assessing the alignment of an LLM response with the corresponding user prompt and its specified constraints.  It provides GPT-40 with the conversation history, the user's prompt, the LLM's response, and a checklist of constraints to verify. The output from GPT-40 consists of a 'yes' or 'no' judgment for each constraint, accompanied by a detailed explanation justifying the decision.", "section": "3.5 Evaluation"}]