[{"figure_path": "https://arxiv.org/html/2502.15011/x1.png", "caption": "Figure 1: CrossOver is a cross-modal alignment method for 3D scenes that learns a unified, modality-agnostic embedding space, enabling a range of tasks. For example, given the 3D CAD model of a query scene and a database of reconstructed point clouds, CrossOver can retrieve the closest matching point cloud and, if object instances are known, it can identify the individual locations of furniture CAD models with matched instances in the retrieved point cloud, using brute-force alignment. This capability has direct applications in virtual and augmented reality.", "description": "CrossOver is a method for aligning different data modalities (images, point clouds, CAD models, floorplans, and text) representing the same 3D scene.  It learns a shared embedding space where similar scenes are close together, regardless of the input modality. This allows for tasks like retrieving matching point clouds given a CAD model, or locating furniture in a point cloud based on its CAD model.  The figure illustrates the overall process of CrossOver, showing how it takes various scene inputs, processes them through modality-specific encoders, merges the results into a unified representation, and enables scene and object retrieval.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2502.15011/x2.png", "caption": "Figure 2: Overview of CrossOver. Given a scene \ud835\udcae\ud835\udcae\\mathcal{S}caligraphic_S and its instances \ud835\udcaaisubscript\ud835\udcaa\ud835\udc56\\mathcal{O}_{i}caligraphic_O start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represented across different modalities \u2110,\ud835\udcab,\u2133,\u211b,\u2131\u2110\ud835\udcab\u2133\u211b\u2131\\mathcal{I},\\mathcal{P},\\mathcal{M},\\mathcal{R},\\mathcal{F}caligraphic_I , caligraphic_P , caligraphic_M , caligraphic_R , caligraphic_F, the goal is to align all modalities within a shared embedding space. The Instance-Level Multimodal Interaction module captures modality interactions at the instance level within the context of a scene. This is further enhanced by the Scene-Level Multimodal Interaction module, which jointly processes all instances to represent the scene with a single feature vector \u2131\ud835\udcaesubscript\u2131\ud835\udcae\\mathcal{F_{S}}caligraphic_F start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT. The Unified Dimensionality Encoders eliminate dependency on precise semantic instance information by learning to process each scene modality independently while interacting with \u2131\ud835\udcaesubscript\u2131\ud835\udcae\\mathcal{F_{S}}caligraphic_F start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT.", "description": "CrossOver is a multimodal scene understanding framework that aligns different modalities (RGB images, point clouds, CAD models, floor plans, and text descriptions) within a shared embedding space.  It uses a two-stage approach: first, an Instance-Level Multimodal Interaction module captures interactions between modalities at the instance level; second, a Scene-Level Multimodal Interaction module combines instance-level information to generate a single scene-level feature vector.  Finally, Unified Dimensionality Encoders process each modality independently, interacting with the scene-level feature vector, to eliminate the need for precise semantic instance information and enable cross-modal alignment at the scene level.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.15011/x3.png", "caption": "Figure 3: Cross-modal Scene Retrieval Inference Pipeline. Given a query modality (\ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P) that represents a scene, we obtain with the corresponding dimensionality encoder its feature vector (\u21313\u2062Dsubscript\u21313\ud835\udc37\\mathcal{F}_{3D}caligraphic_F start_POSTSUBSCRIPT 3 italic_D end_POSTSUBSCRIPT) in the shared cross-modal embedding space. We identify the closest feature vector (\u21312\u2062Dsubscript\u21312\ud835\udc37\\mathcal{F}_{2D}caligraphic_F start_POSTSUBSCRIPT 2 italic_D end_POSTSUBSCRIPT) in the target modality (\u2131\u2131\\mathcal{F}caligraphic_F) and retrieve the corresponding scene from a database of scenes in \u2131\u2131\\mathcal{F}caligraphic_F.", "description": "This figure illustrates the inference pipeline for cross-modal scene retrieval.  It begins with a query scene represented by a specific modality (e.g., a point cloud). A dimensionality-specific encoder processes this query, generating a feature vector in a shared embedding space. This feature vector is then compared to feature vectors of scenes in the target modality (e.g., floorplans) stored in a database. The scene with the closest feature vector in the target modality is identified as the retrieved scene.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2502.15011/x4.png", "caption": "(a) Instance Matching Recall on ScanNet", "description": "This figure shows the performance of CrossOver and other multi-modal methods on instance-level retrieval tasks using the ScanNet dataset.  Specifically, it displays the recall (the percentage of correctly identified instances) for various modality pairs (e.g., image to text, point cloud to mesh). Each bar represents a different modality pair, and the height of the bar indicates the recall rate.  The results showcase CrossOver's ability to achieve high recall across diverse modality combinations, demonstrating its effectiveness in cross-modal instance retrieval. The chart visually compares CrossOver's performance against other state-of-the-art multi-modal methods, highlighting CrossOver's superior performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x5.png", "caption": "(b) Scene-Level Matching Recall on ScanNet and 3RScan", "description": "This figure presents a comparison of scene-level matching recall achieved by different methods on the ScanNet and 3RScan datasets. Scene-level matching recall assesses the ability of a method to retrieve the correct scene from a database given a query scene represented by a specific modality (e.g., RGB images or point clouds). The figure illustrates the superior performance of the proposed CrossOver method compared to several baselines, highlighting its robustness across different datasets and modalities.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x6.png", "caption": "Figure 4: \nCross-Modal Instance Retrieval on ScanNet and 3RScan. (a) Even though CrossOver does not explicitly train all modality combinations, it achieves emergent behavior within the embedding space. The same applies to our Instance Baseline (Ours). CrossOver performs better than our self-baseline since it incorporates more scene context in the fusion of modalities. (b) Our method outperforms all baselines in all datasets, showcasing the robustness of learned cross-modal interactions.", "description": "Figure 4 presents the results of cross-modal instance retrieval experiments conducted on the ScanNet and 3RScan datasets.  Part (a) compares the performance of CrossOver to a simpler 'Instance Baseline' which doesn't leverage scene-level context. This demonstrates that CrossOver's superior performance stems from its ability to learn relationships between modalities at the scene level, not just the object level. Although CrossOver doesn't explicitly train on every possible modality combination, it shows emergent cross-modal behavior.  Part (b) shows that CrossOver significantly outperforms other existing multi-modal methods (ULIP-2 and PointBind) on both datasets, highlighting the robustness and effectiveness of its learned cross-modal interactions.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x7.png", "caption": "Figure 5: Cross-Modal Scene Retrieval Qualitative Results on ScanNet. Given a scene in query modality \u2131\u2131\\mathcal{F}caligraphic_F, we aim to retrieve the same scene in target modality \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P. While PointBind and the Instance Baseline do not retrieve the correct scene within the top-4 matches, CrossOver identifies it as the top-1 match. Notably, temporal scenes appear close together in CrossOver\u2019s embedding space (e.g., k=2\ud835\udc582k=2italic_k = 2, k=3\ud835\udc583k=3italic_k = 3), with retrieved scenes featuring similar object layouts to the query scene, such as the red couch in k=4\ud835\udc584k=4italic_k = 4.", "description": "This figure demonstrates the effectiveness of the CrossOver model in cross-modal scene retrieval.  Using a floorplan as input (query modality \u2131), the model successfully retrieves the corresponding point cloud scene (target modality \ud835\udcab) as the top match.  In contrast, PointBind and a simpler baseline model fail to identify the correct scene among the top four results.  The visualization also highlights a noteworthy characteristic of the CrossOver embedding space: temporal scenes (i.e., the same scene captured at different times) cluster closely together.  This is evident in the examples where similar scene layouts are retrieved with slightly different viewpoints (k=2 and k=3), demonstrating the model's ability to capture and maintain scene-level consistency across time.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x8.png", "caption": "Figure 6: Cross-Modal Scene Retrieval on ScanNet (Scene Matching Recall). Plots showcase the top 1, 5, 10, 20 scene matching recall of different methods on three modality pairs: \u2110\u2192\ud835\udcab\u2192\u2110\ud835\udcab\\mathcal{I}\\rightarrow\\mathcal{P}caligraphic_I \u2192 caligraphic_P, \u2110\u2192\u211b\u2192\u2110\u211b\\mathcal{I}\\rightarrow\\mathcal{R}caligraphic_I \u2192 caligraphic_R, \ud835\udcab\u2192\u211b\u2192\ud835\udcab\u211b\\mathcal{P}\\rightarrow\\mathcal{R}caligraphic_P \u2192 caligraphic_R. Ours and the Instance Baseline have not been explicitly trained on \ud835\udcab\u2192\u211b\u2192\ud835\udcab\u211b\\mathcal{P}\\rightarrow\\mathcal{R}caligraphic_P \u2192 caligraphic_R. Results are computed on a database of 306 scenes and showcase the superior performance of our approach. Once again, the difference between Ours and our self-baseline is attributed to the enhanced cross-modal scene-level interactions achieved with the unified encoders.", "description": "Figure 6 presents a comparative analysis of cross-modal scene retrieval performance using different methods.  The results are shown in plots for top 1, 5, 10, and 20 scene matching recall. Three modality pairs are compared: images to point clouds (I\u2192P), images to text descriptions (I\u2192R), and point clouds to text descriptions (P\u2192R).  The 'Ours' and 'Instance Baseline' methods were not explicitly trained on the P\u2192R pair. The experiment used a database of 306 scenes.  The plots demonstrate that the 'Ours' method significantly outperforms other methods, particularly the 'Instance Baseline', highlighting the improved cross-modal scene interactions enabled by the unified encoders used in the 'Ours' approach.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x9.png", "caption": "(a) Instance Matching Recall", "description": "This figure presents a comparison of instance matching recall across different modalities. The results are shown for various modality pairs, including image-to-point cloud, image-to-mesh, point cloud-to-mesh, and image-to-text. The figure showcases the relative performance of the proposed CrossOver model compared to several baseline methods, highlighting its ability to achieve robust performance across various modality combinations even without explicit object-level alignment.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x10.png", "caption": "(b) Scene-Level Matching Recall", "description": "This figure shows the scene-level matching recall results on the ScanNet and 3RScan datasets. It compares the performance of the proposed CrossOver method with several baselines, including ULIP-2, PointBind, and an instance baseline. The results are presented for different recall thresholds (R@25%, R@50%, R@75%), which represent the percentage of correctly retrieved scenes among the top 25%, 50%, and 75% of retrieved results. The figure visually demonstrates the superior performance of CrossOver across all datasets and metrics.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x11.png", "caption": "Figure 7: Scaled-up training performance on ScanNet. When training on both ScanNet and 3RScan datasets together, results improve from any individual dataset training. As expected, training on 3RScan and evaluating on ScanNet will have limited performance. Note that the 3RScan includes only the \u2110\u2110\\mathcal{I}caligraphic_I, \ud835\udcab\ud835\udcab\\mathcal{P}caligraphic_P, and \u211b\u211b\\mathcal{R}caligraphic_R modalities.", "description": "Figure 7 shows the results of training the CrossOver model on different combinations of the ScanNet and 3RScan datasets.  The graph displays the improvement in both instance-level and scene-level matching recall when training on both datasets compared to training on only one.  The improvement highlights the benefit of using a larger and more diverse dataset for training. It also shows that training only on the 3RScan dataset and testing on ScanNet results in significantly lower performance, as expected, since 3RScan has different modalities (RGB images, point clouds, and text descriptions) than ScanNet.  This demonstrates the model's ability to leverage data from multiple sources to improve overall performance.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2502.15011/x12.png", "caption": "Figure 8: Cross-Modal \u2110\u2192\ud835\udcab\u2192\u2110\ud835\udcab\\mathcal{I}\\rightarrow\\mathcal{P}caligraphic_I \u2192 caligraphic_P Scene Retrieval on ScanNet. Plots showcase scene matching recall (Recall), category recall, temporal recall, and intra-category recall for different number of camera views evaluated on several Top-k\ud835\udc58kitalic_k matches. Note that maximum k\ud835\udc58kitalic_k differs per recall since the amount of eligible matches depends on the criteria for each recall type: scene similarity, category, temporal changes.", "description": "Figure 8 shows the performance of CrossOver on cross-modal scene retrieval using ScanNet.  The model retrieves scenes from point cloud data (P) given an image query (I). The plots illustrate different performance metrics: scene matching recall (overall accuracy), category recall (retrieving scenes of the same type), temporal recall (retrieving the same scene across different time points), and intra-category recall (retrieving a specific scene instance from a set of similar scenes).  The analysis is conducted for varying numbers of camera views (1, 5, 10, 15, 20) used during training to assess impact on retrieval performance, across multiple top-k results (k=1, 5, 10, 20). The maximum k value varies depending on the metric because the number of eligible matches differs based on the criteria for each metric (scene similarity, category, temporal changes).", "section": "4.3. Instance Retrieval"}, {"figure_path": "https://arxiv.org/html/2502.15011/x13.png", "caption": "Figure 9: Cross-Modal \u2110\u2192\ud835\udcab\u2192\u2110\ud835\udcab\\mathcal{I}\\rightarrow\\mathcal{P}caligraphic_I \u2192 caligraphic_P Scene Retrieval on 3RScan. Plots showcase scene matching recall (Recall) and temporal recall for different number of camera views.", "description": "Figure 9 shows the results of cross-modal scene retrieval experiments on the 3RScan dataset.  Specifically, it examines how well the model can retrieve a scene represented by point clouds (\ud835\udcab) given a query scene represented by images (\u2110). The experiments vary the number of camera views used to generate the image representation of the query scene. The plots in the figure show both the scene matching recall and the temporal recall for different numbers of camera views. Scene matching recall measures the accuracy of retrieving the correct scene, while temporal recall measures the accuracy of retrieving the correct scene across different time points (i.e., capturing the same scene at different times).", "section": "4.3. Instance Retrieval"}]