{"importance": "This paper is crucial for LLM safety, revealing a previously overlooked vulnerability in LLMs related to user-specific safety. U-SAFEBENCH offers a way for future research to mitigate these risks. It opens new avenues for personalized safety mechanisms, advancing responsible AI development.", "summary": "LLMs fail to act safely when considering user-specific safety standards, which were made to be solved via new benchmark.", "takeaways": ["Current LLMs fail to act safely when considering user-specific safety standards.", "User-specific safety is a critical, previously unaddressed aspect of LLM safety.", "A chain-of-thought approach can enhance user-specific safety in LLMs."], "tldr": "LLMs exhibit safety vulnerabilities when **user-specific standards** are not considered, relying heavily on general standards. Safety standards for LLMs vary based on **user profiles**, raising the question: Do LLMs act safely regarding these varying standards? Despite its importance, no benchmarks exist for evaluating the **user-specific safety** of LLMs, which are very needed to ensure safety and responsibility of the systems.\n\nThis paper introduces U-SAFEBENCH, a benchmark designed to assess LLM's **user-specific safety.** Evaluations of widely used LLMs reveal a failure to act safely, marking a new discovery. The paper proposes a simple remedy via **chain-of-thought**, effectively improving user-specific safety. The results leads to a clear conclusion: Current LLMs fail to act safely when considering **user-specific safety!**", "affiliation": "KAIST", "categories": {"main_category": "AI Theory", "sub_category": "Safety"}, "podcast_path": "2502.15086/podcast.wav"}