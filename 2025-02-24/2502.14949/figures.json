[{"figure_path": "https://arxiv.org/html/2502.14949/x3.png", "caption": "Figure 1: Overview of the core domains and sub-domains in KITAB-Bench. Our benchmark spans nine major domains (e.g., OCR, charts to JSON, table recognition) and 36 sub-domains (e.g., scanned text, handwritten text, various chart types), providing a comprehensive evaluation framework for modern Arabic document processing and analysis.", "description": "Figure 1 provides a visual representation of the KITAB-Bench benchmark's structure.  It illustrates the nine core domains and 36 sub-domains included in the benchmark. The domains cover key tasks in Arabic document understanding, such as OCR, chart-to-JSON conversion, and table recognition.  The sub-domains further specify the types of documents and data used within each domain (e.g., handwritten text, scanned text, various chart types).  KITAB-Bench's goal is to offer a comprehensive evaluation of Arabic document processing and analysis systems, enabling researchers to assess the performance of their methods across a diverse range of document formats and complexity levels.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.14949/x4.png", "caption": "Figure 2: Overview of different tasks in our benchmark: Eight key components illustrating the task inputs and outputs for table recognition, chart understanding, text recognition, diagram analysis, VQA, line detection, layout analysis, and PDF-to-Markdown conversion, complete with input/output examples for each task.", "description": "Figure 2 presents a comprehensive overview of the eight key tasks included in the KITAB-Bench benchmark.  Each task is visually represented with an example of its input and corresponding output. The tasks cover various aspects of Arabic document understanding, including table recognition (extracting structured data from tables), chart understanding (converting charts into dataframes), text recognition (converting images of text into machine-readable text), diagram analysis (converting diagrams to JSON), visual question answering (VQA), line detection (identifying and bounding lines in documents), layout analysis (detecting the layout structure of a document), and PDF-to-Markdown conversion (converting a PDF document into a Markdown format). This figure provides a visual summary of the types of data and the transformations involved in each task within the benchmark.", "section": "3 KITAB-Bench"}, {"figure_path": "https://arxiv.org/html/2502.14949/x5.png", "caption": "Figure 3: Comparison of model performance across four document understanding tasks (Table Recognition, Image to Text, Diagram to JSON, and Layout Detection) showing successful and failed cases for different models including Ground Truth, EasyOCR, GPT-4, Qwen, Surya, Tesseract, Yolo, and DETR on Arabic document benchmark data.", "description": "This figure displays a comparison of various model performances across four key document understanding tasks: Table Recognition, Image to Text, Diagram to JSON, and Layout Detection.  It showcases both successful and unsuccessful examples for each task, using Arabic benchmark data. Models compared include Ground Truth, EasyOCR, GPT-4, Qwen, Surya, Tesseract, Yolo, and DETR. This provides a visual representation of the strengths and weaknesses of each model in handling different aspects of Arabic document understanding, highlighting the challenges presented by the language's unique characteristics.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.14949/x6.png", "caption": "Figure 4: Synthetic Data Generation Pipeline: A 5-stage process using LLMs to generate topics, create raw data, produce visualization code, render charts, and perform human evaluation for quality control.", "description": "This figure illustrates the five-stage pipeline used to generate synthetic data for charts and diagrams.  The process begins with Large Language Models (LLMs) generating relevant topics.  These topics then inform the generation of raw data by the LLMs. Next, the LLMs create code to visualize this data. This code is then used to render the charts and diagrams. Finally, human evaluators assess the quality of the generated content, ensuring accuracy and adherence to Arabic linguistic conventions. This iterative process ensures high-quality synthetic data for the benchmark.", "section": "3 KITAB-Bench"}, {"figure_path": "https://arxiv.org/html/2502.14949/x7.png", "caption": "Figure 5: Prompts for Different Task Categories.", "description": "Figure 5 displays example prompts used in the KITAB-Bench benchmark for different task categories.  Each prompt is designed to guide an LLM or other model toward a specific output format, ensuring consistent and comparable results across various tasks. The prompts cover detailed instructions on expected output formats, specify the language (Arabic), and address potential ambiguities to minimize human bias in the evaluation process.  The showcased prompts include examples for Chart Type, Chart Topic, Chart Data, PDF to Markdown conversion, OCR, Diagram Type, Diagram Topic, and Diagram Data, as well as Table and Table Data. The prompts are meticulously structured to evaluate different aspects of Arabic document understanding such as visual recognition (charts, diagrams, tables) and text extraction/conversion, highlighting the complexity and nuance required for accurate evaluation.", "section": "3.1 PDF Data Collection"}, {"figure_path": "https://arxiv.org/html/2502.14949/x8.png", "caption": "Figure 6: Prompts for Diagrams and Tables.", "description": "Figure 6 shows example prompts used in the KITAB-Bench benchmark dataset for evaluating diagram and table understanding tasks.  The prompts are designed to guide large language models (LLMs) in generating structured data outputs (JSON for diagrams, CSV and HTML for tables). Each prompt specifies the desired output format and includes instructions for ensuring consistency and accuracy.  The goal is to test the ability of LLMs to correctly interpret diagram and table information and generate machine-readable representations.", "section": "3.1 PDF Data Collection"}]