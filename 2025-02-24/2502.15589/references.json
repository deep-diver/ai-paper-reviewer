{"references": [{"fullname_first_author": "Vaswani", "paper_title": "Attention is all you need", "publication_date": "2017-12-04", "reason": "This paper introduced the Transformer architecture, which is the foundation for many current LLMs, and whose attention mechanism complexity is a central concern of the current paper."}, {"fullname_first_author": "Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-11-28", "reason": "This paper introduced Chain-of-Thought prompting, a method of improving reasoning in LLMs that serves as a key comparison point for LightThinker."}, {"fullname_first_author": "Hu", "paper_title": "Lora: Low-rank adaptation of large language models", "publication_date": "2022-04-25", "reason": "This paper introduced LoRA, a parameter-efficient fine-tuning method whose application to LightThinker the paper suggests as a direction for future work."}, {"fullname_first_author": "Zhang", "paper_title": "H2O: heavy-hitter oracle for efficient generative inference of large language models", "publication_date": "2023-12-10", "reason": "This paper introduces H2O, a training-free acceleration method for LLMs, which this paper uses as a baseline for comparison."}, {"fullname_first_author": "Pang", "paper_title": "Anchor-based large language models", "publication_date": "2024-08-11", "reason": "This paper introduces AnLLM, a method similar to LightThinker that is capable of compressing the thought during inference, which the paper compares LightThinker to directly."}]}