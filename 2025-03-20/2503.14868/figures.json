[{"figure_path": "https://arxiv.org/html/2503.14868/x1.png", "caption": "(a) GPU memory breakdown across various personalization methods.", "description": "This figure (a) shows a detailed breakdown of GPU memory usage across different personalization methods for Stable Diffusion models.  It visually represents the memory allocation for various components, such as the Stable Diffusion model itself (in FP32 and quantized INT8 versions), gradient memory, optimizer state memory, and other memory usage like activations and caches. This breakdown highlights the memory efficiency achieved by the proposed ZOODIP method compared to existing approaches (DreamBooth, Textual Inversion, PEQA, TuneQDM).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14868/x2.png", "caption": "(b) VRAM usage versus image and text alignment scores.", "description": "This figure shows a comparison of the total VRAM (video RAM) usage of different Stable Diffusion personalization methods against their performance. Performance is evaluated using two metrics: image alignment score (CLIP-I) and text alignment score (CLIP-T). The chart visually represents the trade-off between memory efficiency and the quality of image and text alignment achieved by each method.  It allows for a direct comparison of how much VRAM each technique requires to achieve a certain level of performance.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14868/x3.png", "caption": "Figure 1: Analysis of memory consumption and performance of Stable Diffusion personalization methods. (Left) GPU memory breakdown for each method on a Stable Diffusion personalization with a batch size of 1. ZOODiP (Ours) shows significantly higher memory efficiency compared to other methods. (Right) Comparison of memory usage versus performance across methods. Performance is measured with text (CLIP-T) and image (CLIP-I) alignment scores. ZOODiP achieves comparable performance to other methods while using significantly less memory (up to 8.2\u00d78.2\\times8.2 \u00d7 less than DreamBooth). Memory usage was profiled using the PyTorch profiler and nvidia-smi command.", "description": "This figure analyzes the memory usage and performance of various Stable Diffusion personalization methods. The left panel shows a detailed breakdown of GPU memory consumption for each method using a batch size of 1.  It highlights that ZOODIP (the authors' method) uses significantly less memory than other methods like DreamBooth, Textual Inversion, PEQA, and TuneQDM. The right panel presents a comparison of memory usage and model performance, where performance is evaluated using text (CLIP-T) and image (CLIP-I) alignment scores.  ZOODIP achieves comparable performance to other methods while exhibiting up to 8.2 times less memory consumption than DreamBooth.  Memory usage was measured using the PyTorch profiler and the nvidia-smi command.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14868/x4.png", "caption": "(a) Overall illustration of ZOODiP training framework.", "description": "The figure illustrates the training framework of ZOODiP, a memory-efficient personalization method for diffusion models. It shows how a target token is initialized and added to a text prompt, reference images are encoded and processed through a VAE, and the noise is predicted using partial uniform timestep sampling. The gradient is calculated using zeroth-order optimization with subspace gradient and updated with no backpropagation. The overall framework combines zeroth-order optimization with a quantized model to minimize memory usage during the fine-tuning process.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14868/x5.png", "caption": "(b) Illustration of Subspace Gradient (SG) updates.", "description": "This figure illustrates the Subspace Gradient (SG) update mechanism in ZOODiP.  It shows how the algorithm denoises the noisy gradient estimates obtained from the zeroth-order optimization process by projecting them onto a lower-dimensional subspace spanned by the past history of the tokens.  The past tokens are analyzed using Principal Component Analysis (PCA) to identify a low-variance subspace, and a projection matrix is constructed using the eigenvectors corresponding to the retained dimensions. Noisy gradient dimensions are eliminated by projecting out dimensions associated with low variance. The result is a refined gradient that improves training efficiency and stability.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14868/x6.png", "caption": "Figure 2: (a) Illustration of overall ZOODiP framework. A target token is initialized and added to the prompt. Reference images are encoded, and Partial Uniform Timestep Sampling (PUTS)-sampled timestep noise is predicted. The loss is calculated with the original and perturbed token to estimate the gradient. (b) Illustration of Subspace Gradient (SG). Updated tokens from the previous \u03c4\ud835\udf0f\\tauitalic_\u03c4 iterations are stored. PCA identifies low-variance eigenvectors to project out noisy dimensions from the estimated gradient for the next \u03c4\ud835\udf0f\\tauitalic_\u03c4 iterations.", "description": "Figure 2 illustrates the ZOODiP framework.  Panel (a) shows the process of personalization. A new token is added to the text prompt. Reference images are encoded using a Variational Autoencoder (VAE).  PUTS (Partial Uniform Timestep Sampling) selects a subset of timesteps in the diffusion process. The model predicts noise at these timesteps. Then, the model calculates the loss using the original and a perturbed version of the token, estimating the gradient without backpropagation. Panel (b) shows how the Subspace Gradient (SG) method denoises this noisy gradient estimate. Token updates from the previous \u03c4 iterations are collected and used for Principal Component Analysis (PCA).  PCA identifies low-variance directions, which are then projected out of the gradient to reduce noise before the next \u03c4 updates.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14868/x7.png", "caption": "Figure 3: Sparse effective dimension in the token trained with Textual Inversion. Notably, the concept was preserved even when retaining only one-third of the optimized dimensions (k=256\ud835\udc58256k=256italic_k = 256).", "description": "This figure shows the results of a principal component analysis (PCA) performed on token embeddings before and after personalization using Textual Inversion.  The PCA reveals that the most significant changes during personalization are concentrated within a low-dimensional subspace.  Even when only the top one-third of the principal components (k=256) are retained, the core concept of the personalized token is still preserved.  This illustrates that Textual Inversion primarily modifies a small subset of the token's embedding dimensions.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/fig6_style.png", "caption": "Figure 4: Textual Inversion\u00a0[17] with various timestep sampling. When the timestep t\ud835\udc61titalic_t for training is sampled from U\u2062(0,500)\ud835\udc480500U(0,500)italic_U ( 0 , 500 ), key conceptual features such as color and body shape of the reference image are not effectively trained. In contrast, sampling from U\u2062(500,1000)\ud835\udc485001000U(500,1000)italic_U ( 500 , 1000 ) results in successful learning of these features.", "description": "This figure shows the impact of different timestep sampling ranges on the effectiveness of Textual Inversion for image personalization.  Using a uniform distribution between 0 and 500 for sampling the timestep t resulted in poor learning of key features like color and shape from the reference image. In contrast, using a uniform distribution between 500 and 1000 for the timestep resulted in successful learning, indicating that the later stage of the diffusion process is crucial for effectively incorporating the desired features from the reference images.", "section": "3.4. Partial Uniform Timestep Sampling"}, {"figure_path": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/clip_t_PUTS.png", "caption": "Figure 5: Qualitative comparison of image and text alignment. This figure shows how well each method generates images that match the input text prompt while preserving the identity of the personalized subject. ZOODiP generates images that faithfully reflect the prompt while maintaining the concept of the reference image, demonstrating strong image-text alignment.", "description": "Figure 5 presents a qualitative comparison of image and text alignment across various personalization methods.  The figure demonstrates the image generation capabilities of different methods, using the same prompts and reference images. Each method's success in generating images that accurately match the prompt while preserving the identity of the personalized subject is visually demonstrated. ZOODiP's images show particularly strong image-text alignment, closely mirroring both the prompt and the reference images.", "section": "4. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/clip_i_PUTS.png", "caption": "Figure 6: \nQualitative results on style personalization. This figure showcases the results of style personalization achieved through ZOODiP, using few reference images with a consistent style. The outcome highlights ability of ZOODiP to personalize not only the subject but also the style with a high degree of accuracy. This demonstrates the versatility and extensive personalization capabilities of ZOODiP, effectively adapting both stylistic elements and subject details to match the reference images.", "description": "This figure demonstrates ZOODIP's ability to personalize images by style, not just subject.  Using a few reference images with a consistent style, ZOODIP accurately captures and applies that style to new images of the same subject. The results show a high degree of accuracy and demonstrate ZOODIP's versatility in adapting stylistic elements and subject details to closely match the reference images. This showcases its extensive personalization capabilities.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14868/extracted/6288943/figure/dino_PUTS.png", "caption": "(a) CLIP-T scores.", "description": "This figure presents a heatmap visualizing the CLIP-T scores obtained across various combinations of  'TL' and 'TU' parameters, which are used to define the range of timesteps in the Partial Uniform Timestep Sampling (PUTS) method.  The heatmap helps illustrate how different ranges of timesteps sampled during training influence the final text-image alignment score (CLIP-T). The x-axis represents 'TU' (upper bound of timestep range) and the y-axis represents 'TL' (lower bound).  The color intensity of each cell indicates the CLIP-T score achieved with that particular 'TL' and 'TU' combination, allowing for the identification of optimal parameter ranges that maximize performance.", "section": "3.4 Partial Uniform Timestep Sampling"}, {"figure_path": "https://arxiv.org/html/2503.14868/x8.png", "caption": "(b) CLIP-I scores.", "description": "This figure shows the CLIP-I (image alignment) scores for different methods across various personalization methods.  CLIP-I scores measure the cosine similarity between the CLIP embeddings of the reference images and the generated images. Higher scores indicate that the generated images are more similar to the reference images.  The x-axis represents the total VRAM usage (in GB), while the y-axis shows the CLIP-I score. Different personalization methods are represented by different colored markers. This visualization allows for a comparison of the trade-off between memory efficiency and performance for various personalization methods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14868/x9.png", "caption": "(c) DINO scores.", "description": "This figure shows the DINO (Self-Supervised visual representation learning) scores for different combinations of start and end timesteps used in Partial Uniform Timestep Sampling (PUTS).  The heatmap visually represents the performance of the personalization process, with higher scores indicating better alignment between generated images and reference images.", "section": "3.4 Partial Uniform Timestep Sampling"}, {"figure_path": "https://arxiv.org/html/2503.14868/x10.png", "caption": "Figure 7: Heatmap of CLIP-T, CLIP-I and DINO scores across varying TLsubscript\ud835\udc47\ud835\udc3fT_{L}italic_T start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT and TUsubscript\ud835\udc47\ud835\udc48T_{U}italic_T start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT on the <dog6> dataset. x\ud835\udc65xitalic_x-axis is the TUsubscript\ud835\udc47\ud835\udc48T_{U}italic_T start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT and y\ud835\udc66yitalic_y-axis is the TLsubscript\ud835\udc47\ud835\udc3fT_{L}italic_T start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT applied to the sampling distribution.", "description": "This figure visualizes the performance of the ZOODIP model across different ranges of timesteps during training.  The heatmap shows CLIP-T, CLIP-I, and DINO scores (metrics for evaluating text-image alignment and image quality) as a function of the starting timestep (T<sub>L</sub>) and ending timestep (T<sub>U</sub>) of the partial uniform timestep sampling strategy used in the model. Warmer colors represent better performance.  The x-axis represents T<sub>U</sub> and the y-axis represents T<sub>L</sub>, illustrating how the choice of timesteps affects the model's ability to effectively personalize the Stable Diffusion model.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.14868/x11.png", "caption": "Figure 8: Histogram of i\u2217/\u03c4superscript\ud835\udc56\ud835\udf0fi^{*}/\\tauitalic_i start_POSTSUPERSCRIPT \u2217 end_POSTSUPERSCRIPT / italic_\u03c4 ratios for <dog6> and <shiny_sneaker> dataset with hyperparameter \u03c4=128\ud835\udf0f128\\tau=128italic_\u03c4 = 128, \u03bd=10\u22123\ud835\udf08superscript103\\nu=10^{-3}italic_\u03bd = 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT during training with SG. Despite the small \u03bd\ud835\udf08\\nuitalic_\u03bd, a significant portion (>80%absentpercent80>80\\%> 80 %) of dimensions are projected out.", "description": "This figure shows two histograms. Each histogram represents the ratio of the number of principal components retained after PCA to the total number of principal components (\u03c4=128) for each of the two datasets: <dog6> and <shiny_sneaker>. The hyperparameters used are \u03bd=10\u207b\u00b3 and \u03c4=128. A small value of \u03bd is used, which should retain most of the variance. However, the histograms show that a significant portion (more than 80%) of the dimensions are projected out (removed). This indicates that the optimization process primarily focuses on a low-dimensional subspace, which is consistent with the findings described in Section 3.3 (Subspace Gradient).", "section": "3.3 Subspace Gradient"}, {"figure_path": "https://arxiv.org/html/2503.14868/x12.png", "caption": "Figure S1: Generated images with the prompt \u201ca photo of a dog\u201d with various weight precision. While INT8 precision produces results nearly equivalent to full-precision performance, INT4 precision exhibits noticeable degradation in image quality, highlighting the trade-off between lower precision and fidelity.", "description": "This figure displays a comparison of images generated using different levels of weight precision in a Stable Diffusion model.  The prompt used for all generated images was \"a photo of a dog.\"  The images demonstrate the impact of reduced precision on image quality.  The INT8 precision images closely match those generated with full precision (FP32), showing that there's minimal loss in image quality. However, the INT4 images show significantly lower quality and artifacts, indicating a substantial trade-off between reduced memory requirements and image fidelity.  This showcases that while lower-bit quantization saves memory, it can lead to noticeable degradation in the generated image's visual quality.", "section": "S3.2 4-bit quantized models"}, {"figure_path": "https://arxiv.org/html/2503.14868/x13.png", "caption": "Figure S2: Qualitative results of U-Net precision at INT8 and INT4 in <dog6> dataset. ZOODiP works on INT4 and INT8, but performance diminishes due to degradation caused by INT4 quantization.", "description": "This figure presents a qualitative comparison of images generated using 8-bit (INT8) and 4-bit (INT4) quantized U-Net models within the ZOODIP framework.  The results demonstrate the impact of quantization on image quality. While the INT8 model produces high-fidelity images comparable to those generated using full-precision weights, a noticeable degradation in image quality is observed in images produced by the INT4 model. This illustrates the trade-off between reduced memory requirements (achieved through lower-bit quantization) and the fidelity of the generated images.", "section": "S3.2 4-bit quantized models"}, {"figure_path": "https://arxiv.org/html/2503.14868/x14.png", "caption": "Figure S3: Qualitative comparison of the diversity of generated images This figure compares the diversity achieved by different personalization methods. ZOODiP demonstrates the ability to generate highly diverse images while utilizing minimal memory resources.", "description": "Figure S3 presents a qualitative comparison of image diversity resulting from different personalization methods.  The figure displays example images generated by DreamBooth, PEQA, TuneQDM, Textual Inversion, Gradient-Free Textual Inversion, and ZOODiP. By visually inspecting the generated images for each method, one can qualitatively assess the diversity of the output. The caption highlights that ZOODiP produces highly diverse images while using minimal memory resources, emphasizing its efficiency.", "section": "S4.2. Additional qualitative results"}, {"figure_path": "https://arxiv.org/html/2503.14868/x15.png", "caption": "Figure S4: Qualitative results for personalizing SD2.1 and SDXL with ZOODiP. The figure demonstrate that ZOODiP can be applied not only to SD1.5, as discussed in the main paper, but also to various other models. For SD2.1, inference were conducted with images at a resolution of 768\u00d7768768768768\\times 768768 \u00d7 768, while for SDXL, image generation was performed with resolution of 1024\u00d71024102410241024\\times 10241024 \u00d7 1024. However, for SDXL, it was observed that the model\u2019s inherent color interpretation prevents the subject\u2019s colors from being completely replicated. This indicates that the model\u2019s color rendering can vary depending on the environmental context, leading to shifts in the perceived color scheme.", "description": "Figure S4 showcases ZOODIP's adaptability to different diffusion models beyond the SD1.5 model discussed in the main paper.  It demonstrates successful personalization on both SD2.1 (using 768x768 resolution images) and SDXL (using 1024x1024 resolution images).  However, the results highlight a limitation: SDXL's inherent color interpretation sometimes prevents perfectly accurate color replication of the subject, suggesting that environmental context influences color rendering in SDXL.", "section": "S3.4 Generalizability to other models"}, {"figure_path": "https://arxiv.org/html/2503.14868/x16.png", "caption": "Figure S5: Qualitative comparisons on na\u00efve ZO textual inversion without SG and PUTS to ZOODiP (Ours) over iterations. The na\u00efve approach exhibits slower training and tends to produce images that are less aligned with the reference image. In contrast, ZOODiP achieves faster training and generates images that are closely aligned with the reference subject.", "description": "This figure compares the performance of two methods for personalizing diffusion models: a naive zeroth-order optimization approach and the proposed ZOODIP method.  The naive method, lacking the Subspace Gradient (SG) and Partial Uniform Timestep Sampling (PUTS) techniques, shows slower convergence during training and generates images less faithful to the reference image.  In contrast, ZOODIP, utilizing SG and PUTS, demonstrates significantly faster training and produces images that are much better aligned with the subject in the reference image.  The figure visually illustrates this by showing image outputs at various iteration steps for both methods, highlighting the superior performance of ZOODIP.", "section": "S4. Additional Results"}, {"figure_path": "https://arxiv.org/html/2503.14868/x17.png", "caption": "Figure S6: Qualitative comparison of image and text alignment on the <cat> subset of DB dataset.", "description": "This figure presents a qualitative comparison of how well different personalization methods generate images that match both the given text prompt and the visual characteristics of a reference image.  The methods compared include DreamBooth (DB), PEQA, TuneQDM, Textual Inversion (TI), Gradient-Free TI (GF-TI), and the proposed ZOODIP method. The comparison focuses specifically on images of cats, a subset of the DreamBooth (DB) dataset. The goal is to visually demonstrate the effectiveness of each method in capturing both textual and visual aspects of the target concept (the cat) when generating new images.", "section": "4.2. Qualitative Results"}, {"figure_path": "https://arxiv.org/html/2503.14868/x18.png", "caption": "Figure S7: Qualitative comparison of image and text alignment on the <cat2> subset of DB dataset.", "description": "This figure displays a qualitative comparison of the results obtained from different personalization methods on a subset of the DreamBooth dataset focused on a specific cat.  It shows generated images from several methods alongside reference images and text prompts used.  The aim is to visually demonstrate the fidelity of each method's image generation with respect to both visual similarity to the reference cat images and alignment with the textual description.", "section": "S4.2. Additional qualitative results"}]