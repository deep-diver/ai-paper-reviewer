{"references": [{"fullname_first_author": "Brown, T. B.", "paper_title": "Language models are few-shot learners", "publication_date": "2020-XX-XX", "reason": "This paper is foundational for demonstrating the capabilities of large language models (LLMs) as few-shot learners, which is central to the ZebraLogic study."}, {"fullname_first_author": "Chowdhery, A.", "paper_title": "Palm: Scaling language modeling with pathways", "publication_date": "2022-XX-XX", "reason": "This work is highly relevant as it explores scaling language models to improve their performance, a key theme in the ZebraLogic study of LLM scaling limits."}, {"fullname_first_author": "Clark, P.", "paper_title": "Transformers as soft reasoners over language", "publication_date": "2020-XX-XX", "reason": "This paper is important as it explores the use of transformers for logical reasoning tasks, directly relevant to ZebraLogic's focus on LLM reasoning abilities."}, {"fullname_first_author": "Dechter, R.", "paper_title": "Constraint Processing", "publication_date": "2003-XX-XX", "reason": "This book provides the foundational theory of Constraint Satisfaction Problems (CSPs), the framework used in ZebraLogic to generate and analyze logic puzzles."}, {"fullname_first_author": "Liu, H.", "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning", "publication_date": "2020-XX-XX", "reason": "This paper is significant as it introduces LogiQA, an early benchmark dataset for evaluating logical reasoning in LLMs, providing context for ZebraLogic's contribution to LLM evaluation."}]}