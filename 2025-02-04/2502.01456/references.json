{"references": [{"fullname_first_author": "Richard S Sutton", "paper_title": "Learning to predict by the methods of temporal differences", "publication_date": "1988-01-01", "reason": "This paper is foundational to reinforcement learning, introducing the concept of temporal difference learning, which is crucial to many modern RL algorithms including those used in this paper."}, {"fullname_first_author": "John Schulman", "paper_title": "Proximal policy optimization algorithms", "publication_date": "2017-07-01", "reason": "This paper introduced the Proximal Policy Optimization (PPO) algorithm, a widely used and stable reinforcement learning algorithm that is directly compared with in this paper."}, {"fullname_first_author": "Richard S Sutton", "paper_title": "Reinforcement learning: An introduction", "publication_date": "2018-01-01", "reason": "This is a comprehensive textbook on reinforcement learning, providing essential background and context for understanding many of the concepts discussed in the current paper."}, {"fullname_first_author": "Jan Leike", "paper_title": "Scalable agent alignment via reward modeling: a research direction", "publication_date": "2018-11-01", "reason": "This paper is highly relevant because it explores the challenges of reward modeling in reinforcement learning and introduces the idea of using dense rewards to improve efficiency and credit assignment."}, {"fullname_first_author": "Leo Gao", "paper_title": "Scaling laws for reward model overoptimization", "publication_date": "2022-01-01", "reason": "This paper investigates the issue of reward hacking, a significant problem when using dense rewards in reinforcement learning, and proposes solutions that are directly relevant to this paper."}]}