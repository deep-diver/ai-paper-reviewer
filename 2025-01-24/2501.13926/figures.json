[{"figure_path": "https://arxiv.org/html/2501.13926/x3.png", "caption": "Figure 1: Can We Verify and Reinforce Image Generation with Chain-of-Thought (CoT) Reasoning Strategies? Given the success of mathematical CoT reasoning in LLMs\u00a0[1, 2] and LMMs\u00a0[3, 4] (Left), we provide the first investigation to comprehensively explore the potential of applying current reasoning techniques to autoregressive image generation (Right), including test-time verification and preference alignment, with two newly proposed specialized reward models, termed PARM and PARM++.", "description": "This figure illustrates the core idea of the paper: applying chain-of-thought (CoT) reasoning to autoregressive image generation.  The left side shows the successful application of CoT reasoning in LLMs and LMMs for mathematical problem-solving, setting a precedent for the current work. The right side introduces the proposed approach for autoregressive image generation, highlighting the use of test-time verification, preference alignment, and two novel reward models (PARM and PARM++) to enhance the generation process.  It visually represents the conceptual transition from established CoT reasoning in language models to a novel application in image generation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.13926/x4.png", "caption": "Figure 2: Autoregressive Image Generation without (Top) and with (Bottom) Our Reasoning Strategies. We adopt Show-o\u00a0[5] as the baseline model that produces unsatisfactory text-to-image generation. After using our investigated reasoning strategies (integrating PARM with iterative DPO for both reward model guidance and test-time verification), the generation process is effectively enhanced.", "description": "This figure demonstrates the impact of the proposed reasoning strategies on autoregressive image generation. The top row shows examples of images generated by the baseline model Show-o [5] without any reasoning strategies. These examples illustrate the unsatisfactory image generation quality produced by the baseline model. The bottom row shows the improved results after applying the developed reasoning strategies. The reasoning strategies involve integrating the Potential Assessment Reward Model (PARM) with iterative Direct Preference Optimization (DPO) for both reward model guidance and test-time verification. The enhanced generation process significantly improves the quality of the generated images. This figure visually showcases the effectiveness of the proposed methods in enhancing the autoregressive image generation process.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2501.13926/x5.png", "caption": "Figure 3: Comparison of Reward Models as Test-time Verifiers. We adopt Show-o\u00a0[5] as the \u2018Baseline\u2019 and evaluate Best-of-N\ud835\udc41Nitalic_N selection on the GenEval\u00a0[38] benchmark.", "description": "This figure compares the performance of different reward models used for test-time verification in an autoregressive image generation model. The reward models are used to select the best image from a set of candidate images generated by the model. The figure shows the overall score on the GenEval benchmark for different reward models and varying numbers of candidates considered (best-of-N selection).  It shows that using reward models significantly improves the performance compared to a baseline without them. The fine-tuned ORM (Outcome Reward Model) demonstrates the best performance, consistently outperforming the other models, including the zero-shot ORM and PRM (Process Reward Model).", "section": "2.2 ORM vs PRM as Test-time Verifiers"}, {"figure_path": "https://arxiv.org/html/2501.13926/x6.png", "caption": "Figure 4: Investigation of Reward Models in Autoregressive Image Generation. For test-time verification, we implement Outcome Reward Model (ORM) and Process Reward Model (PRM), and introduce a new Potential Assessment Reward Model (PARM) customized for image generation scenarios, which progressively performs three tasks (highlighted in blue) to enhance the reasoning of generation.", "description": "This figure illustrates three reward models used for test-time verification in autoregressive image generation: Outcome Reward Model (ORM), Process Reward Model (PRM), and the novel Potential Assessment Reward Model (PARM).  Each model's architecture is shown, highlighting how they assess the image generation process.  ORM evaluates only the final generated image, while PRM scores each step of the generation process.  PARM combines aspects of both, adaptively evaluating only clear and promising steps to improve overall accuracy and address the limitations of the other models. The three main tasks of PARM are highlighted in blue, showcasing its adaptive assessment approach.", "section": "2 Our Investigation"}, {"figure_path": "https://arxiv.org/html/2501.13926/x7.png", "caption": "Figure 5: The Reflection Mechanism in Potential Assessment Reward Model ++ (PARM++). As an upgraded version of PARM, PARM++ incorporates a reflection evaluation task, enabling the generative model to self-correct its low-quality images.", "description": "The figure illustrates the reflection mechanism incorporated into the enhanced Potential Assessment Reward Model (PARM++).  PARM++ builds upon the original PARM by adding a reflection evaluation step.  This step involves assessing the generated image's alignment with the provided text prompt. If misalignment is detected, PARM++ identifies the issue (e.g., incorrect object attributes, inaccurate spatial relationships) and guides the generative model through a self-correction process to refine the image iteratively until it aligns with the prompt.", "section": "4 Potential Assessment Reward Model ++"}, {"figure_path": "https://arxiv.org/html/2501.13926/x8.png", "caption": "Figure 6: Qualitative Results with Reflection in PARM++. The proposed PARM++ incorporates a reflection evaluation stage to detect text-image misalignments and provides detailed explanations to guide the self-correction process in autoregressive image generation models.", "description": "This figure showcases qualitative examples demonstrating the effectiveness of PARM++'s reflection mechanism in autoregressive image generation.  PARM++ adds a reflection stage after image generation, where it assesses whether the generated image aligns with the text prompt. If misalignment is detected, PARM++ provides specific feedback (as seen in the caption bubbles) and guides a self-correction process. This iterative refinement aims to improve the accuracy and visual fidelity of the generated image.", "section": "4 Potential Assessment Reward Model ++"}, {"figure_path": "https://arxiv.org/html/2501.13926/x9.png", "caption": "Figure 7: Qualitative Results using Our Reasoning Strategies. Show-o\u00a0[5] is adopted as the baseline model, and compared to our best-performing reasoning strategy: integrating PARM with iterative DPO for both reward model guidance and test-time verification.", "description": "This figure showcases a comparison between images generated by the baseline model (Show-o) and images generated using the proposed reasoning strategies (integrating PARM with iterative DPO).  The results demonstrate the improvement in image quality and relevance to the text prompts when the reasoning strategies are employed. The figure visually presents examples of text prompts and then showcases the resulting images generated by both the baseline model and the enhanced model, highlighting the significant difference in image quality and fidelity.", "section": "2.4 DPO Alignment plus Test-time Verifiers"}, {"figure_path": "https://arxiv.org/html/2501.13926/x10.png", "caption": "Figure 8: Qualitative Results using Our Reasoning Strategies. Show-o\u00a0[5] is adopted as the baseline model, and compared to our best-performing reasoning strategy: integrating PARM with iterative DPO for both reward model guidance and test-time verification.", "description": "Figure 8 presents a qualitative comparison of image generation results using different reasoning strategies. The top row shows examples generated by the baseline model, Show-o [5], which demonstrates suboptimal image generation.  The bottom row displays images generated using the authors' best-performing reasoning strategy. This strategy combines the Potential Assessment Reward Model (PARM) with iterative Direct Preference Optimization (DPO) for both reward model guidance and test-time verification. The comparison highlights the significant improvement in image quality and alignment with the text prompt achieved by integrating PARM and iterative DPO into the image generation process. Each text prompt is paired with multiple image samples illustrating both the baseline and enhanced strategies.", "section": "2.4 DPO Alignment plus Test-time Verifiers"}, {"figure_path": "https://arxiv.org/html/2501.13926/x11.png", "caption": "Figure 9: Qualitative Results using Our Reasoning Strategies. Show-o\u00a0[5] is adopted as the baseline model, and compared to our best-performing reasoning strategy: integrating PARM with iterative DPO for both reward model guidance and test-time verification.", "description": "This figure displays a comparison of image generation results between the baseline model (Show-o) and the improved model using the proposed reasoning strategies.  The improved model incorporates the Potential Assessment Reward Model (PARM) with iterative Direct Preference Optimization (DPO) for both reward model guidance and test-time verification.  Each row presents a different text prompt, followed by the image generations from both the baseline and enhanced models. The image generations from the enhanced model demonstrate improved visual quality and alignment with the corresponding text prompts.", "section": "2.4 DPO Alignment plus Test-time Verifiers"}, {"figure_path": "https://arxiv.org/html/2501.13926/x12.png", "caption": "Figure 10: Qualitative Results using Our Reasoning Strategies. Show-o\u00a0[5] is adopted as the baseline model, and compared to our best-performing reasoning strategy: integrating PARM with iterative DPO for both reward model guidance and test-time verification.", "description": "This figure displays a comparison of image generation results between the baseline model (Show-o) and the model enhanced with the authors' best-performing reasoning strategy. The enhanced strategy combines the Potential Assessment Reward Model (PARM) with iterative Direct Preference Optimization (DPO) for both reward model guidance and test-time verification.  The figure presents sets of images generated for various text prompts to illustrate the qualitative difference between the baseline and enhanced models. This showcases how the improved strategy leads to more accurate and coherent image generations, which better match the descriptions in the text prompts.", "section": "2.4 DPO Alignment plus Test-time Verifiers"}, {"figure_path": "https://arxiv.org/html/2501.13926/x13.png", "caption": "Figure 11: Qualitative Results using Our Reasoning Strategies. Show-o\u00a0[5] is adopted as the baseline model, and compared to our best-performing reasoning strategy: integrating PARM with iterative DPO for both reward model guidance and test-time verification.", "description": "This figure showcases the qualitative improvements achieved in image generation by employing the proposed reasoning strategies. The top row displays results from the baseline model (Show-o) without any reasoning, illustrating shortcomings in image generation. The bottom row shows the results after applying the best-performing reasoning strategy: integrating PARM (Potential Assessment Reward Model) with iterative DPO (Direct Preference Optimization). This combination enhances image quality, ensuring better alignment with the text prompts. The improvements are evident across various image generation aspects, demonstrating the effectiveness of the proposed methods in overcoming limitations of the baseline model.", "section": "2.4 DPO Alignment plus Test-time Verifiers"}, {"figure_path": "https://arxiv.org/html/2501.13926/x14.png", "caption": "Figure 12: Visualization of Early-stage and Later-stage Images. We visualize the generated images in the intermediate steps of Show-o\u00a0[5], where the early-stage images are too blurry to interpret, while the later-stage images are too similar to discriminate, posing great challenges for PRMs to evaluate.", "description": "Figure 12 visualizes the intermediate steps of image generation using the Show-o model.  Early-stage images are too blurry for effective evaluation, while later-stage images become visually similar, hindering accurate assessment using Process Reward Models (PRMs).  This illustrates the challenges PRMs face in evaluating the generation process step-by-step, particularly with autoregressive image generation models.", "section": "2.2 ORM vs PRM as Test-time Verifiers"}]