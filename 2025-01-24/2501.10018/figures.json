[{"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig2.png", "caption": "Figure 1: Overview of the proposed video inpainting model DiffuEraser, based on stable diffusion. The main denoising UNet performs the denoising process to generate the final output. The BrushNet branch extracts features from masked images, which are added to the main denoising UNet layer by layer after a zero convolution block. Temporal attention is incorporated after self-attention and cross-attention to improve temporal consistency.", "description": "DiffuEraser, a video inpainting model, uses a stable diffusion architecture.  The core is a denoising U-Net that processes noisy latent representations to generate the final inpainted video frames. A secondary branch, BrushNet, processes masked image regions, extracting features that are integrated into the U-Net via zero-convolution blocks, improving the model's understanding of the masked areas.  To maintain temporal coherence across frames, the architecture incorporates a temporal attention mechanism following the self- and cross-attention layers.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig3-green.png", "caption": "Figure 2: Example of noisy artifacts generated by the model. The masked region above the sea level is not completed correctly and resembles random noise.", "description": "This figure shows an example of the noisy artifacts that can be generated by the model when inpainting videos. In particular, it focuses on a scenario where the masked region is above the sea level. The model fails to correctly reconstruct this area, resulting in a noisy output that closely resembles random noise instead of the expected coherent sea level. This highlights the limitation of the model in handling some scenarios and the need for further improvements in its generative capabilities.", "section": "3.2. Incorporation of Priors"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig4.png", "caption": "Figure 3: Incorporation of priors. We introduce priors during inference by performing DDIM inversion on the outputs of the prior model and adding them to the noisy latent.", "description": "This figure illustrates how prior information is incorporated into the DiffuEraser model during the inference stage to improve the quality of video inpainting.  The process begins with the output of a pre-trained prior model (in this case, ProPainter).  DDIM (Denoising Diffusion Implicit Models) inversion is applied to this output to transform it into a more suitable format for integration with the noisy latent representation of the current frame. This inverted prior is then added to the noisy latent, effectively providing a strong initialization and weak conditioning. This helps to guide the generation process within the diffusion model, reducing noisy artifacts and promoting the generation of coherent and meaningful inpainted content. The addition of priors serves as a form of weak supervision, making the generation process more stable and less prone to hallucinating unrealistic details.", "section": "3.2. Incorporation of Priors"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig5-green.png", "caption": "Figure 4: Comparison of inpainting results before and after incorporating priors.", "description": "This figure demonstrates the impact of incorporating prior information into the diffusion model.  The left side shows inpainting results with only the diffusion model, resulting in noisy artifacts and inconsistencies within the masked regions. The right side shows how adding prior information significantly improves the inpainting quality by producing more coherent and realistic results with fewer artifacts.  This highlights the effectiveness of using priors to initialize and guide the diffusion model, mitigating the generation of unwanted objects.", "section": "3.2. Incorporation of Priors"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig6.png", "caption": "Figure 5: Utilizing the temporal smoothing property of the Video Diffusion Model (VDM) to enhance consistency at the intersections of clips.", "description": "This figure illustrates how the temporal smoothing property inherent in Video Diffusion Models (VDMs) is leveraged to maintain consistency at the boundaries where video clips are joined.  The even-numbered timesteps are generated from the beginning of a clip, while odd-numbered timesteps are generated from the midpoint. This staggered approach uses the VDM's inherent temporal smoothing effect to produce smooth transitions between clips, enhancing the temporal consistency across the entire sequence.", "section": "3.3 Optimizing Temporal Consistency for Long-Sequence Inference"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig7.png", "caption": "Figure 6: Temporal consistency optimization for long-sequence inference.", "description": "This figure illustrates how the temporal consistency of video inpainting is optimized for long sequences.  The issue addressed is that while individual short video clips (e.g., 22 frames) maintain temporal consistency, transitions between clips often exhibit inconsistencies.  The optimization uses the temporal smoothing property of the Video Diffusion Model (VDM). By employing a staggered approach to denoising during inference (even-numbered timesteps from the clip start, odd-numbered from the midpoint), the method blends frames at clip boundaries seamlessly, enhancing overall consistency.", "section": "3.3 Optimizing Temporal Consistency for Long-Sequence Inference"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig89a.png", "caption": "Figure 7: Perform pre-propagation or pre-inference to expand the temporal receptive field of model.", "description": "This figure illustrates the method used to expand the temporal receptive field of the model.  To improve temporal consistency, especially across long sequences, the model performs a pre-processing step before the main frame-by-frame inference.  The pre-processing involves either pre-propagation (a) or pre-inference (b).  Pre-propagation focuses on extending the influence of known pixels across the whole sequence to improve their consistency. In contrast, pre-inference processes the whole sequence as a single unit, using the temporal context of the entire sequence to guide the generation of unknown pixels (new regions that haven't been seen in the video). This combined approach addresses the issue of consistency across segments.", "section": "3.3 Optimizing Temporal Consistency for Long-Sequence Inference"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig89b.png", "caption": "Figure 8: The temporal consistency obtained from pre-propagation or pre-inference is maintained throughout all remaining frames.", "description": "This figure illustrates how pre-propagation and pre-inference improve temporal consistency in long video sequences.  The left side (a) shows pre-propagation, where a portion of the video is processed in advance to establish a strong baseline for consistency. This baseline guides the frame-by-frame processing shown on the right side (b), ensuring that consistency is maintained throughout the entire video.  The process is repeated for consecutive video clips.", "section": "3.3 Optimizing Temporal Consistency for Long-Sequence Inference"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig11.png", "caption": "Figure 9: Texture quality comparison between DiffuEraser and Propainter.", "description": "This figure showcases a comparison of texture quality between the proposed DiffuEraser model and the existing Propainter model.  It provides several examples of video frames with masked regions, displaying the inpainting results from both methods side-by-side. This allows for a visual assessment of how each model handles texture detail, clarity, and overall realism in the inpainted areas.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig10.png", "caption": "Figure 10: Texture quality comparison between DiffuEraser and Propainter.", "description": "This figure presents a qualitative comparison of texture quality between DiffuEraser (the proposed model) and Propainter (a state-of-the-art transformer-based model) for video inpainting.  It shows several examples of video frames with masked regions, alongside the inpainting results produced by each model. The visual comparison highlights that DiffuEraser generates more detailed and refined textures than Propainter, demonstrating its superior generative capabilities, particularly in handling complex textures and object details. The masked regions are indicated in green.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig13.png", "caption": "Figure 11: Temporal consistency comparison between DiffuEraser and Propainter.", "description": "This figure shows a comparison of the temporal consistency achieved by DiffuEraser and Propainter on several video sequences.  Each row displays a sequence of frames with a masked region. The leftmost column ('Masked Frames') shows the original frames with the masked area. The middle column shows the results obtained by Propainter, and the rightmost column shows the results generated by DiffuEraser.  The comparison highlights how DiffuEraser provides better consistency in the inpainted content across consecutive frames, especially apparent in the sequences with larger masked regions or more complex movement.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.10018/extracted/6137439/fig14.png", "caption": "Figure 12: Temporal consistency comparison between DiffuEraser and Propainter.", "description": "This figure compares the temporal consistency of video inpainting results between DiffuEraser and ProPainter.  It visually demonstrates how well each model maintains consistent visual content across consecutive frames of a video after inpainting.  Differences in the consistency of textures and objects across frames highlight the relative strengths and weaknesses of the two models in preserving temporal coherence. The effectiveness of each model in handling temporal consistency is crucial for producing realistic and natural-looking inpainted videos.", "section": "4. Experiments"}]