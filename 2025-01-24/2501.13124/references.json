{"references": [{"fullname_first_author": "Burns", "paper_title": "Weak-to-strong generalization: Eliciting strong capabilities with weak supervision", "publication_date": "2023-12-09", "reason": "This paper introduces the core concept of weak-to-strong generalization, a central theme of the current research."}, {"fullname_first_author": "Bowman", "paper_title": "Measuring progress on scalable oversight for large language models", "publication_date": "2022-11-03", "reason": "This paper explores scalable oversight, a key method to address the limitations of human evaluation in AI alignment, which is directly relevant to the current work."}, {"fullname_first_author": "Leike", "paper_title": "Scalable agent alignment via reward modeling: a research direction", "publication_date": "2018-11-18", "reason": "This paper lays the groundwork for scalable oversight, a crucial topic in the field of AI alignment, which the current study directly builds upon."}, {"fullname_first_author": "Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-12-01", "reason": "This paper details reinforcement learning from human feedback (RLHF), a widely used technique in AI alignment that the current research aims to improve upon."}, {"fullname_first_author": "Michael", "paper_title": "Debate helps supervise unreliable experts", "publication_date": "2023-11-08", "reason": "This paper introduces the use of debate for improving alignment, a core component of the proposed approach in the current research."}]}