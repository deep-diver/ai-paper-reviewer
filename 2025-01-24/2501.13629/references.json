{"references": [{"fullname_first_author": "Tri Dao", "paper_title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "publication_date": "2022-12-01", "reason": "This paper introduces FlashAttention, a more efficient attention mechanism which is used in SIGMA to improve inference speed."}, {"fullname_first_author": "Joshua Ainslie", "paper_title": "GQA: Training generalized multi-query transformer models from multi-head checkpoints", "publication_date": "2023-12-01", "reason": "This paper introduces GQA, a related attention mechanism that is compared against SIGMA's DiffQKV attention to demonstrate performance improvements."}, {"fullname_first_author": "Woosuk Kwon", "paper_title": "Efficient memory management for large language model serving with pagedattention", "publication_date": "2023-12-01", "reason": "This paper discusses memory management techniques for large language models, which addresses a key challenge that SIGMA seeks to improve."}, {"fullname_first_author": "S\u00e9bastien Bubeck", "paper_title": "Sparks of artificial general intelligence: Early experiments with GPT-4", "publication_date": "2023-03-16", "reason": "This paper is a seminal work on evaluating large language models and is referenced by SIGMA to situate its work in the broader context of LLM development."}, {"fullname_first_author": "Mark Chen", "paper_title": "Evaluating large language models trained on code", "publication_date": "2021-07-07", "reason": "This paper presents a benchmark for evaluating LLMs trained on code, which is relevant to SIGMA's focus on the system domain and its AIMICIUS benchmark."}]}