{"references": [{"fullname_first_author": "Anthropic", "paper_title": "Claude Team. Introducing Claude 3.5 Sonnet.", "publication_date": "2024", "reason": "This paper introduces a large language model used as a baseline in the evaluation of Video-MMMU, and it is a key component of the study."}, {"fullname_first_author": "Michael Boratko", "paper_title": "A systematic classification of knowledge, reasoning, and context within the ARC dataset.", "publication_date": "2018", "reason": "This paper introduces a knowledge-driven benchmark that is related to the goals of Video-MMMU."}, {"fullname_first_author": "Dan Hendrycks", "paper_title": "Measuring massive multitask language understanding.", "publication_date": "2021", "reason": "This paper introduces a multi-task language understanding benchmark, relevant to evaluating the broader capabilities of large multimodal models."}, {"fullname_first_author": "Pan Lu", "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering.", "publication_date": "2022", "reason": "This paper introduces a multimodal reasoning benchmark that is relevant to Video-MMMU's focus on knowledge acquisition."}, {"fullname_first_author": "Xiang Yue", "paper_title": "MMMU: A massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI.", "publication_date": "2024", "reason": "This paper introduces a benchmark dataset similar to Video-MMMU, providing a comparative analysis and context for the current work."}]}