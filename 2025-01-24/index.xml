<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-01-24s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/</link><description>Recent content in 2025-01-24s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Thu, 23 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/index.xml" rel="self" type="application/rss+xml"/><item><title>Can We Generate Images with CoT? Let's Verify and Reinforce Image Generation Step by Step</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13926/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13926/</guid><description>Researchers significantly enhanced autoregressive image generation by integrating chain-of-thought reasoning strategies, achieving a remarkable +24% improvement on the GenEval benchmark.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13926/cover.png"/></item><item><title>EchoVideo: Identity-Preserving Human Video Generation by Multimodal Feature Fusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13452/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13452/</guid><description>EchoVideo generates high-fidelity, identity-preserving videos by cleverly fusing text and image features, overcoming limitations of prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13452/cover.png"/></item><item><title>Improving Video Generation with Human Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13918/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13918/</guid><description>Human feedback boosts video generation! New VideoReward model &amp;amp; alignment algorithms significantly improve video quality and user prompt alignment, exceeding prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13918/cover.png"/></item><item><title>Sigma: Differential Rescaling of Query, Key and Value for Efficient Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13629/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13629/</guid><description>SIGMA, a novel large language model, achieves &lt;strong>up to 33.36% faster inference speeds&lt;/strong> by using DiffQKV attention, which differentially optimizes query, key, and value components in the attention mech&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13629/cover.png"/></item><item><title>Temporal Preference Optimization for Long-Form Video Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13919/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13919/</guid><description>Boosting long-form video understanding, Temporal Preference Optimization (TPO) enhances video-LLMs by leveraging preference learning. It achieves this through a self-training method using preference &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13919/cover.png"/></item><item><title>Video-MMMU: Evaluating Knowledge Acquisition from Multi-Discipline Professional Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13826/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13826/</guid><description>Video-MMMU benchmark systematically evaluates Large Multimodal Models’ knowledge acquisition from videos across multiple disciplines and cognitive stages, revealing significant gaps between human and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13826/cover.png"/></item><item><title>Evolution and The Knightian Blindspot of Machine Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13075/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13075/</guid><description>Machine learning overlooks robustness to an unknowable future; this paper contrasts reinforcement learning with biological evolution, revealing that ML&amp;rsquo;s formalisms limit engagement with unknown unkno&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13075/cover.png"/></item><item><title>SRMT: Shared Memory for Multi-agent Lifelong Pathfinding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13200/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13200/</guid><description>SRMT: Shared Recurrent Memory Transformer boosts multi-agent coordination by implicitly sharing information via a global memory, significantly outperforming baselines in complex pathfinding tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13200/cover.png"/></item><item><title>Debate Helps Weak-to-Strong Generalization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13124/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13124/</guid><description>Debate-enhanced weak supervision boosts AI alignment by combining strong and weak models, enabling safer and more reliable AI systems.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.13124/cover.png"/></item><item><title>Step-KTO: Optimizing Mathematical Reasoning through Stepwise Binary Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10799/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10799/</guid><description>STEP-KTO: A novel training framework boosts LLMs&amp;rsquo; mathematical reasoning by providing binary feedback on both intermediate steps and final answers. This ensures logical reasoning trajectories and impr&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10799/cover.png"/></item><item><title>DiffuEraser: A Diffusion Model for Video Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10018/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10018/</guid><description>DiffuEraser: a novel video inpainting model based on stable diffusion, surpasses existing methods by using injected priors and temporal consistency improvements for superior results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10018/cover.png"/></item><item><title>GSTAR: Gaussian Surface Tracking and Reconstruction</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10283/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10283/</guid><description>GSTAR: A novel method achieving photorealistic rendering, accurate reconstruction, and reliable 3D tracking of dynamic scenes with changing topology, even handling surfaces appearing, disappearing, or&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-24/2501.10283/cover.png"/></item></channel></rss>