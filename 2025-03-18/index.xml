<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-03-18s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/</link><description>Recent content in 2025-03-18s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 17 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/index.xml" rel="self" type="application/rss+xml"/><item><title>BlobCtrl: A Unified and Flexible Framework for Element-level Image Generation and Editing</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13434/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13434/</guid><description>BlobCtrl: Precisely edit images at the element level with a unified, flexible framework, bridging the gap between generation and editing.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13434/cover.png"/></item><item><title>DreamRenderer: Taming Multi-Instance Attribute Control in Large-Scale Text-to-Image Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12885/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12885/</guid><description>DreamRenderer: Taming attribute control in large-scale text-to-image models with a plug-and-play, training-free approach for enhanced content creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12885/cover.png"/></item><item><title>Edit Transfer: Learning Image Editing via Vision In-Context Relations</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13327/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13327/</guid><description>Edit Transfer: Learns image edits from a single example and applies it to new images, surpassing text/reference-based methods!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13327/cover.png"/></item><item><title>Free-form language-based robotic reasoning and grasping</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13082/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13082/</guid><description>FreeGrasp: enabling robots to grasp by interpreting instructions and reasoning about object spatial relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13082/cover.png"/></item><item><title>MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13399/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13399/</guid><description>MicroVQA: A new benchmark to test visual-question-answering in microscopy-based research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13399/cover.png"/></item><item><title>Rewards Are Enough for Fast Photo-Realistic Text-to-image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13070/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13070/</guid><description>Rewards Are Enough!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13070/cover.png"/></item><item><title>Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13369/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13369/</guid><description>SIGHTATION: A BLV-aligned dataset utilizing sighted user feedback to enhance diagram descriptions generated by VLMs, improving accessibility for visually impaired learners.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13369/cover.png"/></item><item><title>WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13435/</link><pubDate>Mon, 17 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13435/</guid><description>WideRange4D: A new benchmark &amp;amp; reconstruction method for high-quality 4D scenes with wide-range movements, pushing the boundaries of 4D reconstruction.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.13435/cover.png"/></item><item><title>Basic Category Usage in Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12530/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12530/</guid><description>VLMs exhibit human-like object categorization, favoring basic levels and mirroring biological/expertise nuances, suggesting learned cognitive behaviors.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12530/cover.png"/></item><item><title>Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12533/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12533/</guid><description>Being-0: A humanoid robot agent achieves complex tasks by integrating a vision-language model with modular skills, enhancing efficiency and real-time performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12533/cover.png"/></item><item><title>Investigating Human-Aligned Large Language Model Uncertainty</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12528/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12528/</guid><description>This research explores how well LLM uncertainty measures align with human uncertainty, finding Bayesian and top-k entropy measures show promise.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12528/cover.png"/></item><item><title>Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12605/</link><pubDate>Sun, 16 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12605/</guid><description>A comprehensive survey of multimodal chain-of-thought (MCoT) reasoning, bridging the gap in existing literature and fostering innovation towards multimodal AGI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.12605/cover.png"/></item><item><title>MTV-Inpaint: Multi-Task Long Video Inpainting</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.11412/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.11412/</guid><description>MTV-Inpaint: A unified framework for multi-task long video inpainting, enabling versatile object insertion, scene completion, editing, and removal.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.11412/cover.png"/></item><item><title>V-STaR: Benchmarking Video-LLMs on Video Spatio-Temporal Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.11495/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.11495/</guid><description>V-STaR: A new benchmark to evaluate Video-LLMs in video spatio-temporal reasoning, revealing gaps in current models&amp;rsquo; understanding.</description></item><item><title>Long-Video Audio Synthesis with Multi-Agent Collaboration</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.10719/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.10719/</guid><description>LVAS-Agent: Multi-agent system conquers long-video audio synthesis with collaborative dubbing, script, design, &amp;amp; more!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.10719/cover.png"/></item><item><title>Error Analyses of Auto-Regressive Video Diffusion Models: A Unified Framework</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.10704/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.10704/</guid><description>Unified framework reveals and mitigates error sources in autoregressive video diffusion models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.10704/cover.png"/></item><item><title>DropletVideo: A Dataset and Approach to Explore Integral Spatio-Temporal Consistent Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.06053/</link><pubDate>Sat, 08 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.06053/</guid><description>DropletVideo: A dataset and approach to explore integral spatio-temporal consistent video generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-18/2503.06053/cover.png"/></item></channel></rss>