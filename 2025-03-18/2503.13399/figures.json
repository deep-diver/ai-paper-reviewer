[{"figure_path": "https://arxiv.org/html/2503.13399/x2.png", "caption": "Figure 1: A scientific experimentation workflow drives discovery: researchers analyze experiments, develop hypotheses, and design further experiments to test their ideas. We release MicroVQA, a visual question answering (VQA) benchmark to test these three tasks in the context of biological microscopy. Each of the 1,042 samples is created by a biology expert, and transformed into a multiple choice question (MCQ).", "description": "The figure illustrates the three main reasoning tasks involved in scientific experimentation using biological microscopy images: expert image understanding, hypothesis generation, and experimental proposal.  Each task is represented with an example image and question, highlighting how MicroVQA uses visual question answering to evaluate these reasoning capabilities.  The benchmark comprises 1042 multiple-choice questions, each created by a biology expert, to test the ability to understand, interpret, and reason using microscopy-based data.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2503.13399/x3.png", "caption": "Figure 2: MicroVQA taxonomy of sub-tasks.", "description": "This figure presents a taxonomy that organizes the subtasks within the MicroVQA benchmark into three main categories: expert visual understanding, hypothesis generation, and experimental proposal. Each category is further divided into more specific subtasks to comprehensively evaluate the different aspects of scientific reasoning involved in microscopy-based biological research.  Expert visual understanding tasks include comparing image sets, identifying abnormalities, and more. Hypothesis generation tasks involve exploring causal mechanisms and functional implications. Finally, experimental proposal tasks focus on suggesting new experiments or addressing technical issues.", "section": "3. MicroVQA benchmark"}, {"figure_path": "https://arxiv.org/html/2503.13399/x4.png", "caption": "Figure 3: \nComposition of scientific MLLM benchmarks regarding estimated Bloom\u2019s taxonomy [11]. Higher levels are more cognitively challenging. MicroVQA has more questions at higher levels compared to other benchmarks, for example, MMMU [87] and ScienceQA [53], while perception-driven medical benchmarks like OmniMedVQA are at lower levels.", "description": "Figure 3 is a bar chart comparing the distribution of Bloom's taxonomy levels across several scientific multimodal large language model (MLLM) benchmarks.  Bloom's taxonomy categorizes cognitive skills from simple recall to complex evaluation.  The chart shows that MicroVQA, the benchmark introduced in this paper, has a higher proportion of questions at the higher cognitive levels (analysis, application, evaluation) compared to other benchmarks like MMMU and ScienceQA.  These other benchmarks tend to have a greater proportion of questions at the lower levels (recall, comprehension), reflecting a focus on simpler tasks. Benchmarks focusing primarily on perception, such as OmniMedVQA, also show a concentration of questions at the lower levels. This figure highlights that MicroVQA is designed to assess more advanced reasoning capabilities than many existing benchmarks.", "section": "3. The MicroVQA benchmark"}, {"figure_path": "https://arxiv.org/html/2503.13399/x5.png", "caption": "Figure 4: Constructing the MicroVQA multiple choice questions. (0) We defined tasks with domain biological scientists and created 1,061 raw VQA samples. (1) The raw samples were aligned to an exam-style MCQ by manually transforming a small set and optimizing an LLM prompt to match that alignment. (2) MCQs are further improved using RefineBot, a new iterative method to make MCQs more challenging. The lower panel shows an example MCQ from raw VQA to final: the annotations highlight key changes that we further explore in Sec.\u00a0E.2, where red indicates issues, and  green  indicates good attributes.", "description": "This figure illustrates the three-stage process of generating multiple-choice questions (MCQs) for the MicroVQA benchmark.  Stage 0 involves defining the tasks and creating 1061 raw visual question answering (VQA) samples with the help of biology experts. In Stage 1, these raw VQA samples are transformed into exam-style MCQs.  This is done by manually converting a subset of samples and then optimizing an LLM prompt to automatically convert the remaining samples. Finally, in Stage 2, the MCQs are further refined using the RefineBot, an iterative method that increases question difficulty and removes language shortcuts that could allow models to answer without true multi-modal reasoning.  The lower panel provides a detailed example illustrating the changes made during the refinement process, highlighting improvements (green) and issues (red) that are further discussed in Section E.2 of the paper.", "section": "4. MCQ generation process"}, {"figure_path": "https://arxiv.org/html/2503.13399/extracted/6287634/figures/language-shortcuts.png", "caption": "Figure 5: Performance by sub-task and Bloom\u2019s level for best models: Gemini-1.5-Pro (closed source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (medical).", "description": "This figure presents a detailed breakdown of the performance of three state-of-the-art large language models (LLMs) across various subtasks and Bloom's taxonomy levels within the MicroVQA benchmark.  The subtasks, representing different aspects of scientific reasoning, include comparing image sets, identifying abnormalities, proposing causal mechanisms, exploring functional implications, suggesting new experiments, and addressing technical issues.  The Bloom's taxonomy levels reflect the cognitive complexity of each task, ranging from simple recall to complex evaluation.  The models analyzed are Gemini-1.5-Pro (closed-source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (a medical-specialized model). By visualizing performance across these dimensions, Figure 5 provides key insights into the strengths and weaknesses of each model in tackling various aspects of multimodal scientific reasoning.", "section": "3.3. Analysis of MicroVQA benchmark"}, {"figure_path": "https://arxiv.org/html/2503.13399/x6.png", "caption": "Figure 6: Example data schema.", "description": "The figure displays the schema of the MicroVQA dataset.  It shows the various fields and data types associated with each data point.  The fields include identifiers (image_id), the image data itself, labels, label names, domain and subdomain information, imaging modalities and submodalities, stain type, microns per pixel resolution, and the questions and answers associated with the image. This provides a comprehensive overview of the structure of the dataset, illustrating how diverse data (images, text, metadata) is organized to capture the multifaceted nature of scientific visual question answering.", "section": "D Benchmark details"}, {"figure_path": "https://arxiv.org/html/2503.13399/x7.png", "caption": "Figure 7: Example data instance.", "description": "This figure displays an example of the data schema used in the MicroVQA benchmark.  The schema includes fields such as image ID, image data, label, label name (e.g., pathology), domain, subdomain (e.g., gastrointestinal pathology), modality (e.g., light microscopy), submodality (e.g., brightfield microscopy), stain type (e.g., H&E), microns per pixel, question text, and the correct answer. The question text includes the prompt to be answered and the multiple choice options, along with the correct answer.", "section": "D Benchmark details"}, {"figure_path": "https://arxiv.org/html/2503.13399/x8.png", "caption": "Figure 8: Examples of taxonomy classes used as context to LLM to assign an organism to a question. A YAML file with the full taxonomy will be released with the code.", "description": "Figure 8 shows examples of how taxonomy classes are used as context for LLMs to assign organisms to questions in the MicroVQA benchmark.  The full taxonomy, in YAML format, will be made available with the code release.  This is important because it provides additional context for the LLMs to understand the questions and select the correct answer, which is particularly important in the scientific domain where specialized knowledge is needed.", "section": "D Benchmark details"}, {"figure_path": "https://arxiv.org/html/2503.13399/x9.png", "caption": "Figure 9: Examples of taxonomy classes used as context to LLM to assign a research subject to a question. A YAML file with the full taxonomy will be released with the code.", "description": "This figure shows examples of the taxonomy classes used to provide context to a large language model (LLM) when assigning a research subject to a question within the MicroVQA benchmark.  The full taxonomy, in YAML format, will be available alongside the code release.  The taxonomy helps ensure that the LLMs receive sufficient contextual information for accurate and nuanced reasoning in scientific contexts.  Each example shows a category within the taxonomy (e.g., \"Anatomy,\" \"Biochemistry\") and then lists some relevant keywords.", "section": "D Benchmark details"}, {"figure_path": "https://arxiv.org/html/2503.13399/x10.png", "caption": "Figure 10: Three types of language shortcut relevant to MicroVQA. The target VQA has an image that is fluorescence microscopy stained with TOMM20 which would show a pattern consistent with visualizing mitochondria.", "description": "Figure 10 illustrates three common ways that large language models (LLMs) can \"cheat\" on visual question answering (VQA) tasks by relying on textual information rather than visual reasoning.  The example questions all relate to a fluorescence microscopy image stained with TOMM20, a marker for mitochondria.  Shortcut 1 demonstrates a \"visual giveaway,\" where the question itself provides enough information to answer the question without needing to analyze the image. Shortcut 2 shows \"weak distractors,\" where the incorrect answer options are implausible or easily eliminated based on general knowledge.  Finally, shortcut 3 highlights \"language bias,\" where the question wording or context makes one answer more likely than others, irrespective of the visual data.", "section": "4. MCQ generation process"}, {"figure_path": "https://arxiv.org/html/2503.13399/x11.png", "caption": "Figure 11: Examples of changes to questions and options between stage 1 and stage 2 (RefineBot) of our MCQ generation method. In red elements that need to be improved and in  green  improvements.", "description": "This figure showcases examples of how the two-stage MCQ generation process refines questions and answer options. Stage 1 focuses on creating well-structured MCQs from the original raw data; whereas, stage 2 uses RefineBot to improve the questions by making them more challenging and removing language shortcuts.  The image highlights specific modifications made during the refinement process. Red indicates elements that were improved, while green highlights the improvements themselves.", "section": "MCQ generation details"}, {"figure_path": "https://arxiv.org/html/2503.13399/x12.png", "caption": "Figure 12: Performance by image modality type for the best models: Gemini-1.5-Pro (closed source), VILA1.5-40B (open-source), and LlaVA-Med-Mistral-7B (medical).", "description": "This figure shows a bar chart comparing the performance of three different large language models (LLMs) across three different microscopy image modalities: light microscopy, fluorescence microscopy, and electron microscopy.  The performance metric is likely accuracy or a similar measure of correct responses to questions about the images. The models compared are Gemini-1.5-Pro, VILA1.5-40B, and LlaVA-Med-Mistral-7B, representing closed-source, open-source, and medical-specialized models, respectively. The chart visually demonstrates how each model's performance varies across the different image modalities, highlighting which modality is most challenging for each model and revealing potential differences in model capabilities for handling diverse visual data types in microscopy image analysis.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.13399/x13.png", "caption": "Figure 13:", "description": "This figure shows a cryo-electron tomography (cryoET) image of mammalian cells, highlighting dark circular structures within an organelle. The image was acquired at a resolution of 1.3 \u00c5 and binned by 4, resulting in a 5.3 \u00c5 per pixel resolution. The figure is accompanied by a multiple-choice question asking about the likely identity of these structures, with options including ribosomes, phase-separated condensates, lysosomes, and peroxisomes. The correct answer is stress granules. The reasoning process for identifying the structures is also shown. A key component of the analysis is assessing the size, shape, electron density, and the context of the organelle in which the structures are located. This figure illustrates an error of perception; the model misidentifies the granules.", "section": "Error analysis on MicroVQA"}, {"figure_path": "https://arxiv.org/html/2503.13399/x14.png", "caption": "Figure 14:", "description": "This figure shows examples of how the two-stage MCQ generation process improves the quality of questions.  The left column shows the original questions and options generated in stage 1 (exam-style alignment). The right column shows the refined questions and options after the RefineBot process (stage 2).  The improvements highlight the RefineBot's effectiveness in removing language shortcuts, making the questions more challenging, and improving the distractors' quality by ensuring they're biologically plausible and not easily ruled out by superficial knowledge.", "section": "MCQ generation details"}, {"figure_path": "https://arxiv.org/html/2503.13399/x15.png", "caption": "Figure 15:", "description": "This figure displays two H&E-stained images of schwannoma tissue, showing marked cellular changes and unique tissue structures. The images highlight a cystic change that can be seen in chronic, longstanding schwannomas, and fascicular growth with Verocay bodies. The model incorrectly interprets this as rapid cellular proliferation.", "section": "5.2 Error analysis on MicroVQA"}, {"figure_path": "https://arxiv.org/html/2503.13399/x16.png", "caption": "Figure 16:", "description": "This figure shows a fluorescence microscopy image comparing wounded and unwounded mouse liver tissue.  The tissue is stained with markers for immune response: neutrophil-derived migrasomes (Ly6G, yellow) and platelets (CD41, magenta). The image illustrates the distribution of these markers in both wounded and unwounded tissue, highlighting differences in immune cell localization and activation in response to tissue injury.", "section": "Error analysis on MicroVQA"}, {"figure_path": "https://arxiv.org/html/2503.13399/x17.png", "caption": "Figure 17:", "description": "This figure displays the results of a double immunofluorescence experiment conducted on nerve cells to visualize Sema4B (red marker) and Sox9 (an astrocyte marker, green).  Sema4B's primary localization is surrounding the nuclear area, dispersed in the cytoplasm.  The figure shows three images: one each for Sema4B, Sox9 and a merged image. The question is what potential role Sema4B may play in these cells, based on its observed distribution. The model's response and error analysis are also provided.", "section": "Error analysis"}, {"figure_path": "https://arxiv.org/html/2503.13399/x18.png", "caption": "Figure 18:", "description": "This figure shows a cryo-electron tomography image of fibroblasts from a 70-year-old Alzheimer's patient, highlighting three mitochondria clustered together.  The question associated with this image in the MicroVQA benchmark asks about the most likely reason for this mitochondrial clustering. The various response options provided to the large language model reflect potential causes related to mitochondrial biogenesis, fusion, fission, mitophagy, or other cellular processes. The correct answer is impaired mitophagy (the failure of the cell to properly remove damaged mitochondria), but the model's answer is incorrect because its understanding of mitophagy is shallow.  The analysis of this incorrect answer in the paper reveals the challenges of correctly interpreting complex biological phenomena from microscopic images and applying scientific knowledge appropriately.", "section": "5.2. Error analysis on MicroVQA"}, {"figure_path": "https://arxiv.org/html/2503.13399/x19.png", "caption": "Figure 19:", "description": "This figure shows an H&E-stained image and a vimentin-stained image of a tissue sample from a patient with recurrent seizures.  The vimentin stain highlights cells that are larger than surrounding cells.  The question associated with this figure asks for the identity of the enlarged, vimentin-expressing cells. This image is used in the error analysis to illustrate a knowledge error, where the model fails to correctly identify the cells as reactive astrocytes due to a lack of understanding of the specific cellular changes associated with seizures and brain injury. The image highlights the challenges of multi-modal reasoning in biological microscopy.", "section": "Error analysis"}, {"figure_path": "https://arxiv.org/html/2503.13399/extracted/6287634/figures/collage_1.jpg", "caption": "Figure 20:", "description": "This figure shows a microscopic analysis of a tissue sample from an individual with recurrent seizures, using hematoxylin and eosin stain alongside a vimentin stain. The figure highlights cells with stronger vimentin expression, which are significantly larger than their surrounding counterparts. The question associated with this figure in the paper asks to identify the most probable identity of these prominent vimentin-expressing cells.", "section": "F.6.2 Knowledge errors"}]