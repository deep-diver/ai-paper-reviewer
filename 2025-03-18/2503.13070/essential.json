{"importance": "This paper presents a paradigm shift in conditional image generation, offering a new perspective via regularized reward maximization, **eliminating reliance on complex diffusion losses** and opening doors for efficient, high-quality AIGC.", "summary": "Rewards Are Enough!", "takeaways": ["Rewards alone are sufficient for high-quality, few-step image generation, challenging the necessity of diffusion distillation.", "R0, a novel approach, treats image generation as a direct optimization problem in data space, maximizing rewards.", "Effective regularization techniques, like weight regularization and random \u03b7-sampling, are crucial for preventing reward hacking and ensuring visual quality."], "tldr": "Aligning generated images with text prompts and human preferences is a core challenge in AIGC. Reward-enhanced diffusion distillation is promising for boosting text-to-image models. However, rewards become dominant with stronger conditions, overshadowing diffusion losses. Rewards become the dominant force in generation while diffusion losses become regularization.\n\nTo validate, the paper introduces R0, a conditional generation approach via regularized reward maximization. Treating image generation as an optimization in data space, it searches for images with high compositional rewards. State-of-the-art few-step models are trained with R0, demonstrating the rewards play a dominant role in complex conditions and challenge the conventional diffusion post-training.", "affiliation": "Hong Kong University of Science and Technology", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2503.13070/podcast.wav"}