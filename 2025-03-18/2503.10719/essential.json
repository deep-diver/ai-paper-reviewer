{"importance": "This paper introduces a novel framework and a dedicated benchmark for long-video audio synthesis, addressing key challenges and opening new avenues for research in multi-agent collaborative systems and VLM-based content creation. **It holds importance for researchers in audio processing, video understanding, and AI-driven content generation**.", "summary": "LVAS-Agent: Multi-agent system conquers long-video audio synthesis with collaborative dubbing, script, design, & more!", "takeaways": ["LVAS-Agent, a multi-agent framework, addresses long-video audio synthesis via collaborative roles.", "LVAS-Bench, a new benchmark, enables standardized evaluation of long-video audio synthesis methods.", "The approach surpasses baselines, improving audio-visual alignment."], "tldr": "Current video-to-audio methods falter in **long scenarios due to fragmented synthesis and inadequate cross-scene consistency**. To address this, the paper introduces **LVAS-Agent, a novel multi-agent framework** that emulates professional dubbing workflows through collaborative role specialization. This decomposes long-video synthesis into scene segmentation, script generation, sound design, and audio synthesis, using discussion-correction and generation-retrieval loop. \n\n**LVAS-Agent** mimics dubbing with agents: Storyboarder segments videos, Scriptwriter generates scripts fusing CLIP with dialogue, Designer uses spectral analysis for sound design, and Synthesizer blends neural text-to-speech with diffusion. It introduces **LVAS-Bench for evaluation**. Experiments show the system improves audio-visual alignment, offering a better approach to long-video audio synthesis.", "affiliation": "Hong Kong University of Science and Technology", "categories": {"main_category": "Multimodal Learning", "sub_category": "Audio-Visual Learning"}, "podcast_path": "2503.10719/podcast.wav"}