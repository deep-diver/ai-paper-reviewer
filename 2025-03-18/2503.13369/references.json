{"references": [{"fullname_first_author": "Wan Ju Kang", "paper_title": "Sightation Counts: Leveraging Sighted User Feedback in Building a BLV-aligned Dataset of Diagram Descriptions", "publication_date": "2025-03-17", "reason": "This is a reference to the present work, which is standard practice."}, {"fullname_first_author": "Liu et al", "paper_title": "Improved baselines with visual instruction tuning.", "publication_date": "2024-01-01", "reason": "This work is important because it significantly advances machine intelligence through vision-language models."}, {"fullname_first_author": "Wang et al", "paper_title": "QWEN2-VL series", "publication_date": "2024-01-01", "reason": "The models in this collection matches the performance of GPT-40 and CLAUDE3.5-SONNET in multimodal scenarios."}, {"fullname_first_author": "Gurari et al", "paper_title": "Vizwiz grand challenge: Answering visual questions from blind people.", "publication_date": "2018-01-01", "reason": "VizWiz is important because it contains images and visual QA pairs produced by blind people encouraging the development of more generalized algorithms that can assist the blind."}, {"fullname_first_author": "Rafailov et al", "paper_title": "Direct preference optimization: Your language model is secretly a reward model", "publication_date": "2024-01-01", "reason": "DPO is an important reference because reward models trained on generic data may not accurately represent BLV preferences, and DPO is a widely used algorithm free of reward models."}]}