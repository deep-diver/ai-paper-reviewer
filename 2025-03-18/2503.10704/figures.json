[{"figure_path": "https://arxiv.org/html/2503.10704/x1.png", "caption": "Figure 1: FVDs of short clips generated by different models and methods.", "description": "This figure compares the Frechet Video Distance (FVD) scores of short video clips (16 frames) generated by various autoregressive video diffusion models.  The FVD measures the dissimilarity between the generated videos and real videos. Lower FVD indicates better generation quality. The x-axis represents the time span of frames (from start to end of the clip) and the y-axis shows the FVD scores. By analyzing the trends of FVD across different time spans, we can understand how different models handle the accumulation of errors during autoregressive generation. The graph shows that FVD generally increases over time, indicating that error accumulation is an inherent challenge in autoregressive video generation.", "section": "2 Preliminaries"}, {"figure_path": "https://arxiv.org/html/2503.10704/x2.png", "caption": "Figure 2: Examples of error accumulation (first two rows) and memory bottleneck (last two rows).", "description": "This figure provides visual examples of two main error types affecting Auto-Regressive Video Diffusion Models (ARVDMs). The top two rows showcase error accumulation, where errors compound over time as the model generates subsequent frames of a video. The generated frames become increasingly inconsistent or distorted. The bottom two rows illustrate the memory bottleneck phenomenon, where the model fails to retain and utilize information from earlier frames in the sequence, leading to inconsistencies between frames. The memory bottleneck is specific to ARVDMs that generate long videos, and these errors are not present in video models that process all frames simultaneously.", "section": "2 Preliminaries"}, {"figure_path": "https://arxiv.org/html/2503.10704/x3.png", "caption": "Figure 3: This figure indicates Meta-ARVDM framework. The left part if the initialization stage, which denoises with Minitsubscript\ud835\udc40initM_{{\\text{init}}}italic_M start_POSTSUBSCRIPT init end_POSTSUBSCRIPT steps. We add noise to the output of this stage to form the starting point of the AR generation stage. We then auto-regressively apply Algorithm\u00a03 for denoising. The monotonicity, circularity, and 0\u2212T0\ud835\udc470-T0 - italic_T boundary requirements for plausible implementation are marked in the figure. Here \u210b\u2062({Yi+4+jtjI}j=14)\u210bsuperscriptsubscriptsuperscriptsubscript\ud835\udc4c\ud835\udc564\ud835\udc57superscriptsubscript\ud835\udc61\ud835\udc57I\ud835\udc5714\\mathcal{H}(\\{Y_{i+4+j}^{t_{j}^{\\mathrm{I}}}\\}_{j=1}^{4})caligraphic_H ( { italic_Y start_POSTSUBSCRIPT italic_i + 4 + italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_I end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT ) are all the frames before the execution of the (i/2+2)\ud835\udc5622(i/2+2)( italic_i / 2 + 2 )-th iteration.", "description": "Figure 3 illustrates the Meta-ARVDM framework, a unified framework encompassing most existing autoregressive video diffusion models (ARVDMs). The figure is divided into two main stages: initialization and autoregressive generation.  The initialization stage begins with denoising using a certain number of steps (Minit).  The output is then noised to form the starting point for the autoregressive generation stage. This stage iteratively applies Algorithm 3 to further denoise, generating subsequent frames. Key properties of the ARVDM generation process\u2014monotonicity, circularity, and 0-T boundary conditions\u2014are visually represented and highlighted in the figure. The notation  \u210b({Yi+4+jtjI}j=14) represents all frames preceding the (i/2+2)-th iteration step, which is crucial for understanding the autoregressive process.", "section": "A General Framework of ARVDMs"}, {"figure_path": "https://arxiv.org/html/2503.10704/x4.png", "caption": "Figure 4: The simplified setting for the proof of lower bounds.", "description": "This figure simplifies the scenario for proving the lower bound of the error in autoregressive video diffusion models.  It illustrates a Markov chain representing the relationship between the past frames (X), the input frames (Y), and the output frames (Z). This simplification is used to demonstrate that the memory bottleneck inherent in autoregressive models is unavoidable due to information loss.", "section": "4 Performance Analysis of Meta-ARVDM"}, {"figure_path": "https://arxiv.org/html/2503.10704/x5.png", "caption": "Figure 5: Network structure of adding information of previous frames into each AR step. Here wmsubscript\ud835\udc64\ud835\udc5aw_{m}italic_w start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT is the number of past frames provided for the denoising network, and w\ud835\udc64witalic_w is the number of frames to denoise. The superscript M\ud835\udc40Mitalic_M refers to the past frames and actions (memory).", "description": "Figure 5 illustrates the architecture of the network designed to incorporate information from previous frames into each autoregressive (AR) step during video generation.  The core concept is to leverage the temporal context of the video, avoiding the limitations of only considering the current frame.  The network takes two key inputs: the frames currently being processed ('w' frames) and the preceding frames from the video's history ('w<sub>m</sub>' frames), along with their corresponding actions. The superscript 'M' indicates the incorporation of both past frames and actions into the model's memory.  This enriched context allows the model to better predict the next frames in the sequence, improving temporal consistency and mitigating the 'memory bottleneck' problem associated with autoregressive models that process videos frame-by-frame.", "section": "5 Mitigation of the Memory Bottleneck"}, {"figure_path": "https://arxiv.org/html/2503.10704/x6.png", "caption": "Figure 6: An example of the memory retrieval task in DMLab.", "description": "This figure illustrates a memory retrieval task within the DMLab environment. The objective is to assess the model's ability to recall and utilize information from previous frames to accurately generate subsequent frames. The left side displays the ground truth sequence of frames, while the right side shows the model's generated frames based on the provided context. This test evaluates the model's capacity for long-term memory and consistency in generating temporally coherent video.", "section": "5 Mitigation of the Memory Bottleneck"}, {"figure_path": "https://arxiv.org/html/2503.10704/x7.png", "caption": "Figure 7: Retrieval examples of DMLab. In contrast to Oasis in Figure\u00a02, which fails to generate a consistent scene, the improved models can retrieve the wall/floor color, texture, and decoration (e.g. paint on the wall) from previously seen mazes.", "description": "Figure 7 showcases retrieval examples from the DMLab environment.  The improved models, unlike the Oasis model in Figure 2, successfully generate consistent scenes.  This consistency demonstrates the models' ability to recall and accurately reproduce specific visual details from previously encountered maze environments. These details include the colors of walls and floors, the textures of surfaces, and decorative elements such as paint on walls.  The successful reproduction highlights the models' improved memory and ability to maintain coherence across generated video frames, which is a significant step towards improved video generation capabilities.", "section": "5 Mitigation of the Memory Bottleneck"}, {"figure_path": "https://arxiv.org/html/2503.10704/x8.png", "caption": "Figure 8: Retrieval examples of Minecraft. The memory-enhanced model accurately recalls and retrieves scenes previously encountered in earlier views.", "description": "This figure showcases retrieval examples from the Minecraft environment, demonstrating the capability of a memory-enhanced model.  The images depict various scenes from the game. The model, having been trained with enhanced memory capabilities, accurately recalls and retrieves scenes encountered earlier, highlighting its improved ability to maintain context and consistency over longer sequences of video generation.", "section": "Experimental Results"}, {"figure_path": "https://arxiv.org/html/2503.10704/x11.png", "caption": "Figure 9: Correlation between Memory Bottleneck and Error Accumulation.", "description": "This figure visualizes the relationship between memory bottleneck and error accumulation in autoregressive video diffusion models. The x-axis represents the average retrieval rate (a metric indicating the model's ability to recall past information), and the y-axis represents the maximum PSNR decay (a measure of the increase in error over time).  The plot shows that when the memory bottleneck is effectively mitigated (higher average retrieval rate), the error accumulation is greater (larger PSNR decay). This indicates a trade-off between these two aspects of model performance. Different memory sizes (8, 16, 32, and 48) are compared separately for both DMLab and Minecraft datasets.", "section": "4 Performance Analysis of Meta-ARVDM"}, {"figure_path": "https://arxiv.org/html/2503.10704/x12.png", "caption": "Figure 10: Recall demonstrations on DMLab. The left frames represent the expected ground truth, while the right frames, outlined with a red square, are generated by the model.", "description": "This figure showcases the model's performance on the DMLab environment.  Each pair of images shows the expected ground truth (left) alongside the model's generated output (right, outlined in red). The goal is to assess the model's ability to accurately recall and reconstruct visual scenes from memory. The figure helps to visually demonstrate the effectiveness (or lack thereof) of the model in maintaining visual consistency across time steps. The red box highlights differences between the model's output and the ground truth, allowing for quick visual comparison and evaluation.", "section": "E Demos of Our Methods"}, {"figure_path": "https://arxiv.org/html/2503.10704/x13.png", "caption": "Figure 11: Recall demonstrations on Minecraft. The left frames represent the expected ground truth, while the right frames, outlined with a red square, are generated by the model. The first 4 frames without red squares are provided context.", "description": "This figure showcases the model's ability to generate video frames in Minecraft, given a sequence of initial frames as context.  The left column displays the ground truth frames (what the model ideally should produce), while the right column shows the frames generated by the model. The red boxes highlight the frames generated by the model, which are being evaluated for accuracy against the ground truth. The first four frames in each row (without red boxes) serve as the input context for the model; they are not generated by the model but are provided as a starting point. This visual comparison demonstrates the model's capacity to maintain temporal coherence and generate realistic frames but also reveals instances where it falls short.", "section": "E Demos of Our Methods"}, {"figure_path": "https://arxiv.org/html/2503.10704/x14.png", "caption": "Figure 12: The network structure adopted to compress the past frames and actions.", "description": "This figure illustrates two distinct network architectures designed for compressing past frames and actions within a video generation model. The left panel showcases the 'joint' method, where past frames and actions are concatenated and processed together through feedforward, spatial, and temporal attention modules.  The right panel depicts the 'modulated' method. Here, actions modulate the frames before processing through similar modules. Both methods employ compression by retaining only the final, compressed representations of frames and actions, improving efficiency. These compressed representations are then integrated into subsequent video generation steps.", "section": "5 Mitigation of the Memory Bottleneck"}, {"figure_path": "https://arxiv.org/html/2503.10704/x15.png", "caption": "Figure 13: Minecraft Example Pairs from Minecraft Across Varying SSIM Score Ranges", "description": "This figure visualizes pairs of frames from Minecraft video trajectories, categorized by their Structural Similarity Index (SSIM) scores.  The SSIM score quantifies the visual similarity between two images; higher scores indicate greater similarity. The figure showcases examples across different SSIM ranges: low (SSIM < 0.4), medium (0.4 \u2264 SSIM < 0.7), high (0.7 \u2264 SSIM < 0.9), and near-identical (SSIM \u2265 0.9). Each range illustrates the visual differences (or lack thereof) between the frame pairs, demonstrating how SSIM captures various levels of visual similarity, from nearly indistinguishable to quite different scenes.", "section": "I SSIM Values Benchmark For Minecraft"}, {"figure_path": "https://arxiv.org/html/2503.10704/x16.png", "caption": "Figure 14: The examples of \ud835\udca2\u2062(\u22c5)\ud835\udca2\u22c5\\mathcal{G}(\\cdot)caligraphic_G ( \u22c5 ) and \u210b\u2062(\u22c5)\u210b\u22c5\\mathcal{H}(\\cdot)caligraphic_H ( \u22c5 ).", "description": "This figure illustrates the sets \ud835\udca2(\u22c5) and \u210b(\u22c5) used in the proof of Theorem 4.4, which analyzes the KL-divergence between the generated and true videos.  Specifically, it shows how these sets capture the evolution of noise levels and the denoising process during autoregressive video generation.  The figure uses the examples of  \u0394=2 and \u03c9=4 to illustrate how the sets are built from the noisy video frames, the reference frames, and the results of applying autoregressive steps.  It visualizes the sets of generated random vectors, both inputs and outputs, which are progressively concatenated across iterations of the autoregressive step.  The diagrams visually explain the mathematical notations representing these sets.", "section": "L.1 Proof of Eqn. (4.1)"}]