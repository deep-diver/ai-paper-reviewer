{"references": [{"fullname_first_author": "Federico Bianchi", "paper_title": "Safety-tuned LLAMAs: Lessons from improving the safety of large language models that follow instructions", "publication_date": "2024-00-00", "reason": "This paper provides a strong foundation for understanding and improving the safety of LLMs, which is directly relevant to the task of evaluating and improving the safety of VLMs."}, {"fullname_first_author": "Yang Chen", "paper_title": "Can language models be instructed to protect personal information?", "publication_date": "2023-10-00", "reason": "This paper investigates the ability of language models to protect sensitive information, a crucial aspect of VLM safety given their increasing use in applications handling personal data."}, {"fullname_first_author": "Zhe Chen", "paper_title": "InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "publication_date": "2024-00-00", "reason": "This paper introduces a significant VLM model, InternVL, whose safety is evaluated in the current paper, making it a key reference for understanding the state-of-the-art in VLM technology."}, {"fullname_first_author": "Matt Deitke", "paper_title": "Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models", "publication_date": "2024-09-00", "reason": "This paper releases important open-source VLMs which allows further research and analysis of their safety properties, directly relevant to the focus of the current study."}, {"fullname_first_author": "Lukas Helff", "paper_title": "LlavaGuard: VLM-based safeguards for vision dataset curation and safety assessment", "publication_date": "2024-06-00", "reason": "This paper explores methods for automating VLM safety assessment, a key challenge addressed in the current work, making it a highly relevant reference."}]}