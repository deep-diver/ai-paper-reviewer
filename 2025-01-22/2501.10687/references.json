{"references": [{"fullname_first_author": "Tian, L.", "paper_title": "EMO: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions", "publication_date": "2025", "reason": "This paper is the most important because it is the basis for the proposed two-stage framework and the backbone network used in the current work."}, {"fullname_first_author": "Guo, Y.", "paper_title": "Animatediff: Animate your personalized text-to-image diffusion models without specific tuning", "publication_date": "2023", "reason": "This paper's diffusion model is adopted in stage 2 for video generation, providing a robust foundation for video synthesis."}, {"fullname_first_author": "Zhu, L.", "paper_title": "Tryon-diffusion: A tale of two unets", "publication_date": "2023", "reason": "The ReferenceNet architecture used in the video generation stage is based on this paper, leveraging its image feature extraction capabilities."}, {"fullname_first_author": "Lin, G.", "paper_title": "CyberHost: Taming audio-driven avatar diffusion model with region codebook attention", "publication_date": "2024", "reason": "This work is important as it directly addresses audio-driven full body motion generation, offering a comparative method."}, {"fullname_first_author": "Corona, E.", "paper_title": "Vlogger: Multimodal diffusion for embodied avatar synthesis", "publication_date": "2024", "reason": "This paper's audio-driven gesture synthesis and two-stage method serves as another comparison method to demonstrate progress."}]}