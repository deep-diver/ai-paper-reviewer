<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-01-22s on AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/</link><description>Recent content in 2025-01-22s on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 21 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/index.xml" rel="self" type="application/rss+xml"/><item><title>GPS as a Control Signal for Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12390/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12390/</guid><description>GPS-guided image generation is here! This paper leverages GPS data to create highly realistic images reflecting specific locations, even reconstructing 3D models from 2D photos.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12390/cover.png"/></item><item><title>Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12202/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12202/</guid><description>Hunyuan3D 2.0: A groundbreaking open-source system generating high-resolution, textured 3D assets using scalable diffusion models, exceeding state-of-the-art performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12202/cover.png"/></item><item><title>InternLM-XComposer2.5-Reward: A Simple Yet Effective Multi-Modal Reward Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12368/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12368/</guid><description>InternLM-XComposer2.5-Reward: A novel multi-modal reward model boosting Large Vision Language Model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12368/cover.png"/></item><item><title>MMVU: Measuring Expert-Level Multi-Discipline Video Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12380/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12380/</guid><description>MMVU: a new benchmark pushes multimodal video understanding to expert level, revealing limitations of current models and paving the way for more advanced AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12380/cover.png"/></item><item><title>TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12224/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12224/</guid><description>TokenVerse: Extract &amp;amp; combine visual concepts from multiple images for creative image generation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12224/cover.png"/></item><item><title>UI-TARS: Pioneering Automated GUI Interaction with Native Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12326/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12326/</guid><description>UI-TARS, a novel native GUI agent, achieves state-of-the-art performance by solely using screenshots as input, eliminating the need for complex agent frameworks and expert-designed workflows.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12326/cover.png"/></item><item><title>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12375/</link><pubDate>Tue, 21 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12375/</guid><description>Video Depth Anything achieves consistent depth estimation for super-long videos by enhancing Depth Anything V2 with a spatial-temporal head and a novel temporal consistency loss, setting a new state-o&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.12375/cover.png"/></item><item><title>Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11425/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11425/</guid><description>Agent-R: A novel self-training framework enables language model agents to learn from errors by dynamically constructing training data that corrects erroneous actions, resulting in significantly improv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11425/cover.png"/></item><item><title>Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11733/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11733/</guid><description>Mobile-Agent-E: A self-evolving mobile assistant conquering complex tasks with hierarchical agents and a novel self-evolution module, significantly outperforming prior approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11733/cover.png"/></item><item><title>Reasoning Language Models: A Blueprint</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11223/</link><pubDate>Mon, 20 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11223/</guid><description>Democratizing advanced reasoning in AI, this blueprint introduces a modular framework for building Reasoning Language Models (RLMs), simplifying development and enhancing accessibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.11223/cover.png"/></item><item><title>EMO2: End-Effector Guided Audio-Driven Avatar Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.10687/</link><pubDate>Sat, 18 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.10687/</guid><description>EMO2 achieves realistic audio-driven avatar video generation by employing a two-stage framework: first generating hand poses directly from audio and then using a diffusion model to synthesize full-bod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.10687/cover.png"/></item><item><title>MSTS: A Multimodal Safety Test Suite for Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.10057/</link><pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.10057/</guid><description>New multimodal safety test suite (MSTS) reveals vision-language models&amp;rsquo; vulnerabilities and underscores the unique challenges of multimodal inputs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-22/2501.10057/cover.png"/></item></channel></rss>