[{"figure_path": "https://arxiv.org/html/2501.12390/x4.png", "caption": "Figure 1: What can we do with a GPS-conditioned image generation model?\nWe train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. For example, a model trained on densely sampled geotagged photos from Manhattan can generate images that match a neighborhood\u2019s general appearance and capture key landmarks like museums and parks. We show images sampled from a variety of GPS locations and text prompts. For example, an image with the text prompt \u201cbagel\u201d results in a modern-style sculpture when conditioned on the Museum of Modern Art and an impressionist-style painting when conditioned on the Metropolitan Museum of Art. We also \u201clift\u201d a 3D NeRF of the Statue of Liberty from a landmark-specific 2D GPS-to-image model using score distillation sampling. Please see the project webpage and Sec.\u00a0A.1.1 for more examples.", "description": "This figure demonstrates the capabilities of a GPS-conditioned image generation model.  The model is trained on geotagged photos from Manhattan, learning to generate images representative of different locations and their characteristics, including landmarks and neighborhood styles.  Examples show how text prompts combined with GPS coordinates generate images consistent with both, illustrating compositional generation. For instance, the prompt \u201cbagel\u201d produces diverse results depending on location: a modern sculpture near MoMA and an impressionist painting near the Met.  The figure also highlights the model's ability to reconstruct 3D models from 2D images (a 3D NeRF of the Statue of Liberty) using score distillation sampling.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12390/x5.png", "caption": "Figure 2: Method. (a) After downloading geotagged photos, we train a GPS-to-image generation model conditioned on GPS tags and text prompts. The trained generative model can produce images using both conditioning signals in a compositional manner. (b) We can also extract 3D models from a landmark-specific GPS-to-image model using score distillation sampling. This diffusion model parameterizes the GPS location by the azimuth with respect to a given landmark\u2019s center. + means we concatenate GPS embeddings and text embeddings.", "description": "Figure 2 illustrates the methodology of the proposed GPS-based image and 3D model generation.  Panel (a) details the training process for a GPS-to-image diffusion model.  This model learns to generate images conditioned on both geographical location (GPS coordinates) and textual descriptions. The model's ability to combine these two inputs compositionally is highlighted. Panel (b) shows how 3D models of landmarks are extracted from a 2D GPS-conditioned image model. This is achieved through score distillation sampling, a technique that leverages the model's understanding of how a landmark appears from different viewpoints (parameterized by azimuth relative to the landmark's center) to reconstruct a 3D representation.  The '+' symbol indicates concatenation of GPS and text embeddings during model training.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2501.12390/x7.png", "caption": "Figure 3: 3D Setup Comparison. We extract 3D models from 2D GPS-to-image models. (a) Traditional approaches require running SfM to estimate camera pose, followed by dense geometry estimation. Since they are based on triangulation, they are susceptible to catastrophic errors due to incorrect pose; (b) DreamFusion\u00a0[64] samples images from different poses within a scene using view-dependent prompting. However, text has a limited ability to precisely control the position of the camera. (c) Our method extends DreamFusion with GPS conditioning, reducing pose uncertainty.", "description": "Figure 3 compares three different approaches for generating 3D models from 2D images.  Panel (a) illustrates the traditional Structure from Motion (SfM) pipeline, which involves estimating camera poses and then generating a dense 3D model.  This approach is prone to errors when camera poses are inaccurately estimated. Panel (b) shows DreamFusion's method, which uses text prompts to control viewpoint, but this method has limitations in precisely specifying camera positions. In contrast, Panel (c) presents the proposed method, which incorporates GPS information into DreamFusion to constrain the camera views, leading to more accurate and robust 3D reconstructions.", "section": "3. GPS-guided 3D reconstruction"}, {"figure_path": "https://arxiv.org/html/2501.12390/x9.png", "caption": "Figure 4: Qualitative results for Paris. We show images that have been sampled from our GPS-to-image diffusion model for various locations and prompts within Paris.", "description": "Figure 4 presents qualitative results from the GPS-to-image diffusion model.  It showcases images generated using various text prompts and their corresponding GPS locations within Paris. The goal is to demonstrate the model's ability to generate images that reflect both the specified text and the geographic location, capturing the visual characteristics of different Parisian neighborhoods and landmarks.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12390/x11.png", "caption": "Figure 5: Qualitative comparison for GPS-to-image diffusion. We compare the qualitative results of our method against baselines using specific pairs of text prompts and GPS tags. Each column shows a text prompt and a GPS tag at the top. Text-address-to-image diffusion model is conditioned on a combination of the text prompt and the address name derived from the GPS tag. We also perform nearest neighbor in the training set based on GPS tags. Our GPS-to-image diffusion model uses a text prompt and raw GPS tag as conditioning. Google Street View images are sampled for reference of geolocation. Our method achieves better compositionality and visual quality.", "description": "Figure 5 compares the image generation capabilities of four different methods: the proposed GPS-to-image diffusion model, a text-address-to-image diffusion model, a nearest neighbor approach, and Google Street View.  Each column presents a specific text prompt and GPS coordinates. The text-address method combines the text prompt with the address obtained from the GPS coordinates. The nearest neighbor method selects the most similar image from the training data based on GPS. The proposed model uses both the text prompt and the raw GPS coordinates. Google Street View provides a ground truth reference image for the same location.  The comparison demonstrates the superior compositional ability and visual quality of the proposed GPS-to-image diffusion model.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12390/x12.png", "caption": "Figure 6: Average images. We select five areas for Paris and New York City respectively. Using our GPS-to-image models, we obtain representative images of the concept of \u201cbuilding\u201d within these geographic regions to observe architectural styles. More examples can be found on project webpage for different locations and concepts.", "description": "This figure displays average images generated using GPS-to-image models.  Five areas were selected in each of Paris and New York City.  For each area, the model generated images of buildings, showcasing the architectural styles characteristic of those particular regions. The goal was to demonstrate the model's ability to capture location-specific variations in building styles across different neighborhoods.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12390/x13.png", "caption": "Figure 7: Qualitative comparison for 3D monument reconstruction. We show qualitative results of DreamFusion\u00a0[64] and our method on two monuments: 1) Leaning Tower of Pisa; 2) Arc de Triomphe. Our reconstructed 3D monuments have better visual quality and more accurate 3D structure. We use rendered depth to make the background of RGB rendering white. Please see Appendix\u00a0A.2.2 and project webpage for more examples.", "description": "Figure 7 compares the 3D reconstruction results of two methods: DreamFusion and the proposed GPS-guided approach.  The comparison focuses on two landmarks: the Leaning Tower of Pisa and the Arc de Triomphe.  The visualization includes RGB images and surface normal maps for both methods, highlighting differences in visual quality and 3D structural accuracy. The background is rendered white using depth information for better visualization.  Appendix A.2.2 and the project website offer additional examples.", "section": "4.5. Evaluation of 3D landmark reconstruction"}, {"figure_path": "https://arxiv.org/html/2501.12390/x14.png", "caption": "Figure 8: Ablation. We conducted ablation studies to analyze the effectiveness of different modules in our method for GPS-to-image generation and 3D landmark reconstruction.", "description": "This ablation study investigates the impact of various components within the proposed GPS-to-image generation and 3D landmark reconstruction model.  It specifically examines the contributions of different modules to the overall performance by systematically removing or altering them and measuring the resulting changes in key metrics.  This allows for a quantitative assessment of the importance of each component to image quality and 3D reconstruction accuracy, aiding in understanding the model's architecture and identifying potential areas for improvement.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12390/x15.png", "caption": "Figure 9: Attention visualization. We visualize attention maps for text and GPS tokens.", "description": "Figure 9 visualizes attention maps generated by the model, highlighting which parts of the input (text and GPS coordinates) influence the generation of different image regions.  For example, the attention map for the text prompt \"A tourist\" shows the model focusing on the tourist's features, while the GPS coordinates' attention map highlights the surrounding environment such as buildings in the scene, showcasing the model's ability to effectively integrate both text and location information in image generation.", "section": "4.6. Ablation Study"}]