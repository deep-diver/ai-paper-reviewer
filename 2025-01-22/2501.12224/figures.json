[{"figure_path": "https://arxiv.org/html/2501.12224/x2.png", "caption": "Figure 1: TokenVerse extracts distinct complex visual concepts from a set of concept images (top), and allows users to generate images that depict these concepts in novel versatile compositions (bottom row). Our framework independently processes each concept image, and learns to disentangle its concepts based solely on an accompanying caption, without any additional supervision or masks. This is achieved by learning a personalized representation for each token in the source caption. Our personalized text tokens, extracted from multiple images, are then flexibly incorporated into new text prompts (colored words) to generate novel creative images.", "description": "TokenVerse is a novel framework that enables multi-concept image generation with personalized attributes extracted from multiple source images.  The top row shows example source images, each containing various visual concepts (e.g., objects, attributes, poses, lighting).  TokenVerse independently processes each source image and its caption to learn a disentangled representation for each concept, without needing masks or other supervision.  It achieves this by learning a unique embedding for each word in the caption.  Then, these personalized text tokens (shown in color in the caption below the generated images) are combined flexibly to create new and diverse images (bottom row) that express combinations of learned concepts.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.12224/x3.png", "caption": "Figure 2: Directions in the global modulation space (\u2133\u2133\\mathcal{M}caligraphic_M) and our per-token modulation space (\u2133+superscript\u2133\\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT). \nGiven a generated image (top row), we modify it using text-driven directions in both \u2133\u2133\\mathcal{M}caligraphic_M and \u2133+superscript\u2133\\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT spaces.\n(a) Adding a direction to the vector that is used to modulate all the text and image tokens (i.e. a direction in the space \u2133\u2133\\mathcal{M}caligraphic_M) can be used to effectively modify desired concepts in the generated image. Yet, this often results in non-local changes that also affect other concepts in the generated image. (b)\u00a0Adding a direction only to the modulation vector of a specific text token, like \u201cdog\u201d or \u201cball\u201d (i.e. a direction in the space \u2133+superscript\u2133\\mathcal{M}^{+}caligraphic_M start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT) leads to a localized modification that mostly affects the concept of interest.", "description": "Figure 2 illustrates the effect of modifying the modulation vector in the global modulation space (\u2133) and the per-token modulation space (\u2133+).  The top row shows a generated image. Part (a) demonstrates modifying the global modulation vector to change concepts in the image.  This often causes unintended changes to other concepts. Part (b) shows how modifying the modulation vector for only a single token (e.g., \"dog\") results in localized changes that primarily affect the selected concept. This localized control is key to the proposed approach for multi-concept personalization.", "section": "The M+ space"}, {"figure_path": "https://arxiv.org/html/2501.12224/x4.png", "caption": "Figure 3: TokenVerse overview.\n(a) A pre-trained text-to-image DiT model processes both image and text tokens via a series of DiT blocks. Each block consists of modulation, attention and feed-forward modules. We focus on the modulation block, in which the tokens are modulated via a vector y\ud835\udc66yitalic_y, which is derived from a pooled text embedding. (b) Given a concept image and its corresponding caption, TokenVerse learns a personalized modulation vector offset \u0394\u0394\\Deltaroman_\u0394 for each text token. These offsets represent personalized directions in the modulation space and are learned using a simple reconstruction objective. (c) At inference, the pre-learned direction vectors are used to modulate the text tokens, enabling the injection of personalized concepts into the generated images.", "description": "This figure provides a detailed overview of the TokenVerse framework. Panel (a) illustrates the architecture of a pre-trained text-to-image diffusion transformer (DiT) model, highlighting the modulation, attention, and feed-forward modules within each DiT block.  The modulation block's function of modulating tokens using a vector derived from a pooled text embedding is emphasized. Panel (b) depicts the training process of TokenVerse. It shows how, given a concept image and its caption, personalized modulation vector offsets are learned for each text token.  These offsets represent unique directions within the modulation space, learned through a reconstruction objective.  Panel (c) demonstrates the inference stage. It explains how the pre-learned direction vectors are applied to modulate text tokens during image generation, enabling the incorporation of personalized concepts into the resulting images.", "section": "3. Preliminaries: Diffusion transformers"}, {"figure_path": "https://arxiv.org/html/2501.12224/x5.png", "caption": "Figure 4: Concept isolation loss. When training Concept-Mod we apply an additional concept isolation loss in 50% of the training steps. This loss encourages learning directions that do not interfere with other images by enforcing that the parts in the image that should not be affected by the directions remain similar.", "description": "This figure illustrates the concept isolation loss used in training the Concept-Mod model. During training, the model is presented with a concept image and its caption in 50% of the training steps.  In addition, the model is presented with a second image and its caption that is unrelated to the concept image. The goal is for the model to learn directions in the modulation space that only modify the portions of the generated image corresponding to the concept image, leaving the unrelated image mostly unaffected.  This is done by minimizing a loss function that encourages similarity between the portions of the generated image not relevant to the target concept image and its counterpart in the unrelated image.", "section": "Disentangled concept learning"}, {"figure_path": "https://arxiv.org/html/2501.12224/x6.png", "caption": "Figure 5: Qualitative results.\nEach row begins with a bank of four source images, from which our method independently extracts concepts. To the right, three generated images are shown, demonstrating the seamless combination of these concepts into new, coherent outputs.", "description": "This figure showcases qualitative results of the TokenVerse model. Each row presents four source images, each containing several distinct visual concepts. The model independently extracts these concepts from each source image.  The right side of the figure then displays three newly generated images. These generated images demonstrate the model's ability to seamlessly combine the extracted concepts into novel, coherent compositions, showcasing the versatility and effectiveness of TokenVerse in multi-concept personalization and image generation.", "section": "Qualitative results"}, {"figure_path": "https://arxiv.org/html/2501.12224/x7.png", "caption": "Figure 6: Extreme multi-concept personalization. Our method has no technical constraint on the number of concepts that can be combined in an image. As can be seen, TokenVerse can generate images composing a significant number of concepts.", "description": "Figure 6 showcases the capability of TokenVerse to handle extreme multi-concept personalization.  Unlike other methods, TokenVerse doesn't have limitations on the number of concepts that can be combined within a single generated image. The figure demonstrates this by showing an example image that successfully integrates many different concepts, highlighting the model's flexibility and power.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12224/x8.png", "caption": "Figure 7: Concepts beyond objects.\nWe demonstrate the composition of three types of personalized concepts: object (the bear; concept image not shown), pose (left column) and lighting (top row).\nTokenVerse successfully learns the pose and lighting without overfitting to the identity of the poser or the specific lit scene.", "description": "Figure 7 showcases TokenVerse's ability to personalize and combine various visual concepts beyond just objects.  Three concept types are demonstrated: object (a bear, though its source image isn't displayed), pose (shown in the left column), and lighting (top row). The results highlight that TokenVerse effectively learns distinct representations for pose and lighting without being overly influenced by the specific subject or lighting setup, a significant advancement in disentangled concept learning.", "section": "6.3 Concepts beyond objects"}, {"figure_path": "https://arxiv.org/html/2501.12224/x9.png", "caption": "Figure 8: Qualitative comparisons. Each row depicts two concept images (left) and images containing a combination of those concepts, generated by ConceptExpress\u00a0[14], BAS\u00a0[8], DreamBooth\u00a0[32], OMG\u00a0[23] and our method.\nThe concepts associated with the green and blue words are taken from the left and right concept images, respectively. As can be seen, our method best composes the two concepts while preserving concept fidelity.", "description": "Figure 8 presents a qualitative comparison of different methods for multi-concept image generation. Each row shows two source images (concept images) and several generated images created by different techniques: ConceptExpress, Break-a-Scene, DreamBooth, OMG, and the proposed method (TokenVerse). The generated images combine concepts extracted from the two source images.  The caption highlights that TokenVerse most effectively integrates the concepts while preserving their individual fidelity.", "section": "Qualitative comparison"}, {"figure_path": "https://arxiv.org/html/2501.12224/x10.png", "caption": "Figure 9: Quantitative comparison.\nWe compare our method to other baselines on concept preservation and prompt fidelity (higher is better) using DreamBench++ and a user study. (a) We compare three different settings: (i)\ud835\udc56(i)( italic_i ) composing two concepts from different images (concept composition), (i\u2062i)\ud835\udc56\ud835\udc56(ii)( italic_i italic_i ) decomposing two concepts from the same image (concept decomposition), and (i\u2062i\u2062i)\ud835\udc56\ud835\udc56\ud835\udc56(iii)( italic_i italic_i italic_i ) the combination of the two (full task). (b) We conduct a user study, comparing our method to existing methods on our full task.\nOur method consistently scores best in terms of concept preservation while maintaining high prompt fidelity scores. See\nApp.\u00a0C.2\nfor the exact metrics.", "description": "Figure 9 presents a quantitative comparison of the proposed TokenVerse method against other baselines across three tasks: concept composition (combining concepts from different images), concept decomposition (extracting concepts from a single image), and a full task combining both.  The comparison is based on two metrics: concept preservation and prompt fidelity, both evaluated using DreamBench++ and a user study.  TokenVerse consistently outperforms other methods in concept preservation while achieving high prompt fidelity, as detailed in Appendix C.2.", "section": "6.3 Experiments"}, {"figure_path": "https://arxiv.org/html/2501.12224/x11.png", "caption": "Figure 10: Ablations. The left pane shows all the concepts used to generate the result images. Columns (a) to (d) shows the results of our method as additional components are progressively integrated.", "description": "This ablation study demonstrates the effect of progressively adding components to the TokenVerse model. The leftmost part of the figure displays the source concepts used to generate the images.  Column (a) shows results without using the M+ space, applying direction vectors directly to input text tokens before entering the transformer. Column (b) incorporates the M+ space, adding the directions to modulation vectors of individual text tokens. Column (c) further refines the results by adding per-block directions. Finally, column (d) presents the full TokenVerse model, including the isolation loss to mitigate interference between concepts from different images. The progressive integration of these components highlights their individual contributions to the model's overall performance in generating images with multiple concepts from multiple sources.", "section": "6.4 Ablation study"}, {"figure_path": "https://arxiv.org/html/2501.12224/x12.png", "caption": "Figure 11: Limitations.\nConcept images are shown in the top row, with the generated images using TokenVerse below in each case. While our method supports both disentangled learning and multi-concept composition, limitations remain. (a) Rare blending can occur in specific combinations due to independent training of concepts; We provide analysis and mitigations in\nApp.\u00a0F.\n(b)\u00a0Challenges arise with concepts sharing the same name identifier, which can be mitigated by using distinct terms. (c) Certain incompatible combinations, such as a doll with tiny limbs in a complex pose, may result in undesired outputs.", "description": "Figure 11 demonstrates limitations of the TokenVerse model.  It shows examples where the model struggles with certain combinations of concepts, primarily due to the independent training process for each concept.  Panel (a) shows instances of rare blending between concepts, which can be addressed through joint training. Panel (b) illustrates difficulties when concepts share the same name, resolved by using distinct names.  Lastly, panel (c) shows that incompatible combinations (like a doll with tiny limbs in a complex pose) produce undesirable outputs.", "section": "6.5 Limitations"}, {"figure_path": "https://arxiv.org/html/2501.12224/x13.png", "caption": "Figure 12: Augmentations Ablation. The top row shows the concepts used to generate the result images. Column (a) displays the results of our full method, while column (b) shows the results without text and image augmentations.", "description": "This ablation study investigates the impact of data augmentations on the model's performance.  The top row displays the source concepts used to generate images. Column (a) presents the results obtained using the full method, which incorporates both text and image augmentations during training.  Column (b) shows the results when these augmentations are omitted, highlighting their contribution to improved image generation.", "section": "A. Additional training details"}, {"figure_path": "https://arxiv.org/html/2501.12224/x14.png", "caption": "Figure 13: Progressive composition of concepts. TokenVerse can be used to progressively add concepts into a generated image, while controlling all other aspects of the generated images via text. In each row, the object, pose, lighting, and hair are personalized, while the background is described by text (e.g. \u201cNY city\u201d, \u201cgarden\u201d, and \u201cMars\u201d for the top row.)", "description": "This figure demonstrates the progressive composition of concepts using TokenVerse.  Each row shows a sequence of images, starting with a base image and progressively adding new personalized concepts (object, pose, lighting, and hair) while keeping other aspects consistent through text prompts.  The background changes with each row, showcasing the flexibility of concept control through text input. For example, the top row shows the same subject in various poses and lighting conditions, set against backgrounds specified in the captions such as a city, garden, and mars. ", "section": "Qualitative results"}, {"figure_path": "https://arxiv.org/html/2501.12224/x15.png", "caption": "Figure 14: Limitation \u2013 highly similar modulated tokens.\n(a)\u00a0The common scenario of combining two distinct objects, such as a doll and a dog, into a single image. (b)\u00a0A failure case where independent training of concepts leads to the creation of hybrid objects. (c)\u00a0A potential mitigation for this issue by employing joint training on both concepts.", "description": "This figure illustrates a limitation of the TokenVerse model when highly similar modulation tokens are used.  Panel (a) shows a successful combination of a doll and a dog, representing a typical scenario where the model functions as expected. Panel (b) demonstrates a failure case where independent training leads to the generation of a hybrid object (characteristics of both the doll and the dog are merged). Panel (c) proposes a solution: employing joint training on both concepts during the model's training phase, which addresses the issue of generating hybrid objects by mitigating the similarity of modulation tokens. This demonstrates that training methodology affects the resulting images.", "section": "6.5 Limitations"}, {"figure_path": "https://arxiv.org/html/2501.12224/x16.png", "caption": "Figure 15: Limitations \u2013 colliding captions.\nOur method may fail when handling cases of colliding identifiers, such as two dolls (a). This issue can be easily resolved by assigning distinct identifiers to each object during the initial training (b).", "description": "Figure 15 demonstrates a limitation of the TokenVerse model: when two objects in different images share the same textual identifier (e.g., both are described as \"dolls\"), the model struggles to generate images with both objects distinct and correctly represented. This issue is easily fixed by using different textual identifiers for each object during the initial training phase. The image shows two examples: (a) colliding captions where the model produces an unsatisfactory result; and (b) distinct captions where the model generates a satisfactory result.", "section": "6.5. Limitations"}, {"figure_path": "https://arxiv.org/html/2501.12224/x17.png", "caption": "Figure 16: Qualitative results.\nEach row contains two result images and the source images of the concepts that they contain.", "description": "This figure showcases the qualitative results of the TokenVerse model. Each row presents a set of four source images that exemplify distinct visual concepts (such as a doll, a dog, a man in various poses, lighting conditions, etc.). These concepts are independently extracted by TokenVerse from their respective images and are then seamlessly combined to generate novel image compositions. For each concept image set, the figure displays two generated images demonstrating the flexible and versatile nature of TokenVerse in creating novel image compositions that incorporate multiple visual concepts.", "section": "6.2. Qualitative results"}, {"figure_path": "https://arxiv.org/html/2501.12224/x18.png", "caption": "Figure 17: Qualitative results.\nEach row contains two result images and the source images of the concepts that they contain.", "description": "This figure showcases qualitative results of the TokenVerse model. Each row presents four source images depicting various concepts (e.g., a doll in different poses, a dog with different accessories, a person in various settings, etc.).  To the right of each set of source images, are two images generated by the model. These generated images demonstrate the model's ability to combine and personalize the extracted concepts in novel ways. The generated images show diverse compositions and attributes of the concepts, highlighting the model's flexibility in handling multiple concepts and its capacity for seamless integration of personalized elements from various sources.", "section": "6.2. Qualitative results"}, {"figure_path": "https://arxiv.org/html/2501.12224/x19.png", "caption": "Figure 18: Qualitative results.\nEach row contains two result images and the source images of the concepts that they contain.", "description": "Figure 18 presents qualitative results showcasing TokenVerse's ability to generate images by combining concepts extracted from multiple source images. Each row displays four concept images followed by two generated images. The generated images demonstrate how TokenVerse successfully integrates the concepts from the source images into novel compositions.  This visually demonstrates the model's ability to disentangle and recombine visual attributes (objects, poses, lighting conditions, etc.) from different sources seamlessly.", "section": "6.2. Qualitative results"}, {"figure_path": "https://arxiv.org/html/2501.12224/x20.png", "caption": "Figure 19: Storytelling results.\nDemonstration of our method\u2019s usability for storytelling applications. All the characters, scenes, and poses featured in the story are shown on the left. On the right is the story itself, generated by a language model (LLM). This story was then reprocessed by the LLM to generate prompts, which were used to create the accompanying images.", "description": "This figure demonstrates the application of the TokenVerse method to storytelling.  The left side displays the characters, scenes, and poses used in the story. The story itself, created by a large language model (LLM), is shown on the right.  The LLM further processed the story to generate prompts for image generation using TokenVerse, resulting in the images shown in the figure.", "section": "6.3.3. User study"}, {"figure_path": "https://arxiv.org/html/2501.12224/x21.png", "caption": "Figure 20: An example of the questions asked in the user study. Given a generated image the users are asked about its alignment with both the text and the input concepts", "description": "This figure shows example questions from a user study conducted to evaluate the quality of generated images. Participants were shown a generated image and asked to rate how well it matched a given text description (prompt alignment) and how well it incorporated specific concepts from input images (concept preservation).  Concept preservation was assessed separately for different visual elements present in the image.", "section": "6.3. User study"}]