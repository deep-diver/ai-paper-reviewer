{"references": [{"fullname_first_author": "Shunyu Yao", "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models", "publication_date": "2023-11-01", "reason": "This paper introduces a novel method for improving the reasoning capabilities of LLMs, which is highly relevant to the paper's focus on enhancing LLM agents' decision-making abilities through self-reflection."}, {"fullname_first_author": "Levente Kocsis", "paper_title": "Bandit based monte-carlo planning", "publication_date": "2006-01-01", "reason": "This foundational paper on Monte Carlo Tree Search (MCTS) provides the theoretical basis for the algorithm used in Agent-R to construct and refine trajectories for self-improvement."}, {"fullname_first_author": "Aman Madaan", "paper_title": "Self-refine: Iterative refinement with self-feedback", "publication_date": "2023-11-01", "reason": "This paper explores self-correction in LLMs, a concept central to Agent-R's approach of enabling language agents to dynamically identify and address errors within their trajectories."}, {"fullname_first_author": "Minghao Chen", "paper_title": "Automanual: Generating instruction manuals by llm agents via interactive environmental learning", "publication_date": "2024-05-01", "reason": "This paper addresses interactive agent learning in environments, a key context for Agent-R, offering insights into the challenges and opportunities of training LLMs to interact and adapt in dynamic settings."}, {"fullname_first_author": "Zhiheng Xi", "paper_title": "AgentGym: Evolving large language model-based agents across diverse environments", "publication_date": "2024-06-01", "reason": "This work introduces AgentGym, a platform used for evaluating Agent-R's performance, making it a crucial reference in understanding the experimental setup and comparative analysis within the paper."}]}