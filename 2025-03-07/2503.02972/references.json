{"references": [{"fullname_first_author": "Kojima", "paper_title": "Large language models are zero-shot reasoners", "publication_date": "2022-01-01", "reason": "This paper highlights the zero-shot capabilities of LLMs, a concept heavily discussed in the current paper."}, {"fullname_first_author": "Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models", "publication_date": "2022-01-01", "reason": "This paper investigates chain-of-thought prompting to improve reasoning in LLMs which is later mentioned to not have improved performance on LINGOLY-TOO."}, {"fullname_first_author": "Brown", "paper_title": "Language models are few-shot learners", "publication_date": "2020-01-01", "reason": "This paper's finding on how language models can be few-shot learners has potential to improve model performance on LINGOLY-TOO which requires some form of training."}, {"fullname_first_author": "Mirzadeh", "paper_title": "GSM-Symbolic: Understanding the limitations of mathematical reasoning in large language models", "publication_date": "2024-01-01", "reason": "This paper provides the basis that LLMs do not reason perfectly and shows evidence of memorization, which is what the current paper tries to fix."}, {"fullname_first_author": "McCoy", "paper_title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "publication_date": "2019-01-01", "reason": "This paper discusses how models can be 'right for the wrong reasons', which is the issue being investigated in the current paper."}]}