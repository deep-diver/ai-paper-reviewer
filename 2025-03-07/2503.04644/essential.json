{"importance": "**IFIR** offers a new, specialized benchmark for instruction-following IR that highlights challenges in current models & informs future retriever development. Its detailed analysis and novel evaluation method could greatly improve search tech in expert domains.", "summary": "IFIR: a new benchmark for instruction-following retrieval in expert domains, revealing current model limitations.", "takeaways": ["Current models struggle to effectively follow complex, domain-specific instructions.", "LLM-based retrievers show promise in handling complex retrieval tasks.", "A new LLM-based evaluation metric, INSTFOL, provides a more precise assessment of instruction-following performance."], "tldr": "The study introduces **IFIR**, a benchmark for evaluating instruction-following in specialized domains like finance, law, healthcare & science, where customized directions are critical. IFIR addresses limitations in existing models by incorporating instructions at varying complexity levels. A new LLM-based eval method is also proposed to assess model performance, addressing shortcomings of traditional methods.\n\nExperiments with 15 information retrievers reveal significant challenges in effectively following domain-specific instructions. The paper highlights limitations of current models & potential of LLM-based retrievers. It introduces **INSTFOL**, a more reliable evaluation method and provides in-depth analyses to help future retriever development by offering insights to improve IR systems.", "affiliation": "School of Advanced Interdisciplinary Sciences, University of Chinese Academy of Sciences", "categories": {"main_category": "Natural Language Processing", "sub_category": "Information Extraction"}, "podcast_path": "2503.04644/podcast.wav"}