{"importance": "This paper introduces EAST, a novel weighting method for self-training that enhances reasoning capabilities by emphasizing uncertain data. **This is important for researchers because it offers a new strategy to improve LLMs without manual effort.** It addresses a critical challenge in self-training and opens avenues for exploring adaptive weighting.", "summary": "EAST: Prioritizing uncertainty in self-training refines reasoning of Large Language Models.", "takeaways": ["Entropy-Based Weighting leverages uncertainty information to improve self-training.", "The proposed mapping function controls the extent to which uncertain data is weighted.", "EAST further boosts self-training performance compared to the vanilla method."], "tldr": "Large Language Models(LLMs) leverage self-generated reasoning paths to refine their mathematical problem-solving. **Self-training uses this data, but treats all examples equally, missing the varying value of each data point and limiting learning effectiveness.** This paper asks if reweighting training data could improve reasoning and which data should be prioritized.\n\nTo address this, the paper introduces **Entropy-Based Adaptive Weighting for Self-Training (EAST), which adaptively weights data based on the model's uncertainty.** It uses a mapping function with a tunable parameter to prioritize uncertain data, guiding the model to focus on challenging examples. Evaluated on benchmarks like GSM8K and MATH, EAST shows performance gains over vanilla methods, highlighting its effectiveness.", "affiliation": "University of California Los Angeles", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.23913/podcast.wav"}