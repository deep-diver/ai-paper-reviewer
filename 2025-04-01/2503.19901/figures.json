[{"figure_path": "https://arxiv.org/html/2503.19901/x2.png", "caption": "Figure 1: Introducing TokenHSI, a unified model that enables physics-based characters to perform diverse human-scene interaction tasks. It excels at seamlessly unifying multiple foundational HSI skills within a single transformer network and flexibly adapting learned skills to challenging new tasks, including skill composition, object/terrain shape variation, and long-horizon task completion.", "description": "TokenHSI is a unified model that allows physics-based characters to perform a wide variety of human-scene interaction (HSI) tasks.  The model uses task tokenization to integrate multiple foundational HSI skills into a single transformer network.  This design enables seamless integration and flexible adaptation of learned skills to novel and complex tasks including skill composition (combining multiple skills), handling variations in object and terrain shapes, and completing long-horizon tasks (tasks requiring a sequence of actions over an extended period).", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.19901/x3.png", "caption": "Figure 2: TokenHSI consists of two stages: (left) foundational skill learning and (right) policy adaptation. Through multi-task policy training, the proposed framework learns versatile interaction skills in a single transformer network. Theses learned skills can be flexibly adapted to more challenging HSI tasks by training the lightweight modules, e.g., \ud835\udd4bn\u2062e\u2062wsuperscript\ud835\udd4b\ud835\udc5b\ud835\udc52\ud835\udc64\\mathbb{T}^{new}blackboard_T start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT, \ud835\udd4bcsuperscript\ud835\udd4b\ud835\udc50\\mathbb{T}^{c}blackboard_T start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT, and \u03be\ud835\udd38={\u03be0\ud835\udd38,\u03be1\ud835\udd38}superscript\ud835\udf09\ud835\udd38subscriptsuperscript\ud835\udf09\ud835\udd380subscriptsuperscript\ud835\udf09\ud835\udd381\\xi^{\\mathbb{A}}=\\{\\xi^{\\mathbb{A}}_{0},\\xi^{\\mathbb{A}}_{1}\\}italic_\u03be start_POSTSUPERSCRIPT blackboard_A end_POSTSUPERSCRIPT = { italic_\u03be start_POSTSUPERSCRIPT blackboard_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_\u03be start_POSTSUPERSCRIPT blackboard_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT }.", "description": "TokenHSI's architecture is a two-stage process.  The first stage focuses on foundational skill learning where a unified transformer network learns diverse human-scene interaction (HSI) skills through multi-task training.  The second stage involves policy adaptation where these learned skills are flexibly adapted to handle more challenging HSI tasks. This adaptation is efficient as it only requires training lightweight components: new task tokenizers (for example,  Tnew),  and adapter layers (\u03beA) which are added to the action head. The existing tokenizer for proprioception (Tprop) and encoder (\u03d5) are reused and kept frozen.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.19901/", "caption": "Figure 3: Learning curves comparing the efficiency on skill composition tasks using TokenHSI, policies trained from scratch\u00a0[79], CML\u00a0[108], and its improved version CML (dual). Colored regions denote mean values \u00b1plus-or-minus\\pm\u00b1 a standard deviation based on 3333 models initialized with different random seeds.", "description": "Figure 3 presents a comparison of training efficiency across different methods for skill composition tasks.  The learning curves show the success rate (or a related metric reflecting task performance) over the number of training iterations. Four methods are compared: TokenHSI, a model trained from scratch (Scratch), the original CML method, and an improved version of CML (CML Dual).  The shaded regions around each curve indicate the standard deviation of results across three separate runs, each initialized with different random seeds, providing a measure of variability and confidence in the results. This helps to illustrate how quickly each method learns to perform the skill composition tasks and the relative stability of each approach's performance.", "section": "4.2.1 Skill Composition"}, {"figure_path": "https://arxiv.org/html/2503.19901/x5.png", "caption": "Figure 4: Through policy adaptation, TokenHSI can generalize learned foundational skills to more challenging scene interaction tasks.", "description": "This figure showcases the adaptability of the TokenHSI model.  It demonstrates how foundational skills (learned in a simpler setting) are successfully generalized to more complex and diverse human-scene interaction tasks through the process of policy adaptation.  The six subfigures illustrate examples of skill composition (combining multiple skills), object shape variation (adapting to differently-shaped objects), terrain shape variation (navigating uneven terrain), and long-horizon task completion (executing a sequence of tasks). Each subfigure visually presents a physics-based character performing the specified challenging interaction successfully.", "section": "3.4 Transformer-based Policy Adaptation"}, {"figure_path": "https://arxiv.org/html/2503.19901/x6.png", "caption": "Figure 5: Learning curves comparing the efficiency on object shape variation tasks using TokenHSI, full fine-tuning of pre-trained policies, and AdaptNet\u00a0[109].", "description": "Figure 5 presents a comparison of training efficiency across three different methods for adapting a pre-trained box-carrying policy to handle novel object shapes (chairs and tables).  The learning curves illustrate how quickly each method achieves high success rates.  The methods compared are: TokenHSI (the proposed method), full fine-tuning of the pre-trained policy (a common approach which requires retraining the entire model), and AdaptNet [109] (a state-of-the-art policy adaptation method). The x-axis represents the number of training iterations, and the y-axis represents the success rate. The figure demonstrates TokenHSI's superior efficiency in adapting to new object shapes compared to the other two methods.", "section": "4.2.2. Object Shape Variation"}, {"figure_path": "https://arxiv.org/html/2503.19901/x7.png", "caption": "Figure 6: Learning curves comparing the efficiency on terrain shape variation tasks using TokenHSI, Scratch\u00a0[79], and AdaptNet\u00a0[109]. We ablate the adapter layers during training.", "description": "Figure 6 presents learning curves that compare the training efficiency of three different methods for adapting a physics-based character controller to handle terrain with varying shapes.  The three methods are TokenHSI, Scratch [79] (a method that trains a new policy from scratch), and AdaptNet [109] (a method that incrementally adapts a pre-trained policy). The curves show the success rate achieved by each method over a range of training iterations.  Importantly, the figure also includes results for TokenHSI where the adapter layers (a component designed to improve efficiency of adaptation) have been removed, demonstrating their contribution to the model's overall performance.", "section": "4.2.3. Terrain Shape Variation"}, {"figure_path": "https://arxiv.org/html/2503.19901/x8.png", "caption": "Figure 7: Long-horizon task completion by sequentially executing (a) pre-trained skills and (b) adapted skills by our approach.", "description": "This figure demonstrates the capability of the TokenHSI model to perform a complex, long-horizon task involving multiple skills.  Subfigure (a) shows the execution of the task using only pre-trained skills, highlighting potential limitations due to the lack of adaptation to the specific environment and task complexities. Subfigure (b) shows the improved performance after policy adaptation, where TokenHSI successfully executes the task sequentially and seamlessly transitions between skills.  This showcases the model's ability to adapt to novel scenarios and handle complex tasks that involve integrating multiple skills in a dynamic and coordinated manner.", "section": "4.2.4. Long-horizon Task Completion"}, {"figure_path": "https://arxiv.org/html/2503.19901/x9.png", "caption": "Figure A: Different simulated character models. Building on (a) AMP\u2019s model, we devise two improved versions: (b) and (c), which are used for tasks on flat ground and tasks on stairs terrain, respectively.", "description": "This figure showcases three variations of a simulated character model used in the paper's experiments.  Model (a) is based on the AMP model.  The authors improved upon this design in two ways, resulting in models (b) and (c). Model (b) is optimized for tasks performed on flat ground, while model (c) is specifically designed for tasks involving stairs or uneven terrain, reflecting differences in foot geometry and collision detection capabilities necessary for realistic interaction with stairs.", "section": "A. Simulated Character"}, {"figure_path": "https://arxiv.org/html/2503.19901/x10.png", "caption": "Figure B: Learning curves comparing the efficiency on long-horizon task completion using TokenHSI, Scratch\u00a0[79], and iterative fine-tuning of multiple pre-trained specialist policies, namely Finetune.", "description": "Figure B shows the learning curves for a long-horizon task, comparing three different approaches: TokenHSI, Scratch [79], and Finetune.  The x-axis represents the number of training iterations, while the y-axis indicates the number of completed sub-tasks within the long-horizon task.  The plot illustrates the training efficiency of each method. TokenHSI demonstrates faster convergence and achieves a higher number of completed subtasks compared to the other two approaches. This highlights the effectiveness of TokenHSI in learning complex, multi-stage tasks efficiently.", "section": "D. Quantitative Evaluation on Long-horizon Task Completion"}]