{"importance": "This work introduces KOFFVQA, a novel benchmark addressing the need for **objective VQA evaluation** in Korean. By providing detailed grading criteria, it significantly **improves evaluation reliability**. Also, it offers essential data and insights for **developing more effective VLMs** in Korean.", "summary": "KOFFVQA: Objectively evaluates Korean VLMs with a new free-form VQA benchmark, improving evaluation reliability via detailed grading criteria.", "takeaways": ["KOFFVQA, a new Korean VQA benchmark, enables objective evaluation via detailed grading criteria.", "Using pre-defined criteria improves evaluation consistency compared to comparing with a baseline.", "Direct visual input to VLM judges can reduce evaluation accuracy due to hallucinations."], "tldr": "The paper addresses the challenge of reliably evaluating Large Vision-Language Models(VLMs), particularly in languages other than English. Existing benchmarks often force models to select from pre-defined answers, limiting their open-endedness, or rely on subjective judge models, which can be unreliable. Additionally, there's a lack of benchmarks for Korean VLMs, critical because language models perform differently across languages. To solve this, the paper aims to provide an improved method by focusing on objectively evaluating such open-ended responses in VLM. \n\nThe authors introduce **KOFFVQA**, a new free-form visual question answering benchmark in Korean. It contains carefully designed questions paired with images and grading criteria. An LLM judge scores responses based on these criteria, enhancing reliability. The evaluation is designed in an objective manner, so that even small open-source models can evaluate other models. Experiments show this method is more reliable than existing approaches. By releasing KOFFVQA and its evaluation code, the researchers enhance VLM assessment in Korean, promoting more accurate and reliable model development.", "affiliation": "MAUM AI Inc.", "categories": {"main_category": "Multimodal Learning", "sub_category": "Vision-Language Models"}, "podcast_path": "2503.23730/podcast.wav"}