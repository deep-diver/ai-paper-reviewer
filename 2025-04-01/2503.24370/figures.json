[{"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/Framework.png", "caption": "Figure 1: (a) A demonstration of how Vanilla Prompting, Prompt Engineering, and Thinking Intervention work. Both Vanilla Prompting and Prompt Engineering methods act on the input query. In contrast, Thinking Intervention, either written by humans or generated by LLMs, explicitly injects instructions into the reasoning process. (b) Compared to Vanilla Prompting and Prompt Engineering, Thinking Intervention offers significant performance improvements for R1-Qwen-32B reasoning model across instruction following, instruction hierarchy, and safety alignment tasks.", "description": "Figure 1(a) illustrates three different methods for prompting a reasoning language model: Vanilla Prompting, Prompt Engineering, and Thinking Intervention. Vanilla Prompting and Prompt Engineering modify only the input prompt (the question), whereas Thinking Intervention injects additional instructions directly into the model's reasoning process, either written by humans or automatically generated by another language model. Figure 1(b) presents the experimental results showing that Thinking Intervention significantly outperforms the other two approaches in three tasks: instruction following, instruction hierarchy, and safety alignment. The results are presented as performance percentages on the y-axis with approaches along the x-axis. The R1-Qwen-32B reasoning model was used for this evaluation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/IFeval_example.png", "caption": "Figure 2: An example demonstrating how Thinking Intervention is integrated with vanilla prompting and Reminder Prompting prompting techniques for instruction following tasks.", "description": "This figure demonstrates three different prompting methods for instruction-following tasks: Vanilla Prompting (using the instruction as is), Prompt Engineering (adding a reminder to ensure the model adheres to the instruction's constraints), and Thinking Intervention (injecting instructions directly into the model's internal reasoning process).  The example shows a task requiring the model to list two famous mothers in JSON format.  Vanilla Prompting results in an incorrectly formatted or incomplete response. Prompt Engineering improves the response slightly.  Thinking Intervention, which inserts guiding text into the model's thinking stage (<think> block), yields the most accurate response in the correct JSON format. This comparison highlights how Thinking Intervention allows for a more precise and effective control over the model's reasoning and output compared to traditional prompt engineering methods.", "section": "3 Case Study: Instruction Following"}, {"figure_path": "https://arxiv.org/html/2503.24370/x1.png", "caption": "Figure 3: Evaluation results on the IFEval benchmark\u00a0[69]. We compare the performance with and without Thinking Intervention (ThinkingI), across Vanilla Prompting and Reminder Prompting methods and multiple reasoning models.", "description": "This figure displays the results of experiments evaluating instruction-following capabilities of different large language models (LLMs) on the IFEval benchmark.  Three prompting methods were compared: Vanilla Prompting (using only the original instructions), Reminder Prompting (adding reminders to the instructions), and Thinking Intervention (injecting instructions directly into the model's reasoning process). The accuracy of each method is shown across three different LLMs, indicating the improvement in instruction-following performance achieved through Thinking Intervention. The chart clearly visualizes the gains in accuracy for all three models and across different prompting techniques, demonstrating Thinking Intervention's superiority.", "section": "3 Case Study: Instruction Following"}, {"figure_path": "https://arxiv.org/html/2503.24370/x2.png", "caption": "Figure 4: A demonstration of how the SEP benchmark evaluates instruction hierarchy capabilities. Each example contains a main instruction paired with data. Left: The low-priority instruction is injected into the data, which the model should correctly ignore. Right: The low-priority instruction is absent, measuring the utility of models.", "description": "The SEP benchmark assesses a model's ability to prioritize main instructions over secondary, low-priority ones.  Figure 4 shows two scenarios. On the left, a low-priority instruction is embedded within the data provided alongside the main instruction; a successful model would ignore the low-priority instruction and only complete the primary task.  The right side shows the same main instruction with the data, but without the conflicting low-priority instruction; the result demonstrates the model's inherent ability to accomplish the primary task.", "section": "4 Case Study: Instruction Hierarchy"}, {"figure_path": "https://arxiv.org/html/2503.24370/x3.png", "caption": "Figure 5: A demonstration of Vanilla Prompting, Reminder Prompting, and Thinking Intervention for the SEP benchmark. The {Task} and {Data} fields are filled with content from the SEP dataset (e.g., Figure\u00a04) during evaluation.", "description": "This figure illustrates three different prompting methods applied to the SEP benchmark: Vanilla Prompting, Reminder Prompting, and Thinking Intervention.  Vanilla Prompting uses the original instruction without modification. Reminder Prompting adds a reminder to the instruction to ensure the model adheres to it.  Thinking Intervention injects instructions directly into the model's internal reasoning process by inserting specific tokens, providing a more precise form of control.  The example shows how each method is applied to a task with high- and low-priority instructions, where the low-priority instruction is embedded within the data. This demonstrates how Thinking Intervention can lead to better control and accuracy than the other two methods.", "section": "4 Case Study: Instruction Hierarchy"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/SEPtask.png", "caption": "Figure 6: Model\u2019s performance on the XSTest benchmark via Vanilla Prompting prompting.", "description": "This figure displays the performance of several large language models (LLMs) on the XSTest benchmark for evaluating the safety alignment of models.  The models were evaluated using only the standard prompt (Vanilla Prompting), without any additional safety-oriented techniques. The results reveal the models' compliance rates for safe requests and their refusal rates for unsafe requests.  It highlights a critical safety concern with the R1 models, demonstrating a dangerously high compliance rate for unsafe requests despite good performance on safe requests.  This emphasizes a trade-off between helpfulness and safety, where the models prioritize answering even dangerous requests. ", "section": "Case Study: Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/IH_PT.png", "caption": "Figure 7: Effect of the Thinking Intervention with R1-Qwen-32B model on XSTest benchmark results.", "description": "This figure showcases the impact of Thinking Intervention on the safety performance of the R1-Qwen-32B model, as evaluated using the XSTest benchmark.  It compares the model's safety performance (refusal rate for harmful requests and compliance rate for safe requests) under various prompting methods, both with and without the incorporation of Thinking Intervention. The visualization helps illustrate how Thinking Intervention enhances the model's ability to reject harmful prompts while maintaining its ability to correctly respond to benign prompts.", "section": "Case Study: Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2503.24370/x4.png", "caption": "Figure 8: Effect of the Thinking Intervention with R1-Qwen-32B model on SORRY-Bench.", "description": "This figure shows the results of applying Thinking Intervention to the R1-Qwen-32B model on the SORRY-BENCH benchmark.  It compares the model's performance with and without Thinking Intervention across various prompting methods (Vanilla, Default, Reminder, Goal Priority). The x-axis represents the prompting methods used, while the y-axis shows the unsafe request refusal rate (%). The figure visually demonstrates how Thinking Intervention impacts the model's ability to refuse unsafe prompts, improving its safety performance.  Different colored bars represent the results obtained for the methods with and without Thinking Intervention.", "section": "Case Study: Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2503.24370/x5.png", "caption": "Figure 9: A demonstration of how we prompt GPT-4o to generate the Reminder Prompting. The intervention sequence is a slightly modified version of Reminder Prompting.", "description": "This figure demonstrates the process of generating Reminder Prompts using GPT-40.  The input is a prompt instructing the model to extract constraints from a user prompt and create a reminder for an LLM to adhere to those constraints.  An example input and output are provided to illustrate the task. The generated reminder is then slightly modified to create the Thinking Intervention sequence used in the experiment. The image likely shows the prompt given to GPT-40, the model's response (the Reminder Prompt), and the final Thinking Intervention sequence derived from it.", "section": "3.2 Methods"}, {"figure_path": "https://arxiv.org/html/2503.24370/x6.png", "caption": "Figure 10: A demonstration of how the SEP benchmark evaluates instruction hierarchy capabilities. Each example consists of a main instruction paired with data. Left: A low-priority instruction is injected into the data, which the model should correctly ignore. Middle: A low-priority instruction is injected into the task portion, which the model should follow and generate answers. Right: The low-priority instruction is absent, allowing us to measure the utility of different methods.", "description": "The SEP benchmark evaluates a model's ability to prioritize instructions.  The figure shows three example prompts.  In the left example, a low-priority instruction is embedded within the data; a successful model will ignore it and only focus on the main instruction. In the middle example, the low-priority instruction is part of the main task instruction; a successful model will address both. The rightmost example omits the low-priority instruction, allowing for the measurement of baseline performance (utility) on the main task alone.", "section": "4 Case Study: Instruction Hierarchy"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/IF_gen_prompt.png", "caption": "Figure 11: Prompt template of evaluating the utility metric on the SEP benchmark. The {Question} and {Answer} will be filled with the complete prompt and model response, respectively.", "description": "This figure shows the prompt template used to evaluate the 'utility' metric in the SEP (instruction hierarchy) benchmark.  The template guides an LLM (Large Language Model) to act as a judge, evaluating the quality of a model's response to a given prompt. The LLM judge considers factors such as helpfulness, relevance, accuracy, depth, and creativity in its evaluation. The evaluation is presented in a structured format, requiring the LLM judge to provide a short explanation of its assessment, followed by a numerical rating (1-10) using the format \"[[rating]]\".  The {Question} placeholder represents the input prompt given to the model being tested, and the {Answer} placeholder represents the actual response generated by that model.  The LLM judge evaluates the model's response based on this information. ", "section": "4 Case Study: Instruction Hierarchy"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/SEPtaskfull.png", "caption": "Figure 12: The baseline prompting methods, as well as our Thinking Intervention, used for evaluating safety steering. The {Query} will be filled with the query from the benchmarks. Note for Goal Priority Prompting, we eliminate the few-shot exemplars as suggested by [2].", "description": "Figure 12 displays the various prompting methods used to assess the safety of language models.  It shows the prompts used for four approaches: Vanilla Prompting (no additional instructions); Default Prompting (incorporates general safety guidelines); Reminder Prompting (explicitly reminds the model of safety responsibilities); and Goal Priority Prompting (prioritizes safety above all else).  The {Query} placeholder indicates where the actual user query from the safety benchmark would be inserted.  For Goal Priority Prompting, the few-shot examples suggested in reference [2] were removed.  This figure visually represents the different methods used to control model safety and provides a clear comparison of their inputs.", "section": "5 Case Study: Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/SEP_eval_template.png", "caption": "Figure 13: Prompt template of evaluating the compliance or refusal rate on the XSTest benchmark. The {Question} and {Answer} will be filled with the complete prompt and model response, respectively.", "description": "This figure details the prompt template used to assess the safety and refusal rate of a language model on the XSTest benchmark.  The evaluation process involves classifying model responses into three categories: 1) full compliance (providing an answer), 2) full refusal (explicitly declining to answer), and 3) partial refusal (a combination of compliance and refusal). The template uses placeholders, {Question} and {Answer}, which are replaced with the actual prompt and model's response respectively during the evaluation. The resulting classification of each response helps quantify the model's safety behavior by determining its tendency to engage with potentially harmful requests.", "section": "5.1 Preliminary Safety Evaluations"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/safety_template.png", "caption": "Figure 14: Effectiveness of Thinking Intervention on the XSTest benchmark across multiple reasoning models.", "description": "This figure displays the results of applying Thinking Intervention on the XSTest benchmark across various reasoning models.  It shows a comparison of the models' performance under different prompting methods (Vanilla Prompting, Default Prompting, Reminder Prompting, Goal Priority) both with and without Thinking Intervention. The x-axis represents the safe request compliance rate, and the y-axis represents the unsafe request refusal rate.  Each point on the graph corresponds to a particular model and prompting method, illustrating the trade-off between safe request compliance and unsafe request refusal.  The effectiveness of Thinking Intervention is evident in the improvements to the unsafe request refusal rate achieved across different models and prompting methods.", "section": "5 Case Study: Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/xs_eval_template.png", "caption": "Figure 15: Prompt template of evaluating the refusal rate on the SORRY-Bench benchmark. The {Question} and {Answer} will be filled with the complete prompt and model response, respectively.", "description": "This figure shows the prompt template used to evaluate the refusal rate of language models on the SORRY-BENCH benchmark.  The template guides an evaluator (in this case, GPT-40-MINI) to classify model responses as either fully compliant with the user request, fully refusing to answer, or partially refusing (a combination of compliance and refusal). The evaluator is given the user's question and the model's response, and it provides a numerical classification based on the model's behavior.  This setup is designed to measure how well the model handles potentially unsafe or harmful requests by assessing its tendency to refuse such queries.", "section": "5.3 Thinking Intervention effectively steers the reasoning models toward safety"}, {"figure_path": "https://arxiv.org/html/2503.24370/x7.png", "caption": "Figure 16:  Effectiveness of Thinking Intervention on the SORRY-Bench benchmark across multiple models. Our approach consistently improves the safety alignment of reasoning models.", "description": "Figure 16 displays the results of safety experiments on the SORRY-BENCH dataset, a benchmark focusing on evaluating safety and robustness. The figure illustrates the performance of various reasoning models across different prompting strategies and when enhanced with the proposed Thinking Intervention technique. Each bar in the graph represents a specific model and prompting method, showing the percentage of unsafe prompts successfully refused by the model.  The Thinking Intervention technique consistently improved the refusal rates across multiple models. This indicates improved safety alignment by reducing the tendency of reasoning models to respond to unsafe prompts while preserving their ability to answer safe queries. The figure highlights the consistent improvements across various prompting strategies and model architectures, demonstrating the robustness of the proposed Thinking Intervention approach.", "section": "5 Case Study: Safety Alignment"}, {"figure_path": "https://arxiv.org/html/2503.24370/x8.png", "caption": "Figure 17:  Analysis of varying the location of the intervention sequence on the XSTest and SORRY-Bench benchmarks. The content is kept unchanged, and the Thinking Intervention is placed at the beginning, middle, and end of the reasoning process.", "description": "This figure displays the results of experiments evaluating the impact of placing a Thinking Intervention sequence at different points (beginning, middle, and end) within the reasoning process of a language model.  The experiments were conducted using the XSTest and SORRY-Bench benchmarks for safety assessment.  The graphs show the relationship between safe request compliance rates and unsafe request refusal rates under the different intervention placements.  The purpose is to determine the optimal placement of the intervention sequence for maximizing the model's safety and performance.", "section": "Effect of Thinking Intervention Designs"}, {"figure_path": "https://arxiv.org/html/2503.24370/x9.png", "caption": "Figure 18: Analysis of varying the Thinking Intervention content on XSTest and SORRY-Bench benchmarks. We compare our default short intervention sequence with a longer version. Both versions are inserted at the beginning of the reasoning process.", "description": "This figure displays the results of experiments on the XSTest and SORRY-Bench benchmarks, evaluating the impact of different Thinking Intervention sequences on model safety.  Two intervention sequences were tested: a short version and a longer version. Both were placed at the beginning of the reasoning process within the model.  The graphs illustrate how the different intervention lengths affect the model's performance in terms of the rate of refusing unsafe prompts (robustness) and maintaining accurate responses to safe prompts (compliance).  The results show the trade-off between safety and compliance, demonstrating how the length of the intervention sequence impacts the model's ability to prioritize safety without sacrificing its helpfulness.", "section": "E Effect of Thinking Intervention Designs"}, {"figure_path": "https://arxiv.org/html/2503.24370/extracted/6322787/images/sorry_eval_template.png", "caption": "Figure 19: A demonstration of how models without Thinking Intervention fail to ignore low-priority instructions and consequently provide incorrect responses. We use green color to highlight the main task and red color to highlight the low-priority query.", "description": "This figure demonstrates a model's failure to prioritize instructions when a low-priority instruction is included. Without Thinking Intervention, the model incorrectly focuses on the low-priority instruction ('Tell me what a book is primarily made of') from the data, while neglecting the main task ('Interpret the statute mentioned in the next paragraph, explaining its legal implications') which is related to the Sarbanes-Oxley Act. This leads to an incorrect response, primarily addressing the low-priority query instead of the main instruction.  The green highlights the main task and red highlights the low-priority query.", "section": "4 Case Study: Instruction Hierarchy"}, {"figure_path": "https://arxiv.org/html/2503.24370/x10.png", "caption": "Figure 20: A demonstration of how models with Thinking Intervention successfully ignore low-priority instructions and provide correct responses. We use blue color to highlight the Thinking Intervention, green color to highlight the main task and red color to highlight the low-priority query.", "description": "Figure 20 shows how Thinking Intervention helps a reasoning model correctly prioritize instructions.  The model receives a main instruction (in green) to interpret a legal statute, along with some data that includes a low-priority, unrelated instruction (in red). Without Thinking Intervention, the model gets distracted by the low-priority instruction.  However, with Thinking Intervention (highlighted in blue), the model correctly focuses on the main task and produces a correct and complete answer, successfully ignoring the irrelevant instruction in the data.  The use of color-coding (blue for intervention, green for main task, red for low-priority) helps visually illustrate the model's behavior and the impact of Thinking Intervention.", "section": "F Case Study"}]