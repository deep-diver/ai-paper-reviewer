[{"figure_path": "https://arxiv.org/html/2503.24391/x1.png", "caption": "Figure 1: \nWe present Easi3R, a training-free, plug-and-play approach that efficiently disentangles object and camera motion, enabling the adaptation of DUSt3R for 4D reconstruction.", "description": "Easi3R is a novel method for 4D reconstruction that works without any training.  It takes advantage of the attention mechanisms within the pre-trained DUSt3R model to separate camera motion from object motion.  This disentanglement allows Easi3R to adapt DUSt3R to dynamic scenes, generating accurate 4D point clouds despite the presence of moving objects.  The figure shows a visual representation of this process, illustrating how Easi3R processes video frames to produce a 4D reconstruction by distinguishing between camera and object movements.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.24391/x2.png", "caption": "Figure 2: DUSt3R with Dynamic Video.\nWe process videos using a sliding window and infer the DUSt3R network pairwise. Reconstruction degrades with misalignment when dynamic objects occupy a considerable portion of the frames.", "description": "The figure illustrates how DUSt3R, a method for 3D reconstruction, handles video data.  DUSt3R processes image pairs through a sliding window approach.  However, when moving objects are present, the accuracy of the reconstruction decreases because the relative motion between objects and the camera violates assumptions inherent to the method. The resulting reconstruction becomes misaligned and inaccurate in areas where substantial portions of the frame are occupied by moving objects.", "section": "3.1. DUSt3R with Dynamic Video"}, {"figure_path": "https://arxiv.org/html/2503.24391/extracted/6321542/figures/fig_pipe.png", "caption": "Figure 3: DUSt3R and our Easi3R adaptation. DUSt3R encodes two images Ia,Ibsuperscript\ud835\udc3c\ud835\udc4esuperscript\ud835\udc3c\ud835\udc4fI^{a},I^{b}italic_I start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , italic_I start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT into feature tokens \ud835\udc050a,\ud835\udc050bsuperscriptsubscript\ud835\udc050\ud835\udc4esuperscriptsubscript\ud835\udc050\ud835\udc4f\\mathbf{F}_{0}^{a},\\mathbf{F}_{0}^{b}bold_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , bold_F start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT, which are then decoded into point maps in the reference view coordinate space using two decoders. Our Easi3R aggregates the cross-attention maps from the decoders, producing four semantically meaningful maps: \ud835\udc00\u03bcb=src,\ud835\udc00\u03c3b=src,\ud835\udc00\u03bca=ref,\ud835\udc00\u03c3b=refsubscriptsuperscript\ud835\udc00\ud835\udc4fsrc\ud835\udf07subscriptsuperscript\ud835\udc00\ud835\udc4fsrc\ud835\udf0esubscriptsuperscript\ud835\udc00\ud835\udc4eref\ud835\udf07subscriptsuperscript\ud835\udc00\ud835\udc4fref\ud835\udf0e\\mathbf{A}^{b=\\text{src}}_{\\mu},\\mathbf{A}^{b=\\text{src}}_{\\sigma},\\mathbf{A}^%\n{a=\\text{ref}}_{\\mu},\\mathbf{A}^{b=\\text{ref}}_{\\sigma}bold_A start_POSTSUPERSCRIPT italic_b = src end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03bc end_POSTSUBSCRIPT , bold_A start_POSTSUPERSCRIPT italic_b = src end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03c3 end_POSTSUBSCRIPT , bold_A start_POSTSUPERSCRIPT italic_a = ref end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03bc end_POSTSUBSCRIPT , bold_A start_POSTSUPERSCRIPT italic_b = ref end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_\u03c3 end_POSTSUBSCRIPT. These maps are then used for a second inference pass to enhance reconstruction quality.", "description": "This figure illustrates the architecture of DUSt3R and how the authors' method, Easi3R, adapts it for dynamic scenes. DUSt3R processes two images, creating feature tokens that are decoded into point maps. Easi3R enhances this by aggregating cross-attention maps from DUSt3R's decoders, generating four maps representing different motion aspects (mean and variance of attention from source and reference views).  These new maps are then used in a second inference pass, improving reconstruction quality in dynamic scenes.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.24391/x3.png", "caption": "Figure 4: Visualization for Cross-Attention Maps. We color the normalized values of attention maps, ranging from one to zero. We highlight the patterns captured by each type of attention map using relatively high values. For a more detailed demonstration, we invite reviewers to visit our webpage under easi3r.github.io.", "description": "Figure 4 visualizes cross-attention maps from a deep learning model used for 4D reconstruction.  The maps are color-coded, with brighter colors representing higher attention values, and darker values representing lower attention. The figure showcases different patterns within these attention maps, highlighting how the model attends to various aspects of the scene, including dynamic objects, textured areas, areas with little texture, and under-observed parts. Each column shows a different type of attention pattern.  The caption invites readers to visit a website for more details because the visualization is more effectively shown with interactive elements.", "section": "3.2 Secrets Behind DUSt3R"}, {"figure_path": "https://arxiv.org/html/2503.24391/x8.png", "caption": "Figure 5: Qualitative Results of Dynamic Object Segmentation.\n\u201cOurs\u201d refers to the Easi3Rmonst3rsubscriptEasi3Rmonst3r\\mbox{{Easi3R}}_{\\text{monst3r}}Easi3R start_POSTSUBSCRIPT monst3r end_POSTSUBSCRIPT setting. Here, we present the enhanced setting, where outputs from different methods serve as prompts and are used with SAM2\u00a0[46] for mask inference.", "description": "Figure 5 presents a qualitative comparison of dynamic object segmentation results from several methods, including the proposed Easi3R method using the MonST3R backbone.  The 'enhanced' setting shown uses the output of each method as a prompt for the SAM2 model, which performs mask refinement. The figure demonstrates the improvement in segmentation accuracy achieved by Easi3R compared to other state-of-the-art methods.", "section": "4.1 Dynamic Object Motion"}, {"figure_path": "https://arxiv.org/html/2503.24391/x11.png", "caption": "Figure 6: Qualitative Comparison.\nWe visualize cross-frame globally aligned static scenes with dynamic point clouds at a selected timestamp. Notably, instead of using ground truth dynamic masks in previous work, we apply the estimated per-frame dynamic masks to filter out dynamic points at other timestamps for comparison.\nOur method (top two and bottom two rows as Easi3R dust3r/monst3rdust3r/monst3r{}_{\\text{dust3r/monst3r}}start_FLOATSUBSCRIPT dust3r/monst3r end_FLOATSUBSCRIPT, respectively) achieves temporally consistent reconstruction of both static scenes and moving objects, whereas baselines suffer from static structure misalignment and unstable camera pose estimation, and ghosting artifacts due to inaccuracy estimation of dynamic segmentation.", "description": "Figure 6 presents a qualitative comparison of 4D reconstruction results across different methods.  The top row shows the input video frames. The subsequent rows display the reconstruction results from CUTR3, MonST3R, DAS3R, and the proposed Easi3R method (with DUSt3R and MonST3R backbones).  Easi3R uses estimated per-frame dynamic masks to filter out dynamic points when comparing across different timestamps, unlike previous methods which used ground truth masks. The visualization demonstrates that Easi3R achieves temporally consistent reconstruction of both static and dynamic elements, while the baseline methods suffer from issues like misaligned static structures, unstable camera pose estimation, and ghosting artifacts caused by inaccurate dynamic segmentation.", "section": "4. Qualitative Comparison"}, {"figure_path": "https://arxiv.org/html/2503.24391/x12.png", "caption": "Figure 7: Visualization of estimated camera trajectories.\nOur robust estimated camera trajectory (orange) deviates less from the ground truth (gray) compared to the original backbones (blue).", "description": "Figure 7 presents a comparison of estimated camera trajectories.  The ground truth trajectory is shown in gray.  The trajectories estimated using the original DUSt3R and MonST3R models are shown in blue.  The trajectory generated by the proposed Easi3R method is shown in orange.  The figure visually demonstrates that Easi3R produces a more accurate camera trajectory, exhibiting less deviation from the ground truth compared to the original models.", "section": "4.2 Camera Motion"}]