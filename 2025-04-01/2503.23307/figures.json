[{"figure_path": "https://arxiv.org/html/2503.23307/x2.png", "caption": "Figure 1: MoCha is an end-to-end talking character video generation model that takes only speech and text as input, without requiring any auxiliary conditions. More videos are available on our website: https://congwei1230.github.io/MoCha", "description": "Figure 1 showcases the capabilities of MoCha, a novel end-to-end model for generating talking character videos.  Unlike other methods that often rely on additional input data (e.g., keypoints, reference images), MoCha uniquely generates high-quality video solely from speech and text prompts.  The figure demonstrates examples of MoCha's output across various control aspects, including scene control, emotion control, action control, and multi-character dialogues.  These examples highlight the model's ability to seamlessly integrate speech and text input for realistic character animation and lip synchronization.  For additional examples, viewers are directed to the project website.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.23307/x3.png", "caption": "Figure 2: MoCha Architecture. MoCha is a end-to-end Diffusion Transformer model that generates video frames from the joint conditioning of speech and text, without relying on any auxiliary signals. Both speech and text inputs are projected into token representations and aligned with video tokens through cross-attention.", "description": "MoCha, an end-to-end Diffusion Transformer model, takes both speech and text as input to generate video frames.  It doesn't use any additional signals (like pre-computed poses or images).  The model processes both the speech and text inputs, converting them into token representations.  These tokens are then aligned with video tokens using cross-attention to generate synchronized video output.", "section": "3. Model: MoCha"}, {"figure_path": "https://arxiv.org/html/2503.23307/x4.png", "caption": "Figure 3: MoCha\u2019s Speech-Video Window Cross Attention\nMoCha generates all video frames in parallel using a window cross-attention mechanism, where each video token attends to a local window of audio tokens to improve alignment and lip-sync quality.", "description": "This figure illustrates MoCha's novel Speech-Video Window Cross Attention mechanism. Unlike traditional methods that generate video frames sequentially, MoCha processes all frames in parallel using a cross-attention method.  Crucially, instead of each video token attending to the entire audio sequence, each video token only attends to a localized 'window' of audio tokens. This localized attention significantly improves the alignment between the generated video and the input audio, leading to a much better lip-sync quality.", "section": "3. Model: MoCha"}, {"figure_path": "https://arxiv.org/html/2503.23307/x5.png", "caption": "Figure 4: Multi-character Conversation and Character Tagging. MoCha supports generates multi-character conversion with scene cut. We design a specialized prompt template: it first specifies the number of clips, then introduces the characters along with their descriptions and associated tags. Each clip is subsequently described using only the character tags, simplifying the prompt while preserving clarity. MoCha leverages self-attention across video tokens to ensures character and environment consistency. The audio conditioning signal implicitly guides the model on when to transition between clips.", "description": "Figure 4 illustrates MoCha's capacity for generating multi-character conversations with scene changes.  A structured prompt template is used, beginning with the number of video clips to be generated.  Following this, each character is introduced with a description and a unique tag.  Subsequent descriptions of the clips only utilize these tags, streamlining the prompt while retaining clarity.  MoCha uses self-attention across video tokens to maintain consistency in character appearance and the environment across multiple clips.  The model implicitly switches between clips based on the audio input, indicating a speaker change.", "section": "3. Model: MoCha"}, {"figure_path": "https://arxiv.org/html/2503.23307/x6.png", "caption": "Figure 5: Qualitative results of MoCha on MoCha-Bench.\nMoCha not only generates lip movements that are well-synchronized with the input speech, but also produces natural facial expressions that reflect the prompt along with realistic hand gestures and action movements", "description": "Figure 5 showcases the capabilities of the MoCha model by presenting three example videos generated using different prompts. Each example highlights MoCha's ability to generate realistic and nuanced video output.  The videos demonstrate well-synchronized lip movements with the input audio, natural facial expressions reflecting the specified emotion and context of the prompt, and accurate, realistic hand gestures and full-body actions that appropriately correspond to the text and audio descriptions.", "section": "4. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.23307/x7.png", "caption": "Figure 6: Multi-Stage Training Strategy for MoCha. Text-Speech Joint training starts with close-up shots where speech conditioning has the strongest influence. At each stage, previous data is reduced by 50%, and harder tasks with weaker speech conditioning are introduced. Stage 0 uses text-only video data to establish a foundation for the future stages.", "description": "MoCha's training strategy involves a multi-stage approach.  Training begins with easy tasks: close-up shots of single characters, where speech is highly influential in driving the video generation.  As training progresses, the difficulty increases.  At each stage, the amount of data from the previous stage is halved, and new data is introduced with more challenging conditions, including less explicit speech and more complex shots (e.g. medium shots, multiple characters).  This gradual increase in difficulty helps the model generalize better.  Crucially, Stage 0 uses only text-based video data to create a foundational understanding of video structures before introducing speech.", "section": "3.4 MoCha Training Strategy"}, {"figure_path": "https://arxiv.org/html/2503.23307/x8.png", "caption": "Figure 7: Qualitative comparison between MoCha and baselines on MoCha-Bench.\nMoCha not only produces lip movements that align closely with the input speech\u2014enhancing the clarity and naturalness of articulation\u2014but also generates expressive facial animations and realistic, complex actions that faithfully follow the textual prompt. In contrast, SadTalker and AniPortrait exhibit minimal head motion and limited lip synchronization. Hallo3 mostly follows the lip-syncing but suffers from inaccurate articulation, erratic head movements, and noticeable visual artifacts.\nSince the baselines operate in an image-to-video (I2V) setting, we provide them with the first frame generated by MoCha as input for comparison. The first frame is cropped and resized as needed to meet the requirements of each baseline.", "description": "Figure 7 presents a qualitative comparison of MoCha against three baseline methods (SadTalker, AniPortrait, and Hallo3) on the MoCha-Bench benchmark.  The figure showcases video clips generated by each model in response to the same prompts, highlighting the differences in lip synchronization, facial expressions, and overall motion realism.  MoCha demonstrates superior performance, accurately aligning lip movements with speech, exhibiting natural facial expressions, and generating complex actions consistent with the textual prompts.  In contrast, the baseline models show limitations. SadTalker and AniPortrait exhibit very limited head motion and lip synchronization. Hallo3, while showing better lip sync, suffers from inaccurate articulation, erratic head movements, and noticeable visual artifacts. To ensure a fair comparison, the first frame generated by MoCha was provided as input to the baseline models, which were then tasked with generating the rest of the video sequence.  The first frames were cropped and resized as necessary to meet the individual input requirements of each baseline method.", "section": "4.3. Evaluation"}]