[{"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/newton_preview.png", "caption": "Figure 1: Comparison of feature selection methods on Newton\u2019s Principia text: original image (left), random feature selection retaining 40% of tokens (middle), and our proposed feature selector retaining 40% of tokens (right).", "description": "This figure compares three different approaches to feature selection applied to an image of Newton's Principia text.  The leftmost image shows the original, full-resolution image. The middle image demonstrates the result of randomly selecting and retaining only 40% of the visual tokens (features). The rightmost image shows the result of using the authors' proposed adaptive token reduction method to select and retain 40% of the tokens. This comparison highlights the effectiveness of the proposed method in preserving important information while significantly reducing the number of features.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/S.png", "caption": "Figure 2: Illustration of the Feature Selector in training mode. It uses three Transformer layers and a Gumbel-Softmax head to generate a binary mask where zeros mark tokens for removal and ones for retention. During training, the masked embeddings are replaced by a shared learnable embedding. During inference, the masked embeddings are discarded, while the retained ones are used for downstream tasks, such as image representations in Vision-Language models.", "description": "The Feature Selector, a key component of the proposed model, is illustrated.  It consists of three transformer layers followed by a Gumbel-Softmax head. This head generates a binary mask to identify tokens to keep (marked as '1') or discard (marked as '0').  During training, the discarded tokens are replaced with a shared learnable embedding. Importantly, during inference, these discarded tokens are simply removed, leading to a more efficient representation. The retained tokens are then used for downstream tasks.", "section": "4.1.1 Feature Selector Architecture"}, {"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/R.png", "caption": "Figure 3: Illustration of Feature Reconstructor\u2019s functionality. Its primary objective is to restore the tokens that were replaced with a learned representation.", "description": "The Feature Reconstructor is a crucial part of the proposed feature selection method.  It's a three-layer Transformer network designed to reconstruct the visual tokens that were masked (set to zero) by the Feature Selector.  These masked tokens were temporarily replaced during training with a learned, shared representation. The Reconstructor's job is to take the pruned set of features (those not masked) and generate a reconstruction of the complete feature set, attempting to recover the information lost during the pruning process. The effectiveness of this reconstruction is a key factor in evaluating which tokens are truly essential and which can be discarded without significant performance loss.", "section": "4. Implementation Details"}, {"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/next_text_vertical.png", "caption": "Figure 4: Comparison of LLaVA-NeXT performance with our selector (orange) and random selector (blue) on text-based benchmarks. The green dashed line represents the baseline performance using all features. The red dashed line represents the model\u2019s performance without image input.", "description": "This figure displays a comparison of the LLaVA-NeXT model's performance across various text-based benchmarks using three different feature selection methods: the proposed selector, a random selector, and no feature selection (using all features).  The x-axis represents the percentage of features retained, while the y-axis shows the performance metric (likely accuracy or a similar measure) for each benchmark.  The performance of the model without any image input is also shown as a baseline.  This allows for an evaluation of the effectiveness of the proposed feature selection technique in maintaining performance while reducing computational cost.", "section": "5.1 LLaVA-NeXT: Our Selector vs. Random Selector"}, {"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/next_no_text_vertical.png", "caption": "Figure 5: Comparison of LLaVA-NeXT performance with our selector (orange) and random selector (blue) on non-text benchmarks. The green dashed line represents the baseline performance using all features. The red dashed line represents the model\u2019s performance without image input.", "description": "This figure compares the performance of the LLaVA-NeXT model on several non-text based benchmarks when using different feature selection methods.  The performance is shown for three scenarios: using all features (green dashed line), using features selected by the proposed method (orange line), and using randomly selected features (blue line).  A fourth line (red dashed line) shows the model's performance without any visual input, providing a baseline for assessing the impact of visual information. The x-axis represents the percentage of features retained, and the y-axis represents the performance metric. This allows for a comparison of the effectiveness of the proposed approach versus random feature selection across multiple benchmarks at different levels of feature reduction.", "section": "5.1. LLaVA-NeXT: Our Selector vs. Random Selector"}, {"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/ov_text_vertical.png", "caption": "Figure 6: Comparison of LLaVA-OneVision performance with our trained selector (orange) and random selection (blue) on text-based (OCR-like) benchmarks. The green dashed line represents the baseline performance with all features retained. The red dashed line represents the model\u2019s performance without image input.", "description": "This figure compares the performance of the LLaVA-OneVision model on text-based (OCR-like) benchmarks using three different feature selection methods:  our trained feature selector, random feature selection, and using all features.  The x-axis shows the percentage of features retained, while the y-axis displays the performance metric (likely accuracy).  The orange line represents our trained selector, the blue line represents random selection, the green dashed line shows the baseline performance using all features, and the red dashed line demonstrates the model's performance without any image input.  This visualization helps to understand how effective our feature selection is in maintaining or improving model performance while reducing the number of features required, especially compared to random selection.", "section": "5.1 LLaVA-NeXT: Our Selector vs. Random Selector"}, {"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/ov_no_text_vertical.png", "caption": "Figure 7: Comparison of LLaVA-OneVision performance with our trained selector (orange) and random selection (blue) on non-text benchmarks. The green dashed line represents the baseline performance with all features retained. The red dashed line represents the model\u2019s performance without image input.", "description": "Figure 7 displays a comparison of the LLaVA-OneVision model's performance using different feature selection methods on non-text-based benchmarks.  The model's performance is evaluated when using all available features (baseline), features selected by the proposed trained selector, and randomly selected features. The results highlight how the proposed method compares to random selection across various metrics, showcasing its effectiveness in preserving performance even with reduced feature sets. A control is included to show the model's performance without any image input.", "section": "5.2. LLaVA-OneVision: Our Selector vs. Random Selector"}, {"figure_path": "https://arxiv.org/html/2503.16660/extracted/6297532/images/discussion_2.png", "caption": "Figure 8: Images from three benchmarks illustrating cases where the vision-language model gives correct answers or makes errors. The first column shows the model\u2019s responses using the full visual context, the second column uses a randomly selected set of features, and the third column uses the features selected by our selector. (1) DocVQA: to answer the question selecting the correct features is crucial. (2) MMMU (math): to answer this question, both visual understanding and logical reasoning are important, but the model fails to reason correctly. (3) MMstar: the image details are less important, and the language model plays a dominant role.", "description": "Figure 8 demonstrates the performance of the vision-language model with different feature selection methods on three benchmark datasets: DocVQA, MMMU, and MMStar.  Each row shows an example image from a different benchmark. The first column shows the model's response using all visual features, while the second uses randomly selected features and the third uses features selected by the proposed method.  DocVQA showcases a scenario where selecting the correct features is essential for accurate responses. MMMU presents a task where both visual understanding and reasoning are crucial but the model struggles to reason correctly. In the MMStar example, the image details are less critical than the language model's reasoning capacity, highlighting the importance of language processing in some tasks. This figure effectively illustrates how the importance of image features varies across different task types.", "section": "5. Experiments"}]