[{"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/Figure1.png", "caption": "Figure 1: \nAn example of a multimodal scientific multiple-choice problem.\nThe correct answer is derived based on the reasoning over inputs that include context, question, and diagram.", "description": "This figure shows a multimodal scientific problem, which involves integrating information from multiple modalities such as text and diagrams to arrive at a solution.  The example illustrates a physics problem presented as a multiple-choice question with accompanying text description and diagram. The solution requires a deep understanding of physical principles and an ability to synthesize textual and visual data. Solving such multimodal problems involves complex reasoning and is a current challenge in artificial intelligence.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/Figure3.png", "caption": "Figure 2: The corresponding relation between the Big Seven Personality theory and the seven function-specific agents.", "description": "This figure illustrates the seven function-specific agents used in the MAPS framework and their corresponding relation to the Big Seven Personality theory.  Each agent is assigned a specific role in the problem-solving process, and these roles reflect the traits associated with each personality dimension in the Big Seven model. For example, the Manager agent reflects Conscientiousness, the UserProxy agent reflects Agreeableness, the Interpreter represents Extraversion, and so on. This visual representation helps to understand how the framework integrates different personality traits to achieve a more comprehensive and efficient problem-solving process.", "section": "2.1. Big Seven Personality"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/Figure2.png", "caption": "Figure 3: \nThe overall architecture of MAPS.\nIt illustrates seven functional agents based on the Big Seven Personality theory. It first includes the Manager agent with predefined interaction logic and the UserProxy agent responsible for receiving user inputs. Subsequently,\nfour specialized agents\u2014Interpreter, Aligner, Scholar, and Solver\u2014are introduced, each corresponding to a specific step in solving MSPs. Finally,\nthe Critic agent is presented, providing feedback and corrections to ensure the results are more accurate and interpretable.", "description": "The figure illustrates the MAPS framework's architecture, showing seven agents based on the Big Seven Personality theory.  The UserProxy and Manager agents handle user input and workflow management. Four specialized agents (Interpreter, Aligner, Scholar, Solver) sequentially process multimodal scientific problems (MSPs), with the Critic agent providing feedback for refinement and correction.", "section": "2. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/Figure4.png", "caption": "Figure 4: The schema of the Critic agent, as well as the feedback and backtracking situations of the Critic agent across different datasets.", "description": "Figure 4 illustrates the functionality of the Critic agent within the MAPS framework.  The top panel displays a schematic overview of the Critic agent's scoring and feedback mechanisms. The bottom panel presents a breakdown of the feedback distribution provided by the Critic agent across three different datasets (MathVista, EMMA, and OlympiadBench) to each of the four main agents (Interpreter, Aligner, Scholar, and Solver) as well as scenarios where no regeneration was necessary. This visualization helps to understand the agent's role in refining and iterating on the problem-solving process across various tasks and datasets. The feedback distribution highlights which components of the problem-solving process most frequently required adjustment or correction.", "section": "3.3. Analysis of Critic Agent"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/output1.png", "caption": "Figure 5: Performance Comparison of MAPS on Math, Physics, and Chemistry Subtasks in the EMMA Dataset with GPT-4o, Gemini, and Qwen2.5-VL-72B as Bases.", "description": "Figure 5 presents a comparative analysis of the MAPS model's performance against three different base models (GPT-4, Gemini, and Qwen-72B) across three subtasks within the EMMA dataset: mathematics, physics, and chemistry.  The bar chart visually represents the accuracy achieved by each model on each subtask, showcasing the improvement provided by MAPS over the baseline models. This comparison highlights the effectiveness of MAPS in enhancing the performance of various base models for multimodal scientific problem-solving.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/output.png", "caption": "Figure 6: An analysis of the solving time efficiency across different question types, answer types, question categories, and question difficulties.", "description": "Figure 6 presents a detailed analysis of the time efficiency of the model in solving various types of scientific problems.  The figure breaks down the solving time efficiency based on four key factors: question types (e.g., multiple choice vs. open-ended), answer types (e.g., integer, float, text), question categories (e.g., math, physics, chemistry, general), and question difficulty levels (e.g., before college, college, Olympiad). This comprehensive analysis provides valuable insights into the model's performance characteristics across different problem complexity levels and allows for a better understanding of its strengths and weaknesses in various scientific reasoning scenarios.", "section": "4.3. Time Efficiency"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/Figure6.png", "caption": "Figure 7: The generalization experiments conducted on the DiagramQG physical dataset, which are based on the GPT-4o base model and the incremental part of MAPS.", "description": "This figure displays the results of generalization experiments performed on the DiagramQG physical dataset.  These experiments aimed to assess how well the MAPS framework, built upon the GPT-4 base model, generalizes to a new dataset.  The figure visually compares the accuracy of the GPT-4 base model alone to the accuracy achieved by adding the MAPS framework. The comparison highlights the performance gains provided by MAPS across various sub-tasks within the DiagramQG physical dataset.  Each bar represents a specific sub-task, showing the accuracy improvement resulting from the inclusion of MAPS.", "section": "3. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/Figure5.png", "caption": "Figure 8: A case study of a specific solving process, illustrating the detailed steps involved in solving the problem. This includes the various stages of problem-solving as well as the feedback and backtracking mechanisms that help refine and improve the solution.", "description": "Figure 8 presents a detailed walkthrough of MAPS solving a physics problem, highlighting each agent's role (Interpreter, Aligner, Scholar, Solver, Critic).  It visually demonstrates the multi-step process, including iterative feedback and refinement through Socratic questioning.  The figure shows how each agent contributes sequentially, with the Critic agent providing feedback for improvement and prompting backtracking as needed. This illustrates the dynamic, iterative nature of the MAPS framework.", "section": "2.3. Four-Step MSPs Solving"}, {"figure_path": "https://arxiv.org/html/2503.16905/extracted/6298662/figures/Figure7.png", "caption": "Figure 9: A complete example of the collaborative output from all agents in an iteration, using the multimodal physics problem from the OlympiadBench dataset. This example demonstrates how each agent contributes to the problem-solving process, collaborating to produce a refined solution step by step.", "description": "Figure 9 showcases a detailed, step-by-step illustration of the MAPS framework in action.  It uses a multimodal physics problem from the OlympiadBench dataset as an example, highlighting how each of the seven agents (Manager, UserProxy, Interpreter, Aligner, Scholar, Solver, and Critic) collaborates to solve the problem. The figure visually represents the iterative process, including feedback and refinement, demonstrating the framework's ability to integrate diverse information sources and refine solutions through iterative steps.", "section": "2.3. Four-Step MSPs Solving"}]