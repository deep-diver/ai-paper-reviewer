<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering &#183; HF Daily Paper Reviews by AI</title>
<meta name=title content="ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering &#183; HF Daily Paper Reviews by AI"><meta name=description content="ETVA evaluates text-to-video alignment via fine-grained question generation and answering."><meta name=keywords content="Multimodal Learning,Vision-Language Models,üè¢ Renmin University of China,"><link rel=canonical href=https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16867/><link type=text/css rel=stylesheet href=/ai-paper-reviewer/css/main.bundle.min.595affd4445a931ea6d6e3a5a3c709930fa52a60be10b21c6f81fdb8fecaacea33aacedf80cdc88be45f189be14ed4ce53ea74a1e1406fad9cbf90c5ed409173.css integrity="sha512-WVr/1ERakx6m1uOlo8cJkw+lKmC+ELIcb4H9uP7KrOozqs7fgM3Ii+RfGJvhTtTOU+p0oeFAb62cv5DF7UCRcw=="><script type=text/javascript src=/ai-paper-reviewer/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/ai-paper-reviewer/js/main.bundle.min.efbf3b6b987689fffaf2d7b73173d2690c0279a04d444b0537a77d7f4ff6e6d493445400cb0cf56bc0f0f123e19f15394e63cae34e67f069bd013dd5c73df56e.js integrity="sha512-7787a5h2if/68te3MXPSaQwCeaBNREsFN6d9f0/25tSTRFQAywz1a8Dw8SPhnxU5TmPK405n8Gm9AT3Vxz31bg==" data-copy data-copied></script><script src=/ai-paper-reviewer/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/ai-paper-reviewer/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/ai-paper-reviewer/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/ai-paper-reviewer/favicon-16x16.png><link rel=manifest href=/ai-paper-reviewer/site.webmanifest><meta property="og:url" content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16867/"><meta property="og:site_name" content="HF Daily Paper Reviews by AI"><meta property="og:title" content="ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering"><meta property="og:description" content="ETVA evaluates text-to-video alignment via fine-grained question generation and answering."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="2025-03-24"><meta property="article:published_time" content="2025-03-21T00:00:00+00:00"><meta property="article:modified_time" content="2025-03-21T00:00:00+00:00"><meta property="article:tag" content="Multimodal Learning"><meta property="article:tag" content="Vision-Language Models"><meta property="article:tag" content="üè¢ Renmin University of China"><meta property="og:image" content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16867/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16867/cover.png"><meta name=twitter:title content="ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering"><meta name=twitter:description content="ETVA evaluates text-to-video alignment via fine-grained question generation and answering."><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"2025-03-24s","name":"ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering","headline":"ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering","abstract":"ETVA evaluates text-to-video alignment via fine-grained question generation and answering.","inLanguage":"en","url":"https:\/\/deep-diver.github.io\/ai-paper-reviewer\/2025-03-24\/2503.16867\/","author":{"@type":"Person","name":"Hugging Face Daily Papers"},"copyrightYear":"2025","dateCreated":"2025-03-21T00:00:00\u002b00:00","datePublished":"2025-03-21T00:00:00\u002b00:00","dateModified":"2025-03-21T00:00:00\u002b00:00","keywords":["Multimodal Learning","Vision-Language Models","üè¢ Renmin University of China"],"mainEntityOfPage":"true","wordCount":"3338"}]</script><meta name=author content="Hugging Face Daily Papers"><link href=https://github.com/deep-diver/paper-reviewer/ rel=me><link href=https://twitter.com/algo_diver/ rel=me><script src=/ai-paper-reviewer/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script defer src=/ai-paper-reviewer/lib/typeit/typeit.umd.1b3200cb448f5cd1f548f2781452643d3511a43584b377b82c03a58055da4fdb7bc8f6c6c2ce846480c7677ff25bfd0d75f15823c09443ab18e0fd2cad792587.js integrity="sha512-GzIAy0SPXNH1SPJ4FFJkPTURpDWEs3e4LAOlgFXaT9t7yPbGws6EZIDHZ3/yW/0NdfFYI8CUQ6sY4P0srXklhw=="></script><script defer src=/ai-paper-reviewer/lib/packery/packery.pkgd.min.js integrity></script><script type=text/javascript src=/ai-paper-reviewer/js/shortcodes/gallery.min.9b4cb28f931ed922c26fb9b2510c2debb370f6a63305050c2af81740b2919883715e24efbbdf3a081496718ec751df3a72729d4d0bc71d6071297563a97ce1ee.js integrity="sha512-m0yyj5Me2SLCb7myUQwt67Nw9qYzBQUMKvgXQLKRmINxXiTvu986CBSWcY7HUd86cnKdTQvHHWBxKXVjqXzh7g=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-KX0S6Q55Y7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KX0S6Q55Y7")</script><meta name=theme-color><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-app.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-firestore.js></script><script src=https://www.gstatic.com/firebasejs/8.10.0/firebase-auth.js></script><script>const firebaseConfig={apiKey:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",authDomain:"AIzaSyCv6pUES05bxrampKbfhhmW3y-1pmX3XgE",projectId:"neurips2024-f3065",storageBucket:"neurips2024-f3065.firebasestorage.app",messagingSenderId:"982475958898",appId:"1:982475958898:web:2147e5d7753d6ac091f0eb",measurementId:"G-YQ46HXQ9JS"};var app=firebase.initializeApp(firebaseConfig),db=firebase.firestore(),auth=firebase.auth()</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div class=min-h-[148px]></div><div class="fixed inset-x-0 pl-[24px] pr-[24px]" style=z-index:100><div id=menu-blur class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div><div class="relative max-w-[64rem] ml-auto mr-auto"><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ai-paper-reviewer/ class="text-base font-medium text-gray-500 hover:text-gray-900">HF Daily Paper Reviews by AI</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12"><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>About</p></a><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-21</p></a><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-24</p></a><a href=/ai-paper-reviewer/2025-03-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>2025-03-25</p></a><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Archive</p></a><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title>Tags</p></a><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title></p></a><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span><p class="text-base font-medium" title></p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="ltr:mr-14 rtl:ml-14 flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center space-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400" style=margin-right:5px><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/ai-paper-reviewer/about/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>About</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-21/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-21</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-24/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-24</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/2025-03-25/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>2025-03-25</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/paper-reviews/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Archive</p></a></li><li class=mt-1><a href=/ai-paper-reviewer/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title>Tags</p></a></li><li class=mt-1><a href=https://github.com/deep-diver/paper-reviewer/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li><li class=mt-1><a href=https://twitter.com/algo_diver/ target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></div><p class="text-bg font-bg" title></p></a></li></ul></div></label></div></div></div></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("menu-blur");n.style.opacity=t/300})</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div id=hero class="h-[150px] md:h-[200px]"></div><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/ai-paper-reviewer/2025-03-24/2503.16867/cover_hu10816202405953242363.png)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/>HF Daily Paper Reviews by AI</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/2025-03-24/>2025-03-24s</a><span class="px-1 text-primary-500">/</span></li><li class="inline hidden"><a class="hover:underline decoration-neutral-300 dark:underline-neutral-600" href=/ai-paper-reviewer/2025-03-24/2503.16867/>ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">ETVA: Evaluation of Text-to-Video Alignment via Fine-grained Question Generation and Answering</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2025-03-21T00:00:00+00:00>21 March 2025</time><span class="px-2 text-primary-500">&#183;</span><span>3338 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">16 mins</span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=views_2025-03-24/2503.16867/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=views>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 576 512"><path fill="currentcolor" d="M288 32c-80.8.0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7.0 24.6C17.3 304 48.6 356 95.4 399.4 142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1 3.3-7.9 3.3-16.7.0-24.6-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144 64.5-144 144-144 144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64-11.5.0-22.3-3-31.6-8.4-.2 2.8-.4 5.5-.4 8.4.0 53 43 96 96 96s96-43 96-96-43-96-96-96c-2.8.0-5.6.1-8.4.4 5.3 9.3 8.4 20.1 8.4 31.6z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<span id=likes_2025-03-24/2503.16867/index.md class="animate-pulse inline-block text-transparent max-h-3 rounded-full mt-[-2px] align-middle bg-neutral-300 dark:bg-neutral-400" title=likes>loading</span>
<span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span></span><span class="px-2 text-primary-500">&#183;</span><span>
<button id=button_likes class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400" onclick=process_article()>
<span id=button_likes_heart style=display:none class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M47.6 300.4 228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6.0 115.2.0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"/></svg>
</span></span><span id=button_likes_emtpty_heart class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M244 84l11.1 12 12-11.98C300.6 51.37 347 36.51 392.6 44.1 461.5 55.58 512 115.2 512 185.1V190.9c0 41.5-17.2 81.2-47.6 109.5L283.7 469.1c-7.5 7-17.4 10.9-27.7 10.9S235.8 476.1 228.3 469.1L47.59 300.4C17.23 272.1.0 232.4.0 190.9V185.1c0-69.9 50.52-129.52 119.4-141 44.7-7.59 92 7.27 124.6 39.9C243.1 84 244 84.01 244 84zm11.1 79.9-45-46.8c-21.7-20.82-52.5-30.7-82.8-25.66C81.55 99.07 48 138.7 48 185.1V190.9c0 28.2 11.71 55.2 32.34 74.4L256 429.3l175.7-164c20.6-19.2 32.3-46.2 32.3-74.4V185.1c0-46.4-33.6-86.03-79.3-93.66C354.4 86.4 323.6 96.28 301.9 117.1l-46.8 46.8z"/></svg>
</span></span><span id=button_likes_text>&nbsp;Like</span></button></span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/ai-generated/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">AI Generated
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/categories/-daily-papers/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">ü§ó Daily Papers
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/multimodal-learning/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Multimodal Learning
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/vision-language-models/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Vision-Language Models
</span></span></span><span style=margin-top:.5rem class=mr-2 onclick='window.open("/ai-paper-reviewer/tags/-renmin-university-of-china/","_self")'><span class=flex style=cursor:pointer><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">üè¢ Renmin University of China</span></span></span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Hugging Face Daily Papers" src=/ai-paper-reviewer/img/avatar_hu1570846118988919414.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Hugging Face Daily Papers</div><div class="text-sm text-neutral-700 dark:text-neutral-400">I am AI, and I review papers on HF Daily Papers</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/deep-diver/paper-reviewer/ target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/algo_diver/ target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first lg:ml-auto px-0 lg:order-last ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-[140px]"><details open id=TOCView class="toc-right mt-0 overflow-y-scroll overscroll-contain scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600 rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#t2v-alignment>T2V Alignment</a></li><li><a href=#etva-framework>ETVA Framework</a></li><li><a href=#benchmarking-t2v>Benchmarking T2V</a></li><li><a href=#atomic-question>Atomic Question</a></li><li><a href=#hallucination>Hallucination</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><details class="toc-inside mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#key-takeaways>Key Takeaways</a></li><li><a href=#why-does-it-matter>Why does it matter?</a></li><li><a href=#visual-insights>Visual Insights</a></li></ul></li><li><a href=#in-depth-insights>In-depth insights</a><ul><li><a href=#t2v-alignment>T2V Alignment</a></li><li><a href=#etva-framework>ETVA Framework</a></li><li><a href=#benchmarking-t2v>Benchmarking T2V</a></li><li><a href=#atomic-question>Atomic Question</a></li><li><a href=#hallucination>Hallucination</a></li></ul></li><li><a href=#more-visual-insights>More visual insights</a></li><li><a href=#full-paper>Full paper</a></li></ul></li></ul></nav></div></details><script>var margin=200,marginError=50;(function(){var t=$(window),e=$("#TOCView"),s=e.height();function n(){var n=t.height()-margin;s>=n?(e.css("overflow-y","scroll"),e.css("max-height",n+marginError+"px")):(e.css("overflow-y","hidden"),e.css("max-height","9999999px"))}t.on("resize",n),$(document).ready(n)})()</script></div></div><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><br><div class="flex flex-row flex-wrap items-center space-x-2"><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 48 48" fill="none"><rect width="48" height="48" fill="#fff" fill-opacity=".01"/><path d="M18 43V22c0-3.3137 2.6863-6 6-6s6 2.6863 6 6V43" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M12 40V22c0-6.6274 5.3726-12 12-12s12 5.3726 12 12V40" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M6 35V22C6 12.0589 14.0589 4 24 4s18 8.0589 18 18V35" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 44V31" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/><path d="M24 24.625v-2.75" stroke="#000" stroke-width="4" stroke-linecap="round" stroke-linejoin="round"/></svg>
</span></span><span>2503.16867</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span class=mr-1><span class="relative block icon"><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 511.999 511.999"><g><g><path d="M421.578 190.264l-99.847-99.847c-2.439-2.439-6.391-2.439-8.829.0L82.824 320.495c-2.439 2.439-2.439 6.392.0 8.829l99.847 99.847c2.439 2.439 6.391 2.439 8.829.0l230.078-230.078C424.017 196.655 424.017 192.703 421.578 190.264z"/></g></g><g><g><path d="M506.511 87.672 424.323 5.484c-7.308-7.31-19.175-7.315-26.488.0L348.219 55.1c-2.439 2.439-2.439 6.391.0 8.829l99.847 99.847c2.439 2.437 6.391 2.437 8.829.0l49.616-49.616C513.826 106.847 513.826 94.987 506.511 87.672z"/></g></g><g><g><path d="M508.133 491.11c-1.054-9.556-9.489-16.599-19.104-16.599H111.633l36.058-15.163c4.088-1.719 5.131-7.034 1.994-10.17l-86.854-86.854c-3.137-3.135-8.451-2.094-10.17 1.994C52.224 365.359 2.052 484.66 1.627 485.707c-5.815 13.208 4.855 27.01 18.107 26.263H489.52C500.566 511.97 509.379 502.408 508.133 491.11z"/></g></g></svg>
</span></span><span>Kaisi Guan et el.</span></span></span></div><div class="flex mt-2"><span class="rounded-full bg-primary-500 dark:bg-primary-400 text-neutral-50 dark:text-neutral-800 px-1.5 py-[1px] text-sm font-normal"><span class="flex flex-row items-center"><span>ü§ó 2025-03-24</span></span></span></div></div><p><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://arxiv.org/abs/2503.16867 target=_self role=button>‚Üó arXiv
</a><a class="!rounded-md bg-primary-600 px-4 py-2 !text-neutral !no-underline hover:!bg-primary-500 dark:bg-primary-800 dark:hover:!bg-primary-700" href=https://huggingface.co/papers/2503.16867 target=_self role=button>‚Üó Hugging Face</a></p><audio controls><source src=https://ai-paper-reviewer.com/2503.16867/podcast.wav type=audio/wav>Your browser does not support the audio element.</audio><h3 class="relative group">TL;DR<div id=tldr class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tldr aria-label=Anchor>#</a></span></h3><div class="lead text-neutral-500 dark:text-neutral-400 !mb-9 text-xl"><p>Precisely evaluating semantic alignment between text prompts and generated videos remains a challenge in Text-to-Video (T2V) Generation. Existing text-to-video alignment metrics generate coarse-grained scores without fine-grained details, failing to align with human preference. To address this limitation, this paper introduces a novel Evaluation method of Text-to-Video Alignment via fine-grained question generation and answering. First, a multi-agent system parses prompts into semantic scene graphs to generate atomic questions. Then we design a knowledge-augmented multi-stage reasoning framework for question answering.</p><p>This paper proposes <strong>ETVA</strong>, a novel Evaluation method of Text-to-Video Alignment that contains a multi-agent framework for atomic question generation and a knowledge-augmented multi-stage reasoning framework to emulate human-like reasoning in question answering. The QG part of ETVA consists of three collaborative agents. Based on ETVA, they further construct <strong>ETVA-Bench</strong>, a comprehensive benchmark for text-to-video alignment evaluation. Our findings reveal that these models still struggle in some areas such as camera movements or physics process.</p></div><h4 class="relative group">Key Takeaways<div id=key-takeaways class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#key-takeaways aria-label=Anchor>#</a></span></h4><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-d3ba913deefce05639366771b3a78764></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-d3ba913deefce05639366771b3a78764",{strings:[" ETVA, a new method, achieves higher correlation with human judgment than existing metrics. "],speed:10,lifeLike:!0,startDelay:0,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-92aa5338d99841551e1ae377f5cbdfd4></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-92aa5338d99841551e1ae377f5cbdfd4",{strings:[" The benchmark features 2k diverse prompts and 12k atomic questions for T2V alignment evaluation. "],speed:10,lifeLike:!0,startDelay:1e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><div class="flex px-4 py-3 rounded-md bg-primary-100 dark:bg-primary-900"><span class="text-primary-400 ltr:pr-3 rtl:pl-3 flex items-center"><span class="relative block icon"><svg width="800" height="800" viewBox="0 0 24 24" fill="none"><path d="M9.15316 5.40838C10.4198 3.13613 11.0531 2 12 2s1.5802 1.13612 2.8468 3.40837l.3277.58786C15.5345 6.64193 15.7144 6.96479 15.9951 7.17781c.2806.21302.629999999999999.29209 1.329.45024L17.9605 7.77203C20.4201 8.32856 21.65 8.60682 21.9426 9.54773c.2926.94087-.5458 1.92137-2.2227 3.88217L19.2861 13.9372C18.8096 14.4944 18.5713 14.773 18.4641 15.1177 18.357 15.4624 18.393 15.8341 18.465 16.5776L18.5306 17.2544C18.7841 19.8706 18.9109 21.1787 18.1449 21.7602 17.3788 22.3417 16.2273 21.8115 13.9243 20.7512L13.3285 20.4768C12.6741 20.1755 12.3469 20.0248 12 20.0248S11.3259 20.1755 10.6715 20.4768L10.0757 20.7512c-2.30302 1.0603-3.45452 1.5905-4.22055 1.009C5.08912 21.1787 5.21588 19.8706 5.4694 17.2544L5.53498 16.5776C5.60703 15.8341 5.64305 15.4624 5.53586 15.1177 5.42868 14.773 5.19043 14.4944 4.71392 13.9372L4.2801 13.4299c-1.67685-1.9608-2.51528-2.9413-2.22268-3.88217C2.35002 8.60682 3.57986 8.32856 6.03954 7.77203l.63635-.14398C7.37485 7.4699 7.72433 7.39083 8.00494 7.17781 8.28555 6.96479 8.46553 6.64194 8.82547 5.99623L9.15316 5.40838z" fill="#1c274c"/></svg>
</span></span><span class=dark:text-neutral-300><div id=typeit-4407d1e7c1574f7e7fffb214f1a2e24c></div><script>document.addEventListener("DOMContentLoaded",function(){new TypeIt("#typeit-4407d1e7c1574f7e7fffb214f1a2e24c",{strings:[" The research identifies capabilities and limitations of 15 existing T2V models, paving the way for better T2V generation. "],speed:10,lifeLike:!0,startDelay:2e3,breakLines:!0,waitUntilVisible:!0,loop:!1}).go()})</script></span></div><h4 class="relative group">Why does it matter?<div id=why-does-it-matter class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#why-does-it-matter aria-label=Anchor>#</a></span></h4><p>This paper introduces a novel evaluation metric and benchmark that addresses the limitations of existing methods for assessing text-to-video alignment. By providing a more fine-grained and human-aligned evaluation approach, this research can help researchers to <strong>better understand and improve T2V generation models</strong>. The proposed method opens avenues for developing models that <strong>more accurately capture the nuances of text prompts</strong> and generate videos that align with human expectations, ultimately advancing the field and <strong>potentially leading to more sophisticated T2V applications</strong>.</p><hr><h4 class="relative group">Visual Insights<div id=visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#visual-insights aria-label=Anchor>#</a></span></h4><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x1.png alt></figure></p><blockquote><p>üîº This figure demonstrates the workflow of ETVA (Evaluation of Text-to-Video Alignment) and compares its performance to existing text-to-video alignment metrics. The top half shows a text prompt describing a scene, along with two example videos generated by a text-to-video model. The bottom half displays how existing metrics (BLIP-BLEU, CLIPScore, VideoScore) evaluate these videos, showing that they don&rsquo;t align well with human preferences. In contrast, ETVA&rsquo;s approach is presented: It generates fine-grained questions about specific aspects of the video (e.g., &lsquo;Is there a cup?&rsquo;, &lsquo;Is the water pouring out?&rsquo;) and uses a multi-stage reasoning mechanism, including common-sense knowledge, to determine whether these questions are correctly answered by the video, ultimately leading to an alignment score. ETVA shows significantly better alignment with human judgment compared to existing methods.</p><details><summary>read the caption</summary>Figure 1: Illustration of how ETVA works and comparison with existing metrics.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S4.T1.6.1><thead class=ltx_thead><tr class=ltx_tr id=S4.T1.6.1.1.1><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S4.T1.6.1.1.1.1><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.1.1>Metric</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.2><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.2.1>Existence</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.3><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.3.1>Action</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.4><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.4.1>Material</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.5><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.5.1>Spatial</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.6><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.6.1>Number</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.7><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.7.1>Shape</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.8><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.8.1>Color</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.9><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.9.1>Camera</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.10><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.10.1>Physics</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id=S4.T1.6.1.1.1.11><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.11.1>Other</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S4.T1.6.1.1.1.12><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.1.1.12.1>Overall</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S4.T1.6.1.2.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S4.T1.6.1.2.1.1>BLIP-ROUGE¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.16867v1#bib.bib33 title><span class=ltx_text style=font-size:90%>33</span></a>]</cite></th><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.2>6.0/8.4</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.3>6.5/8.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.4>3.0/5.3</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.5>9.2/12.6</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.6>11.0/15.5</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.7>9.3/14.7</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.8>-4.7/-5.2</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.9>3.3/4.9</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.10>5.9/8.8</td><td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id=S4.T1.6.1.2.1.11>5.3/7.1</td><td class="ltx_td ltx_align_center ltx_border_t" id=S4.T1.6.1.2.1.12>6.3/8.8</td></tr><tr class=ltx_tr id=S4.T1.6.1.3.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.6.1.3.2.1>BLIP-BLEU¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.16867v1#bib.bib33 title><span class=ltx_text style=font-size:90%>33</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.2>9.3/12.9</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.3>8.4/11.6</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.4>6.2/10.1</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.5>11.2/15.4</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.6>10.0/13.6</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.7>9.2/12.7</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.8>6.6/10.5</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.9>10.3/14.7</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.10>10.1/14.4</td><td class="ltx_td ltx_align_center ltx_border_r" id=S4.T1.6.1.3.2.11>8.6/11.4</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.3.2.12>8.5/12.1</td></tr><tr class=ltx_tr id=S4.T1.6.1.4.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.6.1.4.3.1>CLIPScore¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.16867v1#bib.bib42 title><span class=ltx_text style=font-size:90%>42</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.2>10.2/13.7</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.3>10.6/13.6</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.4>9.9/12.8</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.5>12.2/16.1</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.6>10.9/14.8</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.7>14.6/20.8</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.8>11.8/18.8</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.9>12.9/15.2</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.10>9.7/14.2</td><td class="ltx_td ltx_align_center ltx_border_r" id=S4.T1.6.1.4.3.11>10.2/12.9</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.4.3.12>10.3/13.8</td></tr><tr class=ltx_tr id=S4.T1.6.1.5.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.6.1.5.4.1>UMTScore¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.16867v1#bib.bib22 title><span class=ltx_text style=font-size:90%>22</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.2>17.9/24.0</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.3>14.3/19.0</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.4>25.4/34.4</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.5>21.6/28.2</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.6>9.1/13.5</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.7>24.4/32.1</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.8>22.5/31.1</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.9>22.2/29.0</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.10>18.4/23.2</td><td class="ltx_td ltx_align_center ltx_border_r" id=S4.T1.6.1.5.4.11>15.5/20.6</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.5.4.12>17.6/23.5</td></tr><tr class=ltx_tr id=S4.T1.6.1.6.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.6.1.6.5.1>ViCLIPScore¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.16867v1#bib.bib56 title><span class=ltx_text style=font-size:90%>56</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.2>20.2/27.1</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.3>17.9/24.2</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.4>16.2/20.4</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.5>20.6/27.2</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.6>16.9/22.4</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.7>29.4/38.4</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.8>13.8/13.5</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.9>23.6/31.8</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.10>19.8/26.3</td><td class="ltx_td ltx_align_center ltx_border_r" id=S4.T1.6.1.6.5.11>17.0/22.7</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.6.5.12>19.4/25.9</td></tr><tr class=ltx_tr id=S4.T1.6.1.7.6><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S4.T1.6.1.7.6.1>VideoScore¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class=ltx_ref href=https://arxiv.org/html/2503.16867v1#bib.bib12 title><span class=ltx_text style=font-size:90%>12</span></a>]</cite></th><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.2>23.2/30.6</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.3>22.7/30.2</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.4>29.9/37.3</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.5>24.8/31.7</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.6>26.6/35.9</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.7>28.1/35.7</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.8>11.7/16.2</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.9>19.2/26.3</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.10>20.3/23.9</td><td class="ltx_td ltx_align_center ltx_border_r" id=S4.T1.6.1.7.6.11>23.9/31.6</td><td class="ltx_td ltx_align_center" id=S4.T1.6.1.7.6.12>23.7/31.0</td></tr><tr class=ltx_tr id=S4.T1.6.1.8.7><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.1><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.1.1>ETVA</span></th><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.2><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.2.1>47.7/57.4</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.3><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.3.1>38.3/46.6</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.4><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.4.1>55.5/66.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.5><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.5.1>56.0/66.8</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.6><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.6.1>44.0/53.9</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.7><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.7.1>64.1/75.1</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.8><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.8.1>31.5/39.7</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.9><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.9.1>35.5/44.2</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.10><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.10.1>50.6/60.4</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id=S4.T1.6.1.8.7.11><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.11.1>49.0/59.2</span></td><td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id=S4.T1.6.1.8.7.12><span class="ltx_text ltx_font_bold" id=S4.T1.6.1.8.7.12.1>47.2/58.5</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 1 presents the correlation analysis between automatic text-to-video alignment metrics and human judgment. It shows Kendall&rsquo;s tau (œÑ) and Spearman&rsquo;s rho (œÅ) correlation coefficients, indicating the strength and direction of the relationship between each metric&rsquo;s scores and human ratings of alignment quality. The table is broken down by 10 categories of prompts, grouping together prompts that generated the same evaluation questions to ensure a fair comparison. Higher values represent stronger positive correlations. This helps assess how well each metric aligns with human perception of alignment.</p><details><summary>read the caption</summary>Table 1: Correlations between each evaluation metric and human judgment on text alignment, measured by Kendall‚Äôs œÑùúè\tauitalic_œÑ (left) and Spearman‚Äôs œÅùúå\rhoitalic_œÅ (right). The same category denotes groups of prompts that produce the same evaluation questions.</details></blockquote><h3 class="relative group">In-depth insights<div id=in-depth-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#in-depth-insights aria-label=Anchor>#</a></span></h3><h4 class="relative group">T2V Alignment<div id=t2v-alignment class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#t2v-alignment aria-label=Anchor>#</a></span></h4><p>The paper addresses the crucial challenge of <strong>evaluating the semantic alignment between text prompts and generated videos in Text-to-Video (T2V) generation</strong>. Existing metrics, like CLIPScore, are deemed insufficient as they offer only coarse-grained scores, failing to capture fine-grained alignment details and often diverging from human preference. To overcome these limitations, the paper introduces ETVA, a novel evaluation method centered around fine-grained question generation and answering. This innovative approach hinges on a multi-agent system that parses prompts into semantic scene graphs, enabling the generation of atomic questions. A knowledge-augmented multi-stage reasoning framework is designed for question answering, utilizing an auxiliary LLM to retrieve relevant common-sense knowledge, coupled with a video LLM for answering questions through a multi-stage process. The paper includes a new benchmark specifically for text-to-video alignment evaluation, featuring diverse prompts and atomic questions. Through a comprehensive evaluation of existing T2V models, the research identifies their key capabilities and limitations, paving the way for next-generation T2V generation. ETVA demonstrates a higher correlation with human judgment compared to existing metrics, marking a significant advancement in T2V evaluation.</p><h4 class="relative group">ETVA Framework<div id=etva-framework class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#etva-framework aria-label=Anchor>#</a></span></h4><p>The ETVA framework is a novel approach designed for evaluating the semantic alignment between text prompts and generated videos, a significant challenge in the field of Text-to-Video (T2V) generation. Existing metrics often fall short, ETVA leverages a multi-agent system to parse prompts into semantic scene graphs and generate atomic questions, enabling fine-grained alignment analysis. A knowledge-augmented multi-stage reasoning framework, incorporates auxiliary LLMs for commonsense knowledge retrieval, further enhancing question answering accuracy by video LLMs. <strong>ETVA&rsquo;s architecture focuses on generating detailed questions and emulating human-like reasoning</strong>, mitigating hallucinations and improving alignment with human preferences. ETVA is a crucial step towards reliable evaluation, addressing existing metrics&rsquo; limitations.</p><h4 class="relative group">Benchmarking T2V<div id=benchmarking-t2v class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#benchmarking-t2v aria-label=Anchor>#</a></span></h4><p>Benchmarking Text-to-Video (T2V) generation is crucial for assessing model capabilities, identifying limitations, and driving progress in the field. Current benchmarks are categorized into general benchmarks evaluating overall performance (<strong>quality, consistency, aesthetics</strong>) and specific benchmarks focusing on particular aspects (<strong>human actions, multi-object composition, physics phenomenon, time-lapse video</strong>). <strong>General benchmarks often lack fine-grained detail</strong>, while <strong>specific benchmarks offer in-depth analysis of narrow aspects</strong>. A comprehensive benchmark should ideally encompass diverse prompts, atomic questions, and evaluation across various semantic categories (<strong>existence, action, material, spatial, number, shape, color, camera, physics, other</strong>). Developing reliable automatic metrics for T2V alignment is essential, as existing metrics often produce coarse-grained scores and fail to align with human preference. Benchmarking requires careful consideration of question generation and answering strategies, along with human annotation for accurate evaluation. Constructing <strong>structured scene graph</strong> for atomic element is important for achieving overall efficiency.</p><h4 class="relative group">Atomic Question<div id=atomic-question class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#atomic-question aria-label=Anchor>#</a></span></h4><p><strong>Atomic questions</strong> play a crucial role in enhancing the granularity and accuracy of text-to-video alignment evaluation. By breaking down complex prompts into smaller, more manageable questions, the evaluation focuses on assessing specific details within the generated video, addressing limitations of metrics like CLIPScore that offer coarse-grained scores failing to capture fine-grained nuances. This approach ensures a deeper understanding of the video&rsquo;s content and its alignment with the original text prompt. These <strong>atomic questions</strong> systematically verify various aspects of the video, reducing semantic redundancy, ensuring complete coverage, and generating answerable queries for video LLMs. This strategy aids in overcoming issues such as <strong>video LLM hallucination</strong>, where the model might struggle with complex or ambiguous questions, and allows for a more precise and human-aligned assessment.</p><h4 class="relative group">Hallucination<div id=hallucination class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#hallucination aria-label=Anchor>#</a></span></h4><p>Hallucination in text-to-video generation is a significant issue, where models generate content inconsistent with the input prompt. This paper addresses this challenge by focusing on improving the alignment between the text prompt and the generated video content. The <strong>core problem is that existing metrics often fail to capture fine-grained alignment details</strong>, leading to discrepancies between automated evaluations and human preferences. To mitigate this, the authors introduce a novel evaluation method called ETVA, which <strong>employs a multi-agent system for atomic question generation</strong> and a <strong>knowledge-augmented multi-stage reasoning framework for question answering.</strong> This approach aims to simulate human-like reasoning to better assess the semantic alignment between text and video. The key contribution lies in the fine-grained analysis facilitated by generating detailed questions covering various aspects of the video content, ensuring a more comprehensive evaluation than coarse-grained metrics like CLIPScore. By incorporating commonsense knowledge and multi-stage reasoning, ETVA effectively reduces hallucinations and improves the reliability of text-to-video alignment evaluation. Extensive experiments validate the effectiveness of ETVA, demonstrating superior correlation with human judgment compared to existing evaluation metrics. Furthermore, the paper introduces ETVABench, a comprehensive benchmark for text-to-video alignment evaluation, facilitating systematic comparison and analysis of different T2V models. The study highlights the <strong>limitations of current models in accurately simulating real-world physics and camera movements</strong>, paving the way for future research to address these specific challenges and enhance the overall quality and coherence of generated videos.</p><h3 class="relative group">More visual insights<div id=more-visual-insights class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#more-visual-insights aria-label=Anchor>#</a></span></h3><details><summary>More on figures</summary><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x2.png alt></figure></p><blockquote><p>üîº The figure illustrates the overall architecture of the ETVA (Evaluation of Text-to-Video Alignment) model. ETVA uses a two-stage process. The first stage involves a multi-agent system for question generation which takes a text prompt as input. This system parses the prompt into a scene graph, representing entities, attributes, and relationships. Then, a graph traverser systematically traverses this scene graph to generate atomic (fine-grained) yes/no questions. The second stage consists of a knowledge-augmented multi-stage reasoning framework for question answering. Here, an auxiliary LLM provides common sense knowledge that is combined with the video content and the generated questions. The video LLM then answers the questions through a step-by-step reasoning process that involves video understanding, general reflection, and a final conclusion. The final ETVA score is computed based on the combined answers to all the questions.</p><details><summary>read the caption</summary>Figure 2: Overall pipeline of ETVA. ETVA contains a multi-agent framework for generating atomic questions and a knowledge-augmented multi-stage reasoning framework for question answering.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x3.png alt></figure></p><blockquote><p>üîº This radar chart visualizes the performance of ten open-source text-to-video (T2V) models across ten different categories within the ETVABench-2k benchmark. Each category represents a specific aspect of video generation, such as the presence of objects, actions, materials, spatial relationships, numbers, shapes, colors, camera angles, physics, and other miscellaneous elements. The chart allows for a direct comparison of the models&rsquo; capabilities in each category, revealing their strengths and weaknesses in various video generation aspects. A higher score indicates better performance for a dimension.</p><details><summary>read the caption</summary>Figure 3: Evaluating results of 10 opensource T2V models in ETVABench-2k.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x4.png alt></figure></p><blockquote><p>üîº This figure showcases four examples that illustrate how ETVA evaluates text-to-video alignment. Each example highlights a different aspect of the alignment challenge: (1) Physics phenomenon: accurately modeling the behavior of liquids in microgravity; (2) Color transition: smoothly changing the color of an object across a video; (3) Number accuracy: correctly representing the count of objects in the video; and (4) Gestural semantic: correctly interpreting and rendering human actions and gestures. The figure directly compares the results from different text-to-video (T2V) models using ETVA, showing how the models perform in each of these four alignment categories.</p><details><summary>read the caption</summary>Figure 4: Four Cases of using ETVA to evaluate text-to-video alignment in T2V models, covering physics phenomenon, color transition, number accuracy and gestrual semnatic.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x5.png alt></figure></p><blockquote><p>üîº This figure shows a comparison of the prompt category distributions for two benchmark datasets: ETVABench-2k and ETVABench-105. Both datasets are used to evaluate text-to-video alignment models. The figure visually represents the number of prompts belonging to each of the ten defined categories in both datasets. This allows for a comparison of the relative frequency of each prompt type across the two benchmark versions.</p><details><summary>read the caption</summary>Figure 5: Prompt Category Distribution of ETVABench-2k and ETVABench-105</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x6.png alt></figure></p><blockquote><p>üîº This figure provides detailed instructions for human annotators evaluating the alignment between text prompts and generated videos. It outlines the scoring process for assessing semantic consistency, emphasizing that the evaluation should focus solely on the semantic match between text and video, ignoring factors like video quality, resolution, or visual clarity. It also explains the process of evaluating specific binary yes/no questions derived from the text descriptions to assess whether the generated videos correctly answer them.</p><details><summary>read the caption</summary>Figure 6: Instruction for Human Annotation</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x7.png alt></figure></p><blockquote><p>üîº This figure shows the prompt used for the Element Extractor, one of the three agents in the multi-agent question generation framework of ETVA. The prompt instructs the Element Extractor to meticulously analyze the input prompts and extract background information, camera information, entities, attributes, and relationships. It provides a detailed explanation of how each of these data elements should be formatted before being processed by the following agents.</p><details><summary>read the caption</summary>Figure 7: Prompt for Element Extractor.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x8.png alt></figure></p><blockquote><p>üîº This figure shows the prompt used to instruct the Graph Builder agent within the ETVA framework. The Graph Builder agent takes the extracted entities, attributes, and relations from the text prompt as input and constructs a hierarchical scene graph. This graph represents the relationships between elements in the prompt, creating a structured knowledge representation for question generation. The prompt emphasizes the need for a coherent, hierarchical graph that accurately reflects the semantics of the input, ensuring that all mappings between elements are accurate and contextually relevant.</p><details><summary>read the caption</summary>Figure 8: Prompt for Graph Builder.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x9.png alt></figure></p><blockquote><p>üîº This figure details the prompt instructions given to the Graph Traverser agent within the ETVA framework. The Graph Traverser is responsible for generating atomic yes/no questions from the structured scene graph created by previous agents in the system. The prompt provides examples of different question types (content, attribute, relation) and illustrates how the input, question type, and specific content should be formatted. This ensures consistency and facilitates the generation of comprehensive and relevant video evaluation questions.</p><details><summary>read the caption</summary>Figure 9: Prompt for Graph Traverser.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x10.png alt></figure></p><blockquote><p>üîº This figure displays the prompt used for the Knowledge Augmentation module within the ETVA framework. The prompt instructs an LLM to identify implicit knowledge‚Äîcommon sense or relevant physical principles‚Äînot explicitly stated in a video&rsquo;s text prompt but necessary for generating a realistic video. The prompt provides three examples to illustrate how to extract and express this implicit knowledge, focusing on providing detailed descriptions to enrich video understanding.</p><details><summary>read the caption</summary>Figure 10: Prompt for Knowledge Augmentation.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x11.png alt></figure></p><blockquote><p>üîº This figure shows the prompt used for the Multi-stage Reasoning stage of the ETVA framework. The prompt instructs a multimodal assistant to answer a video-related question by engaging in a three-stage process: Video Understanding, Critical Reflection, and Conclusion. The assistant has access to the textual prompt used to generate the video, additional common-sense knowledge, and the video itself. The prompt guides the assistant to analyze how well the video matches both the text and the common sense, identify discrepancies or gaps, and then provide a yes/no answer with a brief justification.</p><details><summary>read the caption</summary>Figure 11: Prompt for Multi-stage Reasoning.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x12.png alt></figure></p><blockquote><p>üîº This figure compares the performance of ETVA (a novel text-to-video alignment evaluation metric) against three existing metrics: BLIP-BLEU, CLIPScore, and VideoScore. The comparison is based on a specific text prompt: &lsquo;Water is slowly pouring out of glass cup in the space station.&rsquo; For each metric, the scores for two generated videos are shown. The figure visually demonstrates how ETVA&rsquo;s fine-grained assessment differs from the coarser-grained scores of the conventional methods, ultimately better reflecting human preferences regarding video-text alignment.</p><details><summary>read the caption</summary>Figure 12: Illustration of a comparative analysis between ETVA and conventional evaluation metrics, based on the text prompt: ‚ÄúWater is slowly pouring out of glass cup in the space station‚Äù. We compare our ETVA score with conventional text-to-video alignment metrics.</details></blockquote><p><figure><img class="my-0 rounded-md" loading=lazy src=https://arxiv.org/html/2503.16867/x13.png alt></figure></p><blockquote><p>üîº Figure 13 presents a comparative analysis of ETVA and conventional evaluation metrics for a specific text prompt: &lsquo;A leaf turns from green to red.&rsquo; It shows the generated videos from various models, alongside their scores from metrics like BLIP-BLEU, CLIPScore, VideoScore, and ETVA. This visual comparison allows for a direct assessment of how well each method aligns with human perception of text-to-video alignment quality. The figure highlights ETVA&rsquo;s ability to more accurately capture the nuanced aspects of semantic correspondence than traditional metrics, which often provide only coarse-grained evaluations.</p><details><summary>read the caption</summary>Figure 13: Illustration of a comparative analysis between ETVA and conventional evaluation metrics, based on the text prompt: ‚ÄùA leaf turns from green to red‚Äù. We compare our ETVA score with conventional text-to-video alignment metrics.</details></blockquote></details><details><summary>More on tables</summary><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T2.4.4><thead class=ltx_thead><tr class=ltx_tr id=S5.T2.2.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id=S5.T2.2.2.2.3><span class="ltx_text ltx_font_bold" id=S5.T2.2.2.2.3.1>Settings</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.1.1.1.1><span class="ltx_text ltx_font_bold" id=S5.T2.1.1.1.1.1>Kendall‚Äôs<math alttext="\tau" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.m1.1a"><mi id="S5.T2.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.m1.1d">italic_œÑ</annotation></semantics></math></span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T2.2.2.2.2><span class="ltx_text ltx_font_bold" id=S5.T2.2.2.2.2.1>Spearsman‚Äôs<math alttext="\rho" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.1.m1.1"><semantics id="S5.T2.2.2.2.2.1.m1.1a"><mi id="S5.T2.2.2.2.2.1.m1.1.1" xref="S5.T2.2.2.2.2.1.m1.1.1.cmml">œÅ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.1.m1.1b"><ci id="S5.T2.2.2.2.2.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.1.m1.1.1">ùúå</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.1.m1.1d">italic_œÅ</annotation></semantics></math></span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T2.4.4.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id=S5.T2.4.4.4.3><span class="ltx_text ltx_font_bold" id=S5.T2.4.4.4.3.1>Multi-agent QG</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.3.3.3.1><span class="ltx_text ltx_font_bold" id=S5.T2.3.3.3.1.2>47.16</span> <span class="ltx_text ltx_font_bold" id=S5.T2.3.3.3.1.1>(<math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T2.3.3.3.1.1.m1.1"><semantics id="S5.T2.3.3.3.1.1.m1.1a"><mi id="S5.T2.3.3.3.1.1.m1.1.1" mathvariant="normal" xref="S5.T2.3.3.3.1.1.m1.1.1.cmml">Œî</mi><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.1.m1.1b"><ci id="S5.T2.3.3.3.1.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.1.m1.1.1">Œî</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.1.1.m1.1d">roman_Œî</annotation></semantics></math> +12.12)</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T2.4.4.4.2><span class="ltx_text ltx_font_bold" id=S5.T2.4.4.4.2.1>57.47(<math alttext="\Delta" class="ltx_Math" display="inline" id="S5.T2.4.4.4.2.1.m1.1"><semantics id="S5.T2.4.4.4.2.1.m1.1a"><mi id="S5.T2.4.4.4.2.1.m1.1.1" mathvariant="normal" xref="S5.T2.4.4.4.2.1.m1.1.1.cmml">Œî</mi><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.2.1.m1.1b"><ci id="S5.T2.4.4.4.2.1.m1.1.1.cmml" xref="S5.T2.4.4.4.2.1.m1.1.1">Œî</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.2.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.2.1.m1.1d">roman_Œî</annotation></semantics></math> +14.67)</span></td></tr><tr class=ltx_tr id=S5.T2.4.4.5.1><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id=S5.T2.4.4.5.1.1>Vannila QG</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.4.4.5.1.2>35.04</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T2.4.4.5.1.3>42.87</td></tr></tbody></table></table></figure><blockquote><p>üîº This ablation study investigates the impact of different question generation methods within the ETVA framework. Specifically, it compares the performance of a multi-agent question generation (QG) approach against a simpler vanilla QG approach. Both methods utilize the same knowledge-augmented multi-stage reasoning framework for question answering; only the question generation process differs. The results show the improvement achieved by the multi-agent QG in terms of Kendall&rsquo;s œÑ and Spearman&rsquo;s œÅ correlation coefficients, demonstrating the effectiveness of the more sophisticated question generation strategy.</p><details><summary>read the caption</summary>Table 2: Ablation Study of ETVA Question Generation. Both multi-agent and vanilla QG are based on the knowledge-augmented multi-stage reasoning framework.</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S5.T3.2.2><thead class=ltx_thead><tr class=ltx_tr id=S5.T3.2.2.2><th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id=S5.T3.2.2.2.3>Setting</th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.2.2.2.4><span class="ltx_text ltx_font_bold" id=S5.T3.2.2.2.4.1>Accuracy</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.1.1.1.1><span class="ltx_text ltx_font_bold" id=S5.T3.1.1.1.1.1>Kendall‚Äôs<math alttext="\tau" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><mi id="S5.T3.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.m1.1d">italic_œÑ</annotation></semantics></math></span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S5.T3.2.2.2.2><span class="ltx_text ltx_font_bold" id=S5.T3.2.2.2.2.1>Spearsman‚Äôs<math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.1.m1.1"><semantics id="S5.T3.2.2.2.2.1.m1.1a"><mi id="S5.T3.2.2.2.2.1.m1.1.1" xref="S5.T3.2.2.2.2.1.m1.1.1.cmml">œÅ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.1.m1.1b"><ci id="S5.T3.2.2.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.2.2.1.m1.1.1">ùúå</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.2.1.m1.1d">italic_œÅ</annotation></semantics></math></span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S5.T3.2.2.3.1 style=background-color:#e6e6e6><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id=S5.T3.2.2.3.1.1><span class="ltx_text ltx_font_bold" id=S5.T3.2.2.3.1.1.1 style=background-color:#e6e6e6>ETVA</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.2.3.1.2><span class="ltx_text ltx_font_bold" id=S5.T3.2.2.3.1.2.1 style=background-color:#e6e6e6>89.27 (+25.20)</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.2.3.1.3><span class="ltx_text ltx_font_bold" id=S5.T3.2.2.3.1.3.1 style=background-color:#e6e6e6>47.16 (+28.98)</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S5.T3.2.2.3.1.4><span class="ltx_text ltx_font_bold" id=S5.T3.2.2.3.1.4.1 style=background-color:#e6e6e6>58.47 (+34.63)</span></td></tr><tr class=ltx_tr id=S5.T3.2.2.4.2><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T3.2.2.4.2.1>¬†¬†¬†¬†¬†¬†w/o. KA</th><td class="ltx_td ltx_align_center" id=S5.T3.2.2.4.2.2>67.34</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.4.2.3>27.34</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.4.2.4>35.54</td></tr><tr class=ltx_tr id=S5.T3.2.2.5.3><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T3.2.2.5.3.1>¬†¬†¬†¬†¬†¬†w/o. VU</th><td class="ltx_td ltx_align_center" id=S5.T3.2.2.5.3.2>82.73</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.5.3.3>37.56</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.5.3.4>44.81</td></tr><tr class=ltx_tr id=S5.T3.2.2.6.4><th class="ltx_td ltx_align_left ltx_th ltx_th_row" id=S5.T3.2.2.6.4.1>¬†¬†¬†¬†¬†¬†w/Only KA</th><td class="ltx_td ltx_align_center" id=S5.T3.2.2.6.4.2>65.48</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.6.4.3>24.72</td><td class="ltx_td ltx_align_center" id=S5.T3.2.2.6.4.4>33.12</td></tr><tr class=ltx_tr id=S5.T3.2.2.7.5><th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id=S5.T3.2.2.7.5.1>Direct answer</th><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.2.7.5.2>63.07</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.2.7.5.3>18.18</td><td class="ltx_td ltx_align_center ltx_border_bb" id=S5.T3.2.2.7.5.4>23.84</td></tr></tbody></table></table></figure><blockquote><p>üîº This table presents the results of an ablation study on the question-answering part of the ETVA model. It analyzes the contribution of three key components: Knowledge Augmentation (KA), Video Understanding (VU), and Critical Reflection (CR). The table shows the accuracy, Kendall&rsquo;s œÑ correlation, and Spearman&rsquo;s œÅ correlation achieved by the full ETVA model and variations where one or more of these components are removed. This allows for a quantitative assessment of the importance of each component in achieving high human alignment scores.</p><details><summary>read the caption</summary>Table 3: Ablation Study of ETVA Question Answering Part: Component Analysis (KA: Knowledge Augmentation; VU: Video Understanding; CR: Critical Reflection)</details></blockquote><figure style=max-width:100%;text-align:center><table style=width:100%><caption style=caption-side:bottom;text-align:left;white-space:normal;display:block;max-width:100%;color:var(--tw-prose-captions);margin-bottom:10px></caption><table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id=S7.T5.2.1><thead class=ltx_thead><tr class=ltx_tr id=S7.T5.2.1.1.1><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id=S7.T5.2.1.1.1.1><span class=ltx_text id=S7.T5.2.1.1.1.1.1 style=font-size:144%>Category</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S7.T5.2.1.1.1.2><span class=ltx_text id=S7.T5.2.1.1.1.2.1 style=font-size:144%>Question Example</span></th><th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id=S7.T5.2.1.1.1.3><span class=ltx_text id=S7.T5.2.1.1.1.3.1 style=font-size:144%>Prompt Example</span></th></tr></thead><tbody class=ltx_tbody><tr class=ltx_tr id=S7.T5.2.1.2.1><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id=S7.T5.2.1.2.1.1><span class=ltx_text id=S7.T5.2.1.2.1.1.1 style=font-size:144%>Existence</span></th><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T5.2.1.2.1.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.2.1.2.1 style=font-size:144%>Is there a penguin in the video?</span></td><td class="ltx_td ltx_align_center ltx_border_t" id=S7.T5.2.1.2.1.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.2.1.3.1 style=font-size:144%>A penguin standing on the left side of a cactus in a desert.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.3.2><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.3.2.1><span class=ltx_text id=S7.T5.2.1.3.2.1.1 style=font-size:144%>Action</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.3.2.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.3.2.2.1 style=font-size:144%>Does the player pass the football?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.3.2.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.3.2.3.1 style=font-size:144%>In a crucial game moment, a player passes the football, dodging opponents.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.4.3><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.4.3.1><span class=ltx_text id=S7.T5.2.1.4.3.1.1 style=font-size:144%>Material</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.4.3.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.4.3.2.1 style=font-size:144%>Is the city made of crystals?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.4.3.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.4.3.3.1 style=font-size:144%>A city made entirely of glowing crystals that change colors based on emotions.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.5.4><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.5.4.1><span class=ltx_text id=S7.T5.2.1.5.4.1.1 style=font-size:144%>Spatial</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.5.4.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.5.4.2.1 style=font-size:144%>Does the penguin stand on the left of the cactus?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.5.4.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.5.4.3.1 style=font-size:144%>A penguin standing on the left side of a cactus in a desert.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.6.5><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.6.5.1><span class=ltx_text id=S7.T5.2.1.6.5.1.1 style=font-size:144%>Number</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.6.5.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.6.5.2.1 style=font-size:144%>Are there three owls in the video?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.6.5.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.6.5.3.1 style=font-size:144%>Three owls perch on a branch during a full moon.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.7.6><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.7.6.1><span class=ltx_text id=S7.T5.2.1.7.6.1.1 style=font-size:144%>Shape</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.7.6.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.7.6.2.1 style=font-size:144%>Is the cloud shaped like a hand?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.7.6.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.7.6.3.1 style=font-size:144%>A cloud shaped like a giant hand that picks up people for transportation.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.8.7><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.8.7.1><span class=ltx_text id=S7.T5.2.1.8.7.1.1 style=font-size:144%>Color</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.8.7.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.8.7.2.1 style=font-size:144%>Does the man‚Äôs hair brown?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.8.7.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.8.7.3.1 style=font-size:144%>There‚Äôs a person, likely in their mid-twenties, with short brown hair.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.9.8><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.9.8.1><span class=ltx_text id=S7.T5.2.1.9.8.1.1 style=font-size:144%>Camera</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.9.8.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.9.8.2.1 style=font-size:144%>Is the camera pushing in?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.9.8.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.9.8.3.1 style=font-size:144%>A girl is walking forward, camera push in.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.10.9><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id=S7.T5.2.1.10.9.1><span class=ltx_text id=S7.T5.2.1.10.9.1.1 style=font-size:144%>Physics</span></th><td class="ltx_td ltx_align_center" id=S7.T5.2.1.10.9.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.10.9.2.1 style=font-size:144%>Is the water pouring out in the space station?</span></td><td class="ltx_td ltx_align_center" id=S7.T5.2.1.10.9.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.10.9.3.1 style=font-size:144%>Water is slowly pouring out of glass cup in the space station.</span></td></tr><tr class=ltx_tr id=S7.T5.2.1.11.10><th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id=S7.T5.2.1.11.10.1><span class=ltx_text id=S7.T5.2.1.11.10.1.1 style=font-size:144%>Other</span></th><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T5.2.1.11.10.2><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.11.10.2.1 style=font-size:144%>Is the video in the Van Gogh style?</span></td><td class="ltx_td ltx_align_center ltx_border_bb" id=S7.T5.2.1.11.10.3><span class="ltx_text ltx_font_italic" id=S7.T5.2.1.11.10.3.1 style=font-size:144%>A beautiful coastal beach waves lapping on sand, Van Gogh style.</span></td></tr></tbody></table></table></figure><blockquote><p>üîº Table 4 presents a comprehensive evaluation of 15 text-to-video (T2V) models on the ETVABench-105 benchmark. The benchmark consists of 105 diverse text prompts, each designed to evaluate specific aspects of video generation quality, categorized into 10 dimensions: Existence, Action, Material, Spatial, Number, Shape, Color, Camera, Physics, and Other. For each prompt, each model generates a short video clip. The table shows the performance scores of each model on each dimension. A higher score indicates better performance in that dimension. Scores are presented as percentages, ranging from 0 to 1. Bold values highlight the overall best performance across all models for each dimension. Underlined values indicate the best performance among the 10 open-source models for each dimension, distinguishing them from the 5 closed-source models. This table provides a detailed comparison of the models&rsquo; capabilities across various aspects of video generation and helps identify strengths and weaknesses across different model architectures.</p><details><summary>read the caption</summary>Table 4: ETVABench-105 evaluation results with 5 close-source T2V models and 10 open-source T2V models. A higher score indicates better performance for a dimension. Bold stands for the best score. Underline indicates the best score in the open-source models.</details></blockquote></details><h3 class="relative group">Full paper<div id=full-paper class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#full-paper aria-label=Anchor>#</a></span></h3><div id=gallery-a15879ed98493a3a754bf4b74177f9cb class=gallery><img src=https://ai-paper-reviewer.com/2503.16867/1.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/2.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/3.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/4.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/5.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/6.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/7.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/8.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/9.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/10.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/11.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/12.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/13.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/14.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/15.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/16.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/17.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/18.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/19.png class="grid-w50 md:grid-w33 xl:grid-w25">
<img src=https://ai-paper-reviewer.com/2503.16867/20.png class="grid-w50 md:grid-w33 xl:grid-w25"></div></div><section class="flex flex-row flex-wrap justify-center pt-4 text-xl"><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16867/&amp;title=ETVA:%20Evaluation%20of%20Text-to-Video%20Alignment%20via%20Fine-grained%20Question%20Generation%20and%20Answering" title="Share on LinkedIn" aria-label="Share on LinkedIn"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="https://twitter.com/intent/tweet/?url=https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16867/&amp;text=ETVA:%20Evaluation%20of%20Text-to-Video%20Alignment%20via%20Fine-grained%20Question%20Generation%20and%20Answering" title="Tweet on Twitter" aria-label="Tweet on Twitter"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"/></svg>
</span></a><a target=_blank class="m-1 rounded bg-neutral-300 p-1.5 text-neutral-700 hover:bg-primary-500 hover:text-neutral dark:bg-neutral-700 dark:text-neutral-300 dark:hover:bg-primary-400 dark:hover:text-neutral-800" href="mailto:?body=https://deep-diver.github.io/ai-paper-reviewer/2025-03-24/2503.16867/&amp;subject=ETVA:%20Evaluation%20of%20Text-to-Video%20Alignment%20via%20Fine-grained%20Question%20Generation%20and%20Answering" title="Send via email" aria-label="Send via email"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></a></section></div><script>var oid="views_2025-03-24/2503.16867/index.md",oid_likes="likes_2025-03-24/2503.16867/index.md"</script><script type=text/javascript src=/ai-paper-reviewer/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/ai-paper-reviewer/2025-03-24/2503.17095/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2025-03-21T00:00:00+00:00>21 March 2025</time>
</span></span></a></span><span></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=pmnxis/pmnxis.github.io issue-term=pathname label=Comment theme=dark-blue crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/ai-paper-reviewer/tags/ title>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=https://deep-diver.github.io/neurips2024/ title>NeurIPS2024</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Hugging Face Daily Papers</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/ai-paper-reviewer/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://deep-diver.github.io/ai-paper-reviewer/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body><script data-name=BMC-Widget data-cfasync=false src=https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js data-id=chansung data-description="Support me on Buy me a coffee!" data-message data-color=#FFDD00 data-position=Left data-x_margin=18 data-y_margin=18></script></html>