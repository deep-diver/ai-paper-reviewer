[{"figure_path": "https://arxiv.org/html/2503.16430/x1.png", "caption": "Figure 1: Comparison of different autoregressive visual generation approaches. (a) Traditional discrete tokenization incorporate quantization during training, resulting in tokenizer training instability and limited vocabulary size that restricts representational capacity. (b) Hybrid continuous AR models preserve rich visual information but need complex distribution modeling (diffusion or GMM) beyond standard categorical prediction. (c) Our approach bridges these paradigms by applying post-training quantization to pretrained continuous features, maintaining the high representational capacity of continuous tokens while enabling simple autoregressive modeling.", "description": "This figure compares three different approaches to autoregressive visual generation.  (a) shows the traditional method using discrete tokens, where quantization during training leads to instability and limits the model's ability to represent fine visual details. (b) illustrates a hybrid approach using continuous tokens, preserving detail but requiring complex distribution modeling like diffusion or Gaussian Mixture Models for prediction. (c) presents the proposed 'TokenBridge' method, which leverages pretrained continuous features and applies post-training quantization to achieve the simplicity of discrete tokens while retaining the representational power of continuous tokens, enabling straightforward autoregressive modeling.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16430/x2.png", "caption": "Figure 2: Generated samples from TokenBridge. Class-conditional generation results on ImageNet\u00a0[6] 256\u00d7256 demonstrating fine details and textures across diverse categories including animals, food, objects, and scenes.", "description": "Figure 2 showcases images generated by the TokenBridge model.  The images are 256x256 pixels and demonstrate the model's ability to generate high-quality, detailed images across a wide range of categories within the ImageNet dataset. These categories include animals, various types of food, different objects, and a variety of scenes. The figure highlights the model's capacity for generating diverse and visually rich outputs.", "section": "2. Related Work"}, {"figure_path": "https://arxiv.org/html/2503.16430/x3.png", "caption": "Figure 3: Illustration of our post-training quantization process. The top row shows the pretrained continuous VAE tokenizer, mapping an input image to continuous latent features \ud835\udc7f\u2208\u211dH\u00d7W\u00d7C\ud835\udc7fsuperscript\u211d\ud835\udc3b\ud835\udc4a\ud835\udc36{\\bm{X}}\\in\\mathbb{R}^{H\\times W\\times C}bold_italic_X \u2208 blackboard_R start_POSTSUPERSCRIPT italic_H \u00d7 italic_W \u00d7 italic_C end_POSTSUPERSCRIPT and reconstructing it through the decoder. Our post-training quantization process (middle) transforms these continuous features into discrete tokens by independently quantizing each channel dimension. The bottom-left shows how our approach preserves the original Gaussian-like distribution (purple curve) in discretized form (purple histogram). The right portion demonstrates the de-quantization process that maps indices back to continuous values for decoding.", "description": "Figure 3 illustrates the TokenBridge's post-training quantization method.  The top panel shows a pretrained continuous Variational Autoencoder (VAE) tokenizer converting an input image into continuous latent features (a tensor with dimensions H x W x C). The VAE's decoder then reconstructs the image from these features. The middle panel details the post-training quantization step, where each channel dimension of the continuous features is independently discretized into discrete tokens. The bottom-left panel compares the original Gaussian-like distribution of the continuous features with its discretized counterpart (histogram), showing how the method preserves the original distribution's shape. Finally, the right panel explains the dequantization process, where the discrete tokens are mapped back to continuous values to allow for decoding using the pretrained VAE decoder.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.16430/x4.png", "caption": "Figure 4: Our autoregressive generation process. At the spatial level, our model autoregressively generates tokens conditioning on previous positions. For each spatial location (highlighted in pink), we apply dimension-wise sequential prediction to efficiently handle the large token space. This approach decomposes the modeling of each token into a series of smaller classification problems while preserving essential inter-dimensional dependencies.", "description": "This figure illustrates the autoregressive generation process used in the TokenBridge model.  The model generates image tokens sequentially, both spatially (across the image) and within each spatial location (dimension-wise). Spatially, the model predicts tokens one at a time, conditioning each prediction on previously generated tokens. Within each spatial location (shown in pink), the model uses a dimension-wise approach: it predicts individual channel values sequentially, thereby efficiently managing the exponentially large token space resulting from post-training quantization. This decomposition into smaller classification problems preserves important relationships between channels while making the model computationally feasible.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.16430/x5.png", "caption": "Figure 5: Reconstruction quality of typical continuous and discrete tokenizers. For discrete baselines, we use VQ from \u00a0[35], and LFQ from \u00a0[19]. Our method achieves reconstruction quality comparable to continuous VAE, preserving more fine details than traditional discrete tokenizers, especially in text and facial features. Zoom in for better comparison.", "description": "Figure 5 presents a comparison of reconstruction quality between continuous and discrete image tokenization methods.  It shows the results of reconstructing images using three different approaches: a continuous Variational Autoencoder (VAE), Vector Quantization (VQ) from [35] as a representative discrete method, and a Learned Fixed-Point Quantization (LFQ) method from [19]. A comparison with the results from the authors' method (TokenBridge) is also included. The images visually demonstrate that TokenBridge achieves reconstruction quality comparable to the continuous VAE, but preserves significantly more fine details than the traditional VQ and LFQ discrete methods. The improvement is particularly visible in areas with fine textural details such as text and facial features.", "section": "4.2. Properties of Our Tokenizer"}, {"figure_path": "https://arxiv.org/html/2503.16430/x6.png", "caption": "Figure 6: Reconstruction quality of different quantization granularities B. Visual comparison showing reconstructions at decreasing quantization levels.\nZoom in for better comparison.", "description": "This figure visually compares the reconstruction quality achieved by TokenBridge at different quantization levels (B).  It presents a series of images reconstructed from the same original images, each reconstruction using a successively lower quantization granularity.  The purpose is to demonstrate how the model's ability to recover fine details is affected as the quantization level decreases, and to show that even with a relatively coarse quantization, the model maintains good reconstruction quality.  Zooming in on the images is recommended for a better evaluation of the subtle differences in detail preservation.", "section": "4.2. Properties of Our Tokenizer"}, {"figure_path": "https://arxiv.org/html/2503.16430/x7.png", "caption": "Figure 7: Token Prediction Strategy. Comparison of dimension-wise token prediction approaches. Top: Parallel prediction produces blurry, inconsistent images. Bottom: Our autoregressive approach sequentially predicts token dimensions, generating coherent, high-quality images. This highlights the interdependence of token dimensions and they cannot be predicted independently.", "description": "This figure compares two methods for predicting image tokens: parallel prediction and the autoregressive approach. The parallel prediction method predicts all token dimensions independently, leading to blurry and inconsistent image generation.  In contrast, the autoregressive approach presented in the paper sequentially predicts each token dimension, conditioning on previously predicted dimensions.  This sequential process generates much more coherent and high-quality images. The figure highlights that token dimensions are highly interdependent and cannot be accurately predicted in isolation.", "section": "3.2. Efficient Large-Vocabulary Token Modeling"}, {"figure_path": "https://arxiv.org/html/2503.16430/x8.png", "caption": "Figure 8: Generation guided by token confidence. Our discrete token approach enables confidence-guided generation, producing clean foreground objects against simple backgrounds by prioritizing high-confidence tokens. This provides a advantage over continuous tokens, which lack explicit token-level confidence scores.", "description": "This figure demonstrates the advantage of using discrete tokens over continuous tokens in autoregressive image generation.  By leveraging confidence scores associated with each discrete token, the model can prioritize high-confidence tokens, resulting in cleaner foreground objects against simpler backgrounds. This selective generation capability is not available with continuous tokens, which lack explicit confidence scores.  The images showcase the difference in generation quality: using confidence scores leads to more focused images with clear subject matter and less noise in the background, while continuous token approaches cannot offer this level of control.", "section": "4.3. Properties of Our Generator"}, {"figure_path": "https://arxiv.org/html/2503.16430/x9.png", "caption": "Figure 9: Additional image generation results of TokenBridge across different ImageNet\u00a0[6] categories.", "description": "This figure showcases a diverse set of images generated by the TokenBridge model. The images represent various categories from the ImageNet dataset, demonstrating the model's ability to generate high-quality, detailed images across a wide range of visual concepts and styles.", "section": "4. Experiments"}]