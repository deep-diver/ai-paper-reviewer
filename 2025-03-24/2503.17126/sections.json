[{"heading_title": "Diversity Tuning", "details": {"summary": "**Diversity tuning in LLMs** for creative writing is crucial due to the non-unique nature of creative tasks.  LLMs should generate diverse, valid outputs, but post-training often prioritizes quality over diversity, leading to homogenous content.  The research explores **post-training approaches to promote both diversity and quality**, introducing the concept of 'deviation'\u2014the difference between a training sample and others with the same prompt\u2014into the objective function.  This helps the model learn from **rare, high-quality instances**.  Diversified DPO and ORPO are implemented to increase semantic and style diversity with minimal quality decrease. The best model matches human-created diversity with GPT-4 quality. Further analysis includes human evaluation and a comparison with DivPO. The idea is to balance the need to learn from frequent and rare instances."}}, {"heading_title": "Deviation Weighting", "details": {"summary": "**Deviation weighting** emerges as a pivotal technique to diversify LLM outputs in creative tasks. By emphasizing rare, high-quality instances, this method addresses the tendency of post-training to prioritize average quality, inadvertently reducing diversity. The core idea involves incorporating a 'deviation' factor, quantifying how much a sample differs from others with the same prompt, directly into the training objective. This approach, implemented via methods like diversified DPO and ORPO, **prioritizes learning from less frequent but valuable examples**. The use of semantic and style deviations captures multifaceted aspects of diversity. By strategically weighting these deviations, the model is incentivized to explore less common pathways, fostering a richer variety of creative outputs. This results in a more nuanced and expressive generation process, without sacrificing overall quality."}}, {"heading_title": "DPO/ORPO + Div", "details": {"summary": "The paper introduces an innovative strategy that combines **Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO) with a deviation-based diversity metric**. This approach aims to enhance both the quality and diversity of generated content from Large Language Models (LLMs), which is particularly useful in creative writing tasks. The core concept involves integrating the degree of difference (**deviation**) between a training sample and other samples with the same prompt into the training objective. This method facilitates learning from rare, high-quality instances that might otherwise be overshadowed. The diversified DPO and ORPO approaches promote both semantic and stylistic variety in the model's output, ensuring that while the quality is maintained, the range of different valid writings expands to achieve on par diversity as human created content. By using deviation as a weighting factor, **rare and unique instances get greater emphasis during training**, leading to a more balanced and diverse generation process. "}}, {"heading_title": "Human Parity", "details": {"summary": "While the paper doesn't explicitly address 'Human Parity', the evaluation metrics and results suggest an implicit pursuit of it. The authors use human-created dataset as a benchmark, implying a desire to reach parity in quality and diversity. **The models strive to generate creative writing that is indistinguishable from human-authored content in terms of validity, quality and creative diversity.** DDPO based models demonstrated better diversity than existing instruction-tuned models, and human evaluations suggest that DDPO approaches can sometimes even be preferred to GPT-40 in terms of quality, achieving parity or even surpassing it."}}, {"heading_title": "Data Is Needed", "details": {"summary": "The paper emphasizes the **critical role of data** in training and evaluating creative writing generation models. It uses the r/writingPrompts dataset, highlighting the importance of diverse writings for a single prompt to enhance model diversity. **Data filtering** is crucial, with the removal of excessively long and non-creative instances. Transforming score data into paired preferences for DPO/ORPO models is essential, avoiding repetition and prioritizing clear winning/losing responses. Details include training and test set sizes, prompt/response lengths, and the distribution of responses per prompt. The thoroughness in data processing underscores its **direct impact on the model's quality and diversity**."}}]