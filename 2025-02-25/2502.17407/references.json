{"references": [{"fullname_first_author": "Wei", "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.", "publication_date": "2022-01-01", "reason": "Introduces chain-of-thought prompting, a technique that significantly boosts model performance, especially in mathematics."}, {"fullname_first_author": "Yang", "paper_title": "Qwen2.5 Technical Report", "publication_date": "2024-12-15", "reason": "Details the architecture and capabilities of Qwen2.5, a key model used in the study as a baseline and for test-time scaling."}, {"fullname_first_author": "Thoppilan", "paper_title": "LaMDA: Language models for dialog applications.", "publication_date": "2022-01-01", "reason": "Demonstrates that scaling pre-training compute is effective for achieving multilinguality."}, {"fullname_first_author": "Muennighoff", "paper_title": "S1: Simple Test-Time Scaling", "publication_date": "2025-01-19", "reason": "Proposes the budget-forcing method to control longer chain-of-thoughts, is key part of the test-time scaling strategies evaluated."}, {"fullname_first_author": "Wang", "paper_title": "Self-consistency improves chain of thought reasoning in language models.", "publication_date": "2022-03-11", "reason": "Introduces self-consistency, a test-time scaling method that generates multiple responses and selects the best one via voting, contributing to expanding the reasoning capacity of models."}]}