{"importance": "This research offers a new benchmark for assessing code critique in LLMs, which is essential for developing coding assistance and review systems. It contributes to advancing LLMs in coding tasks and fosters further research into the analytical capabilities of LLMs.", "summary": "CodeCriticBench: A new benchmark for holistic code critique by Large Language Models.", "takeaways": ["CodeCriticBench covers code generation and QA, with varying difficulty levels.", "It has both basic and advanced critique evaluations for comprehensive ability analysis.", "LLMs face unique challenges in code-related critique tasks."], "tldr": "**Large Language Models (LLMs)** have demonstrated excellent capabilities across various domains, especially in code generation, however, critique benchmarks often lack comprehensive code related tasks. **Existing critique benchmarks** prioritize general reasoning, overlooking vital aspects of software development such as error identification and code QA. This gap hinders the development of robust LLM-based coding assistance and automated code review systems. To address this, **a comprehensive and specialized evaluation** is needed. \nTo address the limitations, The paper introduces **CodeCriticBench**, a holistic benchmark for code critique by LLMs. It covers code generation and QA with varied difficulty, incorporating basic/advanced evaluations for comprehensive ability analysis. **The findings** reveal that LLMs face unique challenges in code-related critique tasks, marking a significant step towards enhancing LLMs for software development and ensuring thorough evaluation.", "affiliation": "M-A-P", "categories": {"main_category": "AI Theory", "sub_category": "Robustness"}, "podcast_path": "2502.16614/podcast.wav"}