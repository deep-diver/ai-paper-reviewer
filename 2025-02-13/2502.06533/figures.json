[{"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/scratchpad_jean.png", "caption": "Figure 1: Illustration of the addition task with scratchpad, for a model pre-trained on numbers up to 3 digits.\nThe highlighted critical tokens are decision points where the model tends to make mistakes, mainly because it is tempted to process the number as if it were shorter. This occurs when the model is faced with a number that is longer than those encountered during the pre-training stage (here, 4 digits instead of 3).", "description": "Figure 1 illustrates an arithmetic addition task given to a language model.  The model, pre-trained on numbers with up to three digits, is shown attempting to add two four-digit numbers. The process is broken down step-by-step using a 'scratchpad' method, where intermediate calculations are displayed. Key tokens ('critical tokens') in the process, highlighted in the figure, represent decision points where the model is particularly prone to error. These errors stem from the model's tendency to treat the four-digit numbers as if they were shorter, a behavior rooted in its pre-training on shorter numbers.  The figure demonstrates the model's struggle with out-of-distribution inputs and highlights where an improved exploration strategy is needed.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/rl_compare_pretrain.png", "caption": "Figure 2: Model accuracy on addition tasks for models trained on numbers up to digit lengths N=7,9,11,13\ud835\udc41791113N=7,9,11,13italic_N = 7 , 9 , 11 , 13.\nResults are shown for varying digit evaluation. Error bars indicate 95% confidence intervals. Full detailed results are provided in Appendix\u00a0D.1.", "description": "This figure displays the accuracy of models trained on addition tasks with varying input digit lengths.  Four different models were pre-trained on numbers with up to 7, 9, 11, and 13 digits, respectively. The graph then shows their accuracy on addition tasks where the number of digits varies from N+1 to N+3 (N being the number of digits the model was originally trained on). The error bars represent the 95% confidence interval, and more detailed data is available in Appendix D.1.", "section": "Experimental results"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/results_kl_trick_7d.png", "caption": "Figure 3: Learning curves of multiple models pre-trained up to N\ud835\udc41Nitalic_N, fine-tuned with RL on N+2\ud835\udc412N+2italic_N + 2.", "description": "This figure displays the learning curves for multiple language models. Each model was initially pre-trained on an arithmetic task involving numbers up to a certain digit length, denoted as N. Subsequently, these pre-trained models underwent fine-tuning using reinforcement learning (RL) on a similar task, but with numbers having two digits more (N+2). The graph illustrates the models' learning progress over multiple training episodes, demonstrating how their accuracy on the N+2 digit task improves with experience. Comparing the curves of models pre-trained on different N values reveals the impact of the initial pre-training stage on the subsequent RL fine-tuning performance.", "section": "5 Experimental results"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/critical_tokens_probas/ct_s1o2_square.png", "caption": "Figure 4: Top: Learning curves of a model fine-tuned with RL on N+1=8 digits. Bottom: Probability of making the right prediction on two critical tokens. Results on more critical tokens are provided in Appendix\u00a0D.2.", "description": "Figure 4 presents a comparative analysis of two learning curves generated during reinforcement learning (RL) fine-tuning of a language model on an arithmetic addition task. The top panel displays the learning curves, illustrating the model's performance improvement over training episodes.  The bottom panel focuses on the probability of accurate predictions for two specific 'critical tokens' (tokens significantly impacting overall accuracy). These tokens, identified earlier in the study, highlight areas of notable challenge for the model during this RL training process. The figure showcases how focusing on these specific tokens,  particularly during RL fine-tuning, influences overall accuracy.  More detailed analysis on additional critical tokens is available in Appendix D.2.", "section": "Experimental results"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/critical_tokens_probas/ct_s2o2_square.png", "caption": "(a)", "description": "This figure shows two examples of addition tasks where the model failed due to errors on critical tokens.  The model, pretrained on numbers up to 3 digits, makes an error on the penultimate digit in (a), and in (b), pretrained on numbers up to 5 digits, it incorrectly inserts a bracket instead of a comma before copying the sixth digit. The colored tokens represent the model's certainty, with green indicating high certainty and red indicating low certainty. This highlights how crucial some tokens are for correct task completion and how low certainty on these tokens leads to incorrect answers.", "section": "3.3 Critical tokens"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/example_uncertainty_3_5.png", "caption": "(b)", "description": "Figure 5(b) shows an example of a failed addition task where the model, pre-trained on numbers up to 5 digits, makes a mistake during the decomposition stage. Instead of correctly continuing the addition process, it prematurely closes the bracket, leading to an incorrect answer. The color-coding of the generated tokens indicates the model's certainty, with green representing high certainty and red representing low certainty. This highlights the critical tokens that significantly affect the final result.", "section": "3.3 Critical tokens"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/example_uncertainty_7.png", "caption": "(c)", "description": "This figure displays an example of an addition task where the model is given two numbers with 11 and 10 digits, respectively, to add. The model uses a scratchpad to perform the calculation step by step. The color coding of the tokens indicates the model's confidence level, with green indicating high confidence and red indicating low confidence. This example illustrates a scenario where the model makes errors due to low confidence in some critical tokens, which can affect the final result.", "section": "3 Problem formulation"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/example_uncertainty_9.png", "caption": "Figure 5: Output examples for addition tasks on N+1\ud835\udc411N+1italic_N + 1 digit lengths (the model is faced with numbers one notch longer than those encountered in pre-training). Each generated token is colored according to its certainty. A green color is a maximal certainty, while a red color is a minimal certainty.", "description": "This figure displays three examples of the model's output when performing addition tasks involving numbers with one more digit than it was trained on (N+1 digits). Each token generated by the model is color-coded to represent its certainty: green indicates high certainty, and red indicates low certainty.  The examples highlight the model's struggles with longer numbers and showcase the emergence of 'critical tokens' (those with low certainty) which significantly impact the final result.  The scratchpad notation used helps illustrate the step-by-step process of the model's addition attempt.", "section": "3 Problem formulation"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/ablation_study_beta.png", "caption": "Figure 6: Fine-tuning results with various values of \u03b2\ud835\udefd\\betaitalic_\u03b2 (averaged over 9 random seeds)", "description": "This figure displays the fine-tuning results obtained using the prioritized KL penalty with different values of beta (\u03b2).  The y-axis represents the Pass@1 metric, indicating the model's accuracy in achieving the correct answer. The x-axis shows the training steps.  Multiple lines represent the results with varying beta values.  The results are averaged over nine separate runs (random seeds), which helps in estimating the robustness and stability of the model's performance under the different beta values used in the prioritized KL penalty.", "section": "5 Experimental results"}, {"figure_path": "https://arxiv.org/html/2502.06533/extracted/6190467/figures/critical_tokens_probas/ct_s1o1.png", "caption": "Figure 7: Evolution of the right prediction probability on multiple critical tokens, during the RL fine-tuning on number length N+1=8\ud835\udc4118N+1=8italic_N + 1 = 8.", "description": "Figure 7 illustrates the evolution of the probability of making the correct prediction on six different critical tokens during the reinforcement learning (RL) fine-tuning process.  The fine-tuning is performed on an arithmetic addition task, where the input numbers have N+1=8 digits.  The graph plots the probability over the course of the RL training (number of steps), differentiating between the results obtained using the standard KL penalty and the prioritized KL penalty (introduced in the paper).  Each line represents a specific critical token (comma positions in the scratchpad), highlighting how each method affects the model's ability to learn to correctly predict these crucial points during the calculation.", "section": "5.3 Impact of the prioritized KL penalty"}]