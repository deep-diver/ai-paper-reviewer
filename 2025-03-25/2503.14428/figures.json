[{"figure_path": "https://arxiv.org/html/2503.14428/x2.png", "caption": "Figure 1: Overall pipeline for MagicComp. (a) Our MagicComp comprises two core modules: Semantic Anchor Disambiguation (SAD) for resolving inter-subject ambiguity during conditioning, and Dynamic Layout Fusion Attention (DLFA) for spatial-attribute binding via fused layout masks in denoising. (b) MagicComp is a training-free framework, which effectively address the challenges (e.g., semantic confusion, misaligned spatial relationship, missing entities) in compositional video generation with minimal additional inference overhead.", "description": "Figure 1 illustrates the MagicComp framework's architecture and capabilities.  Panel (a) details the two main modules: Semantic Anchor Disambiguation (SAD), which clarifies ambiguous relationships between subjects in the input text prompt, and Dynamic Layout Fusion Attention (DLFA), which precisely binds visual attributes and locations to the subjects during video generation. Panel (b) highlights the training-free nature of MagicComp and its effectiveness in overcoming common challenges in compositional video generation\u2014semantic confusion, spatial misalignment, and missing entities\u2014while maintaining efficiency.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.14428/x3.png", "caption": "Figure 2: Detailed architecture of MagicComp. The dual-phase refinement strategy of MagicComp contains two core steps: (a) Semantic Anchor Disambiguation (SAD) module for inter-subject disambiguation during the conditioning stage. We only display disambiguation process of subject \u201ccat\" for simplicity, other subjects follow the similar way. (b) Dynamic Layout Fusion Attention (DLFA) module for precise attribute\u2013location binding of each subject during the denoising stage.", "description": "MagicComp's dual-phase refinement process is illustrated.  The first phase, Semantic Anchor Disambiguation (SAD), resolves ambiguity between subjects in the text prompt by independently processing each subject's semantics (using the example of 'cat' for simplicity) to generate anchor embeddings. These embeddings are then used to adjust the initial text embeddings, reducing ambiguity before the generation process begins. The second phase, Dynamic Layout Fusion Attention (DLFA), precisely binds attributes and locations to each subject during the denoising phase using both pre-defined and model-learned spatial masks. This ensures accurate placement of objects in the generated video.", "section": "3. Method"}, {"figure_path": "https://arxiv.org/html/2503.14428/x4.png", "caption": "Figure 3: Visualization of the disambiguation effect brought by SAD. (a) Cos similarity between the pooled embeddings of \u201cbrown dog\" and \u201cgray cat\" under different settings. \u201cStandard\" indicates the cos similarity is computed when each subject are independently encoded by T5. (b) Cross attention maps between the middle frame video tokens and the pooled subject tokens.", "description": "This figure demonstrates the effectiveness of Semantic Anchor Disambiguation (SAD) in resolving semantic ambiguity.  Part (a) shows a comparison of cosine similarity scores between the embeddings of \"brown dog\" and \"gray cat\" under three conditions: without SAD, with SAD, and with a standard T5 encoding (no SAD).  This illustrates how SAD reduces the similarity between the two subjects, improving the clarity of their distinct semantic representations. Part (b) visualizes cross-attention maps between video tokens from the middle frame and pooled subject tokens. This demonstrates how SAD leads to more distinct attention patterns for \"brown dog\" and \"gray cat,\" preventing semantic confusion during the generation process.", "section": "3.2. Semantic Anchor Disambiguation"}, {"figure_path": "https://arxiv.org/html/2503.14428/x5.png", "caption": "Figure 4: Comparison of different masking strategy. (a) Visualization of prior layout mask and model-adaptive perception layout. (b) Comparison of the generated videos.", "description": "This figure compares different masking strategies used in the Dynamic Layout Fusion Attention (DLFA) module.  Panel (a) visualizes the prior layout mask generated by a large language model (LLM) alongside the model-adaptive perception layout mask. The LLM mask provides a coarse initial estimation of subject locations, while the adaptive mask refines these locations based on the video content and its correlation with the text. Panel (b) then shows the resulting videos generated using each masking method. This comparison highlights how the combination of prior and adaptive masks improves the accuracy and detail in the generated subject-specific regions compared to using only the prior LLM-generated masks.", "section": "3.3 Dynamic Layout Fusion Attention"}, {"figure_path": "https://arxiv.org/html/2503.14428/x6.png", "caption": "Figure 5: Qualitative Comparison on T2V-CompBench. Our MagicComp significantly outperforms existing approaches across various compositional generation tasks, and the methods such as Vico [47] and CogVideoX [48] struggle to capture fine-grained concepts.", "description": "Figure 5 presents a qualitative comparison of different text-to-video (T2V) models on the T2V-CompBench benchmark.  The figure showcases the results of various models on six compositional generation tasks: consistent attribute binding, spatial relationships, motion, action, interaction, and numeracy. Each task is represented with examples of generated video frames using different models, demonstrating MagicComp's superior ability to generate videos that accurately reflect the complex relationships and interactions specified in the textual prompts. In contrast, existing methods like Vico and CogVideoX often fail to correctly capture the fine-grained details and interactions described in the prompts, highlighting MagicComp's advancement in handling complex compositional scenarios.", "section": "4.3. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2503.14428/x7.png", "caption": "Figure 6: Application on complex prompt-based video generation. It is evident that among all models, only MagicComp strictly follows the prompt to generate complex scenarios.", "description": "Figure 6 presents a comparison of video generation results across different models in response to a complex prompt.  The prompt describes a scene with multiple interacting elements and specific attributes (a skeleton pirate, a ghost ship, a dark sea, glowing lanterns, and a treasure map). The figure visually demonstrates that, unlike other models, MagicComp accurately generates the video according to the complex prompt's specifications.", "section": "4.5 Applications"}, {"figure_path": "https://arxiv.org/html/2503.14428/x8.png", "caption": "Figure 7: Application about trajectory-controllable video generation. By incorporating the proposed methods, CogVideoX [48] can achieve trajectory control seamlessly without additional cost.", "description": "Figure 7 demonstrates the application of MagicComp to achieve trajectory-controllable video generation.  The figure showcases that by integrating MagicComp into CogVideoX [48], the model gains the ability to control the trajectory of objects in generated videos without requiring any extra computational cost or additional training. This highlights the efficiency and practical value of MagicComp in enhancing video generation capabilities beyond standard compositional tasks.", "section": "4.5. Applications"}, {"figure_path": "https://arxiv.org/html/2503.14428/x9.png", "caption": "Figure 1: Instruction prompt for prior layout generation.", "description": "This figure shows instructions on how to create a YAML file for describing the layout of video generation.  It details the process of annotating key subjects in video captions and planning their motion paths using bounding boxes.  Specific requirements are provided, including normalization of bounding boxes, consistent movement, attribute binding, and handling of multiple objects.  An example YAML file is given to illustrate the expected format.", "section": "4.1. Experimental Setups"}, {"figure_path": "https://arxiv.org/html/2503.14428/x10.png", "caption": "Figure 2: Qualitative results on Consist-attr.", "description": "This figure displays a qualitative comparison of different models' performance on the 'Consistent Attributes' task.  It shows generated video frames from five different models (MagicComp, CogVideoX-2B, Vico, VideoTetris, and Open-Sora-Plan). Two scenarios are presented: 'Big hearts and small stars floating upwards' and 'Oblong canoe gliding past a circular buoy'. The comparison highlights MagicComp's superior ability to maintain consistent attributes throughout the generated video sequence, compared to other models that struggle with this task.", "section": "4. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2503.14428/x11.png", "caption": "Figure 3: Qualitative results on Consist-attr.", "description": "This figure displays a qualitative comparison of different models' performance on the 'Consistent Attributes' task from the T2V-CompBench benchmark.  It showcases the results of generating videos based on prompts describing objects with specific attributes. Each row represents a different model (MagicComp, CogVideoX-2B, Vico, VideoTetris, and Open-Sora-Plan), and each column displays a sequence of frames from the generated videos for two different prompts.  The prompts used are: \"Star-shaped cookie resting on a round coaster\" and \"Green tractor plowing near a white farmhouse\".  The figure demonstrates MagicComp's superior ability to maintain consistent visual attributes (shape and color) of objects throughout the generated video sequences compared to other methods.", "section": "4. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2503.14428/x12.png", "caption": "Figure 4: Qualitative results on Motion.", "description": "This figure displays qualitative results focusing on the 'Motion' aspect of compositional video generation.  It presents a visual comparison of how different models, including MagicComp and several baselines, handle the generation of videos involving movement and dynamic interactions.  By comparing the video frames generated by each model for the same prompts, the figure helps illustrate the effectiveness and accuracy of the models in capturing motion accurately and consistently.", "section": "4.3. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2503.14428/x13.png", "caption": "Figure 5: Qualitative results on Action & Motion.", "description": "This figure displays qualitative results comparing different models' performance on actions and motion in video generation.  Each row shows the output of a different model (MagicComp, CogVideoX-2B, Vico, VideoTeris, Open-Sora-Plan) for two example prompts: a sheep walking on grass with a hot air balloon above and a boat sailing on the ocean. The image sequence for each model shows how well each model's output captures the specified actions and motions in the prompts. The goal is to illustrate how MagicComp excels in generating videos that accurately reflect the dynamic aspects of action and motion compared to existing methods.", "section": "4.3. Qualitative Comparisons"}, {"figure_path": "https://arxiv.org/html/2503.14428/x14.png", "caption": "Figure 6: Qualitative results on Numeracy.", "description": "This figure showcases a qualitative comparison of various models' performance on numeracy tasks within compositional video generation.  It presents two example prompts: \"two elephants and five buckets in a zoo\" and \"seven bees buzz around a blooming flower bed\". For each prompt, it visually displays the generated video frames produced by different models, allowing for a direct comparison of their ability to accurately represent numerical quantities within a scene. The goal is to highlight the relative strengths and weaknesses of each model in handling numerical concepts during compositional video generation.", "section": "4. Qualitative Comparisons"}]