{"importance": "This paper pioneers a **mechanistic understanding of reasoning in LLMs** by causally linking interpretable components to complex cognitive behaviors, potentially revolutionizing how we design and control AI reasoning.", "summary": "LLMs' reasoning is decoded via sparse autoencoders, revealing key features that, when steered, enhance performance. First mechanistic account of reasoning in LLMs!", "takeaways": ["Sparse Autoencoders (SAEs) can identify reasoning-specific features in LLMs, linking linguistic patterns to cognitive processes.", "ReasonScore effectively identifies SAE features responsible for reasoning, confirmed via interpretability techniques.", "Steering experiments causally demonstrate that amplifying identified features induces reasoning behavior in LLMs."], "tldr": "Large Language Models (LLMs) have shown great success, but their reasoning remains unexplored. This study uses **Sparse Autoencoders (SAEs)** to understand reasoning features in DeepSeek-R1 models.  The method identifies features driving reasoning by learning sparse decompositions of latent representations. It introduces a way to extract candidate \"reasoning features\" from SAE, validated via analysis and interpretability methods, showing a direct link to reasoning. (499 char.)\n\nThe study introduces **ReasonScore**, an evaluation metric for detecting reasoning features from the SAE representations. Features are validated through interpretability and causal interventions, enhancing reasoning by steering these features, showcasing their impact. Systematic feature steering improves reasoning performance, marking the first mechanistic account of reasoning in LLMs. (488 char.)", "affiliation": "AIRI", "categories": {"main_category": "AI Theory", "sub_category": "Interpretability"}, "podcast_path": "2503.18878/podcast.wav"}