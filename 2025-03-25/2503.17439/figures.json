[{"figure_path": "https://arxiv.org/html/2503.17439/x3.png", "caption": "Figure 1: Overview of LEMMA.", "description": "The figure illustrates the LEMMA framework, which enhances LLMs' mathematical reasoning abilities by learning from errors.  It shows how LEMMA constructs a dataset of incorrect solutions paired with their corresponding corrections.  These pairs consist of an incorrect solution with an erroneous step and a reflection connecting it to the correct solution. The process involves systematically analyzing model-generated errors, augmenting errors using a model-aware approach, and then creating paired correct solutions either by directly fixing the errors or starting anew.  The resulting data is used to fine-tune the LLM, enabling it to self-correct errors autonomously during inference.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.17439/x4.png", "caption": "Figure 2: The LEMMA framework. LEMMA uses an error-type grounded mistake augmentation module, and explores two error correction strategies to construct the incorrect-correct revision trajectory as training corpus.", "description": "The LEMMA framework is shown in Figure 2.  It consists of three main components: 1) an error-type grounded mistake augmentation module that generates diverse and representative errors for a variety of mathematical reasoning problems; 2) two error correction strategies, \"Fix & Continue\" and \"Fresh & Restart\", which correct the errors through different approaches; and 3) a training corpus of paired incorrect and correct solutions that are seamlessly connected through model-aware reflection links. These links, annotations that explain the error's origin and justify its correction, provide coherence to the training data, ultimately improving LLMs' ability to reflect on and self-correct their mathematical reasoning processes.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2503.17439/x5.png", "caption": "(a) LLaMA3-8B", "description": "The figure shows the distribution of different error types in the LLaMA3-8B model.  The chart visually represents the percentage of each error category (like Calculation Error, Question Misinterpretation Error, etc.) found in the model's mathematical reasoning outputs.  This allows for a comparison of the relative frequency of different error types within the model's responses.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x6.png", "caption": "(b) Mistral-7B", "description": "This figure shows the distribution of error types for the Mistral-7B model when solving mathematical problems.  The chart visually represents the percentage of each error type in a dataset of model-generated solutions. The different error types (such as Question Misinterpretation (QM), Calculation Error (CA), etc.) are displayed as segments within a circle, with the size of each segment corresponding to its relative frequency. This provides a visual breakdown of the most common types of mistakes made by this model, offering insights into its strengths and weaknesses in mathematical reasoning.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x7.png", "caption": "(c) DeepSeekMath-7B", "description": "This figure shows the distribution of different error types made by the DeepSeekMath-7B model on the MATH dataset.  The error types are categorized using a taxonomy established in the paper, including categories like Question Misinterpretation, Formula Confusion, Calculation Errors, and others. The chart visually represents the proportion of each error type within the model's output, offering insights into the model's strengths and weaknesses in mathematical reasoning.  This allows for targeted improvements and focused data augmentation strategies.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x8.png", "caption": "(d) Qwen2-Math-7B", "description": "This figure shows the distribution of different error types made by the Qwen2-Math-7B model when solving mathematical problems.  The figure displays the percentage of each error type relative to the total number of errors.  Error types include Question Misinterpretation, Formula Confusion, Calculation Errors, Missing Steps, Confusing Concepts, and Nonsensical Output. This visualization helps to understand the common mistakes made by this specific language model, which is useful for developing techniques to improve its performance and to design targeted training data.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x9.png", "caption": "Figure 3: Error type distribution of the different models on the MATH test set, with GPT-4o as the error classification model. Error types that account for less than 1% are omitted to avoid text overlap.", "description": "This figure presents a comparison of error types across four different large language models (LLMs) when solving mathematical problems from the MATH dataset.  The error types are categorized into six main groups (Question Misinterpretation, Formula Confusion, Calculation, Counting, Missing Steps, and Confusing Concepts) using GPT-4 as a classifier.  The bar chart visually shows the proportion of each error type for each LLM, providing insights into the relative strengths and weaknesses of the models in different aspects of mathematical reasoning.  Error types with less than 1% prevalence are excluded for clarity.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x10.png", "caption": "Figure 3: (a) Error type distribution on MATH.", "description": "This figure shows the distribution of different error types made by language models when solving mathematical problems in the MATH dataset. The error types are categorized and their frequencies are displayed. This helps to understand the types of mistakes the models are most prone to make, informing the design of strategies to improve their mathematical reasoning capabilities.  The x-axis represents the different error types while the y-axis represents the count of errors.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x11.png", "caption": "Figure 4: The shortcoming of sampling erroneous trajectories via increasing temerature: Using higher temperatue produces a substantial amount of nonsensical text, which is not observed in normal generation. Solutions are generated by LLaMA3-8B.", "description": "This figure demonstrates the negative impact of using higher sampling temperatures to generate erroneous reasoning trajectories for training LLMs.  Increasing the temperature significantly increases the number of nonsensical or incoherent responses, making them unsuitable for training. The figure compares examples generated at lower versus higher temperatures, highlighting the substantial difference in the quality of generated text.  These results emphasize the importance of using alternative methods to create training data containing meaningful errors.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x12.png", "caption": "(a) Pass@1 on MATH.", "description": "The figure shows a graph plotting the Pass@1 metric on the MATH dataset against different data sizes.  The x-axis represents the size of the training data in multiples of 10,000, and the y-axis shows the Pass@1 score, which is a metric used to evaluate model performance. The graph compares the performance of several different methods including LEMMA, SFT, RefAug, RefAug-90k, and ISC.  The graph demonstrates LEMMA's consistent performance and improvement as the data size increases, while other methods may plateau or even show a decrease in performance.", "section": "4.1 Implementation Details"}, {"figure_path": "https://arxiv.org/html/2503.17439/x13.png", "caption": "(b) Pass@1 on Mathematics.", "description": "The figure shows a graph plotting the Pass@1 accuracy on the Mathematics dataset against the size of training data. This illustrates how the model's performance improves as the amount of training data increases.  It is part of an experimental evaluation to show the effects of different training data sizes on model accuracy.", "section": "4.1 Implementation Details"}, {"figure_path": "https://arxiv.org/html/2503.17439/x14.png", "caption": "Figure 5: Performance comparison with varying data size on LLaMA3-8B. LEMMA consistently demonstrates robust performance improvements in both in-distribution and out-of-distribution tasks, while baseline methods (e.g., ISC and RefAug) tend to plateau or even decline on out-of-distribution datasets.", "description": "This figure displays the performance of LEMMA and several baseline methods on in-distribution and out-of-distribution datasets at different training data sizes.  The results clearly indicate that LEMMA's performance shows consistent improvement as the training data size increases, maintaining a significant lead over the baseline methods.  Conversely, several of the baseline methods show performance plateaus or even declines in accuracy on the out-of-distribution datasets, highlighting LEMMA's superior generalization capabilities.", "section": "Experiments"}, {"figure_path": "https://arxiv.org/html/2503.17439/x15.png", "caption": "(a) LLaMA3-MATH", "description": "This figure shows the distribution of different error types made by the LLaMA3 model on the MATH dataset.  The chart visually represents the frequency of various error categories, such as Calculation Errors (CA), Question Misinterpretation Errors (QM), Formula Confusion Errors (FC), Missing Steps (MS), and others.  It provides a quantitative overview of the model's performance shortcomings, highlighting the areas where the model most frequently makes mistakes during mathematical problem-solving.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x16.png", "caption": "(b) DeepSeekMath-MATH", "description": "Figure 3(b) shows the distribution of different error types in the MATH dataset, specifically for the DeepSeekMath-7B model.  The figure is a bar chart visualizing the frequency of each error type as categorized in Table 1 of the paper.  The error categories include Question Misinterpretation, Formula Confusion, Calculation, Missing Step, and Confusing Concept errors.  This data highlights the types of mistakes that the DeepSeekMath model frequently makes while solving mathematical problems, providing insights into the model's weaknesses.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x17.png", "caption": "Figure 6: Error type changes after fine-tuning. LEMMA consistently decreases the prevalence of all types of errors, while SFT results in an increase of specific error.", "description": "This figure displays the distribution of different error types before and after fine-tuning using two methods: LEMMA and standard Supervised Fine-Tuning (SFT).  It shows that before fine-tuning, various error types are present at different rates.  After fine-tuning with LEMMA, the frequency of all error types is consistently reduced.  In contrast, SFT improves overall accuracy, but leads to an increase in certain specific error types. This illustrates that LEMMA's structured approach to learning from errors effectively mitigates various errors, while SFT may cause an increase in specific error types despite improving overall accuracy. The graphs use bar charts to visually compare the error type distributions in the before, SFT, and LEMMA scenarios for two different datasets.", "section": "5.2 Analysis on Error Type after Fine-tuning"}, {"figure_path": "https://arxiv.org/html/2503.17439/x18.png", "caption": "(a) LLaMA3-8B", "description": "This figure shows the distribution of different error types in model-generated solutions for the LLaMA3-8B model.  It displays a breakdown of the percentage of errors attributed to different categories, providing insights into the types of mistakes the model is most prone to making. This is a key finding used in the error analysis section of the paper to understand model-generated errors.", "section": "3.2 Error Analysis"}, {"figure_path": "https://arxiv.org/html/2503.17439/x19.png", "caption": "(b) Mistral-7B", "description": "Figure 3 presents a breakdown of error types identified in various LLMs while solving mathematical problems.  Panel (b) specifically shows the distribution of these error types within the Mistral-7B language model.  It visually represents the percentage of each error category (such as Question Misinterpretation, Formula Confusion, Calculation Error, etc.) that the Mistral-7B model made. This allows for a comparison of error types across different models, helping to understand the strengths and weaknesses of each model in mathematical reasoning.", "section": "3.2 Error Analysis"}]