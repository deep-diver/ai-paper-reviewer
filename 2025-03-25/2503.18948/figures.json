[{"figure_path": "https://arxiv.org/html/2503.18948/x1.png", "caption": "Figure 1: Illustration of Equivariant Image Generation Framework. The tokenizer translates the image into 1D tokens arranged in columns and an enhanced autoregressive model models the column-wise token distribution.", "description": "This figure illustrates the Equivariant Image Generation Framework, which consists of two main stages: tokenization and modeling.  In the tokenization stage, a 1D tokenizer processes the input image, converting it into a sequence of 1D tokens arranged in columns. This is different from traditional methods that use 2D patches.  Each token represents a vertical slice of image features. The modeling stage uses an enhanced autoregressive model to capture the relationships between these column-wise tokens, thereby predicting the image. This novel approach leverages the translational invariance of natural images, aligning optimization targets across sub-tasks to improve generation quality and efficiency.", "section": "3. Towards Equivariant Autoregressive Image Generation"}, {"figure_path": "https://arxiv.org/html/2503.18948/x2.png", "caption": "Figure 2: Visual Meanings of 1D Tokens. By progressively replacing the randomly initialized token sequence with tokens encoded from the ground truth images, the decoder faithfully reconstructs the original images step by step.", "description": "This figure demonstrates the process of reconstructing an image from 1D token sequences. Starting with a sequence of randomly initialized tokens, the model progressively replaces them with tokens encoded from the ground truth image.  Each step in this replacement process results in a progressively more accurate reconstruction of the original image, visually showcasing how the tokens carry information that contributes to the reconstruction. The figure visually shows how the model successfully recovers the image, confirming the effectiveness of the proposed column-wise tokenization and the overall image generation process.  It highlights that each token encodes a relevant part of the image, and together they represent the whole image.", "section": "3.1 Equivariant Tokenization via Columnization"}, {"figure_path": "https://arxiv.org/html/2503.18948/x3.png", "caption": "Figure 3: Training Loss of Different Models. Left: the training loss of different methods at early (10 epoches) and late (100 epoches) training stage. Right: the relative loss improvement of different methods under different settings compared to the early stage of Multi-task setting. The higher value indicates better performance. The equivariant generation approach can transfer the improvement from a single task to other untrained tasks.", "description": "This figure displays the training loss curves for different image generation models. The left panel shows the absolute training loss at an early stage (10 epochs) and a later stage (100 epochs) of training.  The right panel displays the *relative* improvement in training loss at the later stage compared to the loss at the early stage.  Multiple training scenarios are included, such as the multi-task setting (training on all tasks), and single-task settings (training on only selected tasks).  The comparison highlights how the proposed equivariant generation model achieves better parameter sharing and generalization, leading to consistent performance improvements across subtasks, unlike traditional 2D models where performance improvements in some tasks don't transfer to others.", "section": "4.2. Deep Analysis on Equivariance"}, {"figure_path": "https://arxiv.org/html/2503.18948/x4.png", "caption": "Figure 4: Converged Training Loss on ImageNet vs LHQ. Compared to ImageNet, the visual statics in LHQ demonstrates greater uniformity, as does the task-wise loss distribution.", "description": "Figure 4 compares the converged training loss for different subtasks in the ImageNet and LHQ datasets.  The ImageNet dataset shows a significant difference in the difficulty of different subtasks, with some subtasks consistently exhibiting higher loss than others. This uneven distribution across subtasks is likely due to the inherent bias in the ImageNet dataset towards objects that are centrally located and well-defined. In contrast, the LHQ dataset shows a much more uniform loss distribution across all subtasks, likely due to its higher degree of spatial uniformity. The figure suggests that the inconsistencies in the ImageNet data significantly impact the model's training process.", "section": "4.2. Deep Analysis on Equivariance"}, {"figure_path": "https://arxiv.org/html/2503.18948/x5.png", "caption": "Figure 5: Visual examples of long image generation. We present visual examples of long images with arbitrary lengths, which are generated by our model that has been trained on the Places datasets with fixed length of 256.", "description": "This figure showcases the model's ability to generate images significantly longer than those it was trained on.  The model was trained using images of a fixed 256-pixel length from the Places dataset.  Despite this, the generated examples demonstrate the ability to produce images of arbitrary lengths, maintaining visual coherence and detail across the extended spatial range. This highlights the model's generalization capability and robustness.", "section": "4.3. Equivariance Application"}, {"figure_path": "https://arxiv.org/html/2503.18948/x6.png", "caption": "Figure 6: Interactive Image Generation. During inference, each token is immediately visible and bad generated tokens (circled with orange rectangle) are dropped according to human feedback.", "description": "This figure demonstrates the interactive nature of the image generation process.  Each token, representing a vertical strip of the image, is generated and displayed individually.  The user can then review the generated image and identify any unsatisfactory tokens (highlighted in orange rectangles). These problematic tokens are then discarded, and the model regenerates them, allowing for human-guided refinement of the output image. This process iterates until the user is satisfied with the generated image.", "section": "4.3. Equivariance Application"}, {"figure_path": "https://arxiv.org/html/2503.18948/x7.png", "caption": "(a) w/o random shift", "description": "This figure is a visualization showing the effect of augmented position embedding on image generation.  The \"(a) w/o random shift\" indicates that this specific image was generated without applying random shifts to the position indices during training. This augmentation helps the model better learn consistent relationships across different spatial locations, improving the quality and coherence of generated images, especially those that extend beyond the length of sequences seen during training.", "section": "4.5 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.18948/x8.png", "caption": "(b) with random shift", "description": "This figure visualizes the impact of the augmented position embedding technique on the model's ability to generate coherent and continuous images. The left panel (a) shows results without random position shifts, while the right panel (b) demonstrates the results with random position shifts applied. The use of random position shifts helps to reduce the effect of fixed positions in the model and enhances its generation performance, especially for longer image sequences.", "section": "4.5 Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.18948/x9.png", "caption": "Figure 7: Visualization effect of the Augmented Position Embedding. The model is trained on a subset of the Places dataset.", "description": "This figure compares the results of using a standard positional embedding technique versus an augmented positional embedding technique in a transformer-based image generation model. The model was trained on a subset of the Places dataset. The augmented approach enhances the model's ability to generate images with consistent visual features across spatial positions.", "section": "4.5. Ablation Studies"}, {"figure_path": "https://arxiv.org/html/2503.18948/x10.png", "caption": "Figure 8: Additional Examples about Visual Meanings of 1D Tokens.", "description": "This figure provides additional examples to illustrate how the column-wise tokenization approach preserves the vertical equivariance of natural images.  Each image shows the progressive reconstruction of an image from its 1D token representation. The reconstruction starts with a randomly initialized sequence of tokens and progressively replaces those random tokens with tokens generated from the ground truth image.  This demonstrates how the model reconstructs the image from a column-wise representation, highlighting the effectiveness of the proposed 1D tokenization method in preserving spatial information.", "section": "3.1 Equivariant Tokenization via Columnization"}, {"figure_path": "https://arxiv.org/html/2503.18948/x11.png", "caption": "Figure 9: Visualization of the generation process.", "description": "This figure visualizes the image generation process step by step. Starting from randomly initialized tokens, it progressively replaces these tokens with tokens encoded from the ground truth image. This allows us to see how the decoder faithfully reconstructs the original image, step by step, as the correct token sequences are fed into it. It demonstrates the continuous transition and evolution of the generated image.", "section": "4. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.18948/x12.png", "caption": "Figure 10: Generation Results on the ImageNet-1k Dataset.", "description": "This figure displays a diverse array of images generated by the model trained on the ImageNet-1k dataset.  Each image represents a different object category from the dataset, showcasing the model's ability to generate various objects with high fidelity and detail.  The figure demonstrates the model's capacity for class-conditional image generation, highlighting the quality and diversity of the generated samples.", "section": "4. Experiments"}]