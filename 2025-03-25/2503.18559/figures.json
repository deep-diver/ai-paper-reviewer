[{"figure_path": "https://arxiv.org/html/2503.18559/x1.png", "caption": "Figure 1:  Illustration of the proposed data processing pipeline, which includes video quality assessment, motion filtering, and prompt re-captioning using large language models to improve training data quality.", "description": "This figure illustrates the data processing pipeline used to enhance the quality of the training data for the text-to-video model. The pipeline consists of three main stages: 1) Video Quality Assessment: This stage uses a VQA model to evaluate the quality of videos based on metrics like aesthetics and compression, discarding low-quality videos.  2) Motion Filtering: This stage employs a motion estimation module to filter out videos with undesirable motion characteristics (e.g., dolly zoom effects). This filtering focuses the training data on videos with more natural and consistent movements, enhancing the model's understanding of typical video characteristics. 3) Prompt Re-captioning: In this stage, large language models (LLMs) are utilized to rephrase or re-caption the text prompts associated with each video.  The goal is to refine the prompts to be more accurate, consistent, and effective in guiding the video generation process. The improved data from all these stages are used to feed into the training of the T2V model, improving overall visual quality and model performance.", "section": "3.1 Data Processing Pipeline"}, {"figure_path": "https://arxiv.org/html/2503.18559/x2.png", "caption": "Figure 2: Illustration of the proposed two-stage T2V diffusion model distillation pipeline. The first stage prunes the model\u2019s parameters to improve efficiency, while the second stage enhances visual quality through feedback learning.", "description": "This figure illustrates the two-stage pipeline used to create the efficient text-to-video (T2V) model, called Hummingbird.  The first stage focuses on model pruning, reducing the number of parameters in the U-Net from 1.4 billion to 0.7 billion to improve efficiency. This is done by removing blocks and layers from the original model, followed by fine-tuning using the outputs from the original model. The second stage uses visual feedback learning to enhance the visual quality of the video output after pruning. This feedback mechanism refines the model's ability to generate visually appealing videos with improved details and overall quality.  The visual feedback learning utilizes a combination of reward signals based on image-text and video-text features, ensuring both visual fidelity and temporal coherence.", "section": "3 Method"}]