[{"figure_path": "https://arxiv.org/html/2503.18033/x1.png", "caption": "Figure 1: Overview of OmnimatteZero-From left to right: The video and its corresponding object-masked version are encoded into the latent space. The spatio-temporal footprint of the target object is then identified and extracted from this encoding. Self-attention maps are leveraged to recognize effects associated with the object (such as a cat\u2019s reflection), which are incorporated into both the object mask and the latent encoding. Using this mask, we apply two imperfect video inpainting methods: (a) Background-Preserving Inpainting, which retains background details but may leave behind traces of the object, and (b) Object-Removing Inpainting, which eliminates the object but might introduce background distortions. We refine the results through Attention-based Latent Blending, selectively combining the strengths of each inpainting method. Finally, few denoising steps of the video diffusion model enhances the blended output, producing a high-quality reconstruction with the object removed and the background well-preserved, as indicated by high PSNR values.", "description": "OmnimatteZero processes video by first encoding it and its corresponding mask into latent space.  The object's spatiotemporal footprint is identified and extracted from the encoding, using self-attention maps to incorporate associated effects (like reflections). Two inpainting methods are applied: one preserving background details (potentially leaving object traces), and one removing the object (possibly distorting the background).  These results are blended using attention-based latent blending. Finally, denoising steps in a video diffusion model refine the result, yielding a high-quality reconstruction with the object removed and background preserved (high PSNR).", "section": "4. Method: OmnimatteZero"}, {"figure_path": "https://arxiv.org/html/2503.18033/extracted/6302754/figures/method_self_attn_maps.png", "caption": "Figure 2: Self-attention maps from (a) LTX Video diffusion model and (b) Image based Stable Diffusion. The spatio-temporal video latent \u201dattends to object associated effects\u201d (e.g., shadow, reflection) where, image models struggles to capture these associations.", "description": "This figure demonstrates the key difference in how video diffusion models and image diffusion models process and understand contextual information, specifically focusing on \"object-associated effects\".  Panel (a) shows self-attention maps from the LTX Video diffusion model; these maps highlight the model's ability to connect an object not only to its immediate visual features but also to related contextual elements like shadows and reflections. This demonstrates the model's comprehension of spatio-temporal relationships. In contrast, panel (b) presents self-attention maps from a Stable Diffusion image model, showcasing its inability to make these same object-context connections. The image model focuses only on the object itself, lacking the understanding of the surrounding effects crucial for realistic video editing. This difference underscores the challenges of directly applying image inpainting techniques to video, motivating the development of a dedicated video-centric approach.", "section": "3. Motivation: The failure of image inpainting approaches for videos"}, {"figure_path": "https://arxiv.org/html/2503.18033/extracted/6302754/figures/method_extr_comp.png", "caption": "Figure 3: (a) Foreground Extraction: The target object is extracted by latent code arithmetic, subtracting the background video encoding from the object+background latent (Latent Diff). This initially results in distortions, which are later corrected by replacing pixel values using the original video and a user-provided mask (Latent Diff + Refinement).\n(b) Layer Composition: The extracted object layer is added to a new background latent (Latent Addition). To improve blending, a few steps of SDEdit are applied, yielding a more natural integration of the object into the new scene (Latent Addition + SDEdit).", "description": "Figure 3 illustrates the process of foreground extraction and layer composition using latent video representations. (a) Foreground Extraction: First, the object's latent representation is obtained by subtracting the background's latent representation from the combined latent representation of the object and background. This produces an initial object extraction, but with some distortions. These distortions are corrected by replacing the distorted pixel values with those from the original video based on a mask, leading to a refined object layer. (b) Layer Composition: The refined object layer is added to the latent representation of a new background. To enhance the seamless blending of the object into the new background, several steps of SDEdit are performed, producing a more natural-looking result.", "section": "4. Method: OmnimatteZero"}, {"figure_path": "https://arxiv.org/html/2503.18033/x2.png", "caption": "Figure 4: Qualitative Results: Object removal and background reconstruction. The first row shows input video frames with object masks, while the second row presents the reconstructed backgrounds. Our approach effectively removes objects while preserving fine details, reflections, and textures, demonstrating robustness across diverse scenes. Notice the removal of the cat\u2019s reflection in the mirror and water, the shadow of the dog and bicycle (with the rider), and the bending of the trampoline when removing the jumpers.", "description": "This figure showcases qualitative results of object removal and background reconstruction.  The top row displays input video frames overlaid with object masks, highlighting the areas targeted for removal. The bottom row shows the resulting reconstructed backgrounds after applying the OmnimatteZero method.  The results demonstrate the method's effectiveness in removing objects while preserving fine details such as reflections, shadows, and textures, even across diverse scenes.  Specific examples highlighted include the successful removal of a cat's reflection in a mirror and water, a dog's shadow, and the subtle bending of a trampoline caused by jumpers, all of which are faithfully maintained in the reconstructed background.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2503.18033/extracted/6302754/figures/qualitative_comp.png", "caption": "Figure 5: Qualitative Comparison: Object removal and background reconstruction. Our approach achieves cleaner background reconstructions with fewer artifacts while Generative Omnimatte [15] leaves some residuals, DiffuEraser [16] and ProPainter [36] struggle with noticeable traces (highlighted in red). The last two rows show that OmnimatteZero successfully removes effects where others fail.", "description": "Figure 5 presents a qualitative comparison of OmnimatteZero's object removal and background reconstruction capabilities against three state-of-the-art methods: Generative Omnimatte, DiffuEraser, and ProPainter.  The figure showcases several examples where input videos (with object masks) are processed by each method.  OmnimatteZero consistently produces cleaner background reconstructions with fewer artifacts than the other methods. Generative Omnimatte leaves some residual traces, while DiffuEraser and ProPainter struggle to completely remove objects and leave noticeable artifacts (highlighted in red). Notably, the bottom two rows highlight OmnimatteZero's superior ability to remove not only objects but also associated effects such as shadows and reflections, which the other methods fail to achieve.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2503.18033/extracted/6302754/figures/qualitative_foreground.png", "caption": "Figure 6: Qualitative Comparison: Foreground Extraction. Foreground extraction comparison between OmnimatteZero and OmnimatteRF [17]. Our method accurately captures both the object and its associated effects, such as shadows and reflections, in contrast to OmnimatteRF, often missing or distorting shadows (row 2) and reflections (row 3).", "description": "This figure compares the foreground extraction capabilities of OmnimatteZero and OmnimatteRF.  It demonstrates that OmnimatteZero more accurately captures objects and their associated effects (shadows and reflections). OmnimatteRF, in contrast, frequently misses or distorts these effects, as shown in rows 2 and 3 of the figure which highlight examples of shadow and reflection inaccuracies.", "section": "5. Results"}, {"figure_path": "https://arxiv.org/html/2503.18033/extracted/6302754/figures/qualitative_insertion.png", "caption": "Figure 7: Qualitative Comparison: Layer Composition. The extracted foreground objects, along with their shadows and reflections, are seamlessly integrated into diverse backgrounds, demonstrating the effectiveness of our approach in preserving visual coherence and realism across different scenes.", "description": "This figure demonstrates the ability of the OmnimatteZero method to seamlessly integrate extracted foreground objects (along with their associated shadows and reflections) into various background videos.  It showcases the visual coherence and realism achieved by the approach, highlighting its ability to maintain visual consistency across diverse scenes, even when objects and effects are complex and interact with the background in non-trivial ways.", "section": "5. Results"}]