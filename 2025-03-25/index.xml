<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-03-25s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/</link><description>Recent content in 2025-03-25s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Mon, 24 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/index.xml" rel="self" type="application/rss+xml"/><item><title>Aether: Geometric-Aware Unified World Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18945/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18945/</guid><description>AETHER: a unified framework enabling geometry-aware reasoning in world models, achieving zero-shot generalization from synthetic to real-world data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18945/cover.png"/></item><item><title>AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18769/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18769/</guid><description>AlphaSpace enables robotic actions via semantic tokenization and symbolic reasoning, enhancing spatial intelligence in LLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18769/cover.png"/></item><item><title>AMD-Hummingbird: Towards an Efficient Text-to-Video Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18559/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18559/</guid><description>Hummingbird: An efficient text-to-video model that balances quality and computational efficiency via pruning and visual feedback learning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18559/cover.png"/></item><item><title>CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18886/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18886/</guid><description>CFG-Zero*: A better Classifier-Free Guidance to improve the image quality and text alignment in Flow Matching models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18886/cover.png"/></item><item><title>Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18352/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18352/</guid><description>Diffusion-4K: Synthesizing ultra-high-resolution images with a new benchmark dataset and wavelet-based fine-tuning that makes 4K image creation more detailed and accessible!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18352/cover.png"/></item><item><title>Equivariant Image Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18948/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18948/</guid><description>Aligning image generation subtasks: Equivariant modeling boosts efficiency and generalization by leveraging natural visual signal invariance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18948/cover.png"/></item><item><title>FFN Fusion: Rethinking Sequential Computation in Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18908/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18908/</guid><description>FFN Fusion: Parallelizing sequential computation in large language models for significant speedups!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18908/cover.png"/></item><item><title>I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18878/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18878/</guid><description>LLMs&amp;rsquo; reasoning is decoded via sparse autoencoders, revealing key features that, when steered, enhance performance. First mechanistic account of reasoning in LLMs!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18878/cover.png"/></item><item><title>MetaSpatial: Reinforcing 3D Spatial Reasoning in VLMs for the Metaverse</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18470/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18470/</guid><description>MetaSpatial: RL for 3D Spatial Reasoning in VLMs</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18470/cover.png"/></item><item><title>Training-free Diffusion Acceleration with Bottleneck Sampling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18940/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18940/</guid><description>Bottleneck Sampling: Accelerate diffusion models &lt;em>without&lt;/em> retraining by cleverly using low-resolution priors for efficient inference!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18940/cover.png"/></item><item><title>Verbal Process Supervision Elicits Better Coding Agents</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18494/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18494/</guid><description>CURA: Verbal process supervision improves coding agents.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18494/cover.png"/></item><item><title>Video SimpleQA: Towards Factuality Evaluation in Large Video Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18923/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18923/</guid><description>Video SimpleQA: A New Benchmark for Factuality Evaluation in Large Video Language Models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18923/cover.png"/></item><item><title>Video-T1: Test-Time Scaling for Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18942/</link><pubDate>Mon, 24 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18942/</guid><description>Video-T1 enhances video generation through test-time scaling, improving quality and consistency by viewing generation as a search for optimal video trajectories.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18942/cover.png"/></item><item><title>AgentRxiv: Towards Collaborative Autonomous Research</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18102/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18102/</guid><description>AgentRxiv enables collaborative autonomous research via LLM agent preprint sharing, boosting performance and discovery.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18102/cover.png"/></item><item><title>Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18018/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18018/</guid><description>LLMs falter on culturally adapted math problems, revealing a critical cultural bias.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18018/cover.png"/></item><item><title>OmnimatteZero: Training-free Real-time Omnimatte with Pre-trained Video Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18033/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18033/</guid><description>OmnimatteZero: Real-time omnimatte using pre-trained video diffusion, no training needed!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18033/cover.png"/></item><item><title>Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18013/</link><pubDate>Sun, 23 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18013/</guid><description>Vision-R1: Improves LVLMs via vision-guided reinforcement learning, eliminating the need for human feedback and specialized reward models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.18013/cover.png"/></item><item><title>RDTF: Resource-efficient Dual-mask Training Framework for Multi-frame Animated Sticker Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17735/</link><pubDate>Sat, 22 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17735/</guid><description>RDTF: Efficient animated sticker generation via dual-mask training, outperforming parameter-efficient tuning under constrained resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17735/cover.png"/></item><item><title>LEMMA: Learning from Errors for MatheMatical Advancement in LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17439/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17439/</guid><description>LEMMA: LLMs learn math via mistake analysis and correction, boosting performance without external critics.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17439/cover.png"/></item><item><title>Optimized Minimal 3D Gaussian Splatting</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.16924/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.16924/</guid><description>OMG: optimized minimal 3D Gaussian splatting, enabling fast and efficient rendering with minimal storage.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.16924/cover.png"/></item><item><title>Position: Interactive Generative Video as Next-Generation Game Engine</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17359/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17359/</guid><description>Interactive Generative Video (IGV) can revolutionize game creation by using AI to generate endless, novel content for next-gen game engines.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17359/cover.png"/></item><item><title>V-Seek: Accelerating LLM Reasoning on Open-hardware Server-class RISC-V Platforms</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17422/</link><pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17422/</guid><description>V-SEEK accelerates LLM reasoning on open-hardware RISC-V platforms, achieving up to 3.0x speedup through optimized kernels and memory management.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.17422/cover.png"/></item><item><title>Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.15879/</link><pubDate>Thu, 20 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.15879/</guid><description>Typed-RAG enhances non-factoid QA by type-aware decomposition, refining retrieval and generation for nuanced, user-aligned answers.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.15879/cover.png"/></item><item><title>MagicComp: Training-free Dual-Phase Refinement for Compositional Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.14428/</link><pubDate>Tue, 18 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.14428/</guid><description>MagicComp: Dual-Phase Refinement Enables Training-Free Compositional Video Generation</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-25/2503.14428/cover.png"/></item></channel></rss>