[{"figure_path": "https://arxiv.org/html/2503.10625/x2.png", "caption": "Figure 1: 3D Avatar Reconstruction and Animation Results of our LHM.\nOur method reconstructs an animatable human avatar in a single feed-forward pass in seconds.\nThe resulting model supports real-time rendering and pose-controlled animation.", "description": "This figure showcases the capabilities of the Large Animatable Human Model (LHM).  It presents several examples of human avatars reconstructed from single input images.  The top row illustrates the input image, followed by the reconstructed 3D avatar, the avatar in various poses, demonstrating pose-controlled animation, and finally another reconstructed 3D avatar from a different input image. The reconstruction process takes only a couple of seconds per image using a single feed-forward pass through the model.  The generated avatars are designed for real-time rendering and can be animated with different poses.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10625/x3.png", "caption": "Figure 2: Overview of the proposed LHM.\nOur method extracts body and head image tokens from the input image, and utilizes the proposed Multimodal Body-Head Transformer (MBHT) to fuse the 3D geometric body tokens with the image tokens. After the attention-based fusion process, the geometric body tokens are decoded into Gaussian parameters.", "description": "The figure illustrates the architecture of the Large Animatable Human Model (LHM).  The LHM takes a single image as input and processes it through several steps. First, it extracts image tokens representing both the body and the head from the input image.  Simultaneously, 3D geometric body tokens are generated from a SMPL-X template mesh.  These two types of tokens (image and geometric) are fed into a Multimodal Body-Head Transformer (MBHT) module. Inside the MBHT, the tokens are fused using an attention mechanism, combining visual information with geometric priors. Finally, the resulting fused geometric tokens are decoded into parameters defining a 3D Gaussian splatting representation of the human body, allowing for realistic and animatable 3D rendering.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.10625/x4.png", "caption": "Figure 3: Architecture of the proposed Multimodal Body-Head Transformer Block (MBHT-block).", "description": "The Multimodal Body-Head Transformer (MBHT) block fuses 3D geometric tokens with body and head image features.  Global context features, image tokens, and query point tokens are fed into the MBHT block.  3D head point tokens first fuse with head image features, and then concatenate with 3D body point tokens to interact with body image tokens.  This architecture facilitates balanced attention across different body regions, improving the overall accuracy and detail preservation in the resulting 3D human model.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.10625/x5.png", "caption": "Figure 4: \nSingle-view reconstruction comparisons on DeepFashion\u00a0[33] and in-the-wild images.\nLHM achieves superior appearance fidelity and texture sharpness, particularly evident in facial details and garment wrinkles.", "description": "Figure 4 presents a comparison of single-image 3D human reconstruction results from different methods, including the proposed LHM model.  The comparison uses images from the DeepFashion dataset and also real-world, in-the-wild images. The results visually demonstrate that LHM achieves significantly higher levels of visual fidelity and detail, particularly noticeable in the sharpness of facial features and the accurate rendering of wrinkles and folds in clothing compared to other existing methods.", "section": "5. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10625/x8.png", "caption": "Figure 5: \nSingle-view animatable human reconstruction comparisons on in-the-wild sequences.\nLHM produces more accurate and photorealistic animation results than the baseline methods.\nNote that the results of AniGS are not faithful to the input images.", "description": "Figure 5 presents a comparison of single-image animatable human reconstruction results between LHM and three other methods (AniGS, Ours). The results demonstrate LHM's superior ability to produce more accurate and photorealistic animation sequences compared to existing approaches.  The figure highlights that while other techniques can generate animations, they often fail to accurately capture the details and appearance of the input image, particularly evident in the results from the AniGS method, which deviates significantly from the input.", "section": "5.2. Comparison with Existing Methods"}, {"figure_path": "https://arxiv.org/html/2503.10625/x9.png", "caption": "Figure 6: Ablation study on model design and parameters.", "description": "This ablation study analyzes the effects of different model parameters and architectural choices on the overall performance of the LHM model.  Specifically, it compares results using the multimodal body-head transformer against a simpler alternative.  It also demonstrates the influence of the head token shrinkage regularization technique and the impact of varying the model's size (parameter count) on the quality of the generated 3D avatars. Visual comparisons of the generated avatars highlight the differences in reconstruction quality and detail across these various model configurations.", "section": "5.3 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.10625/x10.png", "caption": "Figure 7: Detailed architecture of Multi-Modal Transformer\u00a0[11].", "description": "This figure presents a detailed breakdown of the Multi-Modal Transformer architecture, a key component of the LHM model.  It shows the flow of information, including query points, image tokens, and global context features, through the various layers including attention mechanisms and adaptive layer normalization. The diagram illustrates how these different input modalities are integrated using multi-head attention to produce a refined output.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.10625/x11.png", "caption": "Figure 8: Architecture of our HFPE for multi-scale facial feature extraction", "description": "Figure 8 illustrates the architecture of the Head Feature Pyramid Encoding (HFPE) module.  The HFPE module is designed to address the challenge of capturing high-frequency details in facial features, particularly given the small size of the face region within the input image and potential spatial downsampling during processing. The figure shows multiple stages: the input image, various feature extractors (ViT blocks), and the final concatenation and projection to output multi-scale facial features.", "section": "4.2. Geometric and Image Feature Encoding"}, {"figure_path": "https://arxiv.org/html/2503.10625/x12.png", "caption": "Figure 9: Ablation for canonical space shape regularization.", "description": "This ablation study visualizes the impact of the canonical space shape regularization loss on the quality of 3D human reconstruction.  The images show that using both the \"as spherical as possible\" loss (LASAP) and the \"as close as possible\" loss (LACAP) results in more accurate and realistic human models, compared to models trained without these losses.  The lack of LASAP leads to artifacts such as semi-transparent boundaries, while the lack of LACAP causes floating points around the human body, indicating the importance of both regularization techniques for robust and accurate reconstruction.", "section": "4.4.2 Canonical Space Regularization"}]