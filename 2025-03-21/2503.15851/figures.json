[{"figure_path": "https://arxiv.org/html/2503.15851/x1.png", "caption": "Figure 1: \n4D Avatar generation with SDS loss\u00a0[58] and our Zero-1-to-A.\nVideo diffusion often suffers from spatial and temporal inconsistencies.\n(a) The SDS loss, aligning avatar with output from video diffusion, produces over-smooth results.\n(b) Zero-1-to-A addresses this issue by synthesizing spatially and temporally consistent datasets for avatar reconstruction.\nIt introduces an updatable dataset to cache video diffusion results and establishes a mutually beneficial relationship between avatar generation and dataset construction to further enhance consistency.", "description": "This figure illustrates the difference between using only score distillation sampling (SDS) loss and the proposed Zero-1-to-A method for 4D avatar generation from video diffusion.  (a) shows the over-smoothed results from directly applying SDS loss, highlighting the spatial and temporal inconsistencies of video diffusion outputs. (b) shows how Zero-1-to-A addresses these issues by creating a consistent dataset through iterative updates. This dataset caches video diffusion results, improving consistency and creating a mutually beneficial relationship between avatar generation and dataset refinement.", "section": "Method"}, {"figure_path": "https://arxiv.org/html/2503.15851/x2.png", "caption": "Figure 2: \nImage to animatable avatars generation by Zero-1-to-A.\nWithout manually annotated data, our method can distill high-fidelity 4D avatars with real-time rendering speed from a pre-trained video diffusion using only one image input.", "description": "This figure demonstrates the Zero-1-to-A model's ability to generate high-fidelity, animatable 4D head avatars from a single input image.  The process leverages a pre-trained video diffusion model without requiring any manually annotated training data. The example shows three different individuals and their corresponding avatars, highlighting the model's ability to capture realistic facial features and expressions and render them in real-time.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.15851/x3.png", "caption": "Figure 3: \nFramework of SymGEN.\nOur method simultaneously builds both the dataset and avatar from scratch through video diffusion.\nIt establishes a mutually beneficial relationship between dataset construction and avatar reconstruction, iteratively updating the synthesized dataset and training the head avatar on the updated dataset to achieve unified results.", "description": "The figure illustrates the SymGEN framework, a novel approach for generating 4D avatars from a single image using a pretrained video diffusion model.  Unlike traditional methods that rely on separate dataset creation and avatar training, SymGEN iteratively refines both simultaneously.  It starts by generating an initial dataset from the video diffusion model. Then, it uses the current avatar model to render videos, which are used to improve the dataset's spatial and temporal consistency. This improved dataset is then used to train an updated avatar model. This cycle of dataset refinement and avatar retraining repeats, creating a mutually beneficial relationship that leads to improved avatar quality and consistency.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.15851/x4.png", "caption": "Figure 4: \nPipeline of Progressive Learning.\nIt sequences learning from simple to complex, facilitating symbiotic generation to create consistent avatars from inconsistent video diffusion.\nThis process divides 4D avatar generation into:\n(1) Spatial Consistency Learning: progressing from frontal to side views with a fixed expression.\n(2) Temporal Consistency Learning: learn from relaxed to hyperbole expressions under a fixed camera.", "description": "This figure illustrates the Progressive Learning pipeline used in the Zero-1-to-A method.  It shows how the model learns to create consistent avatars by starting with simpler tasks and gradually increasing complexity.  The process is divided into two stages:  Spatial Consistency Learning, where the model learns from frontal to side views while maintaining a consistent facial expression; and Temporal Consistency Learning, where the model learns from relaxed to exaggerated expressions while keeping the camera view fixed.  This staged approach helps to mitigate inconsistencies inherent in video diffusion models, ultimately leading to higher-quality, more realistic 4D avatar generation.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.15851/x5.png", "caption": "Figure 5: \nComparison with Static Head Avatar Generation Methods.\nFrom top to bottom: Doctor Strange, Two-Face from DC, Vincent van Gogh, and Black Panther from Marvel.\nGuided by image prompts, our approach captures rich details and demonstrates superior performance in texture and geometry.", "description": "This figure compares the performance of Zero-1-to-A against other state-of-the-art static head avatar generation methods.  Four different subjects are shown: Doctor Strange, Two-Face (from DC Comics), Vincent van Gogh, and Black Panther (from Marvel). Each subject is represented across multiple methods, showcasing the textural and geometric details produced by each approach. Zero-1-to-A demonstrates significantly superior performance in terms of detailed texture and realistic geometry when compared to the other methods.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.15851/x6.png", "caption": "Figure 6: \nComparison with Dynamic Head Avatar Generation Methods.\nYellow circles highlight mouth expression artifacts.\nRendering speed on the same device is shown in the black box.\nThe FLAME mesh of the avatar is visualized bottom left.\nOur method excels in animation quality and rendering speed compared to prior methods.", "description": "This figure compares the performance of Zero-1-to-A against two other dynamic head avatar generation methods (TADA and HeadStudio).  The comparison uses three example avatars, showcasing the rendering speed for each method and highlighting mouth expression artifacts in the competing methods using yellow circles. The bottom-left corner of each example shows the underlying FLAME mesh used for the avatar. The results demonstrate that Zero-1-to-A produces significantly better animation quality and achieves higher rendering speeds compared to the other methods.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.15851/x7.png", "caption": "Figure 7: \nComparison with Portrait Video Diffusion Methods.\nSymbiotic generation enhances portrait video diffusion with improved 3D consistency, temporal smoothness, and expression accuracy.\nIn contrast, traditional portrait video diffusion shows spatial inconsistencies, noted by incorrect eye positioning in side views (green boxes), and temporal inconsistencies, highlighted by significant changes with minor facial expressions (blue boxes).", "description": "This figure compares the results of the proposed method, Zero-1-to-A, against traditional portrait video diffusion methods for generating head avatar animations.  Zero-1-to-A, which uses a symbiotic generation process, produces avatars with better 3D consistency, smoother temporal transitions between frames, and more accurate expression representation. The comparison highlights how traditional methods struggle with spatial inconsistencies (incorrect eye positions in side views) and temporal inconsistencies (significant changes in expression with minimal changes in driving signal). The green boxes highlight spatial errors and the blue boxes highlight temporal inconsistencies in the traditional methods.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.15851/x8.png", "caption": "Figure 8: \nAblation Study.\nProgressive learning is crucial for creating consistent avatars from inconsistent video diffusion, with spatial consistency improving eye and mouth for effective avatar control (green boxes), and temporal consistency enhancing model generalization to new expressions (blue boxes).", "description": "This ablation study demonstrates the importance of progressive learning in generating consistent avatars from inconsistent video diffusion data.  The figure shows the results of different model variations. The \"w/o Progressive Learning\" model highlights the lack of consistency, especially with eye and mouth positioning.  The \"w/o Spatial Consistency Learning\" model shows good mouth consistency but struggles with eyes. The \"w/o Temporal Consistency Learning\" model displays good eye and mouth alignment in simple cases, but fails to generalize to new expressions.  Only the \"Full Model\", which incorporates progressive learning, effectively creates high-quality, consistent avatars across various expressions and views.", "section": "5.2. Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.15851/x9.png", "caption": "Figure 9: \nChallenge Cases.\nOur method exhibits robustness in effectively handling side views (left), eye closure (middle), and facial occlusions (right).\nEach pair shows the driving expression and animation result (right), and the top row contains reference images.", "description": "This figure demonstrates the robustness of the proposed Zero-1-to-A method in handling challenging scenarios during avatar generation. The left column showcases the model's ability to generate avatars from side-view images, maintaining accuracy and detail. The middle column shows that the method can effectively generate avatars even when the eyes are closed in the input image. Finally, the right column demonstrates the capability of Zero-1-to-A to generate high-quality avatars despite the presence of facial occlusions in the input image. Each row displays a pair of images: the driving expression (input image) and the corresponding generated avatar.", "section": "5.2 Ablation Study"}, {"figure_path": "https://arxiv.org/html/2503.15851/x10.png", "caption": "Figure 10: \nLimitation. The animatable Gaussian head\u00a0[60] aligns Gaussians with the FLAME mesh, limiting the modeling of elements beyond the head.", "description": "The image showcases a limitation of the animatable Gaussian head model used in the paper.  Specifically, because the Gaussian points are aligned to the FLAME mesh (a 3D morphable model of a human head), the model struggles to accurately represent elements that extend beyond the typical head region, such as an afro hairstyle.  The inability to effectively model these extra-head features is a key limitation of the method presented in the paper.", "section": "5.3. Limitations"}, {"figure_path": "https://arxiv.org/html/2503.15851/x11.png", "caption": "Figure 11: \nPre-process.\nGiven a portrait image, we first remove its background and then estimate the FLAME parameter.", "description": "This figure illustrates the preprocessing steps involved in preparing portrait images for use in the Zero-1-to-A model.  The process begins with a portrait image containing a background. The first step is background removal using a suitable technique, resulting in an image with the subject isolated. Next, FLAME (Facial Landmark Appearance Model) parameters, including the shape and expression of the face, are estimated using an appropriate method. This provides crucial data for the avatar generation process that follows.", "section": "4. Method"}, {"figure_path": "https://arxiv.org/html/2503.15851/x12.png", "caption": "Figure 12: \n2D Image Generation with SDS-based Loss.\nFrom left to right: reference image, SDS\u00a0[58], ISM\u00a0[42], NFSD\u00a0[35] and video diffusion generation\u00a0[73].", "description": "This figure compares different methods for 2D image generation using a pre-trained video diffusion model.  It demonstrates the results of several approaches when generating images based on a reference image.  The methods compared include Score Distillation Sampling (SDS) [58], Interval Score Matching (ISM) [42], Noise-Free Score Distillation (NFSD) [35], and the baseline video diffusion generation method [73]. The figure visually showcases the differences in generated image quality and fidelity resulting from each method compared to the original reference image. It highlights the impact of various techniques on the visual characteristics of the generated output. ", "section": "9. Discussion with Related Solutions"}, {"figure_path": "https://arxiv.org/html/2503.15851/x13.png", "caption": "Figure 13: \nVisualization of Spatial and Temporal Inconsistencies in Video Diffusion Models.\nPortrait video diffusion exhibits spatial inconsistencies, such as incorrect eye positioning in side views (green boxes), and temporal inconsistencies, evident in significant changes triggered by minor facial expressions (blue boxes).", "description": "Figure 13 visualizes the spatial and temporal inconsistencies present in video diffusion models. The left side shows spatial inconsistencies where incorrect eye positioning is observed in side views (highlighted by green boxes).  The right side demonstrates temporal inconsistencies where minor expression changes lead to significant variations in the generated video (highlighted by blue boxes).  This figure highlights the challenges that arise when directly using the output of video diffusion models for 4D avatar generation, motivating the need for the proposed method to address these issues.", "section": "9.2. Discussion on the Motivation"}, {"figure_path": "https://arxiv.org/html/2503.15851/x14.png", "caption": "Figure 14: \nComparisons with Image-to-3D Methods.\nOur method delivers comparable performance in texture reconstruction while achieving superior 3D consistency.", "description": "Figure 14 presents a comparison of Zero-1-to-A against several state-of-the-art image-to-3D generation methods.  The goal is to demonstrate Zero-1-to-A's ability to generate high-quality 3D models from a single image. The comparison focuses on two key aspects: texture reconstruction quality and the overall 3D consistency of the resulting models. The figure visually shows that Zero-1-to-A achieves comparable results in terms of texture detail to the other methods, but it significantly outperforms them in terms of the 3D structural consistency of the generated avatar.", "section": "10. Additional Experiments"}, {"figure_path": "https://arxiv.org/html/2503.15851/x15.png", "caption": "Figure 15: \nComparisons with Portrait3D\u00a0[24].\nOur method matches the performance of Portrait3D while providing animatable avatars, enabling a wider range of applications.", "description": "Figure 15 presents a qualitative comparison of Zero-1-to-A and Portrait3D [24], highlighting that Zero-1-to-A achieves comparable quality in avatar generation while offering the additional capability of creating animatable avatars, thus expanding the range of potential applications.", "section": "10. Additional Experiments"}, {"figure_path": "https://arxiv.org/html/2503.15851/x16.png", "caption": "Figure 16: \nEvaluation on Different Video Diffusion Models.\nOur method demonstrates its effectiveness by seamlessly adapting to various video diffusion models.", "description": "This figure shows the results of applying the Zero-1-to-A method to different video diffusion models.  It demonstrates the robustness and adaptability of the method by showing that it produces high-quality, animatable avatars regardless of which underlying video diffusion model is used.  The consistency in the results across different models highlights a key strength of the Zero-1-to-A approach.", "section": "5. Experiment"}, {"figure_path": "https://arxiv.org/html/2503.15851/x17.png", "caption": "Figure 17: \nEvaluation on in-the-wild Cases.", "description": "This figure showcases the robustness of the Zero-1-to-A model by testing its performance on a diverse range of real-world images from the FFHQ dataset.  The images represent individuals with varying ethnicities, ages, genders, and facial features. The results demonstrate that the model generalizes well across different demographics, maintaining high-quality avatar generation. Each column shows the reference image and the generated avatar from different expressions.  The diversity in the individuals tested highlights the model's ability to handle in-the-wild scenarios.", "section": "10.3. Additional Ablations"}]