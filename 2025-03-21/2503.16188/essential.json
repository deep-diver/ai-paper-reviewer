{"importance": "This work presents a new way to fine-tune MLLMs for image classification with rule-based RL which avoids catastrophic forgetting and achieves strong performance, offering insights into visual tasks of MLLMs and opening new avenues for RL-based fine-tuning.", "summary": "CLS-RL: Rule-based RL tackles catastrophic forgetting in MLLM image classification, outperforming SFT with better generalization and efficiency.", "takeaways": ["Rule-based reinforcement learning (RL) can effectively fine-tune Multimodal Large Language Models (MLLMs) for image classification tasks, surpassing supervised fine-tuning (SFT) by mitigating catastrophic forgetting.", "The CLS-RL method demonstrates a 'free-lunch' phenomenon, where fine-tuning on one dataset improves performance on others, suggesting a broader learning of image classification fundamentals.", "Reducing the 'thinking process' during fine-tuning, as shown in No-Thinking-CLS-RL, can enhance performance and efficiency, challenging the assumption that extensive reasoning is always beneficial."], "tldr": "Classification is a core task in machine learning, but Multimodal Large Language Models(MLLMs)often struggle with it. Supervised fine-tuning(SFT) can improve performance, but it also leads to catastrophic forgetting and performance degradation. To solve this, the paper proposes **CLS-RL**, uses verifiable signals (class names) as reward to fine-tune MLLMs and format reward to encourage models to think before answering.The paper found that SFT can cause severe catastrophic forgetting issues and may even degrade performance. \n\nTo address this, recent works in inference time thinking, the paper present that **No-Thinking-CLS-RL approach**, which minimizes thinking processes during training by setting an equality accuracy reward. Findings indicate that with much less fine-tuning time, the No-Thinking- CLS-RL method achieves superior in-domain performance and generalization capabilities. Extensive experiments showed that CLS-RL outperforms SFT in most datasets and has a much higher average accuracy on both base-to-new generalization and few-shot learning settings.", "affiliation": "Shanghai AI Laboratory", "categories": {"main_category": "Computer Vision", "sub_category": "Image Classification"}, "podcast_path": "2503.16188/podcast.wav"}