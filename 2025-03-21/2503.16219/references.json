{"references": [{"fullname_first_author": "Niklas Muennighoff", "paper_title": "s1: Simple test-time scaling", "publication_date": "2025-01-01", "reason": "This paper introduces the s1 dataset, which is heavily used as one of the main data sources in the methodology for training the models discussed in the paper."}, {"fullname_first_author": "Zhihong Shao", "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models", "publication_date": "2024-02-01", "reason": "This paper describes DeepSeek-R1, which provides the architecture (DeepSeek-R1-Distill-Qwen-1.5B) used as the base model in the experiments, making it a foundational reference for the work."}, {"fullname_first_author": "An Yang", "paper_title": "Qwen2.5-math technical report: Toward mathematical expert model via self-improvement", "publication_date": "2024-09-01", "reason": "The Qwen series of models are used to filter and refine the datasets used for training, playing a crucial role in the data curation process, also several variants are used as baselines."}, {"fullname_first_author": "Michael Luo", "paper_title": "Deepscaler: Surpassing o1-preview with a 1.5b model by scaling rl.", "publication_date": "2025-01-01", "reason": "This paper presents DeepScaleR-1.5B-Preview, which has the same architecture as the models in the paper and uses a similar Reinforcement Learning approach, used as baseline."}, {"fullname_first_author": "Takeshi Kojima", "paper_title": "Large language models are zero-shot reasoners", "publication_date": "2022-01-01", "reason": "This is one of the original papers on the Chain-of-Thought prompting method, which underpins the 01-preview model used as baseline."}]}