[{"figure_path": "https://arxiv.org/html/2503.16419/x1.png", "caption": "Figure 1: The pipeline of developing efficient reasoning for LLMs. A reasoning model can be trained on the base model using SFT, RL, or a combination of both. While reasoning models demonstrate strong reasoning capabilities, they often suffer from the \u201coverthinking phenomenon\u201d, generating unnecessarily lengthy reasoning steps. To improve efficiency, various methods can be applied to reduce redundant steps while maintaining accuracy, or to fine-tune non-reasoning models to incorporate efficient reasoning capabilities. This approach enables the model to answer questions with concise and effective reasoning steps. In this paper, we explore the latest progress in efficient reasoning for LLMs, aiming to provide insights that can guide future research and the development of reasoning-driven applications across various domains.", "description": "This figure illustrates the process of developing efficient reasoning in large language models (LLMs).  It starts with a base LLM, which is then enhanced through supervised fine-tuning (SFT) and/or reinforcement learning (RL) to create a reasoning model.  However, these reasoning models often produce excessively long reasoning chains, a phenomenon known as 'overthinking'. To address this, various methods are employed to shorten the reasoning process while maintaining accuracy. These methods include reducing redundant steps in the reasoning output, optimizing the reasoning model itself, and designing prompts that guide the model towards more concise reasoning. The ultimate goal is to enable LLMs to answer questions effectively with concise and accurate reasoning steps.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2503.16419/x2.png", "caption": "Figure 2: \nOverview of efficient reasoning methods, which can be summarized as model-oriented (Left: I, II) and reasoning output-oriented (Middle: III, IV), and input prompts-oriented (Right: V, VI) methods. Specifically, (I) Reinforcement Learning with Length Reward Design (Section 3.1); (II) Supervised Fine-Tuning with Variable-Length CoT Data (Section 3.2); (III) Compressing Reasoning Steps into Fewer Latent Representation (Section 4.1); (IV) Dynamic Reasoning Paradigm during Inference (Section 4.2); (V) Prompt-guided Efficient Reasoning (Section 5.1); (VI) Routing Prompts to Optimize Reasoning Efficiency (Section 5.2);", "description": "This figure categorizes efficient reasoning methods for large language models (LLMs) into three main categories based on their approach: model-oriented, reasoning output-oriented, and input prompts-oriented.  Model-oriented methods focus on modifying the model architecture or training process to promote efficient reasoning. Reasoning output-oriented methods aim to optimize the reasoning process during inference by manipulating the output. Input prompts-oriented methods leverage carefully designed input prompts to guide the LLM toward more efficient reasoning. Each category further breaks down into specific techniques, illustrated in the figure with detailed section references from the paper. The model-oriented methods include reinforcement learning with length reward design and supervised fine-tuning with variable-length Chain-of-Thought (CoT) data. The reasoning output-oriented methods cover compressing reasoning steps into fewer latent representations and the dynamic reasoning paradigm during inference. Finally, the input prompts-oriented methods encompass prompt-guided efficient reasoning and routing prompts to optimize reasoning efficiency.", "section": "3 Model-based Efficient Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.16419/x3.png", "caption": "Figure 3:  Taxonomy of existing literature on efficient reasoning for LLMs.", "description": "This figure presents a taxonomy of existing research on efficient reasoning methods for Large Language Models (LLMs).  It categorizes the existing works into three main approaches: model-based efficient reasoning (optimizing model architecture or training directly for conciseness), reasoning output-based efficient reasoning (dynamically adjusting reasoning steps during inference), and input prompt-based efficient reasoning (leveraging prompt properties to influence reasoning efficiency).  Within each category are further sub-categories representing specific techniques or methods, providing a structured overview of the landscape of efficient reasoning research in LLMs.", "section": "3 Model-based Efficient Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.16419/x4.png", "caption": "Figure 4: An example of the \u201coverthinking phenomenon\u201d: when the reasoning model is asked \u201cWhich is larger, 0.9 or 0.11?\u201d, it takes an unnecessarily long time (i.e., 19 seconds for QwQ-32B\u00a0[79] and 42 seconds for DeepSeek-R1\u00a0[31]) to deliver its final answer. The example was tested in March 2025.", "description": "The figure showcases the overthinking phenomenon in large language models (LLMs).  It presents two examples of LLMs, QwQ-32B and DeepSeek-R1, attempting to answer the simple question: \"Which is larger, 0.9 or 0.11?\"  Both models produce verbose and lengthy reasoning processes before arriving at the correct answer, taking 19 and 42 seconds, respectively.  This demonstrates the inefficiency of these LLMs, as they expend significant computational resources on redundant steps, highlighting the need for efficient reasoning techniques.  The test was conducted in March 2025.", "section": "2 Background: Long CoT Reasoning Models and Overthinking Phenomenon"}, {"figure_path": "https://arxiv.org/html/2503.16419/x5.png", "caption": "Figure 5: Illustration of the method for RL fine-tuning with length reward designs. In principle, the length reward assigns higher rewards to short, correct answers and penalizes lengthy or wrong answers to achieve efficient reasoning LLMs.", "description": "This figure illustrates the reinforcement learning (RL) approach used to train large language models (LLMs) for efficient reasoning.  The key modification is incorporating a 'length reward' into the RL training process. This reward mechanism incentivizes the model to produce shorter, correct answers by assigning higher rewards to such outputs, while penalizing longer or incorrect responses. The goal is to train LLMs that can effectively reason with conciseness without sacrificing accuracy, thus achieving efficient reasoning capabilities.", "section": "3.1 RL with Length Reward Design"}, {"figure_path": "https://arxiv.org/html/2503.16419/x6.png", "caption": "Figure 6: Illustration of methods for utilizing SFT with variable-length CoT reasoning datasets.", "description": "This figure illustrates the process of using supervised fine-tuning (SFT) with variable-length chain-of-thought (CoT) reasoning datasets to improve the efficiency of reasoning in large language models (LLMs).  It shows that both long and short CoT data are used in the training, resulting in a more efficient reasoning model.  The process involves constructing variable-length datasets (through various methods described in the paper), followed by fine-tuning an LLM using these datasets. The goal is to teach the LLM to generate concise and effective reasoning steps without compromising accuracy.", "section": "3.2 SFT with Variable-Length CoT Data"}, {"figure_path": "https://arxiv.org/html/2503.16419/x7.png", "caption": "Figure 7: \nComparison of methods of compressing reasoning steps into fewer latent representations.", "description": "This figure compares different methods for compressing reasoning steps in large language models (LLMs) into fewer latent representations.  It illustrates three different approaches: (1) Coconut uses continuous representations and trains the LLM gradually to incorporate these latent representations into its reasoning process.  (2) CODI employs self-distillation to learn latent representations.  (3) CCOT compresses full CoT reasoning steps into a smaller set of compressed tokens which a separate decoding module utilizes to reconstruct concise reasoning.  The figure visually shows the architecture and data flow for each method, highlighting their similarities and differences in how they compress reasoning steps.", "section": "Reasoning Output-based Efficient Reasoning"}]