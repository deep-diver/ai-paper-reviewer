{"importance": "This research introduces a novel benchmark for code LLMs to promote the development of more robust and reliable code generation models, which is important for real-world applications with complexity constraints. It opens up new research avenues to explore code LLMs in a more nuanced way.", "summary": "BIGO(Bench) can help LLMs generate code with controlled time/space complexity, addressing the gap in current evaluations and encouraging further exploration.", "takeaways": ["BIGO(BENCH) is a new benchmark for evaluating code LLMs' ability to generate code with specific time and space complexities.", "The benchmark includes a complexity inference framework and a dataset of coding problems with time and space complexity labels.", "Evaluation of state-of-the-art language models reveals limitations in handling complexity requirements, especially for token-space reasoning models."], "tldr": "Current evaluations of code-generating language models often overlook their ability to understand and produce code that satisfies specific time and space complexity constraints. This oversight leads to a gap between theoretical performance and real-world applicability, where efficiency and scalability are often critical. The ability to optimize and control computational complexity separates novice programmers from experienced ones, yet existing benchmarks do not adequately assess this higher-level reasoning skill in language models. \n\nTo address this gap, this paper introduces **BIGO(BENCH), a novel coding benchmark** designed to evaluate the capabilities of generative language models in understanding and generating code with specified complexities. The benchmark comprises tooling to infer algorithmic complexity, a set of 3,105 coding problems and 1,190,250 solutions annotated with time and space complexity labels. **The study evaluates multiple state-of-the-art language models, revealing strengths and weaknesses in handling complexity requirements**. Token-space reasoning models excel in code generation but lack complexity understanding.", "affiliation": "FAIR at Meta", "categories": {"main_category": "Natural Language Processing", "sub_category": "Large Language Models"}, "podcast_path": "2503.15242/podcast.wav"}