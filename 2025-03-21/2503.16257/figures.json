[{"figure_path": "https://arxiv.org/html/2503.16257/x1.png", "caption": "(a) Layer 5 Key", "description": "This figure displays the magnitude of the key cache in layer 5 of the LLaVA-OV-7B model.  It shows a heatmap visualization of the key cache's values across the channel and token dimensions.  The heatmap allows for the visual identification of channels with significantly larger magnitudes than others. This visualization is used in the paper to support the argument for a mixed-precision quantization strategy for the key cache, where different quantization levels are applied to channels with different magnitude characteristics.", "section": "4.1. KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x2.png", "caption": "(b) Layer 5 Value", "description": "This figure displays the magnitude of the value cache in layer 5 of the LLaVA-OV-7B model.  It shows the distribution of values across both the channel and token dimensions.  The heatmap visualization allows one to understand the variation in magnitude of the values across different channels and tokens, highlighting potential areas of redundancy or sparsity that could be leveraged for quantization.", "section": "4.1. KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x3.png", "caption": "(c) Layer 24 Key", "description": "This figure displays the magnitude of the key cache in layer 24 of the Qwen2.5-VL-7B model.  It is a 3D heatmap showing the channel dimension on the x-axis, the token dimension on the y-axis, and the magnitude of the key values on the z-axis.  The heatmap visualizes the distribution of values within the key cache, highlighting variations across channels and tokens.  This visualization helps to understand the distribution characteristics of the key cache, which is crucial for designing effective quantization strategies.", "section": "4.1. KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x4.png", "caption": "(d) Layer 24 Value", "description": "The figure displays a 3D heatmap visualizing the magnitude of the value cache in layer 24 of a video large language model (VideoLLM). The heatmap shows the magnitude of values across different channels (vertical axis) and tokens (horizontal axis).  The color intensity represents the magnitude, with warmer colors indicating larger magnitudes. This visualization helps to understand the distribution of values within the KV cache, which is crucial for effective quantization and compression techniques. The specific model shown could be LLaVA-OV-7B or Qwen2.5-VL-7B, depending on which sub-figure is referenced in the paper.", "section": "4.1. KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x5.png", "caption": "(f) Layer 5 Value", "description": "This figure displays the magnitude of the value cache in layer 5 of the Qwen2.5-VL-7B model. The heatmap shows the distribution of values across channels and tokens, visualizing the variability and potential for quantization.  Each channel is represented by a line on the y-axis and each token is along the x-axis. The color scale indicates the magnitude of the values, with warmer colors representing larger magnitudes and cooler colors representing smaller magnitudes.", "section": "4.1. KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x6.png", "caption": "(g) Layer 22 Key", "description": "This figure displays the magnitude of the key cache in layer 22 of the Qwen2.5-VL-7B model.  The heatmap shows the values across the channel dimension (y-axis) and token dimension (x-axis). The color intensity represents the magnitude, with warmer colors (red) indicating larger magnitudes and cooler colors (blue) indicating smaller magnitudes. This visualization helps to understand the distribution of values within the key cache, which is important for designing effective quantization strategies.", "section": "4.1. KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x7.png", "caption": "(h) Layer 22 Value", "description": "This figure displays the magnitude of the value cache in layer 22 of the Qwen2.5-VL-7B model. It shows the distribution of values across channels and tokens. The visualization helps to understand the distribution characteristics of the value cache, especially in terms of magnitude variations, which are crucial for designing effective quantization strategies. High magnitude variations across channels may indicate a greater challenge for quantization methods. This figure is part of the analysis of KV cache distributions in video LLMs, which informs the proposed mixed-precision quantization scheme in the VidKV method.", "section": "4.1 KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x8.png", "caption": "Figure 1: Magnitude of KV cache for LLaVA-OV-7B and Qwen2.5-VL-7B.\n(1) In the key cache, certain channels exhibit significantly large magnitudes, while others display abnormal variations across the channel dimension, making them challenging to quantize.\n(2) In the value cache, channels exhibit variations in range.", "description": "Figure 1 visualizes the distribution patterns of key-value (KV) cache data within the LLaVA-OV-7B and Qwen2.5-VL-7B video large language models (VideoLLMs).  The heatmaps illustrate the magnitude of values across both the channel and token dimensions for different layers and heads within each model.  Specifically, the visualization reveals that certain channels in the key cache exhibit exceptionally high magnitudes, presenting a challenge for quantization.  Other channels display irregular variations across the channel dimension, adding further complexity to the quantization process. Conversely, the value cache demonstrates variations primarily in the range of values across channels.  This observation of different distribution patterns in the key and value caches informs the design choices within the VidKV quantization method.", "section": "4. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16257/x9.png", "caption": "(e) Layer 5 Key", "description": "This figure displays the magnitude of the key cache in layer 5 of the Qwen2.5-VL-7B model.  The heatmap shows the values across the channel dimension (y-axis) and token dimension (x-axis).  Different colors represent different magnitudes of the values, with warmer colors (red) representing higher magnitudes and cooler colors (blue) representing lower magnitudes. The visualization helps in understanding the distribution of values within the key cache, which is important for designing efficient quantization strategies.  The visualization highlights which channels are more challenging to quantize due to their larger values or higher variability across tokens.", "section": "4.1 KV Cache Distribution of video LLMs"}, {"figure_path": "https://arxiv.org/html/2503.16257/x12.png", "caption": "Figure 2: Overview of our proposed method VidKV. We implement 1.x-bit mixed-precision quantization for the key cache and 1.58-bit quantization for the value cache. In addition, as shown in the figure, to balance precision and model performance, we protect important visual tokens in the value cache. It is noteworthy that we perform mixed-precision quantization on the key cache along the channel dimension, whereas on the value cache, we apply mixed-precision quantization along the token dimension. All key-value caches are quantized in a per-channel fashion, different from prior KV cache quantization methods for LLMs such as KIVI\u00a0[25].", "description": "This figure illustrates the VidKV method, a 1.x-bit mixed-precision quantization technique for key-value (KV) caches in video large language models (VideoLLMs). VidKV uses a mixed-precision approach: 1.x-bit for the key cache and 1.58-bit for the value cache.  Importantly, for the key cache, mixed-precision quantization is applied along the channel dimension, while for the value cache, it is applied along the token dimension.  To improve performance, VidKV incorporates important token selection in the value cache, protecting crucial visual tokens from aggressive quantization by using higher bit precision. This approach contrasts with previous methods like KIVI [25], which quantized KV caches in a per-token fashion.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.16257/extracted/6296208/ICCV2025-Author-Kit/figures/line_plot.png", "caption": "Figure 3: Analysis of the normal channel in the key cache and its distribution following FFT transformation reveals that the frequency-domain distribution is smoother after using FFT transformation, resulting in reduced quantization error.", "description": "Figure 3 illustrates the impact of Fast Fourier Transform (FFT) on the distribution of elements within the key cache of a video large language model.  The left panel shows the distribution in the time domain, demonstrating sharp fluctuations and significant variations across channels. These variations make it challenging to effectively quantize the data using lower bit representations. In contrast, the right panel shows the distribution after applying FFT, which transforms the data into the frequency domain.  The resulting distribution is significantly smoother, exhibiting less variability.  This smoother frequency-domain representation leads to a reduction in quantization error when using techniques like low-bit quantization.  The FFT-based approach enhances quantization accuracy by mitigating the impact of outliers and improving the stability of the elements across the channels.", "section": "4. Methodology"}]