{"importance": "This paper is important for researchers because it **introduces a novel MoE routing strategy** and **addresses the limitations** of existing methods, enhancing scalability and performance. It **opens new research avenues** in diffusion models.", "summary": "Expert Race: A flexible routing strategy for scaling diffusion transformer with mixture of experts. ", "takeaways": ["Expert Race, a novel MoE routing strategy supports high routing allocation flexibility.", "Router similarity loss optimizes expert collaboration maintaining workload equilibrium.", "Per-layer Regularization ensures effective learning in shallow layers of MoE models."], "tldr": "Diffusion models have become a key framework in visual generation, and integrating Mixture of Experts (MoE) shows promise for improving scalability and performance. However, visual signals in diffusion models have distinct characteristics like high spatial redundancy and temporal variation in denoising task complexity. Previous routing strategies have limitations in adapting to these characteristics, potentially leading to inefficient model utilization and suboptimal performance. \n\nThis paper introduces Race-DiT, a MoE model with a flexible routing strategy called Expert Race, allowing dynamic expert allocation to critical tokens. By letting tokens and experts compete and selecting top candidates, the model dynamically assigns experts to critical tokens. Per-layer regularization addresses shallow layer learning challenges, and router similarity loss prevents mode collapse, ensuring better expert utilization. Experiments on ImageNet validate the approach's effectiveness, showing significant performance gains and promising scaling properties.", "affiliation": "ByteDance Seed", "categories": {"main_category": "Computer Vision", "sub_category": "Image Generation"}, "podcast_path": "2503.16057/podcast.wav"}