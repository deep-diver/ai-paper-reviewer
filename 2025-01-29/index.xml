<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-01-29s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/</link><description>Recent content in 2025-01-29s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Tue, 28 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/index.xml" rel="self" type="application/rss+xml"/><item><title>DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16764/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16764/</guid><description>DIFFSPLAT repurposes 2D image diffusion models to natively generate high-quality 3D Gaussian splats, overcoming limitations in existing 3D generation methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16764/cover.png"/></item><item><title>Histoires Morales: A French Dataset for Assessing Moral Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17117/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17117/</guid><description>HISTOIRESMORALES: a new French dataset tackles the crucial issue of aligning language models with human moral values, providing valuable resources for ethical AI research in a previously underserved l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17117/cover.png"/></item><item><title>Optimizing Large Language Model Training Using FP4 Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17116/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17116/</guid><description>First-ever FP4 training framework for LLMs achieves accuracy comparable to BF16 and FP8, enabling efficient ultra-low precision training.</description></item><item><title>Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16975/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16975/</guid><description>Boosting Large Language Model (LLM) performance, researchers introduce Over-Tokenized Transformers, decoupling input/output vocabularies to improve language modeling. Scaling input vocabularies improv&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16975/cover.png"/></item><item><title>SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17161/</link><pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17161/</guid><description>Reinforcement learning (RL) surpasses supervised fine-tuning (SFT) in fostering generalization in foundation models, while SFT aids RL&amp;rsquo;s stability; a comparative study across text and visual domains r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.17161/cover.png"/></item><item><title>IndicMMLU-Pro: Benchmarking Indic Large Language Models on Multi-Task Language Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.15747/</link><pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.15747/</guid><description>IndicMMLU-Pro: a new benchmark rigorously evaluates large language models&amp;rsquo; multi-task language understanding capabilities across nine major Indian languages, pushing Indic language AI research forward&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.15747/cover.png"/></item><item><title>Low-Rank Adapters Meet Neural Architecture Search for LLM Compression</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16372/</link><pubDate>Thu, 23 Jan 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16372/</guid><description>Low-rank adapters combined with neural architecture search revolutionize LLM compression, enabling efficient fine-tuning and significantly reduced memory footprint.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-01-29/2501.16372/cover.png"/></item></channel></rss>