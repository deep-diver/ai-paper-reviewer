[{"figure_path": "https://arxiv.org/html/2501.17117/x1.png", "caption": "Figure 1: Translation examples of moral and immoral actions with a simple prompt P1, the prompt P2, and the prompt with demonstrations P3.\nIn both cases, translations obtained with P3 are more fluent in French and its cultural context.", "description": "Figure 1 presents three different translations of example sentences from English to French, each using a different prompting technique.  The first (P1) uses a simple translation prompt. The second (P2) adds instructions to adapt to French cultural context.  The third (P3) incorporates example translations with error explanations (demonstrations). The figure highlights that the inclusion of demonstrations (P3) leads to more fluent and culturally appropriate translations in French, compared to the simpler prompts.", "section": "The HISTOIRESMORALES Dataset"}, {"figure_path": "https://arxiv.org/html/2501.17117/x2.png", "caption": "Figure 2: Example of demonstration used in P3.", "description": "This figure shows an example of a demonstration used in Prompt 3 (P3).  Prompt 3 is an improved translation prompt that includes examples (demonstrations) to help the machine translation model improve its accuracy and cultural appropriateness. The example shows a source sentence in English (\"Mike wants to run errands and pick up food items for dinner.\"), a literal translation into French (\"Michel souhaite faire des courses et ramasser des denr\u00e9es alimentaires pour le d\u00eener.\"), and a correction/improvement (\"The translation of 'pick up' into 'ramasser' is too literal. A more fitting translation for the context is 'acheter'.\").  The annotations highlight the importance of selecting natural-sounding and culturally appropriate vocabulary when translating.", "section": "3 The HISTOIRESMORALES Dataset"}, {"figure_path": "https://arxiv.org/html/2501.17117/x3.png", "caption": "Figure 3: Annotation results for the alignment of moral\u00a0norms and actions with French cultural values.", "description": "This figure displays the results of annotating the alignment of moral norms and actions within the HISTOIRESMORALES dataset with French cultural values. The x-axis categorizes the data into norms, moral actions, immoral actions, and their respective consequences. The y-axis represents the percentage of agreement among annotators. The figure visually shows the level of agreement on the alignment of moral concepts between the dataset and the cultural norms in France. The bars illustrate the percentage of annotators who agreed that the moral values in HISTOIRESMORALES aligned with their understanding of French cultural values for each category.  High percentages indicate strong alignment between the dataset's portrayal of morality and the perceptions of native French speakers.", "section": "Dataset Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.17117/x4.png", "caption": "(a) Average PPL\u00a0for DPOM\u00a0 and DPOI\u00a0in French.", "description": "This figure shows the average perplexity scores for the Mistral language model when fine-tuned using Direct Preference Optimization (DPO) to either favor moral actions (DPOM) or immoral actions (DPOI). The perplexity, a measure of the model's uncertainty, is calculated for both moral and immoral actions in French.  Lower perplexity indicates higher confidence in the model's prediction. The graph likely shows how the perplexity changes with different training set sizes used for the DPO fine-tuning, illustrating the model's robustness or susceptibility to shifts in moral preference depending on the training data.", "section": "6 Influencing LLM with Direct Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2501.17117/x5.png", "caption": "(b) Distance of PPLs\u00a0to the baselines for DPOM\u00a0in French and English.", "description": "This figure shows how much the perplexity scores for moral actions (PPLM) change after applying Direct Preference Optimization (DPO) to favor moral actions (DPOM), compared to the baselines (no DPO).  The x-axis represents the size of the training set used for DPO, and the y-axis represents the difference between the perplexity score obtained with DPOM and the baseline perplexity score. Separate lines show the results for French and English, illustrating the impact of language on model robustness to DPO.", "section": "6 Influencing LLM with Direct Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2501.17117/x6.png", "caption": "(c) Ratio of moral\u00a0actions being preferred based on the PPL.", "description": "This figure shows the percentage of times the model selected a moral action over an immoral action, as determined by the perplexity scores (PPL).  A higher percentage indicates a stronger preference for moral actions. The x-axis represents different training set sizes used in the direct preference optimization (DPO) process.  The different colored bars represent results under different conditions (DPOM, DPOi, baseline).", "section": "6 Influencing LLM with Direct Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2501.17117/x7.png", "caption": "Figure 4: Influencing LLM with DPOM\u00a0 or DPOI, using Mistral model. Average results over 5 runs.", "description": "This figure displays the results of experiments conducted to assess the influence of direct preference optimization (DPO) on a language model's moral alignment.  Three subfigures present the key findings: (a) Average perplexity values for moral and immoral actions when the model was fine-tuned using DPO; (b) The difference in perplexity between the model with and without DPO when trained to favor moral actions; and (c) The percentage of times moral actions were favored during the DPO process.  The experiment was performed using the Mistral language model with the results averaged over five runs. The results are shown separately for English and French language data to examine how language may affect the outcomes.", "section": "6 Influencing LLM with Direct Preference Optimization"}, {"figure_path": "https://arxiv.org/html/2501.17117/x8.png", "caption": "Figure 5: Annotation results for the second batch of annotations (\u00a73.4).", "description": "This figure displays the results from the second round of annotations performed in Section 3.4 of the paper. The goal was to evaluate how effective the prompt with demonstrations was compared to the prompt without demonstrations. Two questions were asked to the annotators for each pair of translations (one with demonstrations and one without): 1) which translation is better; 2) how similar are the translations. Results are presented as percentages showing the proportion of times that the translation with demonstrations was selected as better and the degree of similarity between the translations.", "section": "Dataset Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.17117/x9.png", "caption": "(a) Difference of perplexities to the baselines when fine-tuned to prefer immoral\u00a0 actions in French or English.", "description": "This figure shows how much the perplexity scores for moral and immoral actions change after fine-tuning the Mistral language model using Direct Preference Optimization (DPO) to favor immoral actions.  The x-axis represents the size of the training dataset used in the fine-tuning process, while the y-axis shows the difference in perplexity scores between the fine-tuned model and the original, untrained model for both moral and immoral actions. Separate lines are plotted for French and English data to compare the model's robustness to influence in each language.", "section": "6 Influencing LLM with Direct Preference Optimization"}]