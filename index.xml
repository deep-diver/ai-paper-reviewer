<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/</link><description>Recent content on AI Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>© 2024 AI Paper Reviews by AI</copyright><lastBuildDate>Tue, 29 Oct 2024 20:55:37 +0100</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/index.xml" rel="self" type="application/rss+xml"/><item><title>About This Project</title><link>https://deep-diver.github.io/ai-paper-reviewer/about/</link><pubDate>Tue, 29 Oct 2024 20:55:37 +0100</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/about/</guid><description>&lt;h1 class="relative group">Welcome to AI Paper Reviewer!
&lt;div id="welcome-to-ai-paper-reviewer" class="anchor">&lt;/div>
&lt;/h1>
&lt;p>AI Paper Reviewer is a unique platform dedicated to providing insightful reviews and summaries of artificial intelligence research papers. The content is entirely generated by advanced AI systems, offering a novel approach to understanding and disseminating complex scientific literature.&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/about/cover.png"/></item><item><title>Flow-DPO: Improving LLM Mathematical Reasoning through Online Multi-Agent Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22304/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22304/</guid><description>Flow-DPO improves LLM mathematical reasoning by using online multi-agent learning and direct preference optimization to generate high-quality reasoning traces, surpassing existing methods in performan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22304/cover.png"/></item><item><title>Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21845/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21845/</guid><description>Human-in-the-loop RL system achieves near-perfect success rates on diverse dexterous robotic manipulation tasks within just 1-2.5 hours of real-world training, outperforming prior methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21845/cover.png"/></item><item><title>Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22325/</link><pubDate>Tue, 29 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22325/</guid><description>MCR, a novel pre-training method, learns manipulation-centric robotic representations from large-scale robot datasets, significantly boosting real-world robot manipulation success rates.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.22325/cover.png"/></item><item><title>LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21264/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21264/</guid><description>LARP: a novel video tokenizer using learned holistic queries and an autoregressive prior, achieves state-of-the-art video generation, bridging the gap between reconstruction and generation fidelity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21264/cover.png"/></item><item><title>Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20672/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20672/</guid><description>Recursive Transformers, a novel LLM compression method, achieves comparable performance to larger models using efficient parameter sharing and low-rank adaptation, enabling significant throughput gain&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20672/cover.png"/></item><item><title>ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21465/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21465/</guid><description>SHADOWKV boosts long-context LLM inference throughput by up to 3x and supports 6x larger batch sizes using a novel low-rank key cache and value cache offloading strategy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21465/cover.png"/></item><item><title>Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21220/</link><pubDate>Mon, 28 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21220/</guid><description>Vision Search Assistant empowers vision-language models as robust multimodal search engines by effectively integrating web agents for real-time information retrieval, significantly improving performan&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21220/cover.png"/></item><item><title>AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20424/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20424/</guid><description>AutoKaggle: a multi-agent framework automates data science competitions, achieving 85% validation submission and 82% comprehensive score on 8 Kaggle tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20424/cover.png"/></item><item><title>GrounDiT: Grounding Diffusion Transformers via Noisy Patch Transplantation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20474/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20474/</guid><description>GrounDiT achieves precise spatial grounding in text-to-image generation using a novel training-free approach that transplants denoised image patches into specified regions, significantly improving spa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20474/cover.png"/></item><item><title>Language Models And A Second Opinion Use Case: The Pocket Professional</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20636/</link><pubDate>Sun, 27 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20636/</guid><description>LLMs show promise as second opinion tools for complex medical cases, exceeding human accuracy in straightforward cases but demonstrating limitations with nuanced diagnoses; a new benchmark is establis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20636/cover.png"/></item><item><title>Fast Best-of-N Decoding via Speculative Rejection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20290/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20290/</guid><description>Speculative Rejection: A novel algorithm achieves fast, high-quality LLM decoding by strategically rejecting low-scoring partial generations, offering 16-32x speedup over Best-of-N.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20290/cover.png"/></item><item><title>MarDini: Masked Autoregressive Diffusion for Video Generation at Scale</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20280/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20280/</guid><description>MarDini: Asymmetric video diffusion model scales video generation by integrating masked autoregression for temporal planning and diffusion models for spatial generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20280/cover.png"/></item><item><title>Neural Fields in Robotics: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20220/</link><pubDate>Sat, 26 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20220/</guid><description>Neural Fields revolutionize robotics by enabling robots to perceive and interact with their environment more accurately, opening new avenues for perception, planning, and control.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20220/cover.png"/></item><item><title>COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19313/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19313/</guid><description>COAT achieves memory-efficient FP8 training by compressing optimizer states and activations, resulting in 1.54x memory footprint reduction and 1.43x speedup.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19313/cover.png"/></item><item><title>Counting Ability of Large Language Models and Impact of Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/</guid><description>LLM counting abilities are surprisingly sensitive to tokenization; carefully crafted tokenization strategies significantly improve accuracy, bridging the gap between theory and practice.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19730/cover.png"/></item><item><title>FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/</guid><description>FasterCache: a training-free strategy boosts video diffusion model inference speed by 1.67x without sacrificing video quality, using dynamic feature reuse and CFG-Cache.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19355/cover.png"/></item><item><title>Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/</guid><description>PREREQ-TUNE, a novel LLM fine-tuning strategy, disentangles skill and knowledge learning to significantly reduce hallucinations by mitigating knowledge inconsistency between pre-training and fine-tuni&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19290/cover.png"/></item><item><title>GPT-4o System Card</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21276/</link><pubDate>Fri, 25 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21276/</guid><description>GPT-40, an advanced multimodal AI model, boasts impressive speed and capabilities across various modalities, yet faces challenges in safety and bias mitigation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21276/cover.png"/></item><item><title>AgentStore: Scalable Integration of Heterogeneous Agents As Specialized Generalist Computer Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18603/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18603/</guid><description>AgentStore dynamically integrates diverse AI agents for superior task automation, outperforming previous systems by enhancing both generalization and specialization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18603/cover.png"/></item><item><title>Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/</guid><description>LLMs can detect and correct substantial label errors in NLP datasets, significantly improving model performance and highlighting the importance of data quality in NLP.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18889/cover.png"/></item><item><title>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/</guid><description>CAMEL-Bench: a new open-source benchmark rigorously evaluates Arabic LMMs across 8 diverse domains and 38 sub-domains, revealing significant room for improvement even in top models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18976/cover.png"/></item><item><title>CCI3.0-HQ: a large-scale Chinese dataset of high quality designed for pre-training large language models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/</guid><description>CCI3.0-HQ: A new 500GB high-quality Chinese dataset significantly boosts large language model performance, surpassing existing datasets on various benchmarks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18505/cover.png"/></item><item><title>Data Scaling Laws in Imitation Learning for Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/</guid><description>Robotic manipulation policies achieve near 90% success in novel environments and with unseen objects using a data-driven approach that leverages power-law scaling relationships.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18647/cover.png"/></item><item><title>DeCoRe: Decoding by Contrasting Retrieval Heads to Mitigate Hallucinations</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/</guid><description>DeCoRe, a novel training-free decoding strategy, significantly reduces LLM hallucinations by contrasting outputs from masked and unmasked retrieval heads, improving accuracy on various tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18860/cover.png"/></item><item><title>Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18481/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18481/</guid><description>Dialog2Flow (D2F) pre-trains soft-contrastive action-driven sentence embeddings to automatically extract dialog workflows, achieving superior performance on diverse datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18481/cover.png"/></item><item><title>Distill Visual Chart Reasoning Ability from LLMs to MLLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/</guid><description>Researchers synthesize a new multimodal dataset, REACHQA, using code as an intermediary to efficiently distill visual chart reasoning abilities from LLMs to MLLMs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18798/cover.png"/></item><item><title>DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18666/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18666/</guid><description>DreamClear: a high-capacity image restoration model, uses a dual-prompt learning pipeline to create a large-scale dataset and achieves photorealistic restoration of real-world low-quality images.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18666/cover.png"/></item><item><title>Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/</guid><description>This work introduces a new framework that learns object dynamics directly from multi-view videos by explicitly considering robot actions, achieving accurate 3D action-conditioned video prediction and &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18912/cover.png"/></item><item><title>Framer: Interactive Frame Interpolation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/</guid><description>Framer: an interactive frame interpolation tool lets users customize video transitions by adjusting keypoints, yielding smooth, creative results—even handling complex scenarios with an &amp;lsquo;autopilot&amp;rsquo; mod&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18978/cover.png"/></item><item><title>Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/</guid><description>Researchers developed a hybrid approach to collecting preference data for AI alignment, cleverly routing instances to either human or AI annotators based on a predictive model, resulting in improved m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19133/cover.png"/></item><item><title>Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/</guid><description>Infinity-MM, a 40-million-sample multimodal instruction dataset, boosts open-source VLM performance to state-of-the-art levels by combining real-world and synthetic data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18558/cover.png"/></item><item><title>LOGO -- Long cOntext aliGnment via efficient preference Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/</guid><description>LOGO, a novel training strategy, significantly boosts long-context model performance by efficiently optimizing preference alignment, achieving comparable results to GPT-4 with minimal data.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18533/cover.png"/></item><item><title>MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/</guid><description>MMAU benchmark challenges multimodal LLMs with diverse audio tasks, revealing significant gaps in current audio understanding capabilities and driving future advancements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19168/cover.png"/></item><item><title>MotionCLR: Motion Generation and Training-free Editing via Understanding Attention Mechanisms</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/</guid><description>MotionCLR: Training-free, interactive human motion editing via attention mechanism manipulation. Versatile editing, good generation quality, and strong explainability achieved.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18977/cover.png"/></item><item><title>Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/</guid><description>VINE, a novel watermarking method, significantly improves robustness against advanced image editing using generative priors, outperforming existing methods in both image quality and robustness, as val&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18775/cover.png"/></item><item><title>Should We Really Edit Language Models? On the Evaluation of Edited Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/</guid><description>Contrary to popular belief, current language model editing techniques cause inevitable performance decline and safety issues when scaling edits, urging the need for more practical methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18785/cover.png"/></item><item><title>Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/</guid><description>Skywork-Reward achieves state-of-the-art results on RewardBench using a novel data-centric approach, developing high-performing reward models with a significantly smaller dataset (80K pairs) than exis&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18451/cover.png"/></item><item><title>SMITE: Segment Me In TimE</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/</guid><description>SMITE: a new video segmentation method achieving temporally consistent, fine-grained segmentations using only a few reference images, outperforming state-of-the-art alternatives.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18538/cover.png"/></item><item><title>Stable Consistency Tuning: Understanding and Improving Consistency Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/</guid><description>Stable Consistency Tuning (SCT) significantly boosts consistency model training, achieving state-of-the-art results by reducing variance and improving sampling efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18958/cover.png"/></item><item><title>Taipan: Efficient and Expressive State Space Language Models with Selective Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/</guid><description>Taipan, a novel hybrid language model, achieves superior performance and efficiency in handling extremely long text sequences by selectively applying attention, combining the strengths of State Space &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18572/cover.png"/></item><item><title>The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18441/</guid><description>This paper enhances generative AI Transformer models by introducing probabilistic optimization solutions for subword encoding, hyperparameter tuning, attention mechanisms, and quantization, resulting &amp;hellip;</description></item><item><title>Unbounded: A Generative Infinite Game of Character Life Simulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/</guid><description>UNBOUNDED, a generative infinite game, uses AI to create a continuously evolving character life simulation with open-ended interactions and real-time visual generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18975/cover.png"/></item><item><title>Unleashing Reasoning Capability of LLMs via Scalable Question Synthesis from Scratch</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/</guid><description>ScaleQuest: a novel data synthesis method unleashes LLMs&amp;rsquo; reasoning power by generating a massive, high-quality mathematical reasoning dataset from scratch using efficient, open-source models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18693/cover.png"/></item><item><title>VideoWebArena: Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19100/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19100/</guid><description>VideoWebArena benchmark evaluates long-context multimodal agents&amp;rsquo; video understanding abilities via 2021 web tasks, revealing significant performance gaps compared to humans and highlighting key areas&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19100/cover.png"/></item><item><title>WAFFLE: Multi-Modal Model for Automated Front-End Development</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/</guid><description>WAFFLE: a new fine-tuning method dramatically improves UI design-to-HTML code generation by using structure-aware attention and contrastive learning, outperforming current state-of-the-art models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18362/cover.png"/></item><item><title>Why Does the Effective Context Length of LLMs Fall Short?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</link><pubDate>Thu, 24 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/</guid><description>Researchers unveil STRING, a training-free method that boosts large language models&amp;rsquo; long-context performance by cleverly shifting position embeddings, achieving state-of-the-art results on open-sourc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18745/cover.png"/></item><item><title>ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/</guid><description>ADEM-VL boosts vision-language model efficiency by using a parameter-free cross-attention mechanism and an adaptive fusion scheme, achieving state-of-the-art accuracy with reduced computational demand&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17779/cover.png"/></item><item><title>Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/</guid><description>Asynchronous off-policy RLHF accelerates LLM training by 40% without sacrificing performance, achieving compute-optimal scaling by decoupling generation and learning phases.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18252/cover.png"/></item><item><title>DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/</guid><description>DynamicCity generates large-scale, high-quality 4D LiDAR scenes capturing dynamic environments, surpassing existing methods in efficiency and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18084/cover.png"/></item><item><title>Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/</guid><description>SUPE leverages unlabeled prior data to pre-train skills and pseudo-label trajectories for efficient online RL exploration, significantly outperforming existing methods on challenging tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18076/cover.png"/></item><item><title>Lightweight Neural App Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/</guid><description>LiMAC, a novel lightweight architecture, enables efficient mobile app control by combining a small action transformer with a fine-tuned vision-language model, significantly improving accuracy and spee&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17883/cover.png"/></item><item><title>MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/</guid><description>MIA-DPO boosts large vision-language models&amp;rsquo; multi-image understanding by cleverly augmenting single-image data and using attention mechanisms to improve preference alignment, significantly reducing a&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17637/cover.png"/></item><item><title>Multi-Draft Speculative Sampling: Canonical Architectures and Theoretical Limits</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/</guid><description>Researchers boost large language model inference speed by 10x using a novel multi-draft speculative sampling method with theoretical performance guarantees.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18234/cover.png"/></item><item><title>ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/</guid><description>ROCKET-1 masters open-world Minecraft interaction by using visual-temporal context prompting, enabling VLMs to effectively guide low-level policies for complex tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17856/cover.png"/></item><item><title>Scalable Ranked Preference Optimization for Text-to-Image Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/</guid><description>Researchers created a scalable training method for text-to-image models using synthetic, ranked preference data, significantly improving both visual quality and prompt-following.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18013/cover.png"/></item><item><title>Scaling Diffusion Language Models via Adaptation from Autoregressive Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/</guid><description>Researchers efficiently adapt existing large autoregressive language models into competitive diffusion language models, achieving scalability and outperforming prior diffusion models on various benchm&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17891/cover.png"/></item><item><title>TP-Eval: Tap Multimodal LLMs' Potential in Evaluation by Customizing Prompts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/</guid><description>TP-Eval, a novel framework, tackles MLLM evaluation bias by customizing prompts for each model, revealing true capabilities and improving benchmark reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18071/cover.png"/></item><item><title>Value Residual Learning For Alleviating Attention Concentration In Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/</guid><description>ResFormer and SVFormer alleviate Transformer attention concentration, boosting training speed and accuracy by introducing residual value connections and single-layer value sharing, respectively.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17897/cover.png"/></item><item><title>WorldSimBench: Towards Video Generation Models as World Simulators</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/</guid><description>WorldSimBench: a new benchmark rigorously evaluates video generation models as embodied AI agents, using dual evaluation (perceptual and manipulative) and the novel HF-Embodied Dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18072/cover.png"/></item><item><title>ZIP-FIT: Embedding-Free Data Selection via Compression-Based Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</link><pubDate>Wed, 23 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/</guid><description>ZIP-FIT uses gzip compression to efficiently select task-relevant training data for language models, drastically improving fine-tuning speed and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18194/cover.png"/></item><item><title>Aligning Large Language Models via Self-Steering Optimization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/</guid><description>Self-Steering Optimization (SSO) autonomously generates high-quality preference signals for aligning large language models, significantly improving performance across various benchmarks without manual&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17131/cover.png"/></item><item><title>Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/</guid><description>Inf-CL shatters memory limits in contrastive learning, enabling training with massive batch sizes (millions) using a novel tile-based computation strategy for unprecedented accuracy and speed.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17243/cover.png"/></item><item><title>Frontiers in Intelligent Colonoscopy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/</guid><description>This study advances intelligent colonoscopy by creating ColonINST, a large-scale multimodal dataset, and ColonGPT, a multimodal language model, to improve colonoscopic scene perception.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17241/cover.png"/></item><item><title>JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/</guid><description>JMMMU, a new benchmark, rigorously evaluates large multimodal models&amp;rsquo; Japanese language and cultural understanding, revealing significant performance gaps and highlighting the need for culturally awar&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17250/cover.png"/></item><item><title>LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/</guid><description>LongVU, a novel spatiotemporal compression mechanism, enables efficient processing of long videos by LLMs, improving video understanding performance significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17434/cover.png"/></item><item><title>LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/</guid><description>LVSM, a novel transformer-based model, achieves state-of-the-art novel view synthesis by eliminating 3D inductive biases, enabling superior quality, scalability, and zero-shot generalization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17242/cover.png"/></item><item><title>Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/</guid><description>Math Neurosurgery precisely isolates math reasoning parameters within LLMs using only forward passes, boosting performance without affecting non-math skills.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16930/cover.png"/></item><item><title>MiniPLM: Knowledge Distillation for Pre-Training Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/</guid><description>MINIPLM: A novel knowledge distillation framework boosts pre-trained language models&amp;rsquo; performance by efficiently refining the training data distribution using teacher LM knowledge, achieving significa&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17215/cover.png"/></item><item><title>PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/</guid><description>PyramidDrop boosts Large Vision-Language Model efficiency by 40% during training and 55% during inference, achieving comparable performance by progressively reducing image token redundancy in deeper l&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17247/cover.png"/></item><item><title>SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</link><pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/</guid><description>SpectroMotion: a novel approach to reconstruct dynamic specular scenes by combining 3D Gaussian Splatting with physically-based rendering and deformation fields, outperforming existing methods in view&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.17249/cover.png"/></item><item><title>3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/</guid><description>3DGS-Enhancer boosts 3D scene rendering from sparse views by cleverly using video diffusion priors to improve view consistency, resulting in superior quality and performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16266/cover.png"/></item><item><title>Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/</guid><description>Agent-to-Sim (ATS) learns realistic 3D agent behavior models from casual, longitudinal videos by reconstructing a persistent 4D representation and training a generative model, enabling real-to-sim tra&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16259/cover.png"/></item><item><title>Alchemy: Amplifying Theorem-Proving Capability through Symbolic Mutation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/</guid><description>Alchemy: Amplifying Theorem-Proving with Symbolic Mutation synthesizes formal mathematical theorems, boosting neural theorem-proving performance by up to 5%.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15748/cover.png"/></item><item><title>AutoTrain: No-code training for state-of-the-art models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/</guid><description>AutoTrain: a no-code, open-source library simplifies training state-of-the-art models on custom datasets for various tasks, democratizing access to advanced AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15735/cover.png"/></item><item><title>Can Knowledge Editing Really Correct Hallucinations?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/</guid><description>HalluEditBench: A new benchmark reveals knowledge editing&amp;rsquo;s limitations in truly fixing LLM hallucinations, offering valuable insights for future improvements.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16251/cover.png"/></item><item><title>CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/</guid><description>CompassJudger-1: An open-source, all-in-one judge LLM offering unitary scoring, model comparison, critique generation, and diverse task execution, significantly advancing LLM evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16256/cover.png"/></item><item><title>Continuous Speech Synthesis using per-token Latent Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/</guid><description>SALAD, a novel per-token latent diffusion model, achieves superior zero-shot speech synthesis, surpassing discrete methods in intelligibility while maintaining speech quality and speaker similarity.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16048/cover.png"/></item><item><title>FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/</guid><description>FrugalNeRF: achieving high-fidelity 3D scene reconstruction from minimal data with unprecedented speed, eliminating the need for pre-trained models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16271/cover.png"/></item><item><title>Improve Vision Language Model Chain-of-thought Reasoning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/</guid><description>Researchers enhance vision-language model reasoning by distilling rationales from GPT-4, fine-tuning with a new dataset, and applying reinforcement learning, achieving significant performance gains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16198/cover.png"/></item><item><title>Language Models are Symbolic Learners in Arithmetic</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/</guid><description>LLMs don&amp;rsquo;t calculate; they&amp;rsquo;re symbolic learners in arithmetic, mastering tasks through subgroup pattern recognition, prioritizing easy-to-hard pattern selection.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15580/cover.png"/></item><item><title>LLM-based Optimization of Compound AI Systems: A Survey</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/</guid><description>This survey reveals how Large Language Models (LLMs) efficiently optimize complex AI systems by acting as end-to-end optimizers, bypassing gradient calculations and generating intricate instructions.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16392/cover.png"/></item><item><title>Mitigating Object Hallucination via Concentric Causal Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/</guid><description>Concentric Causal Attention (CCA) significantly reduces object hallucination in large vision-language models by mitigating the negative effects of long-term decay in Rotary Position Encoding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15926/cover.png"/></item><item><title>Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/</guid><description>PANGEA: A fully open multilingual, multimodal LLM trained on a diverse 6M instruction dataset, significantly outperforming existing models in multilingual and culturally diverse scenarios.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16153/cover.png"/></item><item><title>Pantograph: A Machine-to-Machine Interaction Interface for Advanced Theorem Proving, High Level Reasoning, and Data Extraction in Lean 4</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/</guid><description>Pantograph: a new Lean 4 interface boosts machine-assisted theorem proving by enabling efficient proof search and high-level reasoning via novel features, including draft-sketch-proof (DSP) support.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16429/cover.png"/></item><item><title>Pre-training Distillation for Large Language Models: A Design Space Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16215/</guid><description>Boosting large language model pre-training: This research explores pre-training distillation, systematically optimizing its design to significantly improve student LLM performance.</description></item><item><title>Reflection-Bench: probing AI intelligence with reflection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/</guid><description>Reflection-Bench, a new benchmark, reveals current LLMs lack true reflection abilities, highlighting a critical gap in achieving human-level AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16270/cover.png"/></item><item><title>RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/</guid><description>RM-BENCH, a novel benchmark, rigorously evaluates reward models&amp;rsquo; sensitivity to subtle content and style biases, showing a strong correlation with policy model performance and revealing significant ro&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16184/cover.png"/></item><item><title>SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/</guid><description>SAM2Long enhances video object segmentation by adding a training-free memory tree, significantly improving accuracy and robustness in complex, long-term videos without needing further training.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16268/cover.png"/></item><item><title>Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/</guid><description>GATEAU, a novel framework, efficiently selects high-quality long-context samples for LLM training by using Homologous Models&amp;rsquo; Guidance and Contextual Awareness Measurement, significantly boosting perf&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15633/cover.png"/></item><item><title>Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/</guid><description>SPARE, a training-free method, uses sparse autoencoders to precisely steer LLMs&amp;rsquo; knowledge selection, resolving context-memory conflicts and significantly improving accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15999/cover.png"/></item><item><title>Teach Multimodal LLMs to Comprehend Electrocardiographic Images</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/</guid><description>PULSE, a new MLLM, achieves state-of-the-art accuracy in ECG image interpretation, exceeding existing models by 15-30%, thanks to a novel million-sample instruction tuning dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.19008/cover.png"/></item><item><title>xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</link><pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/</guid><description>xGen-MM-Vid efficiently captures temporal information in videos using only 32 tokens, achieving state-of-the-art accuracy with significantly reduced computational cost.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.16267/cover.png"/></item><item><title>Hallucination Detox: Sensitive Neuron Dropout (SeND) for Large Language Model Training</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15460/</guid><description>New training method, Sensitive Neuron Dropout (SeND), reduces large language model hallucinations by up to 40% while improving efficiency.</description></item><item><title>Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/</guid><description>Ichigo, a new real-time voice assistant, leverages a novel mixed-modal early-fusion approach for superior speed and accuracy in speech-based tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15316/cover.png"/></item><item><title>M-RewardBench: Evaluating Reward Models in Multilingual Settings</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</link><pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15522/</guid><description>M-REWARDBENCH: A new multilingual benchmark reveals significant performance gaps in reward models across languages, highlighting the need for improved multilingual LLM development.</description></item><item><title>Baichuan Alignment Technical Report</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/</guid><description>Baichuan Alignment unveils cutting-edge techniques for aligning large language models, resulting in significant performance improvements and valuable insights for advancing AI research.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14940/cover.png"/></item><item><title>DM-Codec: Distilling Multimodal Representations for Speech Tokenization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/</guid><description>DM-Codec, a novel speech tokenizer, leverages combined language and speech model distillation to achieve state-of-the-art performance in speech tokenization, reducing error rates significantly.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15017/cover.png"/></item><item><title>How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</link><pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/</guid><description>Researchers discover the &amp;lsquo;imitation threshold&amp;rsquo; in text-to-image models: around 200-600 training examples of a concept are needed before reliable imitation occurs.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.15002/cover.png"/></item><item><title>Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14677/</guid><description>AI-generated text detection is flawed; this paper reveals dataset quality issues, proposes evaluation methods, and shows how high-quality generated data can improve detection model accuracy.</description></item><item><title>BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/</guid><description>BiGR: A novel image generation model using compact binary codes, unifying generation and discrimination for superior performance and zero-shot generalization across various vision tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14672/cover.png"/></item><item><title>EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14649/</guid><description>EvoPress: A new evolutionary search method achieves optimal dynamic LLM compression, surpassing current techniques in accuracy and efficiency across various compression methods.</description></item><item><title>How Do Training Methods Influence the Utilization of Vision Models?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/</guid><description>Training methods profoundly alter how neural networks utilize their layers, revealing that efficient training prioritizes early layers while adversarial training emphasizes deeper ones.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14470/cover.png"/></item><item><title>Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/</guid><description>Montessori-Instruct optimizes synthetic training data for LLMs by aligning it with student learning preferences, significantly boosting student model performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14208/cover.png"/></item><item><title>NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/</guid><description>NaturalBench: a new benchmark exposes VLMs&amp;rsquo; vulnerabilities to natural adversarial samples, highlighting compositionality challenges &amp;amp; bias issues, and promoting dynamic VLM evaluation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14669/cover.png"/></item><item><title>Teaching Models to Balance Resisting and Accepting Persuasion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</link><pubDate>Fri, 18 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/</guid><description>LLMs are taught to both resist harmful and accept helpful persuasion using Persuasion-Balanced Training, resulting in more reliable and collaborative AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14596/cover.png"/></item><item><title>$γ-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13859/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13859/</guid><description>γ-MoD efficiently adapts Mixture-of-Depths to existing MLLMs, drastically reducing computational costs without significant performance loss, paving the way for more practical multimodal AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13859/cover.png"/></item><item><title>A Comparative Study on Reasoning Patterns of OpenAI's o1 Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13639/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13639/</guid><description>OpenAI&amp;rsquo;s o1 model surpasses other LLMs in reasoning tasks by employing unique inference strategies, revealing six novel reasoning patterns analyzed across various benchmarks.</description></item><item><title>A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13841/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13841/</guid><description>Researchers reveal a unified framework for editing post-trained large language model parameters, improving efficiency and performance by mathematically analyzing the impact of different editing operat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13841/cover.png"/></item><item><title>ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/</guid><description>ARKit LabelMaker creates the largest real-world 3D dataset with dense semantic annotations, boosting performance of 3D semantic segmentation models and accelerating progress in indoor scene understand&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13924/cover.png"/></item><item><title>BenTo: Benchmark Task Reduction with In-Context Transferability</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13804/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13804/</guid><description>BENTO efficiently reduces LLM benchmark size by 95% using in-context transferability, achieving 97% evaluation accuracy, saving computational costs without compromising quality.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13804/cover.png"/></item><item><title>Can MLLMs Understand the Deep Implication Behind Chinese Images?</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13854/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13854/</guid><description>CII-Bench, a new benchmark, reveals that current MLLMs struggle to understand the deeper implications within Chinese images, particularly those related to traditional culture, showcasing a significant&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13854/cover.png"/></item><item><title>CBT-Bench: Evaluating Large Language Models on Assisting Cognitive Behavior Therapy</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/</guid><description>CBT-BENCH: a new benchmark reveals LLMs&amp;rsquo; potential and limitations in assisting Cognitive Behavioral Therapy, highlighting the need for further research in AI-driven mental healthcare.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13218/cover.png"/></item><item><title>Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/</guid><description>New framework, CIA Suite, enables accurate, automated cross-lingual evaluation of multilingual LLMs using a novel test set and evaluator LLMs, bridging the gap in multilingual NLP assessment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13394/cover.png"/></item><item><title>DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking Head Video Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/</guid><description>DAWN: a new framework for generating realistic talking head videos from a single image and audio, using a fast non-autoregressive diffusion model to overcome limitations of previous approaches.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13726/cover.png"/></item><item><title>Diffusion Curriculum: Synthetic-to-Real Generative Curriculum Learning via Image-Guided Diffusion</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/</guid><description>Boosting AI&amp;rsquo;s learning from limited or poor-quality data, this paper introduces DisCL, a novel curriculum learning method using image-guided diffusion models to generate diverse synthetic training dat&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13674/cover.png"/></item><item><title>Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13334/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13334/</guid><description>LLM safety mechanisms, while aiming to prevent harmful outputs, paradoxically introduce biases that enable &amp;lsquo;jailbreaks&amp;rsquo;; this research quantifies these biases and proposes a novel defense.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13334/cover.png"/></item><item><title>DPLM-2: A Multimodal Diffusion Protein Language Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/</guid><description>DPLM-2: A multimodal diffusion model revolutionizes protein structure &amp;amp; sequence generation, achieving superior accuracy and diversity via efficient training and structure tokenization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13782/cover.png"/></item><item><title>DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13830/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13830/</guid><description>DreamVideo-2 achieves zero-shot video customization with precise motion control by using a novel mask-guided motion module and masked reference attention, overcoming the limitations of previous method&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13830/cover.png"/></item><item><title>Failing Forward: Improving Generative Error Correction for ASR with Synthetic Data and Retrieval Augmentation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13198/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13198/</guid><description>DARAG boosts ASR accuracy by 8-33% using synthetic data and retrieval augmentation to improve Generative Error Correction, overcoming limitations of traditional GEC models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13198/cover.png"/></item><item><title>FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/</guid><description>FiTv2, an enhanced flexible vision transformer, achieves state-of-the-art image generation by dynamically processing images as sequences of tokens, overcoming resolution limitations of prior models.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13925/cover.png"/></item><item><title>Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13863/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13863/</guid><description>FLUID, a 10.5B parameter autoregressive model using continuous tokens and random order generation, achieves state-of-the-art text-to-image generation, demonstrating that careful model design can unloc&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13863/cover.png"/></item><item><title>Harnessing Webpage UIs for Text-Rich Visual Understanding</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13824/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13824/</guid><description>MultiUI, a massive dataset of 7.3M multimodal instructions synthesized from web UIs, significantly boosts text-rich visual understanding model performance across diverse tasks, exceeding specialized m&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13824/cover.png"/></item><item><title>In-context learning and Occam's razor</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14086/</guid><description>In-context learning&amp;rsquo;s success is explained by its implicit minimization of both training error and model complexity, akin to Occam&amp;rsquo;s Razor, achieved through a data compression lens.</description></item><item><title>Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13848/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13848/</guid><description>Janus, a novel autoregressive framework, unifies multimodal understanding and generation by decoupling visual encoding, surpassing previous unified models and achieving state-of-the-art results.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13848/cover.png"/></item><item><title>LoLDU: Low-Rank Adaptation via Lower-Diag-Upper Decomposition for Parameter-Efficient Fine-Tuning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13618/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13618/</guid><description>LoLDU, a novel parameter-efficient fine-tuning method, drastically reduces trainable parameters in large language models using Lower-Diag-Upper decomposition, achieving comparable performance to full &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13618/cover.png"/></item><item><title>Looking Inward: Language Models Can Learn About Themselves by Introspection</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/</guid><description>Language Models can learn about themselves through introspection, outperforming other models in self-prediction tasks, suggesting a form of internal self-awareness.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13787/cover.png"/></item><item><title>MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/</guid><description>MagicTailor empowers text-to-image models with component-level control over personalized concepts, enabling fine-grained customization and high-quality image generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13370/cover.png"/></item><item><title>MedINST: Meta Dataset of Biomedical Instructions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/</guid><description>MEDINST, a novel biomedical instruction meta-dataset with 133 tasks and 7M samples, significantly improves LLMs&amp;rsquo; cross-task generalization in medical analysis.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13458/cover.png"/></item><item><title>MixEval-X: Any-to-Any Evaluations from Real-World Data Mixtures</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13754/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13754/</guid><description>MixEval-X: a new benchmark standardizes multi-modal AI evaluations using real-world data mixtures, improving consistency and reducing bias in model rankings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13754/cover.png"/></item><item><title>MobA: A Two-Level Agent System for Efficient Mobile Task Automation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13757/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13757/</guid><description>MobA, a novel two-level agent system, significantly improves mobile task automation efficiency by combining multimodal LLMs with a sophisticated task planning and reflection mechanism.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13757/cover.png"/></item><item><title>PopAlign: Diversifying Contrasting Patterns for a More Comprehensive Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13785/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13785/</guid><description>PopAlign improves LLM alignment by diversifying contrasting patterns across prompt, model, and pipeline levels, resulting in more comprehensive and robust alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13785/cover.png"/></item><item><title>PUMA: Empowering Unified MLLM with Multi-granular Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/</guid><description>PUMA: a unified multi-granular MLLM excels at diverse visual tasks by seamlessly integrating image generation and understanding, addressing varying granularity demands.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13861/cover.png"/></item><item><title>Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13360/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13360/</guid><description>RAP-MLLMs: Personalize AI assistants in real-time without retraining, using a retrieval-augmented framework and a new dataset for infinite visual concept understanding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13360/cover.png"/></item><item><title>Retrospective Learning from Interactions</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13852/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13852/</guid><description>RESPECT: a novel method improves language models by learning from implicit user feedback in multi-turn interactions, boosting task completion rates without external annotation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13852/cover.png"/></item><item><title>Roadmap towards Superhuman Speech Understanding using Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13268/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13268/</guid><description>New roadmap &amp;amp; benchmark for superhuman speech understanding using LLMs, revealing key limitations in handling abstract acoustic knowledge and non-semantic information.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13268/cover.png"/></item><item><title>Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/</guid><description>Router-Tuning and MindSkip boost Transformer efficiency by dynamically adjusting computation depth, achieving 21% speedup with minimal performance loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13184/cover.png"/></item><item><title>SBI-RAG: Enhancing Math Word Problem Solving for Students through Schema-Based Instruction and Retrieval-Augmented Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13293/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13293/</guid><description>SBI-RAG enhances math word problem solving by integrating schema-based instruction with a large language model, improving reasoning clarity and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13293/cover.png"/></item><item><title>SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/</guid><description>SeerAttention learns intrinsic attention sparsity, achieving significant speedups in LLMs without sacrificing accuracy, via a novel learnable gating mechanism and customized FlashAttention.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13276/cover.png"/></item><item><title>SemiEvol: Semi-supervised Fine-tuning for LLM Adaptation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/</guid><description>SEMIEVOL, a novel semi-supervised framework, significantly improves large language model adaptation by effectively leveraging both limited labeled and abundant unlabeled data, achieving superior perfo&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14745/cover.png"/></item><item><title>Steering Your Generalists: Improving Robotic Foundation Models via Value Guidance</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/</guid><description>Boosting robot performance at deployment time, Value-Guided Policy Steering (V-GPS) re-ranks actions from existing policies using a value function learned via offline RL, consistently improving perfor&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13816/cover.png"/></item><item><title>UCFE: A User-Centric Financial Expertise Benchmark for Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/</guid><description>UCFE benchmark realistically evaluates LLMs&amp;rsquo; financial expertise via user-centric design and dynamic interactions, revealing performance gaps and highlighting human-preference alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.14059/cover.png"/></item><item><title>VidPanos: Generative Panoramic Videos from Casual Panning Videos</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13832/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13832/</guid><description>VidPanos generates realistic panoramic videos from casual panning videos by cleverly using generative video models to fill in unseen parts of the scene, offering a significant step towards immersive v&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13832/cover.png"/></item><item><title>Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</link><pubDate>Thu, 17 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/</guid><description>Boosting LLM-based web agents: This work introduces world models, improving efficiency and cost in web navigation by simulating action outcomes before execution.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13232/cover.png"/></item><item><title>AERO: Softmax-Only LLMs for Efficient Private Inference</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/</guid><description>AERO achieves 4.23x communication and 1.94x latency reduction in private AI inference by developing a Softmax-only LLM architecture with novel entropy regularization.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13060/cover.png"/></item><item><title>Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/</guid><description>KeyNMF, a novel topic modeling approach, effectively analyzes information dynamics in Chinese diaspora media, revealing the PRC&amp;rsquo;s potential influence on European elections.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12791/cover.png"/></item><item><title>DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12628/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12628/</guid><description>DocLayout-YOLO: Blazing-fast document layout analysis via diverse synthetic data and adaptive perception, exceeding state-of-the-art speed and accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12628/cover.png"/></item><item><title>Exploring Model Kinship for Merging Large Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12613/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12613/</guid><description>Researchers improve large language model capabilities by introducing &amp;lsquo;model kinship&amp;rsquo; – a metric measuring LLM similarity, which guides a novel merging strategy for enhanced performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12613/cover.png"/></item><item><title>HumanEval-V: Evaluating Visual Understanding and Reasoning Abilities of Large Multimodal Models Through Coding Tasks</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12381/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12381/</guid><description>HumanEval-V: A new benchmark rigorously evaluates large multimodal models&amp;rsquo; visual understanding and reasoning abilities through carefully designed coding tasks, revealing significant limitations in cu&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12381/cover.png"/></item><item><title>Insights from the Inverse: Reconstructing LLM Training Goals Through Inverse RL</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12491/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12491/</guid><description>Researchers used inverse reinforcement learning to reveal hidden reward functions in large language models, achieving up to 80% accuracy in predicting human preferences and offering new insights into &amp;hellip;</description></item><item><title>JudgeBench: A Benchmark for Evaluating LLM-based Judges</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/</guid><description>JudgeBench: a new benchmark objectively evaluates LLM-based judges on complex tasks, revealing that even top models struggle, highlighting the need for more advanced AI judges.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12784/cover.png"/></item><item><title>Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/</guid><description>Long-LRM: A groundbreaking 3D reconstruction model generating photorealistic, wide-coverage scenes from 32 images in 1.3 seconds using a novel hybrid architecture.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12781/cover.png"/></item><item><title>Meta-Chunking: Learning Efficient Text Segmentation via Logical Perception</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/</guid><description>Meta-Chunking boosts RAG performance by intelligently segmenting text into logically coherent chunks, improving knowledge retrieval and question answering.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12788/cover.png"/></item><item><title>MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/</guid><description>MMed-RAG significantly boosts medical vision-language model factuality by using domain-aware retrieval, adaptive context selection, and RAG-based preference fine-tuning, achieving an average 43.8% imp&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.13085/cover.png"/></item><item><title>MuVi: Video-to-Music Generation with Semantic Alignment and Rhythmic Synchronization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/</guid><description>MuVi: a novel framework generating synchronized music for videos, achieving superior semantic alignment and rhythmic synchronization through contrastive learning and flow-matching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12957/cover.png"/></item><item><title>Open Materials 2024 (OMat24) Inorganic Materials Dataset and Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/</guid><description>Meta FAIR released OMat24, a massive open dataset of inorganic materials with 110M+ DFT calculations and state-of-the-art Equiformer V2 models, accelerating AI-driven materials discovery.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12771/cover.png"/></item><item><title>ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12405/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12405/</guid><description>ProSA assesses LLM prompt sensitivity using a new metric, revealing that larger models are more robust but subjective evaluations are also affected by prompt variations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12405/cover.png"/></item><item><title>Revealing the Barriers of Language Agents in Planning</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12409/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12409/</guid><description>Language agents struggle with planning due to limited constraint understanding and the diminishing influence of goals, hindering human-level performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12409/cover.png"/></item><item><title>Stabilize the Latent Space for Image Autoregressive Modeling: A Unified Perspective</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12490/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12490/</guid><description>By stabilizing the latent space using a novel discrete image tokenizer, researchers achieve superior performance in image autoregressive modeling, surpassing previous state-of-the-art methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12490/cover.png"/></item><item><title>The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12787/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12787/</guid><description>Large multimodal models are prone to hallucinations; this work systematically investigates these, pinpointing key causes and introducing a benchmark for improved model reliability.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12787/cover.png"/></item><item><title>Tracking Universal Features Through Fine-Tuning and Model Merging</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12391/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12391/</guid><description>Researchers tracked feature evolution in small language models through fine-tuning and model merging, discovering surprising feature instability and uncovering interpretable persistent features like v&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12391/cover.png"/></item><item><title>TransAgent: Transfer Vision-Language Foundation Models with Heterogeneous Agent Collaboration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/</guid><description>TransAgent empowers vision-language models by collaboratively distilling knowledge from diverse expert agents, achieving state-of-the-art performance on visual recognition tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12183/cover.png"/></item><item><title>WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/</guid><description>WORLDCUISINES: a massive multilingual VQA benchmark on global cuisines, reveals cultural knowledge gaps in current vision-language models and provides a valuable resource for advancing research in thi&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12705/cover.png"/></item><item><title>WorldMedQA-V: a multilingual, multimodal medical examination dataset for multimodal language models evaluation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12722/</link><pubDate>Wed, 16 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12722/</guid><description>WorldMedQA-V: a new multilingual, multimodal medical exam dataset helps fairly evaluate AI&amp;rsquo;s performance in diverse healthcare settings.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12722/cover.png"/></item><item><title>Improving Long-Text Alignment for Text-to-Image Diffusion Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11817/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11817/</guid><description>LongAlign enhances text-to-image diffusion models by introducing segment-level encoding and decomposed preference optimization, achieving superior long-text alignment.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11817/cover.png"/></item><item><title>Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/</guid><description>Mini-Omni2 is an open-source, multi-modal language model closely replicating GPT-40&amp;rsquo;s vision, speech, and duplex capabilities, trained efficiently on a limited dataset.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11190/cover.png"/></item><item><title>MoH: Multi-Head Attention as Mixture-of-Head Attention</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11842/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11842/</guid><description>MoH improves Transformer efficiency by dynamically routing attention heads, enhancing inference speed and reducing computational costs without accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11842/cover.png"/></item><item><title>OMCAT: Omni Context Aware Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12109/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12109/</guid><description>OMCAT, a new model, excels at cross-modal temporal understanding by using a novel dataset (OCTAV) and ROTE, an enhanced version of RoPE, achieving state-of-the-art results on AVQA tasks.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.12109/cover.png"/></item><item><title>SHAKTI: A 2.5 Billion Parameter Small Language Model Optimized for Edge AI and Low-Resource Environments</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11331/</guid><description>Shakti, a 2.5B parameter language model, achieves high performance on edge devices using innovative techniques like VGQA and SwiGLU, outperforming larger models in several benchmarks.</description></item><item><title>VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11623/</link><pubDate>Tue, 15 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11623/</guid><description>VidEgoThink: A new benchmark reveals that current large language models struggle with egocentric video understanding, highlighting the need for advancements in embodied AI.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11623/cover.png"/></item><item><title>FLARE: Faithful Logic-Aided Reasoning and Exploration</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11900/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11900/</guid><description>FLARE, a novel interpretable approach, leverages LLMs and logic programming to achieve state-of-the-art results in complex reasoning tasks by enhancing model faithfulness and providing insights into r&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11900/cover.png"/></item><item><title>HART: Efficient Visual Generation with Hybrid Autoregressive Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/</guid><description>HART: A hybrid autoregressive transformer achieves state-of-the-art image generation quality at significantly higher speeds than diffusion models, thanks to its innovative hybrid tokenizer and residua&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10812/cover.png"/></item><item><title>Large Language Model Evaluation via Matrix Nuclear-Norm</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10672/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10672/</guid><description>Researchers developed Matrix Nuclear-Norm, a fast, accurate LLM evaluation metric that efficiently measures information compression, surpassing the computationally expensive Matrix Entropy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10672/cover.png"/></item><item><title>Minimum Tuning to Unlock Long Output from LLMs with High Quality Data as the Key</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10210/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10210/</guid><description>High-quality data, not sheer volume, is key to unlocking LLMs&amp;rsquo; potential for generating long, coherent outputs, as demonstrated by significant performance improvements with minimal tuning.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.10210/cover.png"/></item><item><title>Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11081/</link><pubDate>Mon, 14 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11081/</guid><description>Researchers stabilize &amp;amp; scale continuous-time consistency models for faster, high-quality image generation, achieving state-of-the-art results on ImageNet.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11081/cover.png"/></item><item><title>ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09870/</link><pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09870/</guid><description>Researchers developed CHROKNOWBENCH, a new benchmark, and CHROKNOWLEDGE, a framework, to effectively evaluate and enhance large language models&amp;rsquo; understanding of chronological knowledge across various&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09870/cover.png"/></item><item><title>Taming Overconfidence in LLMs: Reward Calibration in RLHF</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09724/</link><pubDate>Sun, 13 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09724/</guid><description>Researchers introduce novel reward calibration methods for RLHF, effectively reducing LLM overconfidence and enhancing reliability without sacrificing performance.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09724/cover.png"/></item><item><title>FlatQuant: Flatness Matters for LLM Quantization</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09426/</link><pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09426/</guid><description>FLATQUANT achieves state-of-the-art LLM quantization, minimizing accuracy loss (&amp;lt;1%) and latency (up to 2.3x speedup) through fast, learnable affine transformations and efficient kernel fusion.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09426/cover.png"/></item><item><title>Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09347/</link><pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09347/</guid><description>Researchers developed Condition Contrastive Alignment (CCA), a novel guidance-free method for high-quality autoregressive visual generation, significantly boosting performance while slashing sampling &amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09347/cover.png"/></item><item><title>Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.08968/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.08968/</guid><description>Controllable Safety Alignment (CoSA) lets large language models adapt to diverse safety needs at inference time without retraining, boosting practical use.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.08968/cover.png"/></item><item><title>MedMobile: A mobile-sized language model with expert-level clinical capabilities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09019/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09019/</guid><description>MedMobile: A mobile-ready 3.8B parameter language model achieves expert-level clinical performance, surpassing USMLE benchmarks with unprecedented efficiency and accessibility.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.09019/cover.png"/></item><item><title>ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.08584/</link><pubDate>Fri, 11 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.08584/</guid><description>ZipVL boosts large vision-language model efficiency by 2.6x via dynamic token sparsfication and 50% memory reduction using KV cache compression, all while maintaining minimal accuracy loss.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.08584/cover.png"/></item><item><title>DyVo: Dynamic Vocabularies for Learned Sparse Retrieval with Entities</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.07722/</link><pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.07722/</guid><description>DyVo boosts learned sparse retrieval by dynamically adding Wikipedia entities to the vocabulary, significantly improving accuracy and relevance in entity-rich datasets.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.07722/cover.png"/></item><item><title>Neural Metamorphosis</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11878/</link><pubDate>Thu, 10 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11878/</guid><description>NeuMeta learns a continuous weight manifold for neural networks, enabling the generation of any-sized network without retraining, even for unseen configurations.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11878/cover.png"/></item><item><title>Bi-Level Motion Imitation for Humanoid Robots</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.01968/</link><pubDate>Wed, 02 Oct 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.01968/</guid><description>Bi-Level Motion Imitation (BMI) enhances humanoid robot policy learning by cleverly modifying human motion capture data to be physically feasible, resulting in more robust and realistic robot movement&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.01968/cover.png"/></item><item><title>From Commands to Prompts: LLM-based Semantic File System for AIOS</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11843/</link><pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11843/</guid><description>Researchers developed LSFS, an LLM-based semantic file system for AIOS, enabling natural language file management via prompts, significantly improving user experience and efficiency.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.11843/cover.png"/></item><item><title>Leveraging Locality to Boost Sample Efficiency in Robotic Manipulation</title><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.10615/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.10615/</guid><description>SGRv2: Action locality boosts sample efficiency in robot manipulation!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2406.10615/cover.png"/></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_10_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_10_0/</guid><description>&lt;table id='0' style='font-size:18px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>pl_score&lt;/td>&lt;td>responses_pl&lt;/td>&lt;td>Average Score&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mixtral-8x7b&lt;/td>&lt;td>7.64&lt;/td>&lt;td>1.00&lt;/td>&lt;td>7.64&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral-Nemo-Instruct-2407&lt;/td>&lt;td>7.37&lt;/td>&lt;td>1.00&lt;/td>&lt;td>7.37&lt;/td>&lt;/tr>&lt;tr>&lt;td>openchat-3.5-0106-gemma&lt;/td>&lt;td>6.51&lt;/td>&lt;td>0.96&lt;/td>&lt;td>6.81&lt;/td>&lt;/tr>&lt;tr>&lt;td>Meta-Llama-3.1-8B-Instruct&lt;/td>&lt;td>6.24&lt;/td>&lt;td>1.00&lt;/td>&lt;td>6.24&lt;/td>&lt;/tr>&lt;tr>&lt;td>Starling-LM-7B-alpha&lt;/td>&lt;td>6.05&lt;/td>&lt;td>0.93&lt;/td>&lt;td>6.49&lt;/td>&lt;/tr>&lt;tr>&lt;td>openchat-3.5-0106&lt;/td>&lt;td>6.03&lt;/td>&lt;td>0.94&lt;/td>&lt;td>6.39&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral-7B-Instruct-v0.3&lt;/td>&lt;td>5.75&lt;/td>&lt;td>0.98&lt;/td>&lt;td>5.82&lt;/td>&lt;/tr>&lt;tr>&lt;td>Bielik-7B-Instruct-v0.1&lt;/td>&lt;td>5.40&lt;/td>&lt;td>0.89&lt;/td>&lt;td>6.08&lt;/td>&lt;/tr>&lt;tr>&lt;td>dolphin-2.9.1-llama-3-8b&lt;/td>&lt;td>5.24&lt;/td>&lt;td>0.89&lt;/td>&lt;td>5.86&lt;/td>&lt;/tr>&lt;tr>&lt;td>Polka-Mistral-7B-SFT&lt;/td>&lt;td>4.43&lt;/td>&lt;td>0.98&lt;/td>&lt;td>4.52&lt;/td>&lt;/tr>&lt;tr>&lt;td>trurl-2-7b&lt;/td>&lt;td>2.75&lt;/td>&lt;td>0.99&lt;/td>&lt;td>2.76&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral-7B-Instruct-v0.2&lt;/td>&lt;td>2.05&lt;/td>&lt;td>0.31&lt;/td>&lt;td>6.56&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_10_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_10_1/</guid><description>&lt;table id='2' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>Coding&lt;/td>&lt;td>Extraction&lt;/td>&lt;td>Humanities&lt;/td>&lt;td>Mathematics&lt;/td>&lt;td>Reasoning&lt;/td>&lt;td>Role-playing&lt;/td>&lt;td>Stem&lt;/td>&lt;td>Writing&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mixtral-8x7b&lt;/td>&lt;td>5.20&lt;/td>&lt;td>8.15&lt;/td>&lt;td>9.45&lt;/td>&lt;td>5.65&lt;/td>&lt;td>5.80&lt;/td>&lt;td>8.95&lt;/td>&lt;td>8.55&lt;/td>&lt;td>9.35&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral-Nemo-Instruct-2407&lt;/td>&lt;td>5.85&lt;/td>&lt;td>8.95&lt;/td>&lt;td>9.50&lt;/td>&lt;td>6.70&lt;/td>&lt;td>5.80&lt;/td>&lt;td>7.45&lt;/td>&lt;td>8.30&lt;/td>&lt;td>6.40&lt;/td>&lt;/tr>&lt;tr>&lt;td>openchat-3.5-0106-gemma&lt;/td>&lt;td>5.35&lt;/td>&lt;td>6.90&lt;/td>&lt;td>8.80&lt;/td>&lt;td>4.55&lt;/td>&lt;td>5.40&lt;/td>&lt;td>7.97&lt;/td>&lt;td>8.47&lt;/td>&lt;td>7.05&lt;/td>&lt;/tr>&lt;tr>&lt;td>Meta-Llama-3.1-8B-Instruct&lt;/td>&lt;td>4.60&lt;/td>&lt;td>9.10&lt;/td>&lt;td>8.82&lt;/td>&lt;td>5.30&lt;/td>&lt;td>2.50&lt;/td>&lt;td>5.60&lt;/td>&lt;td>6.30&lt;/td>&lt;td>7.70&lt;/td>&lt;/tr>&lt;tr>&lt;td>Starling-LM-7B-alpha&lt;/td>&lt;td>4.75&lt;/td>&lt;td>7.35&lt;/td>&lt;td>8.50&lt;/td>&lt;td>4.15&lt;/td>&lt;td>3.90&lt;/td>&lt;td>6.90&lt;/td>&lt;td>8.85&lt;/td>&lt;td>7.55&lt;/td>&lt;/tr>&lt;tr>&lt;td>openchat-3.5-0106&lt;/td>&lt;td>5.05&lt;/td>&lt;td>6.90&lt;/td>&lt;td>9.30&lt;/td>&lt;td>3.80&lt;/td>&lt;td>3.90&lt;/td>&lt;td>6.00&lt;/td>&lt;td>8.40&lt;/td>&lt;td>7.75&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral- 7B-Instruct-v0.3&lt;/td>&lt;td>4.30&lt;/td>&lt;td>7.30&lt;/td>&lt;td>6.75&lt;/td>&lt;td>2.35&lt;/td>&lt;td>3.80&lt;/td>&lt;td>7.25&lt;/td>&lt;td>7.45&lt;/td>&lt;td>7.35&lt;/td>&lt;/tr>&lt;tr>&lt;td>Bielik-7B-Instruct-v0.1&lt;/td>&lt;td>3.00&lt;/td>&lt;td>4.35&lt;/td>&lt;td>8.47&lt;/td>&lt;td>4.10&lt;/td>&lt;td>6.15&lt;/td>&lt;td>7.83&lt;/td>&lt;td>6.90&lt;/td>&lt;td>7.85&lt;/td>&lt;/tr>&lt;tr>&lt;td>dolphin-2.9. 1-llama-3-8b&lt;/td>&lt;td>4.60&lt;/td>&lt;td>6.15&lt;/td>&lt;td>8.80&lt;/td>&lt;td>4.80&lt;/td>&lt;td>3.30&lt;/td>&lt;td>7.40&lt;/td>&lt;td>6.35&lt;/td>&lt;td>5.50&lt;/td>&lt;/tr>&lt;tr>&lt;td>Polka-Mistral-7B-SFT&lt;/td>&lt;td>2.95&lt;/td>&lt;td>5.25&lt;/td>&lt;td>5.60&lt;/td>&lt;td>2.95&lt;/td>&lt;td>2.45&lt;/td>&lt;td>4.90&lt;/td>&lt;td>6.80&lt;/td>&lt;td>5.25&lt;/td>&lt;/tr>&lt;tr>&lt;td>trurl-2-7b&lt;/td>&lt;td>1.80&lt;/td>&lt;td>3.50&lt;/td>&lt;td>3.95&lt;/td>&lt;td>1.70&lt;/td>&lt;td>2.05&lt;/td>&lt;td>3.30&lt;/td>&lt;td>2.65&lt;/td>&lt;td>3.15&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral-7B-Instruct-v0.2&lt;/td>&lt;td>4.25&lt;/td>&lt;td>7.40&lt;/td>&lt;td>8.40&lt;/td>&lt;td>3.20&lt;/td>&lt;td>5.00&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_11_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_11_0/</guid><description>&lt;table id='6' style='font-size:14px'>&lt;tr>&lt;td>Quant.&lt;/td>&lt;td>imatrix&lt;/td>&lt;td>Size [GiB]&lt;/td>&lt;td>PPL&lt;/td>&lt;td>△PPL&lt;/td>&lt;td>KLD&lt;/td>&lt;td>Mean △p&lt;/td>&lt;td>RMS △p&lt;/td>&lt;td>Same top p [%]&lt;/td>&lt;/tr>&lt;tr>&lt;td>FP16&lt;/td>&lt;td>-&lt;/td>&lt;td>13.49&lt;/td>&lt;td>3,9393&lt;/td>&lt;td>-&lt;/td>&lt;td>&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q8_0&lt;/td>&lt;td>No&lt;/td>&lt;td>7.17&lt;/td>&lt;td>3.9422&lt;/td>&lt;td>0.0029&lt;/td>&lt;td>0.0010&lt;/td>&lt;td>-0.0070&lt;/td>&lt;td>0.9800&lt;/td>&lt;td>98.6890&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q8_0&lt;/td>&lt;td>Yes&lt;/td>&lt;td>7.17&lt;/td>&lt;td>3.9422&lt;/td>&lt;td>0.0029&lt;/td>&lt;td>0.0010&lt;/td>&lt;td>-0.0070&lt;/td>&lt;td>0.9800&lt;/td>&lt;td>98.6890&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q6_K&lt;/td>&lt;td>No&lt;/td>&lt;td>5.53&lt;/td>&lt;td>3,9450&lt;/td>&lt;td>0.0057&lt;/td>&lt;td>0.0051&lt;/td>&lt;td>-0.0420&lt;/td>&lt;td>2.1850&lt;/td>&lt;td>97.2410&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q6_K&lt;/td>&lt;td>Yes&lt;/td>&lt;td>5.53&lt;/td>&lt;td>3,9406&lt;/td>&lt;td>0.0013&lt;/td>&lt;td>0.0037&lt;/td>&lt;td>-0.0030&lt;/td>&lt;td>1.8490&lt;/td>&lt;td>97.6130&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q5_K_M&lt;/td>&lt;td>No&lt;/td>&lt;td>4.78&lt;/td>&lt;td>3.9520&lt;/td>&lt;td>0.0127&lt;/td>&lt;td>0.0106&lt;/td>&lt;td>-0.0680&lt;/td>&lt;td>3,1320&lt;/td>&lt;td>96.0510&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q5_K_M&lt;/td>&lt;td>Yes&lt;/td>&lt;td>4.78&lt;/td>&lt;td>3,9473&lt;/td>&lt;td>0.0080&lt;/td>&lt;td>0.0086&lt;/td>&lt;td>-0.0250&lt;/td>&lt;td>2.8320&lt;/td>&lt;td>96.4670&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q4_K_M&lt;/td>&lt;td>No&lt;/td>&lt;td>4.07&lt;/td>&lt;td>3,9876&lt;/td>&lt;td>0.0483&lt;/td>&lt;td>0.0286&lt;/td>&lt;td>-0.2690&lt;/td>&lt;td>5,1300&lt;/td>&lt;td>93.6550&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q4_K_M&lt;/td>&lt;td>Yes&lt;/td>&lt;td>4.07&lt;/td>&lt;td>3.9727&lt;/td>&lt;td>0.0333&lt;/td>&lt;td>0.0220&lt;/td>&lt;td>-0.1440&lt;/td>&lt;td>4.4880&lt;/td>&lt;td>94.4700&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q3_K_M&lt;/td>&lt;td>No&lt;/td>&lt;td>3.28&lt;/td>&lt;td>4.0915&lt;/td>&lt;td>0.1522&lt;/td>&lt;td>0.0826&lt;/td>&lt;td>-0.9160&lt;/td>&lt;td>8.6880&lt;/td>&lt;td>89.5780&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q3_K_M&lt;/td>&lt;td>Yes&lt;/td>&lt;td>3.28&lt;/td>&lt;td>4.0458&lt;/td>&lt;td>0.1065&lt;/td>&lt;td>0.0683&lt;/td>&lt;td>-0.3860&lt;/td>&lt;td>7.8390&lt;/td>&lt;td>90.6290&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q2_K&lt;/td>&lt;td>No&lt;/td>&lt;td>2.53&lt;/td>&lt;td>4,7045&lt;/td>&lt;td>0.7652&lt;/td>&lt;td>0.2852&lt;/td>&lt;td>-3.8050&lt;/td>&lt;td>16.3760&lt;/td>&lt;td>81.1100&lt;/td>&lt;/tr>&lt;tr>&lt;td>Q2_K&lt;/td>&lt;td>Yes&lt;/td>&lt;td>2.53&lt;/td>&lt;td>4.3522&lt;/td>&lt;td>0.4128&lt;/td>&lt;td>0.1939&lt;/td>&lt;td>-1.8980&lt;/td>&lt;td>13.4190&lt;/td>&lt;td>84.5580&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_17_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_17_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>1m&lt;/td>&lt;td>eval --model hf --model_args&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="4">&lt;/td>&lt;td>pretrained=speakleash / Bielik -7B-&lt;/td>&lt;/tr>&lt;tr>&lt;td>Instruct -v0. 1 --tasks&lt;/td>&lt;/tr>&lt;tr>&lt;td>polish_generate --num_fewshot 0&lt;/td>&lt;/tr>&lt;tr>&lt;td>output_path results / --log_samples&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="4">lm_&lt;/td>&lt;td>eval --model hf --model_ args&lt;/td>&lt;/tr>&lt;tr>&lt;td>pretrained=speakleash / Bielik -7B-&lt;/td>&lt;/tr>&lt;tr>&lt;td>Instruct -v0. 1 --tasks polish_mc&lt;/td>&lt;/tr>&lt;tr>&lt;td>num_fewshot 0 --output_path results / -- log_samples&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">lm_eval&lt;/td>&lt;td>--model hf --model_args&lt;/td>&lt;/tr>&lt;tr>&lt;td>pretrained=speakleash / Bielik -7B-&lt;/td>&lt;/tr>&lt;tr>&lt;td>Instruct -v0. 1 --tasks&lt;/td>&lt;/tr>&lt;tr>&lt;td>polish_generate_few --num_fewshot 5&lt;/td>&lt;/tr>&lt;tr>&lt;td>- output_path results / --log_samples&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="4">1m&lt;/td>&lt;td>eval --model hf --model_args&lt;/td>&lt;/tr>&lt;tr>&lt;td>pretrained=speakleash / Bielik -7B-&lt;/td>&lt;/tr>&lt;tr>&lt;td>Instruct -v⌀. 1 --tasks polish_mc&lt;/td>&lt;/tr>&lt;tr>&lt;td>num_fewshot 5 --output_path results / --log_samples&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_2_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_2_0/</guid><description>&lt;table id='3' style='font-size:18px'>&lt;tr>&lt;td>Parameter&lt;/td>&lt;td>Value&lt;/td>&lt;/tr>&lt;tr>&lt;td>Layers&lt;/td>&lt;td>32&lt;/td>&lt;/tr>&lt;tr>&lt;td>Model Dimension&lt;/td>&lt;td>4096&lt;/td>&lt;/tr>&lt;tr>&lt;td>Attention Heads&lt;/td>&lt;td>32&lt;/td>&lt;/tr>&lt;tr>&lt;td>Key/Value Heads&lt;/td>&lt;td>8&lt;/td>&lt;/tr>&lt;tr>&lt;td>Head Size&lt;/td>&lt;td>128&lt;/td>&lt;/tr>&lt;tr>&lt;td>Intermediate Size&lt;/td>&lt;td>14336&lt;/td>&lt;/tr>&lt;tr>&lt;td>Activation Function&lt;/td>&lt;td>SwiGLU&lt;/td>&lt;/tr>&lt;tr>&lt;td>Vocabulary Size&lt;/td>&lt;td>32000&lt;/td>&lt;/tr>&lt;tr>&lt;td>Positional Embeddings&lt;/td>&lt;td>RoPE (0 = 10000)&lt;/td>&lt;/tr>&lt;tr>&lt;td>Context Length&lt;/td>&lt;td>8192&lt;/td>&lt;/tr>&lt;tr>&lt;td>Sliding Window&lt;/td>&lt;td>4096&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_3_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_3_0/</guid><description>&lt;table id='0' style='font-size:16px'>&lt;tr>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td colspan="3">Polish&lt;/td>&lt;td colspan="3">English&lt;/td>&lt;/tr>&lt;tr>&lt;td>Tokenizer&lt;/td>&lt;td>Vocab Size&lt;/td>&lt;td>Avg tokens&lt;/td>&lt;td>Tokens&lt;/td>&lt;td>CpT&lt;/td>&lt;td>TpW&lt;/td>&lt;td>Tokens&lt;/td>&lt;td>CpT&lt;/td>&lt;td>TpW&lt;/td>&lt;/tr>&lt;tr>&lt;td>APT3&lt;/td>&lt;td>31980&lt;/td>&lt;td>480&lt;/td>&lt;td>344&lt;/td>&lt;td>5.22&lt;/td>&lt;td>1.48&lt;/td>&lt;td>615&lt;/td>&lt;td>3.15&lt;/td>&lt;td>1.93&lt;/td>&lt;/tr>&lt;tr>&lt;td>Llama2&lt;/td>&lt;td>32000&lt;/td>&lt;td>554&lt;/td>&lt;td>681&lt;/td>&lt;td>2.63&lt;/td>&lt;td>2.94&lt;/td>&lt;td>427&lt;/td>&lt;td>4.53&lt;/td>&lt;td>1.34&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral v0.1&lt;/td>&lt;td>32000&lt;/td>&lt;td>578&lt;/td>&lt;td>747&lt;/td>&lt;td>2.40&lt;/td>&lt;td>3.22&lt;/td>&lt;td>408&lt;/td>&lt;td>4.75&lt;/td>&lt;td>1.28&lt;/td>&lt;/tr>&lt;tr>&lt;td>Llama2 + APT3&lt;/td>&lt;td>57362&lt;/td>&lt;td>442&lt;/td>&lt;td>441&lt;/td>&lt;td>4.07&lt;/td>&lt;td>1.90&lt;/td>&lt;td>442&lt;/td>&lt;td>4.38&lt;/td>&lt;td>1.39&lt;/td>&lt;/tr>&lt;tr>&lt;td>Mistral v0.1 + APT3&lt;/td>&lt;td>58690&lt;/td>&lt;td>450&lt;/td>&lt;td>493&lt;/td>&lt;td>3.64&lt;/td>&lt;td>2.12&lt;/td>&lt;td>407&lt;/td>&lt;td>4.76&lt;/td>&lt;td>1.28&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_4_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_4_0/</guid><description>&lt;table id='9' style='font-size:20px'>&lt;tr>&lt;td>Precision&lt;/td>&lt;td>Recall&lt;/td>&lt;td>F1&lt;/td>&lt;/tr>&lt;tr>&lt;td>0.8640&lt;/td>&lt;td>0.8285&lt;/td>&lt;td>0.8431&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_8_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_8_0/</guid><description>&lt;table id='0' style='font-size:22px'>&lt;tr>&lt;td>Framework&lt;/td>&lt;td>Configuration&lt;/td>&lt;td>Total Batch Size&lt;/td>&lt;td>Throughput&lt;/td>&lt;/tr>&lt;tr>&lt;td>TinyLlama&lt;/td>&lt;td>8xA100 40GB&lt;/td>&lt;td>2,097,152 tokens&lt;/td>&lt;td>24,390 tokens/GPU/sec&lt;/td>&lt;/tr>&lt;tr>&lt;td>ALLaMo&lt;/td>&lt;td>8xA100 40GB&lt;/td>&lt;td>2,097,152 tokens&lt;/td>&lt;td>26,150 tokens/GPU/sec (+7.2%)&lt;/td>&lt;/tr>&lt;tr>&lt;td>ALLaMo&lt;/td>&lt;td>8xA100 40GB&lt;/td>&lt;td>2,359,296 tokens&lt;/td>&lt;td>26,550 tokens/GPU/sec (+8.8%)&lt;/td>&lt;/tr>&lt;tr>&lt;td>TinyLlama&lt;/td>&lt;td>16xA100 40GB&lt;/td>&lt;td>2,097,152 tokens&lt;/td>&lt;td>24,000 tokens/GPU/sec 1&lt;/td>&lt;/tr>&lt;tr>&lt;td>ALLaMo&lt;/td>&lt;td>16xA100 40GB&lt;/td>&lt;td>2,097,152 tokens&lt;/td>&lt;td>25,850 tokens/GPU/sec (+7.7%)&lt;/td>&lt;/tr>&lt;tr>&lt;td>ALLaMo&lt;/td>&lt;td>16xA100 40GB&lt;/td>&lt;td>2,359,296 tokens&lt;/td>&lt;td>26,000 tokens/GPU/sec (+8.3%)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_9_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.18565/tables/table_9_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>All tasks&lt;/td>&lt;td>RAG Reranking&lt;/td>&lt;td>RAG Reader&lt;/td>&lt;td>Perplexity&lt;/td>&lt;/tr>&lt;tr>&lt;td colspan="5">7B parameters models:&lt;/td>&lt;/tr>&lt;tr>&lt;td>berkeley-nest/Starling-LM-7B-alpha&lt;/td>&lt;td>47.46&lt;/td>&lt;td>75.73&lt;/td>&lt;td>82.86&lt;/td>&lt;td>1438.04&lt;/td>&lt;/tr>&lt;tr>&lt;td>openchat/openchat-3.5-0106&lt;/td>&lt;td>47.32&lt;/td>&lt;td>74.71&lt;/td>&lt;td>83.60&lt;/td>&lt;td>1106.56&lt;/td>&lt;/tr>&lt;tr>&lt;td>Nexusflow/Starling-LM-7B-beta&lt;/td>&lt;td>45.69&lt;/td>&lt;td>74.58&lt;/td>&lt;td>81.22&lt;/td>&lt;td>1161.54&lt;/td>&lt;/tr>&lt;tr>&lt;td>openchat/openchat-3.5-1210&lt;/td>&lt;td>44.17&lt;/td>&lt;td>71.76&lt;/td>&lt;td>82.15&lt;/td>&lt;td>1923.83&lt;/td>&lt;/tr>&lt;tr>&lt;td>teknium/OpenHermes-2.5-Mistral-7B&lt;/td>&lt;td>42.64&lt;/td>&lt;td>70.63&lt;/td>&lt;td>80.25&lt;/td>&lt;td>1463.00&lt;/td>&lt;/tr>&lt;tr>&lt;td>mistralai/Mistral-7B-Instruct-v0.2&lt;/td>&lt;td>40.29&lt;/td>&lt;td>72.58&lt;/td>&lt;td>79.39&lt;/td>&lt;td>2088.08&lt;/td>&lt;/tr>&lt;tr>&lt;td>Bielik-7B-Instruct-v0.1&lt;/td>&lt;td>39.28&lt;/td>&lt;td>61.89&lt;/td>&lt;td>86.00&lt;/td>&lt;td>277.92&lt;/td>&lt;/tr>&lt;tr>&lt;td>internlm/internlm2-chat-7b&lt;/td>&lt;td>37.64&lt;/td>&lt;td>72.29&lt;/td>&lt;td>71.17&lt;/td>&lt;td>3892.50&lt;/td>&lt;/tr>&lt;tr>&lt;td>internlm/internlm2-chat-7b-sft&lt;/td>&lt;td>36.97&lt;/td>&lt;td>73.22&lt;/td>&lt;td>69.96&lt;/td>&lt;td>4269.63&lt;/td>&lt;/tr>&lt;tr>&lt;td>HuggingFaceH4/zephyr-7b-alpha&lt;/td>&lt;td>33.97&lt;/td>&lt;td>71.47&lt;/td>&lt;td>73.35&lt;/td>&lt;td>4464.45&lt;/td>&lt;/tr>&lt;tr>&lt;td>HuggingFaceH4/zephyr-7b-beta&lt;/td>&lt;td>33.15&lt;/td>&lt;td>71.65&lt;/td>&lt;td>71.27&lt;/td>&lt;td>3613.14&lt;/td>&lt;/tr>&lt;tr>&lt;td>szymonrucinski/Curie-7B-v1&lt;/td>&lt;td>26.72&lt;/td>&lt;td>55.58&lt;/td>&lt;td>85.19&lt;/td>&lt;td>389.17&lt;/td>&lt;/tr>&lt;tr>&lt;td>mistralai/Mistral-7B-Instruct-v0.1&lt;/td>&lt;td>26.42&lt;/td>&lt;td>56.35&lt;/td>&lt;td>73.68&lt;/td>&lt;td>6909.94&lt;/td>&lt;/tr>&lt;tr>&lt;td>meta-Ilama/Llama-2-7b-chat-hf&lt;/td>&lt;td>21.04&lt;/td>&lt;td>54.65&lt;/td>&lt;td>72.93&lt;/td>&lt;td>4018.74&lt;/td>&lt;/tr>&lt;tr>&lt;td>Voicelab/trurl-2-7b&lt;/td>&lt;td>18.85&lt;/td>&lt;td>60.67&lt;/td>&lt;td>77.19&lt;/td>&lt;td>1098.88&lt;/td>&lt;/tr>&lt;tr>&lt;td>Baseline (majority class)&lt;/td>&lt;td>0.00&lt;/td>&lt;td>53.36&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;/tr>&lt;tr>&lt;td colspan="5">Models with different sizes:&lt;/td>&lt;/tr>&lt;tr>&lt;td>upstage/SOLAR-10.7B-Instruct-v1.0 (10.7B)&lt;/td>&lt;td>46.07&lt;/td>&lt;td>76.93&lt;/td>&lt;td>82.86&lt;/td>&lt;td>789.58&lt;/td>&lt;/tr>&lt;tr>&lt;td>Voicelab/trurl-2-13b-academic (13B)&lt;/td>&lt;td>29.45&lt;/td>&lt;td>68.19&lt;/td>&lt;td>79.88&lt;/td>&lt;td>733.91&lt;/td>&lt;/tr>&lt;tr>&lt;td>Azurro/APT3-1B-Instruct-v1 (1B)&lt;/td>&lt;td>-13.80&lt;/td>&lt;td>52.11&lt;/td>&lt;td>12.23&lt;/td>&lt;td>739.09&lt;/td>&lt;/tr>&lt;tr>&lt;td colspan="5">7B parameters pretrained and continuously pretrained models:&lt;/td>&lt;/tr>&lt;tr>&lt;td>alpindale/Mistral-7B-v0.2-hf&lt;/td>&lt;td>33.05&lt;/td>&lt;td>60.23&lt;/td>&lt;td>85.21&lt;/td>&lt;td>932.60&lt;/td>&lt;/tr>&lt;tr>&lt;td>internlm/internlm2-7b&lt;/td>&lt;td>33.03&lt;/td>&lt;td>69.39&lt;/td>&lt;td>73.63&lt;/td>&lt;td>5498.23&lt;/td>&lt;/tr>&lt;tr>&lt;td>mistralai/Mistral-7B-v0.1&lt;/td>&lt;td>30.67&lt;/td>&lt;td>60.35&lt;/td>&lt;td>85.39&lt;/td>&lt;td>857.32&lt;/td>&lt;/tr>&lt;tr>&lt;td>Bielik-7B-v0.1&lt;/td>&lt;td>29.38&lt;/td>&lt;td>62.13&lt;/td>&lt;td>88.39&lt;/td>&lt;td>123.31&lt;/td>&lt;/tr>&lt;tr>&lt;td>internlm/intermlm2-base-7b&lt;/td>&lt;td>20.68&lt;/td>&lt;td>52.39&lt;/td>&lt;td>69.85&lt;/td>&lt;td>3110.92&lt;/td>&lt;/tr>&lt;tr>&lt;td>meta-llama/Llama-2-7b-hf&lt;/td>&lt;td>12.73&lt;/td>&lt;td>54.02&lt;/td>&lt;td>77.92&lt;/td>&lt;td>850.45&lt;/td>&lt;/tr>&lt;tr>&lt;td>OPI-PG/Qra-7b&lt;/td>&lt;td>11.13&lt;/td>&lt;td>54.40&lt;/td>&lt;td>75.25&lt;/td>&lt;td>203.36&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20011/tables/table_3_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20011/tables/table_3_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>Technique&lt;/td>&lt;td>General Mechanism&lt;/td>&lt;td colspan="6">Compute Size Space Runtime Training Dataset Inference Memory Storage Latency&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">Model Architectures (Sec. 2)&lt;/td>&lt;td>Lightweight Models (Sec. 2.1)&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;/tr>&lt;tr>&lt;td>Efficient Self-Attention (Sec. 2.2)&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;/tr>&lt;tr>&lt;td>Neural Arch. Search (Sec. 2.3)&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">Training Techniques (Sec. 3)&lt;/td>&lt;td>Pre-training (Sec. 3.1)&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>Finetuning (Sec. 3.2)&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">Model Compression (Sec. 4)&lt;/td>&lt;td>Pruning (Sec. 4.1)&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;/tr>&lt;tr>&lt;td>Quantization (Sec. 4.2)&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;/tr>&lt;tr>&lt;td>Knowledge Distillation (Sec. 4.3)&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20011/tables/table_8_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20011/tables/table_8_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>Setting&lt;/td>&lt;td>Constraints&lt;/td>&lt;td>Datasets&lt;/td>&lt;td>Metrics&lt;/td>&lt;/tr>&lt;tr>&lt;td>Efficient Inference&lt;/td>&lt;td>Latency&lt;/td>&lt;td>SuperGLUE (Sarlin et al., 2020), SQuAD (Ra- jpurkar et al., 2016), TriviaQA (Joshi et al., 2017), CoQA (Reddy et al., 2019), Natural Questions (NQ) (Kwiatkowski et al., 2019)&lt;/td>&lt;td>Inference Time (Narayanan et al⌀, 2023), Throughput (Arora et al., 2024)&lt;/td>&lt;/tr>&lt;tr>&lt;td>On-device/Mobile&lt;/td>&lt;td>Memory&lt;/td>&lt;td>TinyBERT (Jiao et al., 2020) and OpenOrca (Lian et al., 2023)&lt;/td>&lt;td>Peak Memory Usage (Lee et al., 2024a), Memory Footprint, Compression Ratio (Cao et al., 2024)&lt;/td>&lt;/tr>&lt;tr>&lt;td>Privacy-Preserving&lt;/td>&lt;td>Privacy&lt;/td>&lt;td>PrivacyGLUE (Shankar et al., 2023), MIMIC (John- son et al., 2020)&lt;/td>&lt;td>Privacy Budget (Yu et al., 2024), Noise Level (Havrilla et al., 2024)&lt;/td>&lt;/tr>&lt;tr>&lt;td>Energy-Efficient AI&lt;/td>&lt;td>Energy Optimiza- tion&lt;/td>&lt;td>-&lt;/td>&lt;td>Energy Efficiency Ratio (Stojkovic et al., 2024b), Thermal Efficiency, Idle Power Consumption (Patel et al., 2024)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20011/tables/table_9_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.20011/tables/table_9_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>Category&lt;/td>&lt;td>Application&lt;/td>&lt;td>Need for SLM Application&lt;/td>&lt;td colspan="5">Runtime Overhead Space Inference Memory Storage Latency Comm.&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">Real-Time Interaction&lt;/td>&lt;td>Chatbots&lt;/td>&lt;td>Real-time response needed, lightweight&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;/tr>&lt;tr>&lt;td>Voice Interfaces&lt;/td>&lt;td>Low latency required for real-time&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>Translation&lt;/td>&lt;td>Real-time translation with low-resources&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">Content Generation &amp; Processing&lt;/td>&lt;td>Text Summarization&lt;/td>&lt;td>Faster inference, minimal resource use&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>Sentiment Analysis&lt;/td>&lt;td>Efficient analysis in low-resource envir.&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>Text Classification&lt;/td>&lt;td>Low latency, on-the-fly processing&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>NLP for Search&lt;/td>&lt;td>Low latency for real-time search&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>Autocompletion&lt;/td>&lt;td>Fast prediction with low memory&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>V&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_21_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_21_0/</guid><description>&lt;br>&lt;table id='7' style='font-size:16px'>&lt;tr>&lt;td>Dataset&lt;/td>&lt;td>Class&lt;/td>&lt;td>Instance&lt;/td>&lt;td>Document Type&lt;/td>&lt;td>Language&lt;/td>&lt;/tr>&lt;tr>&lt;td>PRImA [310]&lt;/td>&lt;td>10&lt;/td>&lt;td>305&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>BCE-Atabic-v1 [311]&lt;/td>&lt;td>3&lt;/td>&lt;td>1833&lt;/td>&lt;td>Arabic books&lt;/td>&lt;td>Arabic&lt;/td>&lt;/tr>&lt;tr>&lt;td>Diva-hisdb [312]&lt;/td>&lt;td>Text Block&lt;/td>&lt;td>150&lt;/td>&lt;td>Handwritten Historical Doc- ument&lt;/td>&lt;td>Multiple Languages&lt;/td>&lt;/tr>&lt;tr>&lt;td>DSSE200 [313]&lt;/td>&lt;td>6&lt;/td>&lt;td>200&lt;/td>&lt;td>Magazines, Academic pa- pers&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>OHG [314]&lt;/td>&lt;td>6&lt;/td>&lt;td>596&lt;/td>&lt;td>Handwritten Historical Doc- ument&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>CORD [315]&lt;/td>&lt;td>5&lt;/td>&lt;td>1000&lt;/td>&lt;td>Receipts&lt;/td>&lt;td>Indonesian&lt;/td>&lt;/tr>&lt;tr>&lt;td>FUNSD [316]&lt;/td>&lt;td>4&lt;/td>&lt;td>199&lt;/td>&lt;td>Form document&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>PubLayNet [317]&lt;/td>&lt;td>5&lt;/td>&lt;td>360000&lt;/td>&lt;td>Academic papers&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>Chn [318]&lt;/td>&lt;td>5&lt;/td>&lt;td>8005&lt;/td>&lt;td>Chinese Wikipedia pages&lt;/td>&lt;td>Chinese&lt;/td>&lt;/tr>&lt;tr>&lt;td>DocBank [319]&lt;/td>&lt;td>13&lt;/td>&lt;td>500000&lt;/td>&lt;td>Academic papers&lt;/td>&lt;td>English, Chinese&lt;/td>&lt;/tr>&lt;tr>&lt;td>BCE-Atabic-v1 [320]&lt;/td>&lt;td>21&lt;/td>&lt;td>9000&lt;/td>&lt;td>Arabic books&lt;/td>&lt;td>Arabic&lt;/td>&lt;/tr>&lt;tr>&lt;td>DAD [321]&lt;/td>&lt;td>5&lt;/td>&lt;td>5980&lt;/td>&lt;td>Articles&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>DocLayNet [322]&lt;/td>&lt;td>11&lt;/td>&lt;td>80863&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>Primarily English&lt;/td>&lt;/tr>&lt;tr>&lt;td>D4LA [27]&lt;/td>&lt;td>27&lt;/td>&lt;td>11092&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>M6Doc [323]&lt;/td>&lt;td>74&lt;/td>&lt;td>9080&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English, Chinese&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_22_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_22_0/</guid><description>&lt;table id='3' style='font-size:14px'>&lt;tr>&lt;td>Dataset&lt;/td>&lt;td>Instance&lt;/td>&lt;td>Task&lt;/td>&lt;td>Feature&lt;/td>&lt;td>Language&lt;/td>&lt;/tr>&lt;tr>&lt;td>IIIT5K [324]&lt;/td>&lt;td>5000&lt;/td>&lt;td>TR&lt;/td>&lt;td>Real-world scene text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>Street View Text [325]&lt;/td>&lt;td>647&lt;/td>&lt;td>TD&lt;/td>&lt;td>Street View&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>Street View Text Per- spective [326]&lt;/td>&lt;td>645&lt;/td>&lt;td>TD&lt;/td>&lt;td>Street View with per- spective distortion&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR 2003 [327]&lt;/td>&lt;td>507&lt;/td>&lt;td>TD &amp; TR&lt;/td>&lt;td>Real-world short scene text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR 2013 [328]&lt;/td>&lt;td>462&lt;/td>&lt;td>TD &amp; TR&lt;/td>&lt;td>Real-world short scene text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>MSRA-TD500 [329]&lt;/td>&lt;td>500&lt;/td>&lt;td>TD&lt;/td>&lt;td>Rotated text&lt;/td>&lt;td>English, Chinese&lt;/td>&lt;/tr>&lt;tr>&lt;td>CUTE80 [330]&lt;/td>&lt;td>13000&lt;/td>&lt;td>TD &amp; TR&lt;/td>&lt;td>Curved text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>COCO-Text [331]&lt;/td>&lt;td>63,686&lt;/td>&lt;td>TD &amp; TR&lt;/td>&lt;td>Real-world short scene text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR 2015 [332]&lt;/td>&lt;td>1500&lt;/td>&lt;td>TD &amp; TR &amp; TS&lt;/td>&lt;td>Incidental Scene Text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>SCUT-CTW1500 [333]&lt;/td>&lt;td>1500&lt;/td>&lt;td>TD&lt;/td>&lt;td>Curved text&lt;/td>&lt;td>English, Chinese&lt;/td>&lt;/tr>&lt;tr>&lt;td>Total-Text [334]&lt;/td>&lt;td>1555&lt;/td>&lt;td>TD &amp; TR&lt;/td>&lt;td>Multi-oriented scene text&lt;/td>&lt;td>English, Chinese&lt;/td>&lt;/tr>&lt;tr>&lt;td>SynthText [335]&lt;/td>&lt;td>800,000&lt;/td>&lt;td>TD &amp; TR&lt;/td>&lt;td>Synthetic images&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>SynthAdd [336]&lt;/td>&lt;td>1,200,000&lt;/td>&lt;td>TD &amp; TR&lt;/td>&lt;td>Synthetic images&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>Occlusion Scene Text [80]&lt;/td>&lt;td>4832&lt;/td>&lt;td>TD&lt;/td>&lt;td>Occlusion text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>WordArt [337]&lt;/td>&lt;td>6316&lt;/td>&lt;td>TR&lt;/td>&lt;td>Artistic text&lt;/td>&lt;td>English&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR2019-ReCTS [338]&lt;/td>&lt;td>25,000&lt;/td>&lt;td>TD &amp; TR &amp; TS&lt;/td>&lt;td>TD &amp; TR &amp; Document Structure Analysis&lt;/td>&lt;td>Chinese&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_0/</guid><description>&lt;table id='1' style='font-size:18px'>&lt;tr>&lt;td>Dataset&lt;/td>&lt;td>Image&lt;/td>&lt;td>Instance&lt;/td>&lt;td>Type&lt;/td>&lt;td>Task&lt;/td>&lt;/tr>&lt;tr>&lt;td>UW-III [339]&lt;/td>&lt;td>100&lt;/td>&lt;td>/&lt;/td>&lt;td>Inline and displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>InftyCDB-1 [340]&lt;/td>&lt;td>467&lt;/td>&lt;td>21000&lt;/td>&lt;td>Inline and displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>Marmo [341]t&lt;/td>&lt;td>594&lt;/td>&lt;td>9500&lt;/td>&lt;td>Inline and displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR-2017 POD [342]&lt;/td>&lt;td>3900&lt;/td>&lt;td>5400&lt;/td>&lt;td>Only displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>TFD-ICDAR 2019 [343]&lt;/td>&lt;td>851&lt;/td>&lt;td>38000&lt;/td>&lt;td>Inline and displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR-2021 IBEM [344]&lt;/td>&lt;td>8900&lt;/td>&lt;td>166000&lt;/td>&lt;td>Inline and displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>FormulaNet [345]&lt;/td>&lt;td>46,672&lt;/td>&lt;td>1000,00&lt;/td>&lt;td>Inline and displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>ArxivFormula [91]&lt;/td>&lt;td>700000&lt;/td>&lt;td>813.3&lt;/td>&lt;td>Inline and displayed Formula&lt;/td>&lt;td>MED&lt;/td>&lt;/tr>&lt;tr>&lt;td>Pix2tex [346]&lt;/td>&lt;td colspan="2">189117&lt;/td>&lt;td>Printed&lt;/td>&lt;td>MER&lt;/td>&lt;/tr>&lt;tr>&lt;td>CROHME [347]&lt;/td>&lt;td colspan="2">12178&lt;/td>&lt;td>Handwritten&lt;/td>&lt;td>MER&lt;/td>&lt;/tr>&lt;tr>&lt;td>HME100K [348]&lt;/td>&lt;td colspan="2">99109&lt;/td>&lt;td>Handwritten&lt;/td>&lt;td>MER&lt;/td>&lt;/tr>&lt;tr>&lt;td>UniMERNet [96]&lt;/td>&lt;td colspan="2">1,061,791&lt;/td>&lt;td>Printed and Handwritten&lt;/td>&lt;td>MER&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_23_1/</guid><description>&lt;table id='3' style='font-size:16px'>&lt;tr>&lt;td>Dataset&lt;/td>&lt;td>Instance&lt;/td>&lt;td>Type&lt;/td>&lt;td>Language&lt;/td>&lt;td>Task&lt;/td>&lt;td>Feature&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR2013 [349]&lt;/td>&lt;td>150&lt;/td>&lt;td>Government Documents&lt;/td>&lt;td>English&lt;/td>&lt;td>TD &amp; TSR&lt;/td>&lt;td>Covers complex structures and cross-page tables&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR2017 POD [342]&lt;/td>&lt;td>1548&lt;/td>&lt;td>Scientific papers&lt;/td>&lt;td>English&lt;/td>&lt;td>TD&lt;/td>&lt;td>Includes shape and formula detec- tion&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICDAR2019 [350]&lt;/td>&lt;td>2439&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English&lt;/td>&lt;td>TD &amp; TSR&lt;/td>&lt;td>Includes historical and modern ta- bles&lt;/td>&lt;/tr>&lt;tr>&lt;td>TABLE2LATEX-450K [124]&lt;/td>&lt;td>140000&lt;/td>&lt;td>Scientific papers&lt;/td>&lt;td>English&lt;/td>&lt;td>TSR&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>RVL-CDIP (subset) [351]&lt;/td>&lt;td>518&lt;/td>&lt;td>Receipts&lt;/td>&lt;td>English&lt;/td>&lt;td>TD&lt;/td>&lt;td>Derived from RVL-CDIP&lt;/td>&lt;/tr>&lt;tr>&lt;td>IIIT-AR-13K [352]&lt;/td>&lt;td>17,000 (not only tables)&lt;/td>&lt;td>Annual Reports&lt;/td>&lt;td>Multi-langugae&lt;/td>&lt;td>TD&lt;/td>&lt;td>Does not only contain tables&lt;/td>&lt;/tr>&lt;tr>&lt;td>CamCap [353]&lt;/td>&lt;td>85&lt;/td>&lt;td>Table images&lt;/td>&lt;td>English&lt;/td>&lt;td>TD &amp; TSR&lt;/td>&lt;td>Used for evaluating table detection in camera-captured images&lt;/td>&lt;/tr>&lt;tr>&lt;td>UNLV Table [354]&lt;/td>&lt;td>2889&lt;/td>&lt;td>Journals, Newspapers, Business Letters&lt;/td>&lt;td>English&lt;/td>&lt;td>TD&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>UW-3 Table [355]&lt;/td>&lt;td>1,600 (around 120 tables)&lt;/td>&lt;td>Books, Magazines&lt;/td>&lt;td>English&lt;/td>&lt;td>TD&lt;/td>&lt;td>Manually labeled bounding boxes&lt;/td>&lt;/tr>&lt;tr>&lt;td>Marmot [356]&lt;/td>&lt;td>2000&lt;/td>&lt;td>Conference Papers&lt;/td>&lt;td>English and Chinese&lt;/td>&lt;td>TD&lt;/td>&lt;td>Includes diversified table types; still expanding&lt;/td>&lt;/tr>&lt;tr>&lt;td>TableBank [357]&lt;/td>&lt;td>417234&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English&lt;/td>&lt;td>TD &amp; TSR&lt;/td>&lt;td>Automatically created by weakly su- pervised methods&lt;/td>&lt;/tr>&lt;tr>&lt;td>DeepFigures [287]&lt;/td>&lt;td>5,500,000 (tables and figures)&lt;/td>&lt;td>Scientific papers&lt;/td>&lt;td>English&lt;/td>&lt;td>TD&lt;/td>&lt;td>Supports figure extraction&lt;/td>&lt;/tr>&lt;tr>&lt;td>PubTabNet [125]&lt;/td>&lt;td>568000&lt;/td>&lt;td>Scientific papers&lt;/td>&lt;td>English&lt;/td>&lt;td>TSR&lt;/td>&lt;td>Structure and content recognition of tables&lt;/td>&lt;/tr>&lt;tr>&lt;td>PubTables-1M [358]&lt;/td>&lt;td>1000000&lt;/td>&lt;td>Scientific papers&lt;/td>&lt;td>English&lt;/td>&lt;td>TSR [122]&lt;/td>&lt;td>Evaluates the oversegmentation is- sue&lt;/td>&lt;/tr>&lt;tr>&lt;td>SciTSR [359]&lt;/td>&lt;td>15000&lt;/td>&lt;td>Scientific papers&lt;/td>&lt;td>English&lt;/td>&lt;td>TSR&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>FinTable [359]&lt;/td>&lt;td>112887&lt;/td>&lt;td>Scientific and Financial Tables&lt;/td>&lt;td>English&lt;/td>&lt;td>TD &amp; TSR&lt;/td>&lt;td>Automatic Annotation methods&lt;/td>&lt;/tr>&lt;tr>&lt;td>SynthTabNet [360]&lt;/td>&lt;td>600000&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English&lt;/td>&lt;td>TD &amp; TSR&lt;/td>&lt;td>Synthetic tables&lt;/td>&lt;/tr>&lt;tr>&lt;td>Wired Table in the Wild [121]&lt;/td>&lt;td>14582 (pages)&lt;/td>&lt;td>Photos, Files, and Web Pages&lt;/td>&lt;td>English&lt;/td>&lt;td>TSR&lt;/td>&lt;td>Deformed and occluded images&lt;/td>&lt;/tr>&lt;tr>&lt;td>WikiTableSet [361]&lt;/td>&lt;td>50000000&lt;/td>&lt;td>Wikipedia&lt;/td>&lt;td>English, Japanese, French&lt;/td>&lt;td>TSR&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>STDW [362]&lt;/td>&lt;td>7000&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English&lt;/td>&lt;td>TD&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>TableGraph-350K [363]&lt;/td>&lt;td>358,767&lt;/td>&lt;td>Academic Table&lt;/td>&lt;td>English&lt;/td>&lt;td>TSR&lt;/td>&lt;td>including TableGraph-24K&lt;/td>&lt;/tr>&lt;tr>&lt;td>TabRecSet [364]&lt;/td>&lt;td>38100&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English and Chinese&lt;/td>&lt;td>TSR&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>DECO [365]&lt;/td>&lt;td>1165&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>English&lt;/td>&lt;td>TD&lt;/td>&lt;td>Enron document electronic table files&lt;/td>&lt;/tr>&lt;tr>&lt;td>iFLYTAB [366]&lt;/td>&lt;td>17291&lt;/td>&lt;td>Multiple Types&lt;/td>&lt;td>Chinese and English&lt;/td>&lt;td>TD &amp; TSR&lt;/td>&lt;td>Online and offline tables from vari- ous scenarios&lt;/td>&lt;/tr>&lt;tr>&lt;td>FinTab [367]&lt;/td>&lt;td>1,600&lt;/td>&lt;td>Financial Table&lt;/td>&lt;td>Chinese&lt;/td>&lt;td>TSR&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_25_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_25_0/</guid><description>&lt;table id='1' style='font-size:14px'>&lt;tr>&lt;td>Dataset&lt;/td>&lt;td>Year&lt;/td>&lt;td>Instance&lt;/td>&lt;td>Class&lt;/td>&lt;td>Task&lt;/td>&lt;td>Feature&lt;/td>&lt;/tr>&lt;tr>&lt;td>DeepChart [368]&lt;/td>&lt;td>2015&lt;/td>&lt;td>5000&lt;/td>&lt;td>5&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>VIEW [369]&lt;/td>&lt;td>2012&lt;/td>&lt;td>300&lt;/td>&lt;td>3&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>-&lt;/td>&lt;/tr>&lt;tr>&lt;td>ReVision [288]&lt;/td>&lt;td>2011&lt;/td>&lt;td>2601&lt;/td>&lt;td>10&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Based on ChartSense dataset&lt;/td>&lt;/tr>&lt;tr>&lt;td>CHART 2019 [370] - PMC&lt;/td>&lt;td>2019&lt;/td>&lt;td>4242&lt;/td>&lt;td>multi-class&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Real charts from scientific publications&lt;/td>&lt;/tr>&lt;tr>&lt;td>CHART 2019 - Syn- thetic [371]&lt;/td>&lt;td>2019&lt;/td>&lt;td>202,550&lt;/td>&lt;td>multi-class&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Synthetic charts&lt;/td>&lt;/tr>&lt;tr>&lt;td>DocFigure [370]&lt;/td>&lt;td>2019&lt;/td>&lt;td>33000&lt;/td>&lt;td>28&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Includes various figure images&lt;/td>&lt;/tr>&lt;tr>&lt;td>UB-PMC 2019 [370]&lt;/td>&lt;td>2019&lt;/td>&lt;td>4242&lt;/td>&lt;td>7&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Competition dataset&lt;/td>&lt;/tr>&lt;tr>&lt;td>UB-PMC 2020 [372]&lt;/td>&lt;td>2020&lt;/td>&lt;td>2123&lt;/td>&lt;td>4&lt;/td>&lt;td>Chart Data Extraction&lt;/td>&lt;td>Real charts from PubMedCentra&lt;/td>&lt;/tr>&lt;tr>&lt;td>UM-PMC 2021 [373]&lt;/td>&lt;td>2021&lt;/td>&lt;td>22924&lt;/td>&lt;td>15&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Competition dataset&lt;/td>&lt;/tr>&lt;tr>&lt;td>UB-PMC 2022 [136]&lt;/td>&lt;td>2022&lt;/td>&lt;td>33186&lt;/td>&lt;td>15&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Competition dataset&lt;/td>&lt;/tr>&lt;tr>&lt;td>Synth 2020 [373]&lt;/td>&lt;td>2020&lt;/td>&lt;td>9600&lt;/td>&lt;td>4&lt;/td>&lt;td>Chart Data Extraction&lt;/td>&lt;td>Synthetic charts&lt;/td>&lt;/tr>&lt;tr>&lt;td>LINEEX430k [300]&lt;/td>&lt;td>2023&lt;/td>&lt;td>430,000&lt;/td>&lt;td>Line charts&lt;/td>&lt;td>Chart Data Extraction&lt;/td>&lt;td>Focused on line charts&lt;/td>&lt;/tr>&lt;tr>&lt;td>ICPR 2022 [136]&lt;/td>&lt;td>2022&lt;/td>&lt;td>26,596&lt;/td>&lt;td>15&lt;/td>&lt;td>Chart Classification&lt;/td>&lt;td>Charts with embedded text&lt;/td>&lt;/tr>&lt;tr>&lt;td>ExcelChart400K&lt;/td>&lt;td>[3742021&lt;/td>&lt;td>400,0000&lt;/td>&lt;td>Pie and bar charts&lt;/td>&lt;td>Chart Data Extraction&lt;/td>&lt;td>Extracted from Excel charts with JSON annotations&lt;/td>&lt;/tr>&lt;tr>&lt;td>CHARTER [375]&lt;/td>&lt;td>2021&lt;/td>&lt;td>32334&lt;/td>&lt;td>4&lt;/td>&lt;td>Chart Data Extraction&lt;/td>&lt;td>Sourced from document pages, web pages, PubMed, FigureQA, etc.&lt;/td>&lt;/tr>&lt;tr>&lt;td>StructChart dataset [376]&lt;/td>&lt;td>2023&lt;/td>&lt;td>16466&lt;/td>&lt;td>Organization and structure charts&lt;/td>&lt;td>Chart Structure Extraction&lt;/td>&lt;td>-&lt;/td>&lt;/tr>&lt;tr>&lt;td>OneChart [377]&lt;/td>&lt;td>2023&lt;/td>&lt;td>10000000&lt;/td>&lt;td>5&lt;/td>&lt;td>Chart Information Extraction, QA, and Inference&lt;/td>&lt;td>Synthesized using Matplotlib&lt;/td>&lt;/tr>&lt;tr>&lt;td>Chart-to-Text [378]&lt;/td>&lt;td>2023&lt;/td>&lt;td>8305&lt;/td>&lt;td>6&lt;/td>&lt;td>Chart Information Extraction&lt;/td>&lt;td>Contains chart samples and corresponding data&lt;/td>&lt;/tr>&lt;tr>&lt;td>ChartLlama [379]&lt;/td>&lt;td>2023&lt;/td>&lt;td>1500&lt;/td>&lt;td>10&lt;/td>&lt;td>7 comprehensive chart tasks in- cluding chart information extrac- tion&lt;/td>&lt;td>GPT-4 generates charts and instruction data&lt;/td>&lt;/tr>&lt;tr>&lt;td>ChartX [299]&lt;/td>&lt;td>2024&lt;/td>&lt;td>48000&lt;/td>&lt;td>18&lt;/td>&lt;td>7 comprehensive chart tasks in- cluding chart information extrac- tion&lt;/td>&lt;td>Automatically generated by GPT-4 and manually checked&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_26_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_26_0/</guid><description>&lt;table id='6' style='font-size:14px'>&lt;tr>&lt;td>Metric&lt;/td>&lt;td>Definition&lt;/td>&lt;td>Description&lt;/td>&lt;/tr>&lt;tr>&lt;td>IoU&lt;/td>&lt;td>Area of Overlap IoU = Area of Union TP&lt;/td>&lt;td>Measures the overlap between predicted and ground truth boxes.&lt;/td>&lt;/tr>&lt;tr>&lt;td>ReCall&lt;/td>&lt;td>ReCall = TP + FN N&lt;/td>&lt;td>Measures how many true positive samples are correctly predicted by the model.&lt;/td>&lt;/tr>&lt;tr>&lt;td>mAP&lt;/td>&lt;td>1 mAP = APi N i=1 M 1&lt;/td>&lt;td>Average precision across all classes, assessing overall model performance.&lt;/td>&lt;/tr>&lt;tr>&lt;td>mAP@IoU[a:b]&lt;/td>&lt;td>mAP@IoU[a:b] = mAPI⌀U j M j=1&lt;/td>&lt;td>Computes over a range of IoU thresholds [a, b], calculating at specified intervals and averaged.&lt;/td>&lt;/tr>&lt;tr>&lt;td>F1-score&lt;/td>&lt;td>Precision X Recall F1-score = 2 x Precision + Recall&lt;/td>&lt;td>Balances precision and recall and useful in imbalanced class scenarios.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_0/</guid><description>&lt;table id='1' style='font-size:14px'>&lt;tr>&lt;td>Metric&lt;/td>&lt;td>Definition&lt;/td>&lt;td>Description&lt;/td>&lt;/tr>&lt;tr>&lt;td>CER&lt;/td>&lt;td>S+ D + I CER N&lt;/td>&lt;td>Measures the character-level discrepancy between recognized and ground truth text, suitable for OCR tasks requiring high precision.&lt;/td>&lt;/tr>&lt;tr>&lt;td>Edit Distance&lt;/td>&lt;td>D(i-1,j)+1 D(i,j) = min [ D(i,j -1)+1 D(i - 1, j -1) + Cost(s1 [i], s2[j]) N&lt;/td>&lt;td>Measures the minimum edit distance needed to convert recognized text into ground truth text.&lt;/td>&lt;/tr>&lt;tr>&lt;td>BLEU&lt;/td>&lt;td>BLEU = BP x exp M Wn log Pn ) n=1&lt;/td>&lt;td>Measures the minimum edit distance needed to convert recognized text into ground truth text.&lt;/td>&lt;/tr>&lt;tr>&lt;td>METEOR&lt;/td>&lt;td>METEOR = Fmean x (1 - Penalty)&lt;/td>&lt;td>Accounts for both precision and recall, and supports stem and synonym matching.&lt;/td>&lt;/tr>&lt;tr>&lt;td>ROUGE-N&lt;/td>&lt;td>� ngramE Reference min(Countmatch (ngram), Countcandidate (ngram)) ROUGE-N = ngramEReference Countreference (ngram)&lt;/td>&lt;td>An improved version of BLEU that focuses on recall rather than precision.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_27_1/</guid><description>&lt;table id='4' style='font-size:16px'>&lt;tr>&lt;td>Metric&lt;/td>&lt;td>Definition&lt;/td>&lt;td>Description&lt;/td>&lt;/tr>&lt;tr>&lt;td>ExpRate&lt;/td>&lt;td>Number of exact matches ExpRate Total number of samples&lt;/td>&lt;td>Measures the proportion of samples that are com- pletely correct, suitable for scenarios requiring high accuracy.&lt;/td>&lt;/tr>&lt;tr>&lt;td>MSE&lt;/td>&lt;td>m n 1 MSE ��(I(i,j) - K(i,j))2 mn i=1 j=1&lt;/td>&lt;td>Measures the average squared difference between corresponding pixels in two images.&lt;/td>&lt;/tr>&lt;tr>&lt;td>SSIM&lt;/td>&lt;td>(2�x�� + C1)(2�xy + C2) SSIM(x, y) = (사로 + 사립 + C1)(⌀2 + ⌀2 + C2)&lt;/td>&lt;td>Measures the structural similarity of images, taking into account brightness, contrast, and structural in- formation.&lt;/td>&lt;/tr>&lt;tr>&lt;td>CDM&lt;/td>&lt;td>2x TP CDM = 2 x TP + FP + FN&lt;/td>&lt;td>Converts LaTeX mathematical expression into im- age and matches it with the corresponding image structure.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_28_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_28_0/</guid><description>&lt;table id='2' style='font-size:14px'>&lt;tr>&lt;td>Metric&lt;/td>&lt;td>Definition&lt;/td>&lt;td>Description&lt;/td>&lt;/tr>&lt;tr>&lt;td>Purity&lt;/td>&lt;td>k 1 Purity = max ICinL⌀l N j i=1 k 1&lt;/td>&lt;td>Measures the level of noise contained in the detected results.&lt;/td>&lt;/tr>&lt;tr>&lt;td>Completeness&lt;/td>&lt;td>Completeness max |Lj n Cil N 2 j=1&lt;/td>&lt;td>Measure the proportion of table areas detected within the tables.&lt;/td>&lt;/tr>&lt;tr>&lt;td>CAR&lt;/td>&lt;td>�i=1 1(predicted adjacency(Ci) = true adjacency(Ci)) CAR n&lt;/td>&lt;td>Evaluates boundary detection and relative positioning of table cells, reflecting the structural relationships of the table.&lt;/td>&lt;/tr>&lt;tr>&lt;td>TEDS&lt;/td>&lt;td>TED(T1 , T2) TEDS(T1,T2) = 1 - max(size(T1), size(T2)) AcolEd (i)H&lt;/td>&lt;td>Measures similarity based on tree edit distance, focusing on table structure, including tags and content.&lt;/td>&lt;/tr>&lt;tr>&lt;td>Aall&lt;/td>&lt;td>K⌀il ArowSt (i) n ArowEd (i) n AcolSt(i) n Aall = N&lt;/td>&lt;td>A cell's prediction is considered correct if and only if all four of its logical positions are accurately predicted.&lt;/td>&lt;/tr>&lt;tr>&lt;td>F_beta&lt;/td>&lt;td>(1 +0.52) . H . Aall F�=0.5 = 0.52 . H + Aall&lt;/td>&lt;td>Combines spatial positioning and logical accuracy, balancing evaluation better than F1-score. layout and spatial location&lt;/td>&lt;/tr>&lt;tr>&lt;td>WAF&lt;/td>&lt;td>� =1 IoUi · F�= 1 @IoUi W AF = �1=1 IoU⌀&lt;/td>&lt;td>Evaluates adjacency relation prediction based on intersection over union (IoU).&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_29_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_29_0/</guid><description>&lt;br>&lt;table id='2' style='font-size:14px'>&lt;tr>&lt;td>Tools&lt;/td>&lt;td>Developer&lt;/td>&lt;td>Time&lt;/td>&lt;td>Introduction&lt;/td>&lt;/tr>&lt;tr>&lt;td>GROBID&lt;/td>&lt;td>Patrice Lopez&lt;/td>&lt;td>2011&lt;/td>&lt;td>A machine learning library that focuses on extracting and restructuring original documents, converting them into structured formats such as XML/TEI encoding.&lt;/td>&lt;/tr>&lt;tr>&lt;td>PyMuPDF&lt;/td>&lt;td>Jorj X. McKie&lt;/td>&lt;td>2011&lt;/td>&lt;td>A Python library for extracting, analyzing, converting, and processing data from PDFs and other documents, supporting tables, figures, and other types of content.&lt;/td>&lt;/tr>&lt;tr>&lt;td>doc2text&lt;/td>&lt;td>Joe Sutherland&lt;/td>&lt;td>2016.9&lt;/td>&lt;td>Specializes in extracting low-quality documents; only ensures compatibility in Linux.&lt;/td>&lt;/tr>&lt;tr>&lt;td>pdfplumber&lt;/td>&lt;td>Jeremy Singer- Vine&lt;/td>&lt;td>2019.1&lt;/td>&lt;td>Tools for extraction and parsing of characters, images, lines, tables, and other elements from digital PDF documents.&lt;/td>&lt;/tr>&lt;tr>&lt;td>Parsr&lt;/td>&lt;td>axa-group&lt;/td>&lt;td>2019.8&lt;/td>&lt;td>A tool for cleaning, parsing, and extracting content from various document types, with outputs including JSON, Markdown, CSV/pandasDF, and txt formats.&lt;/td>&lt;/tr>&lt;tr>&lt;td>PP-StructureV2&lt;/td>&lt;td>Baidu&lt;/td>&lt;td>2021.8&lt;/td>&lt;td>Intelligent document analysis system, supports layout analysis of Chinese and English documents, table recognition, and semantic recognition.&lt;/td>&lt;/tr>&lt;tr>&lt;td>DocxChain&lt;/td>&lt;td>Alibaba&lt;/td>&lt;td>2023.9&lt;/td>&lt;td>A system for non-structured or semi-structured document conversion into various information and formats, including complex document applications based on computational capabilities.&lt;/td>&lt;/tr>&lt;tr>&lt;td>pdf2htmlEX&lt;/td>&lt;td>Lu Wang&lt;/td>&lt;td>2023.12&lt;/td>&lt;td>A project to convert PDF documents into HTML format.&lt;/td>&lt;/tr>&lt;tr>&lt;td>MinerU&lt;/td>&lt;td>OpenDataLab&lt;/td>&lt;td>2024.4&lt;/td>&lt;td>A system for extracting content from PDF and converting it into markdown or JSON formats.&lt;/td>&lt;/tr>&lt;tr>&lt;td>PDF-Extract-Kit&lt;/td>&lt;td>OpenDataLab&lt;/td>&lt;td>2024.7&lt;/td>&lt;td>A system based on MinerU to extract various content from PDF, including layout analysis, OCR, table recognition, and formula recognition tasks.&lt;/td>&lt;/tr>&lt;tr>&lt;td>OmniParser&lt;/td>&lt;td>Adithya s Kolavi&lt;/td>&lt;td>2024.6&lt;/td>&lt;td>A platform for extracting and parsing any unstructured data, transforming it into structured, actionable data optimized for GenAI applications.&lt;/td>&lt;/tr>&lt;tr>&lt;td>LLM_aided_ocr&lt;/td>&lt;td>Jeff Emanuel&lt;/td>&lt;td>2024.8&lt;/td>&lt;td>Uses Tesseract for document OCR, followed by LLM-based error correction, with final output in markdown or similar formats.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_38_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_38_0/</guid><description>&lt;table id='0' style='font-size:18px'>&lt;tr>&lt;td>[94]&lt;/td>&lt;td>Jianshu Zhang, Jun Du, and Lirong Dai. Multi-scale attention with dense encoder for hand- written mathematical expression recognition. In 2018 24th international conference on pattern recognition (ICPR), pages 2245-2250. IEEE, 2018.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[95]&lt;/td>&lt;td>Zhe Li, Lianwen Jin, Songxuan Lai, and Yecheng Zhu. Improving attention-based handwritten mathematical expression recognition with scale augmentation and drop attention. In 2020 17th International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 175-180. IEEE, 2020.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[96]&lt;/td>&lt;td>Bin Wang, Zhuangcheng Gu, Chao Xu, Bo Zhang, Botian Shi, and Conghui He. Unimernet: A universal network for real-world mathematical expression recognition. arXiv preprint arXiv:2404.15254, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[97]&lt;/td>&lt;td>Wei Zhang, Zhiqiang Bai, and Yuesheng Zhu. An improved approach based on cnn-rnns for mathematical expression recognition. In Proceedings of the 2019 4th international conference on multimedia systems and signal processing, pages 57-61, 2019.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[98]&lt;/td>&lt;td>Wenqi Zhao, Liangcai Gao, Zuoyu Yan, Shuai Peng, Lin Du, and Ziyin Zhang. Handwritten mathematical expression recognition with bidirectionally trained transformer. In Document analysis and recognition-ICDAR 2021: 16th international conference, Lausanne, Switzerland, September 5-10, 2021, proceedings, part II 16, pages 570-584. Springer, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[99]&lt;/td>&lt;td>Wenqi Zhao and Liangcai Gao. Comer: Modeling coverage for transformer-based handwritten mathematical expression recognition. In European conference on computer vision, pages 392-408. Springer, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[100]&lt;/td>&lt;td>Bohan Li, Ye Yuan, Dingkang Liang, Xiao Liu, Zhilong Ji, Jinfeng Bai, Wenyu Liu, and Xiang Bai. When counting meets hmer: counting-aware network for handwritten mathematical expression recognition. In European conference on computer vision, pages 197-214. Springer, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[101]&lt;/td>&lt;td>Jianhua Zhu, Liangcai Gao, and Wenqi Zhao. Ical: Implicit character-aided learning for enhanced handwritten mathematical expression recognition. In International Conference on Document Analysis and Recognition, pages 21-37. Springer, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[102]&lt;/td>&lt;td>Chungkwong Chan. Stroke extraction for offline handwritten mathematical expression recog- nition. IEEE Access, 8:61565-61575, 2020.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[103]&lt;/td>&lt;td>Jiaming Wang, Jun Du, Jianshu Zhang, and Zi-Rui Wang. Multi-modal attention network for handwritten mathematical expression recognition. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1181-1186. IEEE, 2019.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[104]&lt;/td>&lt;td>Leipeng Hao, Liangcai Gao, Xiaohan Yi, and Zhi Tang. A table detection method for pdf documents based on convolutional neural networks. In 2016 12th IAPR Workshop on Document Analysis Systems (DAS), pages 287-292. IEEE, 2016.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[105]&lt;/td>&lt;td>Azka Gilani, Shah Rukh Qasim, Imran Malik, and Faisal Shafait. Table detection using deep learning. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 771-776. IEEE, 2017.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[106]&lt;/td>&lt;td>Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 1162-1167. IEEE, 2017.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[107]&lt;/td>&lt;td>Shoaib Ahmed Siddiqui, Muhammad Imran Malik, Stefan Agne, Andreas Dengel, and Sheraz Ahmed. Decnt: Deep deformable cnn for table detection. IEEE access, 6:74151-74161, 2018.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[108]&lt;/td>&lt;td>Yilun Huang, Qinqin Yan, Yibo Li, Yifan Chen, Xiong Wang, Liangcai Gao, and Zhi Tang. A yolo-based table detection method. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 813-818. IEEE, 2019.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[109]&lt;/td>&lt;td>Bin Xiao, Murat Simsek, Burak Kantarci, and Ala Abu Alkheir. Table detection for visually rich document images. Knowledge-Based Systems, 282:111080, 2023.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_50_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_50_0/</guid><description>&lt;table id='0' style='font-size:18px'>&lt;tr>&lt;td>[282]&lt;/td>&lt;td>Zengyuan Guo, Yuechen Yu, Pengyuan Lv, Chengquan Zhang, Haojie Li, Zhihui Wang, Kun Yao, Jingtuo Liu, and Jingdong Wang. Trust: An accurate and end-to-end table structure recognizer using splitting-based transformers. arXiv preprint arXiv:2208.14687, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[283]&lt;/td>&lt;td>Tao Zhang, Yi Sui, Shunyao Wu, Fengjing Shao, and Rencheng Sun. Table structure recog- nition method based on lightweight network and channel attention. Electronics, 12(3):673, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[284]&lt;/td>&lt;td>Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. Deplot: One-shot visual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[285]&lt;/td>&lt;td>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[286]&lt;/td>&lt;td>Christopher Clark and Santosh Divvala. Pdffigures 2.0: Mining figures from research papers. In Proceedings of the 16th ACM/IEEE-CS on Joint Conference on Digital Libraries, pages 143-152, 2016.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[287]&lt;/td>&lt;td>Noah Siegel, Nicholas Lourie, Russell Power, and Waleed Ammar. Extracting scientific figures with distantly supervised neural networks. In Proceedings of the 18th ACM/IEEE on joint conference on digital libraries, pages 223-232, 2018.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[288]&lt;/td>&lt;td>Manolis Savva, Nicholas Kong, Arti Chhajta, Li Fei-Fei, Maneesh Agrawala, and Jeffrey Heer. Revision: Automated classification, analysis and redesign of chart images. In Proceedings of the 24th annual ACM symposium on User interface software and technology, pages 393-402, 2011.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[289]&lt;/td>&lt;td>Ales Mishchenko and Natalia Vassilieva. Chart image understanding and numerical data extraction. In 2011 Sixth International Conference on Digital Information Management, pages 115-120. IEEE, 2011.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[290]&lt;/td>&lt;td>Haixia Liu and Tim Brailsford. Reproducing show, attend and tell: Neural image caption generation with visual attention. In Journal of Physics: Conference Series, volume 2589, page 012012. IOP Publishing, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[291]&lt;/td>&lt;td>Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha, and Changshui Zhang. Aligning where to see and what to tell: image caption with region-based attention and scene factorization. arXiv preprint arXiv:1506.06272, 2015.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[292]&lt;/td>&lt;td>Sameer Antani, Dina Demner-Fushman, Jiang Li, Balaji V Srinivasan, and George R Thoma. Exploring use of images in clinical articles for decision support in evidence-based medicine. In Document Recognition and Retrieval XV, volume 6815, pages 230-239. SPIE, 2008.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[293]&lt;/td>&lt;td>Beibei Cheng, Sameer Antani, R Joe Stanley, and George R Thoma. Automatic segmentation of subfigure image panels for multimodal biomedical document retrieval. In Document Recognition and Retrieval XVIII, volume 7874, pages 294-304. SPIE, 2011.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[294]&lt;/td>&lt;td>Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. Empowering 1000 tokens/second on-device llm prefilling with mllm-npu. arXiv preprint arXiv:2407.05858, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[295]&lt;/td>&lt;td>Weihua Huang, Chew Lim Tan, and Wee Kheng Leow. Associating text and graphics for scientific chart understanding. In Eighth International Conference on Document Analysis and Recognition (ICDAR '05), pages 580-584. IEEE, 2005.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[296]&lt;/td>&lt;td>Weihua Huang and Chew Lim Tan. A system for understanding imaged infographics and its applications. In Proceedings of the 2007 ACM symposium on Document engineering, pages 9-18, 2007.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[297]&lt;/td>&lt;td>Sagnik Ray Choudhury, Shuting Wang, Prasenjit Mitra, and C Lee Giles. Automated data extraction from scholarly line graphs. In Proc. Int. Workshop Graph. Recognit, 2015.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_51_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_51_0/</guid><description>&lt;table id='0' style='font-size:18px'>&lt;tr>&lt;td>[298]&lt;/td>&lt;td>Chinmayee Rane, Seshasayee Mahadevan Subramanya, Devi Sandeep Endluri, Jian Wu, and C Lee Giles. Chartreader: Automatic parsing of bar-plots. In 2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI), pages 318-325. IEEE, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[299]&lt;/td>&lt;td>Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et al. Chartx &amp; chartvlm: A versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[300]&lt;/td>&lt;td>Muhammad Yusuf Hassan, Mayank Singh, et al. Lineex: data extraction from scientific line charts. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6213-6221, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[301]&lt;/td>&lt;td>Ceres Carton, Aurelie Lemaitre, and Bertrand Couasnon. Fusion of statistical and structural information for flowchart recognition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1210-1214. IEEE, 2013.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[302]&lt;/td>&lt;td>Mar�al Rusinol, Lluis-Pere de las Heras, Joan Mas, Oriol Ramos Terrades, Dimosthenis Karatzas, Anjan Dutta, Gemma Sanchez, and Josep Llados. Cvc-uab's participation in the flowchart recognition task of clef-ip 2012. In CLEF (Online Working Notes/Labs/Workshop), 2012.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[303]&lt;/td>&lt;td>Hugh A Chipman, Edward I George, Robert E McCulloch, and Thomas S Shively. mbart: multidimensional monotone bart. Bayesian Analysis, 17(2):515-544, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[304]&lt;/td>&lt;td>Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large vision-language models. arXiv preprint arXiv:2312.06109, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[305]&lt;/td>&lt;td>Jiawei Wang, Kai Hu, Zhuoyao Zhong, Lei Sun, and Qiang Huo. Detect-order-construct: A tree construction based approach for hierarchical document structure analysis. arXiv preprint arXiv:2401.11874, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[306]&lt;/td>&lt;td>Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, and Zhibo Yang. Omniparser: A unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15641-15653, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[307]&lt;/td>&lt;td>Christos Papadopoulos, Stefan Pletschacher, Christian Clausner, and Apostolos Antonacopou- los. The impact dataset of historical document images. In Proceedings of the 2Nd international workshop on historical document imaging and processing, pages 123-130, 2013.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[308]&lt;/td>&lt;td>Mukkai Krishnamoorthy, George Nagy, Sharad Seth, and Mahesh Viswanathan. Syntactic segmentation and labeling of digitized pages from technical journals. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(7):737-747, 1993.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[309]&lt;/td>&lt;td>David Lewis, Gady Agam, Shlomo Argamon, Ophir Frieder, David Grossman, and Jefferson Heard. Building a test collection for complex document information processing. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 665-666, 2006.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[310]&lt;/td>&lt;td>Apostolos Antonacopoulos, David Bridson, Christos Papadopoulos, and Stefan Pletschacher. A realistic dataset for performance evaluation of document layout analysis. In 2009 10th International Conference on Document Analysis and Recognition, pages 296-300. IEEE, 2009.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[311]&lt;/td>&lt;td>Rana SM Saad, Randa I Elanwar, NS Abdel Kader, Samia Mashali, and Margrit Betke. Bce-arabic-v1 dataset: Towards interpreting arabic document images for people with vi- sual impairments. In Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments, pages 1-8, 2016.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[312]&lt;/td>&lt;td>Fotini Simistira, Manuel Bouillon, Mathias Seuret, Marcel W�rsch, Michele Alberti, Rolf Ingold, and Marcus Liwicki. Icdar2017 competition on layout analysis for challenging medieval manuscripts. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 1361-1370. IEEE, 2017.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_56_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21169/tables/table_56_0/</guid><description>&lt;table id='0' style='font-size:18px'>&lt;tr>&lt;td>[374]&lt;/td>&lt;td>Junyu Luo, Zekun Li, Jinpeng Wang, and Chin- Yew Lin. Chartocr: Data extraction from charts images via a deep hybrid framework. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 1917-1925, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[375]&lt;/td>&lt;td>Joseph Shtok, Sivan Harary, Ophir Azulai, Adi Raz Goldfarb, Assaf Arbelle, and Leonid Karlin- sky. Charter: heatmap-based multi-type chart data extraction. arXiv preprint arXiv:2111.14103, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[376]&lt;/td>&lt;td>Renqiu Xia, Bo Zhang, Haoyang Peng, Hancheng Ye, Xiangchao Yan, Peng Ye, Botian Shi, Yu Qiao, and Junchi Yan. Structchart: Perception, structuring, reasoning for visual chart understanding. arXiv preprint arXiv:2309.11268, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[377]&lt;/td>&lt;td>Jinyue Chen, Lingyu Kong, Haoran Wei, Chenglong Liu, Zheng Ge, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Onechart: Purify the chart structural extraction via one auxiliary token. arXiv preprint arXiv:2404.09987, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[378]&lt;/td>&lt;td>Jason Obeid and Enamul Hoque. Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model. arXiv preprint arXiv:2010.09142, 2020.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[379]&lt;/td>&lt;td>Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: A multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[380]&lt;/td>&lt;td>Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1516- 1520. IEEE, 2019.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[381]&lt;/td>&lt;td>Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. Docbench: A benchmark for evaluating llm-based document reading systems. arXiv preprint arXiv:2407.10701, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[382]&lt;/td>&lt;td>Karin Verspoor, Dat Quoc Nguyen, Saber A Akhondi, Christian Druckenbrodt, Camilo Thorne, Ralph Hoessel, Jiayuan He, and Zenan Zhai. Chemu dataset for information extraction from chemical patents. Mendeley Data, 2(10):17632, 2020.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[383]&lt;/td>&lt;td>Shivalika Tanwar, Patrick Auberger, Germain Gillet, Mario DiPaola, Katya Tsaioun, and Bruno 0 Villoutreix. A new chembl dataset for the similarity-based target fishing engine fasttargetpred: Annotation of an exhaustive list of linear tetrapeptides. Data in Brief, 42: 108159, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[384]&lt;/td>&lt;td>Jan Hajic and Pavel Pecina. The muscima++ dataset for handwritten optical music recognition. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 39-46. IEEE, 2017.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[385]&lt;/td>&lt;td>Lukas Tuggener, Ismail Elezi, Jurgen Schmidhuber, Marcello Pelillo, and Thilo Stadelmann. Deepscores-a dataset for segmentation, detection and classification of tiny objects. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 3704-3709. IEEE, 2018.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[386]&lt;/td>&lt;td>Zelun Wang and Jyh-Charn Liu. Translating math formula images to latex sequences using deep neural networks with sequence-level training. International Journal on Document Analysis and Recognition (IJDAR), 24(1):63-75, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[387]&lt;/td>&lt;td>Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, and Conghui He. Cdm: A reliable metric for fair and accurate formula recognition evaluation. arXiv preprint arXiv:2409.03643, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[388]&lt;/td>&lt;td>Pratik Kayal, Mrinal Anand, Harsh Desai, and Mayank Singh. Tables to latex: structure and content extraction from scientific tables. International Journal on Document Analysis and Recognition (IJDAR), 26(2):121-130, 2023.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_0/</guid><description>&lt;table id='0' style='font-size:16px'>&lt;tr>&lt;td>Dataset&lt;/td>&lt;td>Task Type&lt;/td>&lt;td>#Data&lt;/td>&lt;td>Avg Len&lt;/td>&lt;td>Language&lt;/td>&lt;td>Metric&lt;/td>&lt;td>Judge Model&lt;/td>&lt;/tr>&lt;tr>&lt;td colspan="7">Long-context Benchmark&lt;/td>&lt;/tr>&lt;tr>&lt;td>LongBench-Chat&lt;/td>&lt;td>Multi-Task&lt;/td>&lt;td>50&lt;/td>&lt;td>35,571&lt;/td>&lt;td>English/Chinese&lt;/td>&lt;td>Point-wise Rate&lt;/td>&lt;td>GPT-4o&lt;/td>&lt;/tr>&lt;tr>&lt;td>&lt;/td>&lt;td>Single-Doc QA&lt;/td>&lt;td>750&lt;/td>&lt;td>8,573&lt;/td>&lt;td>English/Chinese&lt;/td>&lt;td>Point-wise Rate&lt;/td>&lt;td>GPT-4o&lt;/td>&lt;/tr>&lt;tr>&lt;td>LongBench&lt;/td>&lt;td>Multi-Doc QA&lt;/td>&lt;td>800&lt;/td>&lt;td>1,0255&lt;/td>&lt;td>English/Chinese&lt;/td>&lt;td>Point-wise Rate&lt;/td>&lt;td>GPT-4o&lt;/td>&lt;/tr>&lt;tr>&lt;td>&lt;/td>&lt;td>Summarization&lt;/td>&lt;td>800&lt;/td>&lt;td>9,210&lt;/td>&lt;td>English/Chinese&lt;/td>&lt;td>Point-wise Rate&lt;/td>&lt;td>GPT-4o&lt;/td>&lt;/tr>&lt;tr>&lt;td colspan="7">Short-context Benchmark&lt;/td>&lt;/tr>&lt;tr>&lt;td>MT-Bench&lt;/td>&lt;td>Instruction Following&lt;/td>&lt;td>80&lt;/td>&lt;td>-&lt;/td>&lt;td>English&lt;/td>&lt;td>Point-wise Rate&lt;/td>&lt;td>GPT-4&lt;/td>&lt;/tr>&lt;tr>&lt;td>AlpacaEval2&lt;/td>&lt;td>Instruction Following&lt;/td>&lt;td>805&lt;/td>&lt;td>-&lt;/td>&lt;td>English&lt;/td>&lt;td>LC Win Rate&lt;/td>&lt;td>GPT-4-turbo&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_6_1/</guid><description>&lt;table id='2' style='font-size:14px'>&lt;tr>&lt;td rowspan="2">Model&lt;/td>&lt;td rowspan="2">Method&lt;/td>&lt;td rowspan="2">LongBench-Chat&lt;/td>&lt;td colspan="3">LongBench&lt;/td>&lt;td rowspan="2">Avg&lt;/td>&lt;/tr>&lt;tr>&lt;td>S-Doc QA&lt;/td>&lt;td>M-Doc QA&lt;/td>&lt;td>Summ&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">Llama-3.1-8B&lt;/td>&lt;td>officially post-trained&lt;/td>&lt;td>60.2&lt;/td>&lt;td>59.3&lt;/td>&lt;td>42.9&lt;/td>&lt;td>35.3&lt;/td>&lt;td>49.4&lt;/td>&lt;/tr>&lt;tr>&lt;td>SFT&lt;/td>&lt;td>69.8&lt;/td>&lt;td>66.1&lt;/td>&lt;td>44.5&lt;/td>&lt;td>39.6&lt;/td>&lt;td>55.0&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ SRM&lt;/td>&lt;td>67.4&lt;/td>&lt;td>65.0&lt;/td>&lt;td>49.6&lt;/td>&lt;td>42.7&lt;/td>&lt;td>56.2&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ Contrast&lt;/td>&lt;td>70.6&lt;/td>&lt;td>67.8&lt;/td>&lt;td>46.2&lt;/td>&lt;td>40.3&lt;/td>&lt;td>56.2&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ LongReward&lt;/td>&lt;td>72.6&lt;/td>&lt;td>67.8&lt;/td>&lt;td>55.8&lt;/td>&lt;td>43.2&lt;/td>&lt;td>59.9&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">GLM-4-9B&lt;/td>&lt;td>officially post-trained&lt;/td>&lt;td>68.6&lt;/td>&lt;td>67.8&lt;/td>&lt;td>56.9&lt;/td>&lt;td>47.9&lt;/td>&lt;td>60.3&lt;/td>&lt;/tr>&lt;tr>&lt;td>SFT&lt;/td>&lt;td>64.8&lt;/td>&lt;td>68.4&lt;/td>&lt;td>50.9&lt;/td>&lt;td>42.1&lt;/td>&lt;td>56.6&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ SRM&lt;/td>&lt;td>66.6&lt;/td>&lt;td>67.5&lt;/td>&lt;td>57.4&lt;/td>&lt;td>48.2&lt;/td>&lt;td>59.9&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ Contrast&lt;/td>&lt;td>68.2&lt;/td>&lt;td>67.8&lt;/td>&lt;td>58.0&lt;/td>&lt;td>47.8&lt;/td>&lt;td>60.5&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ LongReward&lt;/td>&lt;td>69.2&lt;/td>&lt;td>71.9&lt;/td>&lt;td>58.8&lt;/td>&lt;td>48.5&lt;/td>&lt;td>62.1&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>Method&lt;/td>&lt;td>#Facts&lt;/td>&lt;td>FactScore&lt;/td>&lt;/tr>&lt;tr>&lt;td>Llama-3.1-8B&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>SFT&lt;/td>&lt;td>21.76&lt;/td>&lt;td>91.94&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ LongReward&lt;/td>&lt;td>32.86&lt;/td>&lt;td>92.85&lt;/td>&lt;/tr>&lt;tr>&lt;td>GLM-4-9B&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>SFT&lt;/td>&lt;td>18.41&lt;/td>&lt;td>91.43&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ LongReward&lt;/td>&lt;td>28.05&lt;/td>&lt;td>93.62&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_1/</guid><description>&lt;table id='2' style='font-size:14px'>&lt;tr>&lt;td>&lt;/td>&lt;td>Win&lt;/td>&lt;td>Tie&lt;/td>&lt;td>Loss&lt;/td>&lt;td>△(Win-Loss)&lt;/td>&lt;/tr>&lt;tr>&lt;td>Helpfulness&lt;/td>&lt;td>0.14&lt;/td>&lt;td>0.84&lt;/td>&lt;td>0.02&lt;/td>&lt;td>0.12&lt;/td>&lt;/tr>&lt;tr>&lt;td>Logicality&lt;/td>&lt;td>0.14&lt;/td>&lt;td>0.86&lt;/td>&lt;td>0.00&lt;/td>&lt;td>0.14&lt;/td>&lt;/tr>&lt;tr>&lt;td>Faithfulness&lt;/td>&lt;td>0.32&lt;/td>&lt;td>0.64&lt;/td>&lt;td>0.04&lt;/td>&lt;td>0.28&lt;/td>&lt;/tr>&lt;tr>&lt;td>Completeness&lt;/td>&lt;td>0.26&lt;/td>&lt;td>0.64&lt;/td>&lt;td>0.10&lt;/td>&lt;td>0.16&lt;/td>&lt;/tr>&lt;tr>&lt;td>Overall&lt;/td>&lt;td>0.54&lt;/td>&lt;td>0.38&lt;/td>&lt;td>0.08&lt;/td>&lt;td>0.46&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_7_2/</guid><description>&lt;br>&lt;table id='6' style='font-size:14px'>&lt;tr>&lt;td>Method&lt;/td>&lt;td>MT-Bench&lt;/td>&lt;td>AlpacaEval2&lt;/td>&lt;/tr>&lt;tr>&lt;td>Llama-3.1-8B&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>officially post-trained&lt;/td>&lt;td>8.13&lt;/td>&lt;td>22.9&lt;/td>&lt;/tr>&lt;tr>&lt;td>SFT&lt;/td>&lt;td>7.12&lt;/td>&lt;td>12.4&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ SRM&lt;/td>&lt;td>7.58&lt;/td>&lt;td>13.7&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ Contrast&lt;/td>&lt;td>7.58&lt;/td>&lt;td>13.8&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ LongReward&lt;/td>&lt;td>7.24&lt;/td>&lt;td>14.2&lt;/td>&lt;/tr>&lt;tr>&lt;td>GLM-4-9B&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>officially post-trained&lt;/td>&lt;td>8.09&lt;/td>&lt;td>22.4&lt;/td>&lt;/tr>&lt;tr>&lt;td>SFT&lt;/td>&lt;td>7.37&lt;/td>&lt;td>12.5&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ SRM&lt;/td>&lt;td>7.50&lt;/td>&lt;td>14.2&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ Contrast&lt;/td>&lt;td>7.54&lt;/td>&lt;td>14.5&lt;/td>&lt;/tr>&lt;tr>&lt;td>DPO w/ LongReward&lt;/td>&lt;td>7.58&lt;/td>&lt;td>15.4&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td rowspan="2">Model&lt;/td>&lt;td rowspan="2">Preference Data&lt;/td>&lt;td colspan="2">Long Benchmark&lt;/td>&lt;td colspan="2">Short Benchmark&lt;/td>&lt;/tr>&lt;tr>&lt;td>LongBench-Chat&lt;/td>&lt;td>LongBench&lt;/td>&lt;td>MT-Bench&lt;/td>&lt;td>AlpacaEval2&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">Llama-3.1-8B&lt;/td>&lt;td>Short&lt;/td>&lt;td>70.6&lt;/td>&lt;td>54.5&lt;/td>&lt;td>7.48&lt;/td>&lt;td>15.8&lt;/td>&lt;/tr>&lt;tr>&lt;td>Long&lt;/td>&lt;td>72.6&lt;/td>&lt;td>55.6&lt;/td>&lt;td>7.24&lt;/td>&lt;td>14.2&lt;/td>&lt;/tr>&lt;tr>&lt;td>Short + Long&lt;/td>&lt;td>73.0&lt;/td>&lt;td>57.3&lt;/td>&lt;td>7.51&lt;/td>&lt;td>14.9&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">GLM-4-9B&lt;/td>&lt;td>Short&lt;/td>&lt;td>67.0&lt;/td>&lt;td>56.3&lt;/td>&lt;td>7.62&lt;/td>&lt;td>14.7&lt;/td>&lt;/tr>&lt;tr>&lt;td>Long&lt;/td>&lt;td>69.2&lt;/td>&lt;td>59.7&lt;/td>&lt;td>7.58&lt;/td>&lt;td>15.2&lt;/td>&lt;/tr>&lt;tr>&lt;td>Short + Long&lt;/td>&lt;td>70.2&lt;/td>&lt;td>58.7&lt;/td>&lt;td>7.61&lt;/td>&lt;td>15.4&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21252/tables/table_8_1/</guid><description>&lt;table id='2' style='font-size:16px'>&lt;tr>&lt;td>Method&lt;/td>&lt;td>Accuracy&lt;/td>&lt;/tr>&lt;tr>&lt;td>SRM&lt;/td>&lt;td>0.583&lt;/td>&lt;/tr>&lt;tr>&lt;td>Paired comparison&lt;/td>&lt;td>0.571&lt;/td>&lt;/tr>&lt;tr>&lt;td>LongReward&lt;/td>&lt;td>0.662&lt;/td>&lt;/tr>&lt;tr>&lt;td>w/o Helpfulness&lt;/td>&lt;td>0.631&lt;/td>&lt;/tr>&lt;tr>&lt;td>w/o Logicality&lt;/td>&lt;td>0.623&lt;/td>&lt;/tr>&lt;tr>&lt;td>w/o Faithfulness&lt;/td>&lt;td>0.578&lt;/td>&lt;/tr>&lt;tr>&lt;td>w/o Completeness&lt;/td>&lt;td>0.578&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_10_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_10_0/</guid><description>&lt;table id='4' style='font-size:14px'>&lt;tr>&lt;td>Compression method&lt;/td>&lt;td>Config&lt;/td>&lt;td>r&lt;/td>&lt;td>W-bit of EoRA&lt;/td>&lt;td>Model Size (GB)&lt;/td>&lt;td>Wikitext2&lt;/td>&lt;td>ARC-E&lt;/td>&lt;td>ARC-C&lt;/td>&lt;td>MathQA&lt;/td>&lt;/tr>&lt;tr>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>15.08&lt;/td>&lt;td>6.13&lt;/td>&lt;td>80.09&lt;/td>&lt;td>50.42&lt;/td>&lt;td>40.10&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="7">SparseGPT&lt;/td>&lt;td rowspan="7">2:4&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>9.12&lt;/td>&lt;td>12.32&lt;/td>&lt;td>62.75&lt;/td>&lt;td>30.11&lt;/td>&lt;td>26.43&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">128&lt;/td>&lt;td>16&lt;/td>&lt;td>9.77&lt;/td>&lt;td>11.07&lt;/td>&lt;td>68.22&lt;/td>&lt;td>34.64&lt;/td>&lt;td>29.91&lt;/td>&lt;/tr>&lt;tr>&lt;td>4&lt;/td>&lt;td>9.28&lt;/td>&lt;td>11.15&lt;/td>&lt;td>67.55&lt;/td>&lt;td>34.47&lt;/td>&lt;td>29.91&lt;/td>&lt;/tr>&lt;tr>&lt;td>3&lt;/td>&lt;td>9.24&lt;/td>&lt;td>11.31&lt;/td>&lt;td>68.01&lt;/td>&lt;td>34.72&lt;/td>&lt;td>29.71&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">512&lt;/td>&lt;td>16&lt;/td>&lt;td>11.70&lt;/td>&lt;td>9.04&lt;/td>&lt;td>74.49&lt;/td>&lt;td>41.89&lt;/td>&lt;td>34.17&lt;/td>&lt;/tr>&lt;tr>&lt;td>4&lt;/td>&lt;td>9.77&lt;/td>&lt;td>9.12&lt;/td>&lt;td>74.62&lt;/td>&lt;td>41.46&lt;/td>&lt;td>33.63&lt;/td>&lt;/tr>&lt;tr>&lt;td>3&lt;/td>&lt;td>9.64&lt;/td>&lt;td>9.32&lt;/td>&lt;td>72.30&lt;/td>&lt;td>40.35&lt;/td>&lt;td>32.66&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="14">GPTQ&lt;/td>&lt;td rowspan="7">W4&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>5.35&lt;/td>&lt;td>7.00&lt;/td>&lt;td>78.11&lt;/td>&lt;td>45.90&lt;/td>&lt;td>34.07&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">128&lt;/td>&lt;td>16&lt;/td>&lt;td>6.01&lt;/td>&lt;td>6.80&lt;/td>&lt;td>78.07&lt;/td>&lt;td>47.44&lt;/td>&lt;td>37.21&lt;/td>&lt;/tr>&lt;tr>&lt;td>4&lt;/td>&lt;td>5.50&lt;/td>&lt;td>6.83&lt;/td>&lt;td>78.78&lt;/td>&lt;td>47.35&lt;/td>&lt;td>36.78&lt;/td>&lt;/tr>&lt;tr>&lt;td>3&lt;/td>&lt;td>5.46&lt;/td>&lt;td>6.90&lt;/td>&lt;td>78.24&lt;/td>&lt;td>47.18&lt;/td>&lt;td>36.52&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">512&lt;/td>&lt;td>16&lt;/td>&lt;td>7.85&lt;/td>&lt;td>6.50&lt;/td>&lt;td>79.75&lt;/td>&lt;td>48.29&lt;/td>&lt;td>38.72&lt;/td>&lt;/tr>&lt;tr>&lt;td>4&lt;/td>&lt;td>6.01&lt;/td>&lt;td>6.61&lt;/td>&lt;td>78.87&lt;/td>&lt;td>48.80&lt;/td>&lt;td>38.92&lt;/td>&lt;/tr>&lt;tr>&lt;td>3&lt;/td>&lt;td>5.90&lt;/td>&lt;td>6.75&lt;/td>&lt;td>78.49&lt;/td>&lt;td>46.92&lt;/td>&lt;td>36.88&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="7">W3&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>4.63&lt;/td>&lt;td>15.64&lt;/td>&lt;td>36.78&lt;/td>&lt;td>20.90&lt;/td>&lt;td>22.37&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">128&lt;/td>&lt;td>16&lt;/td>&lt;td>5.28&lt;/td>&lt;td>10.06&lt;/td>&lt;td>60.14&lt;/td>&lt;td>31.74&lt;/td>&lt;td>29.11&lt;/td>&lt;/tr>&lt;tr>&lt;td>4&lt;/td>&lt;td>4.78&lt;/td>&lt;td>10.26&lt;/td>&lt;td>61.53&lt;/td>&lt;td>31.48&lt;/td>&lt;td>28.64&lt;/td>&lt;/tr>&lt;tr>&lt;td>3&lt;/td>&lt;td>4.74&lt;/td>&lt;td>11.68&lt;/td>&lt;td>56.52&lt;/td>&lt;td>29.18&lt;/td>&lt;td>26.70&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">512&lt;/td>&lt;td>16&lt;/td>&lt;td>7.16&lt;/td>&lt;td>8.53&lt;/td>&lt;td>71.00&lt;/td>&lt;td>38.82&lt;/td>&lt;td>31.89&lt;/td>&lt;/tr>&lt;tr>&lt;td>4&lt;/td>&lt;td>5.28&lt;/td>&lt;td>8.67&lt;/td>&lt;td>68.35&lt;/td>&lt;td>40.01&lt;/td>&lt;td>31.69&lt;/td>&lt;/tr>&lt;tr>&lt;td>3&lt;/td>&lt;td>5.18&lt;/td>&lt;td>10.19&lt;/td>&lt;td>66.70&lt;/td>&lt;td>35.40&lt;/td>&lt;td>30.45&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_6_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_6_0/</guid><description>&lt;table id='4' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>Sparsity&lt;/td>&lt;td>Compensation Method&lt;/td>&lt;td>Wikitext2&lt;/td>&lt;td>ARC-E&lt;/td>&lt;td>ARC-C&lt;/td>&lt;td>MathQA&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="8">LLaMA3-8B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>6.13&lt;/td>&lt;td>80.09&lt;/td>&lt;td>50.42&lt;/td>&lt;td>40.10&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">50%&lt;/td>&lt;td>-&lt;/td>&lt;td>8.25&lt;/td>&lt;td>72.13&lt;/td>&lt;td>39.84&lt;/td>&lt;td>32.69&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>7.99 7.98 (-0.01)&lt;/td>&lt;td>73.90 75.88 (+1.98)&lt;/td>&lt;td>41.38 43.60 (+2.22)&lt;/td>&lt;td>32.96 34.90 (+1.94)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">60%&lt;/td>&lt;td>-&lt;/td>&lt;td>12.00&lt;/td>&lt;td>63.38&lt;/td>&lt;td>30.54&lt;/td>&lt;td>27.00&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>10.93&lt;/td>&lt;td>64.64&lt;/td>&lt;td>30.97&lt;/td>&lt;td>28.40&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>10.71 (-0.22)&lt;/td>&lt;td>68.77 (+4.13)&lt;/td>&lt;td>34.98 (+4.01)&lt;/td>&lt;td>31.62 (+3.22)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">2:4&lt;/td>&lt;td>-&lt;/td>&lt;td>12.32&lt;/td>&lt;td>62.75&lt;/td>&lt;td>30.11&lt;/td>&lt;td>26.43&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>11.31 11.07 (-0.24)&lt;/td>&lt;td>64.89 68.22 (+3.33)&lt;/td>&lt;td>31.99 34.64 (+2.65)&lt;/td>&lt;td>26.49 29.91 (+3.42)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="10">LLaMA2-7B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>5.47&lt;/td>&lt;td>69.31&lt;/td>&lt;td>39.84&lt;/td>&lt;td>27.67&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">50%&lt;/td>&lt;td>-&lt;/td>&lt;td>6.48&lt;/td>&lt;td>64.14&lt;/td>&lt;td>35.92&lt;/td>&lt;td>26.90&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>6.34&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>6.31 (-0.03)&lt;/td>&lt;td>63.51 66.45 (+2.94)&lt;/td>&lt;td>36.26 38.22 (+1.96)&lt;/td>&lt;td>26.39 27.10 (+0.71)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">60%&lt;/td>&lt;td>-&lt;/td>&lt;td>8.35&lt;/td>&lt;td>59.72&lt;/td>&lt;td>30.11&lt;/td>&lt;td>25.15&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>7.81&lt;/td>&lt;td>61.61&lt;/td>&lt;td>32.42&lt;/td>&lt;td>25.09&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>7.69 (-0.12)&lt;/td>&lt;td>62.66 (+1.05)&lt;/td>&lt;td>34.12 (+1.70)&lt;/td>&lt;td>25.99 (+0.9)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">2:4&lt;/td>&lt;td>-&lt;/td>&lt;td>8.77&lt;/td>&lt;td>60.47&lt;/td>&lt;td>30.11&lt;/td>&lt;td>24.65&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>8.15&lt;/td>&lt;td>60.98&lt;/td>&lt;td>30.54&lt;/td>&lt;td>24.89&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>7.97 (-0.18)&lt;/td>&lt;td>63.42 (+2.44)&lt;/td>&lt;td>32.67 (+2.13)&lt;/td>&lt;td>25.59 (+0.70)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="10">LLaMA2-13B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>4.88&lt;/td>&lt;td>73.23&lt;/td>&lt;td>45.56&lt;/td>&lt;td>29.91&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">50%&lt;/td>&lt;td>-&lt;/td>&lt;td>5.65&lt;/td>&lt;td>68.81&lt;/td>&lt;td>39.24&lt;/td>&lt;td>27.30&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>5.54&lt;/td>&lt;td>69.69&lt;/td>&lt;td>39.59&lt;/td>&lt;td>27.63&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>5.54&lt;/td>&lt;td>71.63 (+1.94)&lt;/td>&lt;td>41.97 (+2.38)&lt;/td>&lt;td>28.27 (+0.64)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">60%&lt;/td>&lt;td>-&lt;/td>&lt;td>6.93&lt;/td>&lt;td>63.21&lt;/td>&lt;td>33.70&lt;/td>&lt;td>26.86&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>6.59&lt;/td>&lt;td>65.44&lt;/td>&lt;td>34.12&lt;/td>&lt;td>26.06&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>6.52 (-0.07)&lt;/td>&lt;td>67.25 (+1.81)&lt;/td>&lt;td>37.71 (+3.59)&lt;/td>&lt;td>27.16 (+1.10)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">2:4&lt;/td>&lt;td>-&lt;/td>&lt;td>7.10&lt;/td>&lt;td>66.32&lt;/td>&lt;td>34.30&lt;/td>&lt;td>25.92&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>6.82&lt;/td>&lt;td>66.28&lt;/td>&lt;td>33.61&lt;/td>&lt;td>25.12&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>6.75 (-0.07)&lt;/td>&lt;td>68.47 (+2.19)&lt;/td>&lt;td>37.54 (+3.93)&lt;/td>&lt;td>27.53 (+2.41)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_7_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_7_0/</guid><description>&lt;table id='2' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>W-bit&lt;/td>&lt;td>Compensation Method&lt;/td>&lt;td>Wikitext2&lt;/td>&lt;td>ARC-E&lt;/td>&lt;td>ARC-C&lt;/td>&lt;td>MathQA&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">LLaMA3-8B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>6.13&lt;/td>&lt;td>80.09&lt;/td>&lt;td>50.42&lt;/td>&lt;td>40.10&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">W4&lt;/td>&lt;td>-&lt;/td>&lt;td>7.00&lt;/td>&lt;td>78.11&lt;/td>&lt;td>45.90&lt;/td>&lt;td>34.07&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>6.80 6.80&lt;/td>&lt;td>77.48 78.07 (+0.59)&lt;/td>&lt;td>45.24 47.44 (+2.20)&lt;/td>&lt;td>36.51 37.21 (+0.7)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">W3&lt;/td>&lt;td>-&lt;/td>&lt;td>15.64&lt;/td>&lt;td>36.78&lt;/td>&lt;td>20.90&lt;/td>&lt;td>22.37&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>10.24 10.06 (-0.18)&lt;/td>&lt;td>57.19 60.14 (+2.95)&lt;/td>&lt;td>30.02 31.74 (+1.72)&lt;/td>&lt;td>26.43 29.11 (+2.68)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">LLaMA2-7B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>5.47&lt;/td>&lt;td>69.31&lt;/td>&lt;td>39.84&lt;/td>&lt;td>27.67&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">W4&lt;/td>&lt;td>-&lt;/td>&lt;td>5.75&lt;/td>&lt;td>67.67&lt;/td>&lt;td>38.13&lt;/td>&lt;td>26.73&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>5.68 5.68&lt;/td>&lt;td>66.96 68.18 (+1.22)&lt;/td>&lt;td>37.62 38.05 (+0.43)&lt;/td>&lt;td>27.06 27.13 (+0.07)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">W3&lt;/td>&lt;td>-&lt;/td>&lt;td>7.76&lt;/td>&lt;td>58.41&lt;/td>&lt;td>31.65&lt;/td>&lt;td>23.50&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>6.84 6.84&lt;/td>&lt;td>63.97 65.69 (+1.72)&lt;/td>&lt;td>34.47 35.83 (+1.36)&lt;/td>&lt;td>23.90 25.79 (+1.89)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="7">LLaMA2-13B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>4.88&lt;/td>&lt;td>73.23&lt;/td>&lt;td>45.56&lt;/td>&lt;td>29.91&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">W4&lt;/td>&lt;td>-&lt;/td>&lt;td>5.06&lt;/td>&lt;td>71.33&lt;/td>&lt;td>44.28&lt;/td>&lt;td>29.10&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>5.03 5.03&lt;/td>&lt;td>71.88 71.80&lt;/td>&lt;td>44.19 44.53 (+0.34)&lt;/td>&lt;td>28.97 28.90&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">W3&lt;/td>&lt;td>-&lt;/td>&lt;td>5.99&lt;/td>&lt;td>63.04&lt;/td>&lt;td>37.28&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;td>26.26&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>5.76 5.75 (-0.01)&lt;/td>&lt;td>64.64 65.86 (+1.22)&lt;/td>&lt;td>37.54 39.50 (+1.96)&lt;/td>&lt;td>26.83 27.20 (+0.37)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_7_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_7_1/</guid><description>&lt;table id='6' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>Sparsity&lt;/td>&lt;td>W-bit&lt;/td>&lt;td>Compensation Method&lt;/td>&lt;td>Wikitext2&lt;/td>&lt;td>ARC-E&lt;/td>&lt;td>ARC-C&lt;/td>&lt;td>MathQA&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">LLaMA3-8B&lt;/td>&lt;td colspan="2">Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>6.13&lt;/td>&lt;td>80.09&lt;/td>&lt;td>50.42&lt;/td>&lt;td>40.10&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">2:4&lt;/td>&lt;td rowspan="2">W4&lt;/td>&lt;td>-&lt;/td>&lt;td>86.15&lt;/td>&lt;td>34.59&lt;/td>&lt;td>18.34&lt;/td>&lt;td>19.89&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>12.84 12.60 (-0.24)&lt;/td>&lt;td>62.12 65.9 (+3.78)&lt;/td>&lt;td>29.35 31.22 (+1.87)&lt;/td>&lt;td>26.86 29.58 (+2.72)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">LLaMA2-7B&lt;/td>&lt;td colspan="2">Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>5.47&lt;/td>&lt;td>69.31&lt;/td>&lt;td>39.84&lt;/td>&lt;td>27.67&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">2:4&lt;/td>&lt;td rowspan="2">W4&lt;/td>&lt;td>-&lt;/td>&lt;td>9.37&lt;/td>&lt;td>58.41&lt;/td>&lt;td>29.43&lt;/td>&lt;td>23.88&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>8.42 8.24 (-0.18)&lt;/td>&lt;td>59.09 62.33 (+3.24)&lt;/td>&lt;td>29.94 31.14 (+1.20)&lt;/td>&lt;td>24.42 25.39 (+0.97)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">LLaMA2-13B&lt;/td>&lt;td colspan="2">Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>4.88&lt;/td>&lt;td>73.23&lt;/td>&lt;td>45.56&lt;/td>&lt;td>29.91&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="2">2:4&lt;/td>&lt;td rowspan="2">W4&lt;/td>&lt;td>-&lt;/td>&lt;td>7.27&lt;/td>&lt;td>64.09&lt;/td>&lt;td>33.10&lt;/td>&lt;td>24.75&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD EoRA&lt;/td>&lt;td>6.98 6.89 (-0.09)&lt;/td>&lt;td>66.41 66.58 (+0.17)&lt;/td>&lt;td>33.27 35.06 (+1.79)&lt;/td>&lt;td>25.29 27.06 (+1.77)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_8_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_8_0/</guid><description>&lt;table id='4' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>Sparsity&lt;/td>&lt;td>r&lt;/td>&lt;td>Compensation Method&lt;/td>&lt;td>Wikitext2&lt;/td>&lt;td>ARC-E&lt;/td>&lt;td>ARC-C&lt;/td>&lt;td>MathQA&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="6">LLaMA3-8B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>6.13&lt;/td>&lt;td>80.09&lt;/td>&lt;td>50.42&lt;/td>&lt;td>40.10&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">2:4&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>12.32&lt;/td>&lt;td>62.75&lt;/td>&lt;td>30.11&lt;/td>&lt;td>26.43&lt;/td>&lt;/tr>&lt;tr>&lt;td>64&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>11.76 11.67 (-0.10)&lt;/td>&lt;td>62.83 65.86 (+3.03)&lt;/td>&lt;td>30.97 33.1 (+2.13)&lt;/td>&lt;td>26.39 28.57 (+2.18)&lt;/td>&lt;/tr>&lt;tr>&lt;td>128&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>11.31 11.07 (-0.24)&lt;/td>&lt;td>64.89 68.22 (+3.33)&lt;/td>&lt;td>31.99 34.64 (+2.65)&lt;/td>&lt;td>26.49 29.91 (+3.42)&lt;/td>&lt;/tr>&lt;tr>&lt;td>256&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>10.54 10.25 (-0.30)&lt;/td>&lt;td>68.01 71.00 (+2.99)&lt;/td>&lt;td>34.55 37.96 (+3.41)&lt;/td>&lt;td>28.74 31.59 (+2.85)&lt;/td>&lt;/tr>&lt;tr>&lt;td>512&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>9.38 9.04 (-0.34)&lt;/td>&lt;td>71.46 74.49 (+3.03)&lt;/td>&lt;td>38.73 41.89 (+3.16)&lt;/td>&lt;td>30.38 34.17 (+3.79)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="6">LLaMA2-7B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>5.47&lt;/td>&lt;td>69.31&lt;/td>&lt;td>39.84&lt;/td>&lt;td>27.67&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">2:4&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>8.77&lt;/td>&lt;td>60.47&lt;/td>&lt;td>30.11&lt;/td>&lt;td>24.65&lt;/td>&lt;/tr>&lt;tr>&lt;td>64&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>8.37 8.29 (-0.08)&lt;/td>&lt;td>60.18 62.58 (+2.40)&lt;/td>&lt;td>30.2 32.16 (+1.96)&lt;/td>&lt;td>24.48 25.62 (+1.14)&lt;/td>&lt;/tr>&lt;tr>&lt;td>128&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>8.15 7.97 (-0.18)&lt;/td>&lt;td>60.98 63.42 (+2.44)&lt;/td>&lt;td>30.54 32.67 (+2.13)&lt;/td>&lt;td>24.89 25.59 (+0.70)&lt;/td>&lt;/tr>&lt;tr>&lt;td>256&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>7.74 7.45 (-0.29)&lt;/td>&lt;td>62.71 65.44 (+2.73)&lt;/td>&lt;td>31.99 34.47 (+2.48)&lt;/td>&lt;td>25.19 26.06 (+0.87)&lt;/td>&lt;/tr>&lt;tr>&lt;td>512&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>7.09 6.80 (-0.29)&lt;/td>&lt;td>65.44 66.91 (+1.47)&lt;/td>&lt;td>34.72 36.77 (+2.05)&lt;/td>&lt;td>24.38 25.96 (+1.58)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="6">LLaMA2-13B&lt;/td>&lt;td>Uncompressed&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>4.88&lt;/td>&lt;td>73.23&lt;/td>&lt;td>45.56&lt;/td>&lt;td>29.91&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="5">2:4&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>7.10&lt;/td>&lt;td>66.32&lt;/td>&lt;td>34.30&lt;/td>&lt;td>25.92&lt;/td>&lt;/tr>&lt;tr>&lt;td>64&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>6.95 6.92 (-0.03)&lt;/td>&lt;td>66.24 67.50 (+1.26)&lt;/td>&lt;td>33.95 36.00 (+2.05)&lt;/td>&lt;td>25.56 26.80 (+1.24)&lt;/td>&lt;/tr>&lt;tr>&lt;td>128&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>6.82 6.75 (-0.07)&lt;/td>&lt;td>66.28 68.47 (+2.19)&lt;/td>&lt;td>33.61 37.54 (+3.93)&lt;/td>&lt;td>25.12 27.53 (+2.41)&lt;/td>&lt;/tr>&lt;tr>&lt;td>256&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>6.57 6.46 (-0.11)&lt;/td>&lt;td>66.32 70.07 (+3.75)&lt;/td>&lt;td>35.06 38.73 (+3.67)&lt;/td>&lt;td>26.06 27.77 (+1.71)&lt;/td>&lt;/tr>&lt;tr>&lt;td>512&lt;/td>&lt;td>SVD EoRA&lt;/td>&lt;td>6.20 6.07 (-0.13)&lt;/td>&lt;td>68.72 71.54 (+2.82)&lt;/td>&lt;td>36.51 40.61 (+4.10)&lt;/td>&lt;td>26.39 29.17 (+2.78)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_9_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_9_0/</guid><description>&lt;table id='3' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>Compression Method&lt;/td>&lt;td>Compression Setting&lt;/td>&lt;td>LoRA initialization&lt;/td>&lt;td>ARC-E&lt;/td>&lt;td>ARC-C&lt;/td>&lt;td>MathQA&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="14">LLaMA3-8B&lt;/td>&lt;td rowspan="2">Uncompressed&lt;/td>&lt;td rowspan="2">&lt;/td>&lt;td>w/o finetuning&lt;/td>&lt;td>80.09&lt;/td>&lt;td>50.42&lt;/td>&lt;td>40.10&lt;/td>&lt;/tr>&lt;tr>&lt;td>Standard&lt;/td>&lt;td>84.55&lt;/td>&lt;td>56.39&lt;/td>&lt;td>53.56&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="4">SparseGPT&lt;/td>&lt;td rowspan="4">2:4&lt;/td>&lt;td>w/o finetuning&lt;/td>&lt;td>62.75&lt;/td>&lt;td>30.11&lt;/td>&lt;td>26.43&lt;/td>&lt;/tr>&lt;tr>&lt;td>Standard&lt;/td>&lt;td>73.82&lt;/td>&lt;td>41.30&lt;/td>&lt;td>45.42&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>74.45&lt;/td>&lt;td>43.68&lt;/td>&lt;td>48.77&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>76.01 (+1.56)&lt;/td>&lt;td>48.54 (+4.86)&lt;/td>&lt;td>54.67 (+5.90)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="4">GPTQ&lt;/td>&lt;td rowspan="4">W4&lt;/td>&lt;td>w/o finetuning&lt;/td>&lt;td>78.11&lt;/td>&lt;td>45.90&lt;/td>&lt;td>34.07&lt;/td>&lt;/tr>&lt;tr>&lt;td>Standard&lt;/td>&lt;td>81.69&lt;/td>&lt;td>54.09&lt;/td>&lt;td>51.42&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>82.49&lt;/td>&lt;td>54.52&lt;/td>&lt;td>53.96&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>83.04 (+0.55)&lt;/td>&lt;td>55.46 (+0.94)&lt;/td>&lt;td>56.04 (+2.08)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="4">GPTQ&lt;/td>&lt;td rowspan="4">W3&lt;/td>&lt;td>w/o finetuning&lt;/td>&lt;td>36.78&lt;/td>&lt;td>20.90&lt;/td>&lt;td>22.37&lt;/td>&lt;/tr>&lt;tr>&lt;td>Standard&lt;/td>&lt;td>57.87&lt;/td>&lt;td>30.29&lt;/td>&lt;td>34.10&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>75.54&lt;/td>&lt;td>44.70&lt;/td>&lt;td>48.17&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>76.93 (+1.39)&lt;/td>&lt;td>47.44 (+2.74)&lt;/td>&lt;td>53.90 (+5.73)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_9_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21271/tables/table_9_1/</guid><description>&lt;table id='7' style='font-size:14px'>&lt;tr>&lt;td>Model&lt;/td>&lt;td>Dataset Ratio&lt;/td>&lt;td>LoRA initialization&lt;/td>&lt;td>ARC-E&lt;/td>&lt;td>ARC-C&lt;/td>&lt;td>MathQA&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="10">LLaMA3-8B&lt;/td>&lt;td>-&lt;/td>&lt;td>-&lt;/td>&lt;td>80.09&lt;/td>&lt;td>50.42&lt;/td>&lt;td>40.10&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">100%&lt;/td>&lt;td>Standard&lt;/td>&lt;td>73.82&lt;/td>&lt;td>41.30&lt;/td>&lt;td>45.42&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>74.45&lt;/td>&lt;td>43.68&lt;/td>&lt;td>48.77&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>76.01 (+1.56)&lt;/td>&lt;td>48.54 (+4.86)&lt;/td>&lt;td>54.67 (+5.90)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">50%&lt;/td>&lt;td>Standard&lt;/td>&lt;td>71.67&lt;/td>&lt;td>38.56&lt;/td>&lt;td>40.23&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>72.18&lt;/td>&lt;td>41.46&lt;/td>&lt;td>42.51&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>75.42 (+3.24)&lt;/td>&lt;td>46.41 (+4.95)&lt;/td>&lt;td>48.91 (+6.40)&lt;/td>&lt;/tr>&lt;tr>&lt;td rowspan="3">30%&lt;/td>&lt;td>Standard&lt;/td>&lt;td>69.82&lt;/td>&lt;td>36.77&lt;/td>&lt;td>36.71&lt;/td>&lt;/tr>&lt;tr>&lt;td>SVD&lt;/td>&lt;td>72.01&lt;/td>&lt;td>39.76&lt;/td>&lt;td>40.60&lt;/td>&lt;/tr>&lt;tr>&lt;td>EoRA&lt;/td>&lt;td>73.86 (+1.85)&lt;/td>&lt;td>43.85 (+4.09)&lt;/td>&lt;td>44.79 (+4.19)&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_11_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_11_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>[10]&lt;/td>&lt;td>Wanhua Li, Jiwen Lu, Jianjiang Feng, Chunjing Xu, Jie Zhou, and Qi Tian. Bridgenet: A continuity-aware probabilistic network for age estimation. In CVPR, pages 1145-1154, 2019.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[11]&lt;/td>&lt;td>Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In ICCV, pages 7262-7272, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[12]&lt;/td>&lt;td>Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213-229. Springer, 2020.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[13]&lt;/td>&lt;td>Junnan Li, Yongkang Wong, Qi Zhao, and Mohan s Kankanhalli. Dual-glance model for deciphering social relationships. In ICCV, pages 2650-2659, 2017.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[14]&lt;/td>&lt;td>Yunfei Guo, Fei Yin, Wei Feng, Xudong Yan, Tao Xue, Shuqi Mei, and Cheng-Lin Liu. Social relation reasoning based on triangular constraints. In AAAI, pages 737-745, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[15]&lt;/td>&lt;td>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael s Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[16]&lt;/td>&lt;td>Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[17]&lt;/td>&lt;td>Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[18]&lt;/td>&lt;td>Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In CVPR, pages 20051-20060, 2024.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[19]&lt;/td>&lt;td>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748-8763. PMLR, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[20]&lt;/td>&lt;td>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc v Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 35:24824-24837, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[21]&lt;/td>&lt;td>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[22]&lt;/td>&lt;td>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc v Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[23]&lt;/td>&lt;td>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[24]&lt;/td>&lt;td>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[25]&lt;/td>&lt;td>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In EMNLP, pages 4222-4235, 2020.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[26]&lt;/td>&lt;td>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684-10695, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[27]&lt;/td>&lt;td>OpenAI. Gpt-4 technical report, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[28]&lt;/td>&lt;td>Wanhua Li, Xiaoke Huang, Zheng Zhu, Yansong Tang, Xiu Li, Jie Zhou, and Jiwen Lu. Ordinalclip: Learning rank prompts for language-guided ordinal regression. NeurIPS, 35:35313-35325, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[29]&lt;/td>&lt;td>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_13_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_13_0/</guid><description>&lt;table id='0' style='font-size:14px'>&lt;tr>&lt;td>[50]&lt;/td>&lt;td>Sheldon Cohen. Social relationships and health. American psychologist, 59(8):676, 2004.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[51]&lt;/td>&lt;td>Hope R Conte and Robert Plutchik. A circumplex model for interpersonal personality traits. Journal of personality and social psychology, 40(4):701, 1981.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[52]&lt;/td>&lt;td>Daphne Blunt Bugental. Acquisition of the algorithms of social life: a domain-based approach. Psycholog- ical bulletin, 126(2):187, 2000.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[53]&lt;/td>&lt;td>Alan P Fiske. The four elementary forms of sociality: framework for a unified theory of social relations. Psychological review, 99(4):689, 1992.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[54]&lt;/td>&lt;td>Arushi Goel, Keng Teck Ma, and Cheston Tan. An end-to-end network for generating social relationship graphs. In CVPR, pages 11186-11195, 2019.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[55]&lt;/td>&lt;td>Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C Schmidt. Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. arXiv preprint arXiv:2303.07839, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[56]&lt;/td>&lt;td>Shima Imani, Liang Du, and Harsh Shrivastava. Mathprompter: Mathematical reasoning using large language models. arXiv preprint arXiv:2303.05398, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[57]&lt;/td>&lt;td>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In NAACL, pages 2080-2094, 2021.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[58]&lt;/td>&lt;td>Timo Schick, Jane Dwivedi- Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[59]&lt;/td>&lt;td>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[60]&lt;/td>&lt;td>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877-1901, 2020.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[61]&lt;/td>&lt;td>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, pages 11048-11064, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[62]&lt;/td>&lt;td>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In NAACL, pages 2655-2671, 2022.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[63]&lt;/td>&lt;td>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In ICLR, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[64]&lt;/td>&lt;td>Reid Pryzant, Dan Iter, Jerry Li, Yin Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with "gradient descent" and beam search. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, EMNLP, pages 7957-7968, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[65]&lt;/td>&lt;td>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[66]&lt;/td>&lt;td>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[67]&lt;/td>&lt;td>Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.&lt;/td>&lt;/tr>&lt;tr>&lt;td>[68]&lt;/td>&lt;td>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_6_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_6_0/</guid><description>&lt;table id='1' style='font-size:14px'>&lt;tr>&lt;td>Input:&lt;/td>&lt;td>Initial segments W1:M, training dataset T, iteration number N&lt;/td>&lt;/tr>&lt;tr>&lt;td>Build the candidate set Wm for each segment Wm repeat N times&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>batch from&lt;/td>&lt;td>sample&lt;/td>&lt;/tr>&lt;tr>&lt;td>&lt;/td>&lt;td>Randomly a of data D T for m = 1, · · · , M do Um := Top-k(- EZED ▽ L(w1:M; z)) hwm Compute top-k promising segment&lt;/td>&lt;/tr>&lt;tr>&lt;td>&lt;/td>&lt;td>▷ substitutions for b = 0, 1, · · · , K * M - 1 do&lt;/td>&lt;/tr>&lt;tr>&lt;td>&lt;/td>&lt;td>▷ Initialization&lt;/td>&lt;/tr>&lt;tr>&lt;td>(b) w : = W1:M&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>1:M w⌀ := Ui([b/M]), where 2 = (b mod M) + 1&lt;/td>&lt;td>replacement segment&lt;/td>&lt;/tr>&lt;tr>&lt;td>Select&lt;/td>&lt;td>▷ one replacement&lt;/td>&lt;/tr>&lt;tr>&lt;td>W1:M := wi⌀M, where 6* = argmin⌀ EZED L(W1:M, z) ▷&lt;/td>&lt;td>best&lt;/td>&lt;/tr>&lt;tr>&lt;td>Compute Output: Optimized segments W1:M&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_7_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_7_0/</guid><description>&lt;br>&lt;table id='2' style='font-size:16px'>&lt;tr>&lt;td>Methods&lt;/td>&lt;td>ZS&lt;/td>&lt;td>Acc (%)&lt;/td>&lt;td>&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>All attributes + SVM 1&lt;/td>&lt;td>X&lt;/td>&lt;td>57.2&lt;/td>&lt;td>Methods&lt;/td>&lt;td>Acc (%)&lt;/td>&lt;/tr>&lt;tr>&lt;td>Pair CNN 13&lt;/td>&lt;td>X&lt;/td>&lt;td>58.0&lt;/td>&lt;td>SocialGPT&lt;/td>&lt;td>61.58&lt;/td>&lt;/tr>&lt;tr>&lt;td>Dual-Glance 13&lt;/td>&lt;td>X&lt;/td>&lt;td>59.6&lt;/td>&lt;td>- Dense Captions&lt;/td>&lt;td>52.63&lt;/td>&lt;/tr>&lt;tr>&lt;td>SRG-GN 54&lt;/td>&lt;td>X&lt;/td>&lt;td>53.6&lt;/td>&lt;td>- Task-oriented Captions&lt;/td>&lt;td>59.89&lt;/td>&lt;/tr>&lt;tr>&lt;td>GRM 6&lt;/td>&lt;td>X&lt;/td>&lt;td>62.3&lt;/td>&lt;td>- Symbol → Object Coordinate&lt;/td>&lt;td>57.68&lt;/td>&lt;/tr>&lt;tr>&lt;td>MGR 2&lt;/td>&lt;td>X&lt;/td>&lt;td>64.4&lt;/td>&lt;td>- Symbol → Object Caption - Social Story&lt;/td>&lt;td>59.83 45.31&lt;/td>&lt;/tr>&lt;tr>&lt;td>GR2N 3&lt;/td>&lt;td>X&lt;/td>&lt;td>64.3&lt;/td>&lt;td>Segment {System}&lt;/td>&lt;td>&lt;/td>&lt;/tr>&lt;tr>&lt;td>TRGAT 14&lt;/td>&lt;td>X&lt;/td>&lt;td>65.3&lt;/td>&lt;td>- SocialPrompt - SocialPrompt Segment {Expectation }&lt;/td>&lt;td>60.23 59.19&lt;/td>&lt;/tr>&lt;tr>&lt;td>SocialGPT (w/ GPT-3.5)&lt;/td>&lt;td>&lt;/td>&lt;td>64.1&lt;/td>&lt;td>- SocialPrompt Segment {Context}&lt;/td>&lt;td>61.18&lt;/td>&lt;/tr>&lt;tr>&lt;td>SocialGPT (w/ Vicuna-13B)&lt;/td>&lt;td>&lt;/td>&lt;td>66.7&lt;/td>&lt;td>- SocialPrompt Segment {Guidance}&lt;/td>&lt;td>43.56&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_8_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_8_0/</guid><description>&lt;br>&lt;table id='1' style='font-size:20px'>&lt;tr>&lt;td>Methods&lt;/td>&lt;td>ZS&lt;/td>&lt;td>Acc (%)&lt;/td>&lt;/tr>&lt;tr>&lt;td>Pair CNN 13&lt;/td>&lt;td>X&lt;/td>&lt;td>46.30&lt;/td>&lt;/tr>&lt;tr>&lt;td>GRM&lt;/td>&lt;td>X&lt;/td>&lt;td>64.18&lt;/td>&lt;/tr>&lt;tr>&lt;td>GR2N 3&lt;/td>&lt;td>X&lt;/td>&lt;td>64.70&lt;/td>&lt;/tr>&lt;tr>&lt;td>SocialGPT (w/ GPT-3.5)&lt;/td>&lt;td>&lt;/td>&lt;td>53.43&lt;/td>&lt;/tr>&lt;tr>&lt;td>SocialGPT (w/ Vicuna-13B)&lt;/td>&lt;td>&lt;/td>&lt;td>65.12&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_8_1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_8_1/</guid><description>&lt;br>&lt;table id='3' style='font-size:18px'>&lt;tr>&lt;td>Methods&lt;/td>&lt;td>Acc (%)&lt;/td>&lt;/tr>&lt;tr>&lt;td>BLIP-2 41&lt;/td>&lt;td>35.84&lt;/td>&lt;/tr>&lt;tr>&lt;td>LLaVA 68&lt;/td>&lt;td>45.12&lt;/td>&lt;/tr>&lt;tr>&lt;td>GPT-4V 55&lt;/td>&lt;td>59.67&lt;/td>&lt;/tr>&lt;tr>&lt;td>SocialGPT&lt;/td>&lt;td>66.70&lt;/td>&lt;/tr>&lt;/table></description></item><item><title/><link>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_9_0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/paper-reviews/2410.21411/tables/table_9_0/</guid><description>&lt;br>&lt;table id='1' style='font-size:16px'>&lt;tr>&lt;td rowspan="2">Model&lt;/td>&lt;td colspan="3">PIPA&lt;/td>&lt;td colspan="3">PISC&lt;/td>&lt;/tr>&lt;tr>&lt;td>SocialGPT&lt;/td>&lt;td>+ GSPO&lt;/td>&lt;td>△&lt;/td>&lt;td>SocialGPT&lt;/td>&lt;td>+ GSPO&lt;/td>&lt;td>△&lt;/td>&lt;/tr>&lt;tr>&lt;td>Vicuna-7B&lt;/td>&lt;td>61.58&lt;/td>&lt;td>62.99&lt;/td>&lt;td>+1.41&lt;/td>&lt;td>45.13&lt;/td>&lt;td>49.79&lt;/td>&lt;td>+4.66&lt;/td>&lt;/tr>&lt;tr>&lt;td>Vicuna-13B&lt;/td>&lt;td>66.70&lt;/td>&lt;td>69.23&lt;/td>&lt;td>+2.53&lt;/td>&lt;td>65.12&lt;/td>&lt;td>66.19&lt;/td>&lt;td>+1.07&lt;/td>&lt;/tr>&lt;tr>&lt;td>Llama2-7B&lt;/td>&lt;td>31.91&lt;/td>&lt;td>34.07&lt;/td>&lt;td>+2.16&lt;/td>&lt;td>36.71&lt;/td>&lt;td>38.04&lt;/td>&lt;td>+1.33&lt;/td>&lt;/tr>&lt;tr>&lt;td>Llama2-13B&lt;/td>&lt;td>37.86&lt;/td>&lt;td>41.27&lt;/td>&lt;td>+3.41&lt;/td>&lt;td>42.74&lt;/td>&lt;td>48.39&lt;/td>&lt;td>+5.65&lt;/td>&lt;/tr>&lt;/table></description></item></channel></rss>