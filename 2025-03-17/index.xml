<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025-03-17s on HF Daily Paper Reviews by AI</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/</link><description>Recent content in 2025-03-17s on HF Daily Paper Reviews by AI</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>Â© 2025 Hugging Face Daily Papers</copyright><lastBuildDate>Fri, 14 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/index.xml" rel="self" type="application/rss+xml"/><item><title>Adversarial Data Collection: Human-Collaborative Perturbations for Efficient and Robust Robotic Imitation Learning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11646/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11646/</guid><description>ADC: Human-robot collaboration revolutionizes data collection, slashing data needs and boosting robot learning!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11646/cover.png"/></item><item><title>API Agents vs. GUI Agents: Divergence and Convergence</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11069/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11069/</guid><description>API vs. GUI Agents: Understanding the divergence and convergence in LLM-based automation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11069/cover.png"/></item><item><title>ReCamMaster: Camera-Controlled Generative Rendering from A Single Video</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11647/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11647/</guid><description>ReCamMaster: Re-shoots videos via generative rendering, controlling camera movement from a single source, for novel perspectives and enhanced video creation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11647/cover.png"/></item><item><title>VGGT: Visual Geometry Grounded Transformer</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11651/</link><pubDate>Fri, 14 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11651/</guid><description>VGGT: a fast, end-to-end transformer that infers complete 3D scene attributes from multiple views, outperforming optimization-based methods.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.11651/cover.png"/></item><item><title>ETCH: Generalizing Body Fitting to Clothed Humans via Equivariant Tightness</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10624/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10624/</guid><description>ETCH: Equivariantly fitting bodies to clothed humans through tightness for better pose and shape accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10624/cover.png"/></item><item><title>FlowTok: Flowing Seamlessly Across Text and Image Tokens</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10772/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10772/</guid><description>FlowTok: Seamlessly flows across text and image tokens!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10772/cover.png"/></item><item><title>From TOWER to SPIRE: Adding the Speech Modality to a Text-Only LLM</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10620/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10620/</guid><description>SPIRE: Adds speech to text-only LLMs, maintaining text performance via discretized speech and continued pre-training.</description></item><item><title>Kolmogorov-Arnold Attention: Is Learnable Attention Better For Vision Transformers?</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10632/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10632/</guid><description>KArAt: Can Learnable Attention Beat Standard Attention in Vision Transformers?</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10632/cover.png"/></item><item><title>Large-scale Pre-training for Grounded Video Caption Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10781/</link><pubDate>Thu, 13 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10781/</guid><description>GROVE: Pre-training on large-scale data for grounded video caption generation.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10781/cover.png"/></item><item><title>Cockatiel: Ensembling Synthetic and Human Preferenced Training for Detailed Video Caption</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.09279/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.09279/</guid><description>Cockatiel: Ensembling synthetic &amp;amp; human-preferred training boosts detailed video captioning, setting new SOTA on VDCSCORE.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.09279/cover.png"/></item><item><title>Group-robust Machine Unlearning</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.09330/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.09330/</guid><description>Group-robust machine unlearning via MIU reduces perf. degradation in dominant groups after unlearning, preserving model robustness without compromising accuracy.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.09330/cover.png"/></item><item><title>Neighboring Autoregressive Modeling for Efficient Visual Generation</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10696/</link><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10696/</guid><description>NAR: Neighboring Autoregressive Modeling for efficient visual generation by locality-preserved, parallel decoding.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10696/cover.png"/></item><item><title>MaRI: Material Retrieval Integration across Domains</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.08111/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.08111/</guid><description>MaRI: Accurately retrieves textures from images by bridging the gap between visual representations and material properties across diverse domains.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.08111/cover.png"/></item><item><title>Open-World Skill Discovery from Unsegmented Demonstrations</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10684/</link><pubDate>Tue, 11 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10684/</guid><description>SBD: Self-supervised skill discovery from unsegmented videos!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.10684/cover.png"/></item><item><title>PLADIS: Pushing the Limits of Attention in Diffusion Models at Inference Time by Leveraging Sparsity</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.07677/</link><pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.07677/</guid><description>PLADIS: Sparsity boosts attention for diffusion models, enhancing text-to-image generation at inference time!</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.07677/cover.png"/></item><item><title>ARMOR v0.1: Empowering Autoregressive Multimodal Understanding Model with Interleaved Multimodal Generation via Asymmetric Synergy</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06542/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06542/</guid><description>ARMOR: Empowers MLLMs with interleaved multimodal generation via asymmetric synergy, using limited resources.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06542/cover.png"/></item><item><title>Learning Few-Step Diffusion Models by Trajectory Distribution Matching</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06674/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06674/</guid><description>TDM: a new diffusion distillation paradigm unifying trajectory distillation and distribution matching, surpassing teachers in a data-free manner with state-of-the-art performance and low training cost&amp;hellip;</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06674/cover.png"/></item><item><title>ProJudge: A Multi-Modal Multi-Discipline Benchmark and Instruction-Tuning Dataset for MLLM-based Process Judges</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06553/</link><pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.06553/</guid><description>ProJudge: MLLM judges&amp;rsquo; benchmark for sci-reasoning &amp;amp; instruction-tuning data to boost performance!</description></item><item><title>GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving</title><link>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.05689/</link><pubDate>Fri, 07 Mar 2025 00:00:00 +0000</pubDate><guid>https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.05689/</guid><description>GoalFlow: A novel approach to enhance multimodal trajectory generation for autonomous driving using goal-driven flow matching.</description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://deep-diver.github.io/ai-paper-reviewer/2025-03-17/2503.05689/cover.png"/></item></channel></rss>