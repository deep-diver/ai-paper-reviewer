{"references": [{"fullname_first_author": "Liu", "paper_title": "KAN: Kolmogorov-Arnold Networks", "publication_date": "2025-01-01", "reason": "This paper introduces KANs (Kolmogorov-Arnold Networks), which is the main focus of this paper and the architecture being explored and modified for attention mechanisms."}, {"fullname_first_author": "Vaswani", "paper_title": "Attention Is All You Need", "publication_date": "2017-01-01", "reason": "This paper introduced the Transformer architecture, upon which Vision Transformers (ViTs) are based, making it a fundamental reference for any work involving ViTs and attention mechanisms."}, {"fullname_first_author": "Dosovitskiy", "paper_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "publication_date": "2020-01-01", "reason": "This paper introduces Vision Transformers (ViTs) for image recognition, the primary architecture being studied and modified in the current paper, making it essential for baseline comparison."}, {"fullname_first_author": "Yang", "paper_title": "Kolmogorov-Arnold Transformer", "publication_date": "2025-01-01", "reason": "This paper is one of the most related works, as it directly explores KANs in Transformers, similarly to the current paper's investigation of KANs in attention mechanisms."}, {"fullname_first_author": "Krizhevsky", "paper_title": "Learning Multiple Layers of Features from Tiny Images", "publication_date": "2009-01-01", "reason": "This paper introduces the CIFAR-10 and CIFAR-100 datasets, which are used extensively in this paper for benchmarking performance, making it crucial for the experimental validation of the proposed methods."}]}