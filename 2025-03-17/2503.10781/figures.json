[{"figure_path": "https://arxiv.org/html/2503.10781/x2.png", "caption": "Figure 1: Output of our GROunded Video caption gEneration (GROVE) model on an instructional video. The model outputs a video-level caption (bottom) with key noun phrases in the caption coloured and localised (grounded) in the video by temporally consistent bounding boxes (top). Note how the objects are consistently annotated (with the same color) despite changes in scale and viewpoint and how the person is marked as occluded (orange box not present) in frames 1 and 4 when the person (or their hand) is not visible.", "description": "This figure showcases the GROVE model's output for an instructional video.  The model generates a video-level caption (at the bottom) and highlights key objects mentioned in the caption with color-coded bounding boxes (at the top).  These boxes track the objects consistently across different frames, even as the objects change size and position.  Importantly, the model correctly identifies when a person is occluded, represented by the absence of a bounding box in frames where the person or their hand is not visible.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2503.10781/x3.png", "caption": "Figure 2: A method for automatic annotation of spatio-temporally grounded captions. In the first stage (left), we apply a still-image grounded caption generation model on individual video frames producing temporally inconsistent outputs. In the second stage (middle), the captions from individual frames are aggregated into a single video-level caption describing the most salient actions/objects in the video. Third (right), individual frame-level phrases and bounding boxes are associated over time into a temporally consistent and dense labelling of object bounding boxes over the video.", "description": "This figure illustrates a three-stage automatic annotation method for generating spatio-temporally grounded video captions.  Stage 1 shows frame-wise grounded caption generation using a still-image model, resulting in temporally inconsistent outputs. Stage 2 aggregates these frame-level captions into a single, consistent video-level caption, focusing on the most important actions and objects.  Finally, Stage 3 associates frame-level phrases and bounding boxes across time to create temporally consistent, dense bounding box annotations for the entire video.", "section": "3. Large-scale generation of grounded captions"}, {"figure_path": "https://arxiv.org/html/2503.10781/x4.png", "caption": "Figure 3: An overview of our GROVE model for grounded video caption generation.\nDashed red rectangles outline the key technical contributions enabling grounded caption generation in video and include: (i) spatio-temporal adapters; (ii) the bounding box decoder and (iii) the temporal objectness head.", "description": "This figure illustrates the architecture of the GROVE model, which is designed for generating grounded video captions.  Key components highlighted are spatio-temporal adapters that efficiently model temporal information in video, a bounding box decoder for generating spatially accurate bounding boxes, and a temporal objectness head to handle occluded or temporarily absent objects. The model takes a video clip as input and outputs a natural language caption along with spatio-temporal bounding boxes that ground the objects mentioned in the caption. ", "section": "4. The GROVE Model"}, {"figure_path": "https://arxiv.org/html/2503.10781/x5.png", "caption": "Table 4: Results on the validation set of ActivityNet-Entities\u00a0[45]. Large-scale pretraining (PT+FT) results in an improvement over fine-tuning only (FT) for our model GROVE.", "description": "This table presents a comparison of the performance of the GROVE model on the ActivityNet-Entities dataset [45], a benchmark dataset for spatio-temporal grounding. The comparison is done between three training scenarios: only fine-tuning (FT), only pre-training (PT), and both pre-training and fine-tuning (PT+FT).  The results show that the model achieves improved performance (indicated by the metrics in the table, which are not included in this JSON since their detail is not provided in the question) when both pre-training and fine-tuning are employed, highlighting the benefit of large-scale pre-training using the automatically generated HowToGround1M dataset.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10781/x6.png", "caption": "Figure 4: Qualitative examples showing predictions of our GROVE model. Please note that GROVE is able to: (i) produce video-level natural language captions describing the main action in the video; (ii) ground multiple objects; and (iii) produce spatio-temporally consistent bounding box predictions.\nPlease note that the second row shows an example of model prediction that is partly incorrect as the blue box, while temporally consistent, does not depict a \u201cyarn\u201d.\nMore qualitative results including video results are in the supp. mat.", "description": "Figure 4 presents example outputs from the GROVE model, highlighting its ability to generate video-level captions that accurately describe the main action, ground multiple objects within the video by associating them with phrases in the caption, and maintain temporal consistency in the bounding box annotations across the video frames.  The model successfully localizes objects even when there are changes in viewpoint or scale.  However, the figure also points out a limitation where, in one instance, the temporal consistency of bounding boxes doesn't perfectly align with the semantic meaning of the caption.", "section": "Qualitative Examples"}, {"figure_path": "https://arxiv.org/html/2503.10781/x7.png", "caption": "Figure 5: Results after pre-training (PT) vs. after fine-tuning and pre-training (PT+FT) as a function of the pre-training dataset size. Results are reported on the iGround validation set.", "description": "This figure shows the performance of the GROVE model on the iGround validation set as a function of the size of the pre-training dataset.  Two scenarios are presented: one where the model is only pre-trained (PT) and another where it's both pre-trained and fine-tuned (PT+FT) on the iGround dataset. The metrics used are CIDEr, AP50, and recall, all commonly used in image captioning and object detection tasks. The graph visually demonstrates how increasing the size of the pre-training data improves the model's performance across all three metrics, highlighting the benefit of larger pre-training datasets for this task.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10781/x8.png", "caption": "Table 5: Ablation of spatio-temporal adapters (AD) and unfreezing the bounding box decoder and projection layers (unfreeze). We report results on the iGround validation set.", "description": "This ablation study investigates the impact of two key components of the GROVE model on the iGround validation set performance.  The first component evaluated is the spatio-temporal adapters (AD), which help the model efficiently process spatio-temporal video information. The second component is the bounding box decoder and its associated projection layers.  The table shows the model's performance (measured by METEOR, CIDEr, AP50, and Recall) under different configurations: with both AD and unfreezing enabled, with only one of them enabled, and with neither enabled.  This analysis reveals the contribution of each component to the overall accuracy and efficiency of the model.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10781/x9.png", "caption": "Table 6: Benefits of temporal objectness. AP50 (left) and recall (right) of our model for different temporal objectness thresholds. Results are reported on the iGround validation set.", "description": "This figure displays the impact of using a temporal objectness threshold on the performance of the GROVE model. The left panel shows the Average Precision at 50% Intersection over Union (AP50), while the right panel shows the recall, both evaluated on the iGround validation dataset.  Different lines represent different thresholds applied to the temporal objectness scores, demonstrating how the model's performance changes with varying thresholds for determining object presence/absence in a frame.", "section": "6. Experiments"}, {"figure_path": "https://arxiv.org/html/2503.10781/x10.png", "caption": "Table 7: Comparison of our automatic annotation approach vs. its alternatives: \u201cAlt. Stage 1\u201d is based on GIT\u00a0[37], Llama3\u00a0[8] and OWLv2\u00a0[23]. In \u201cAlt. Stage 2\u201d, we use the full caption instead of Subject-Verb-Object triplets. \u201cAlt. Stage 3\u201d is based on CoTracker3\u00a0[14] for tracking the objects of interest. Results are reported on the iGround validation set.", "description": "This table compares the performance of the proposed automatic annotation method with three alternative approaches. The alternatives modify different stages of the annotation pipeline.  \"Alt. Stage 1\" replaces the initial grounded caption generation stage with a combination of GIT [37] for generating captions, Llama3 [8] for extracting noun phrases and OWLv2 [23] for object localization. \"Alt. Stage 2\" uses full frame-level captions in the caption aggregation step, instead of using just subject-verb-object triplets. Finally, \"Alt. Stage 3\" substitutes the temporal bounding box association with CoTracker3 [14], a visual tracker.  The evaluation metric used is the iGround validation set.", "section": "6. Experiments"}]