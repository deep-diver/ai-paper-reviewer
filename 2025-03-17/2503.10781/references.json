{"references": [{"fullname_first_author": "Jean-Baptiste Alayrac", "paper_title": "Flamingo: a visual language model for few-shot learning", "publication_date": "2022-01-01", "reason": "This paper is important because it introduces the Flamingo visual language model, which is a foundational model for multimodal learning that enables few-shot learning capabilities, relevant to the broader context of grounded caption generation."}, {"fullname_first_author": "Antoine Miech", "paper_title": "HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips", "publication_date": "2019-01-01", "reason": "This is a core reference as the work heavily relies on the HowTo100M dataset and also creates the HowToGround1M dataset from the HowTo100M dataset."}, {"fullname_first_author": "Alec Radford", "paper_title": "Learning transferable visual models from natural language supervision", "publication_date": "2021-01-01", "reason": "This is a key reference for the CLIP model, which is used as a backbone for the GROVE model in generating and grounding video captions."}, {"fullname_first_author": "Hanoona Rasheed", "paper_title": "GLaMM: Pixel grounding large multimodal model", "publication_date": "2024-01-01", "reason": "This is one of the most important reference since it is a state-of-the-art grounding model and the proposed GROVE model in this paper is built on top of this GLaMM model."}, {"fullname_first_author": "Luowei Zhou", "paper_title": "Grounded video description", "publication_date": "2019-01-01", "reason": "This paper has also been referred to a few times, and is a related work for the video captioning and grounding tasks."}]}