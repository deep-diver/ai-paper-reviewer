[{"figure_path": "https://arxiv.org/html/2503.08111/extracted/6269952/fig/dataset.jpg", "caption": "Figure 1: \nExamples from the MaRI Gallery, showcasing (a) synthetic and (b) real-world datasets we constructed.\n(c) MaRI: A groundbreaking framework for accurately retrieving textures from images, bridging the gap between visual representations and material properties.", "description": "Figure 1 demonstrates the MaRI (Material Retrieval Integration) framework's capability by showcasing examples from its gallery.  Subfigure (a) displays synthetic datasets created by rendering various 3D objects with different materials under controlled lighting. Subfigure (b) shows real-world datasets, where images of materials were captured and processed. Subfigure (c) provides a visual overview of the MaRI framework.  It's described as a groundbreaking method that accurately retrieves materials from images by connecting visual representations (images) with actual material properties, effectively bridging a gap in traditional material retrieval methods.", "section": "Introduction"}, {"figure_path": "https://arxiv.org/html/2503.08111/extracted/6269952/fig/framework.jpg", "caption": "Figure 2: Overview of our dataset construction pipeline. (a) Synthetic materials are generated from 3D models obtained from Objaverse, combined with textures from AmbientCG, and rendered with HDR images. (b) Real-world materials are selected and segmented using Grounded-SAM and then transformed into material spheres via the ZeST method.", "description": "Figure 2 illustrates the process of creating the MaRI dataset, which consists of synthetic and real-world materials. (a) shows how synthetic materials are generated: 3D models from Objaverse are combined with textures from AmbientCG, and rendered using HDR images to create a variety of material samples under diverse lighting conditions.  (b) depicts the creation of the real-world dataset: real-world images of materials are selected, the material regions are segmented using Grounded-SAM, and the ZeST method transforms these segments into standardized material spheres. This two-pronged approach ensures a balanced and comprehensive dataset for training and evaluating the MaRI material retrieval framework.", "section": "3.2 Dataset"}, {"figure_path": "https://arxiv.org/html/2503.08111/extracted/6269952/fig/comparison.jpg", "caption": "Figure 3: The architecture of the MaRI framework for contrastive fine-tuning in material retrieval. MaRI uses DINOv2-based encoders for both image and material feature extraction, fine-tuning only the last Transformer block, while keeping the rest of the model frozen. During inference, cosine similarity between image and material embeddings is used to retrieve the most relevant materials from the library.", "description": "MaRI's architecture is a dual-encoder framework using two DINOv2-based encoders: one for image features and one for material features.  Only the final Transformer block of each encoder is fine-tuned during training, keeping the pre-trained weights frozen to maintain generalizability. The training utilizes a contrastive loss function to align image and material embeddings in a shared space. During inference, the cosine similarity between the image and material embeddings is calculated to retrieve the most relevant materials from a material library.", "section": "3. Methodology"}, {"figure_path": "https://arxiv.org/html/2503.08111/extracted/6269952/fig/unseen.jpg", "caption": "Figure 4: Qualitative comparison of material retrieval results using the Trained Materials dataset as the gallery.", "description": "This figure shows a qualitative comparison of material retrieval results using the 'Trained Materials' dataset as the gallery.  It presents a real-world image query alongside the top-1 and top-5 retrieval results from different methods: DINOv2, Make-it-Real, MaPa, and MaRI. The goal is to visually demonstrate the relative performance of each method in accurately retrieving materials that match the visual characteristics of the query image. This is done for two example queries, \"Marble\" and \"Bark\", allowing for a direct comparison across the methods, highlighting MaRI's superior performance in accurately retrieving visually similar materials.", "section": "4. Experiments"}]