{"references": [{"fullname_first_author": "Jiajie Zhang", "paper_title": "LongCite: Enabling LLMs to Generate Fine-Grained Citations in Long-Context QA", "publication_date": "2024-09-02", "reason": "This paper introduces the LongBench-Cite benchmark and several baseline methods for citation generation, which the current work directly builds upon and improves."}, {"fullname_first_author": "Benjamin Cohen-Wang", "paper_title": "ContextCite: Attributing Model Generation to Context", "publication_date": "2024-12-01", "reason": "This paper introduces the ContextCite method for contributing context attribution, which inspired the current work's self-supervised reward approach."}, {"fullname_first_author": "Yu Meng", "paper_title": "SimPO: Simple Preference Optimization with a Reference-free Reward", "publication_date": "2024-12-01", "reason": "This paper proposes the SimPO algorithm for preference optimization, which is used in the current work to further improve citation quality."}, {"fullname_first_author": "Abhimanyu Dubey", "paper_title": "The LLaMA 3 Herd of Models", "publication_date": "2024-07-21", "reason": "This paper introduces the LLaMA 3 language model family, which is used as the base model for the current work's experiments."}, {"fullname_first_author": "Long Ouyang", "paper_title": "Training language models to follow instructions with human feedback", "publication_date": "2022-03-02", "reason": "This paper is a foundational work in aligning language models using human feedback, providing context for the current work's focus on self-supervised alignment."}]}