[{"figure_path": "https://arxiv.org/html/2502.14856/x1.png", "caption": "Figure 1: Comparison of the drafting and verification times of EAGLE-2 implemented by three frameworks (Huggingface, SGLang, and our optimized implementation) for two vocabulary sizes: 32k (Llama-2-7B) and 128k (Llama-3-8B).", "description": "This figure compares the time spent on drafting and verification stages of the EAGLE-2 speculative sampling algorithm across three different implementations (Huggingface, SGLang, and an optimized implementation developed by the authors).  The comparison is shown for two different vocabulary sizes: 32k (representing the Llama-2-7B model) and 128k (representing the Llama-3-8B model). The figure visually illustrates how the drafting and verification times change depending on the implementation and the vocabulary size of the language model. This helps to understand the computational bottlenecks and the effectiveness of the authors' optimized implementation.", "section": "1 Introduction"}, {"figure_path": "https://arxiv.org/html/2502.14856/x2.png", "caption": "Figure 2: Token frequency distribution, statistically analyzed using the tokenizer of Llama-3-8B on a subset of 1B tokens randomly sampled from the SlimPajama-627B dataset\u00a0Soboleva et\u00a0al. (2023). As shown in the figure, 75% of the vocabulary tokens account for less than 5% of all token occurrences in the dataset, presenting a \u201cLong Tail\u201d effect.", "description": "This figure shows the distribution of token frequencies in the Llama-3-8B vocabulary.  The data was obtained by analyzing one billion tokens randomly selected from the SlimPajama-627B dataset. The distribution exhibits a long-tail effect, meaning that a small percentage of tokens (25%) account for most (95%) of the token occurrences in the dataset, while the vast majority of tokens (75%) are very rarely used. This uneven distribution highlights the sparsity in the vocabulary.", "section": "3 Methodology"}, {"figure_path": "https://arxiv.org/html/2502.14856/x3.png", "caption": "Figure 3: (Left) The drafting process of EAGLE-2 when promptP=\ud835\udc43absent~{}P=italic_P =\u201cIt\u201d, beam w\u2062i\u2062d\u2062t\u2062h=2\ud835\udc64\ud835\udc56\ud835\udc51\ud835\udc61\u210e2width=2italic_w italic_i italic_d italic_t italic_h = 2 and search d\u2062e\u2062p\u2062t\u2062h=3\ud835\udc51\ud835\udc52\ud835\udc5d\ud835\udc61\u210e3depth=3italic_d italic_e italic_p italic_t italic_h = 3. It picks out the top K=8\ud835\udc3e8K=8italic_K = 8 probability tokens (purple) as the draft tree. (Right) The drafting process of FR-Spec, where the LM Head is cropped during the drafting process while the beam search procedure remains the same.", "description": "Figure 3 illustrates the drafting processes of both EAGLE-2 and FR-Spec.  The left panel shows EAGLE-2's drafting process with a prompt of \"It\", a beam width of 2, and a search depth of 3.  The model generates a draft tree by selecting the top 8 most probable tokens (shown in purple).  The right panel displays FR-Spec's modification to this process. FR-Spec optimizes the drafting process by removing the LM Head (Language Model Head), thereby reducing computational cost while maintaining the beam search methodology unchanged from EAGLE-2. The verification process remains identical between both methods.", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2502.14856/x4.png", "caption": "Figure 4: The illustration of the verification process for EAGLE-2 and FR-Spec, given the draft in Figure\u00a03. FR-Spec\u00a0solely modifies the drafting process while the verification process remains consistent with EAGLE-2.", "description": "This figure illustrates the verification process used in both EAGLE-2 and FR-Spec.  It shows how the target LLM (the full model) verifies the candidate token sequences generated by the draft model (a smaller, faster model) during speculative sampling. The key difference is that FR-Spec uses a reduced vocabulary in its draft model, impacting the drafting process's speed and efficiency without changing the verification process. The figure visually demonstrates how an attention mask is used to selectively attend to the valid draft tokens. This mask helps direct the LLM's attention, ensuring the mathematical equivalence of the output distribution to the original method and enabling the verification process to be consistent across methods. ", "section": "Methodology"}, {"figure_path": "https://arxiv.org/html/2502.14856/x5.png", "caption": "Figure 5: Comparison of Python-based implementation and C-based implementation. X, Y, and Z represent three different short-duration computational tasks.", "description": "This figure compares the performance of Python-based and C-based implementations for three short-duration computational tasks (labeled X, Y, and Z) within the speculative sampling framework.  It highlights the performance overhead introduced by Python's interpreted nature, showcasing the significant speed improvements achieved through native C and CUDA implementations. This is done to isolate the core algorithmic performance from the implementation-related overhead in order to get accurate performance analysis of speculative sampling.", "section": "3.1 Identifying Key Bottlenecks for Speculative Sampling"}, {"figure_path": "https://arxiv.org/html/2502.14856/x6.png", "caption": "Figure 6: Time breakdown of the drafting process of EAGLE-2. We profile the EAGLE-2 trained on Llama-2-7B (32k vocabulary) and the EAGLE-2 trained on Llama-3-8B (128k vocabulary).", "description": "This figure shows a breakdown of the time spent during the drafting process in the EAGLE-2 speculative sampling method.  It compares the time spent on different components of the model for two different LLMs: Llama-2-7B (with a 32k vocabulary) and Llama-3-8B (with a 128k vocabulary). The breakdown shows the proportion of time spent on embedding, the transformer layers, and the LM head (including softmax).  It highlights how the computational bottleneck shifts from the transformer layers in Llama-2-7B to the LM Head in Llama-3-8B as the vocabulary size increases, emphasizing the impact of vocabulary size on the drafting process efficiency.", "section": "3.1 Identifying Key Bottlenecks for Speculative Sampling"}, {"figure_path": "https://arxiv.org/html/2502.14856/x7.png", "caption": "Figure 7: Decoding speed (token/s) of FR-Spec\u00a0and EAGLE-2 for Llama-3-8B under different frameworks.", "description": "This figure compares the decoding speed, measured in tokens per second, of the FR-Spec method and the EAGLE-2 baseline when used with Llama-3-8B, a large language model. The comparison is done across different implementation frameworks: Hugging Face, SGLang, and a custom-optimized implementation.  The results show the significant speed improvements achieved by FR-Spec in all three frameworks, demonstrating the effectiveness of the proposed method.", "section": "4 Experiments"}, {"figure_path": "https://arxiv.org/html/2502.14856/x8.png", "caption": "Figure 8: Decoding speed (token/s) of FR-Spec\u00a0and EAGLE-2 for Llama-3.2-1B under different implementation framework.", "description": "This figure compares the decoding speed, measured in tokens per second, of the FR-Spec model and the baseline EAGLE-2 model for the Llama-3.2-1B language model.  The comparison is made across three different implementation frameworks: Huggingface, SGLang, and a custom optimized implementation developed by the authors. The chart visually represents the performance gains achieved by FR-Spec over EAGLE-2 within each framework, illustrating the impact of different implementation choices on speed improvements.  It showcases FR-Spec's significant speedup, especially when compared to the Huggingface and SGLang implementations of EAGLE-2.", "section": "4.3 Decoding Speed"}]