[{"figure_path": "https://arxiv.org/html/2503.02878/x1.png", "caption": "Figure 1: Self-taught lookahead self-improves the value model by learning from state-transition dynamics. During the data generation phase (top left), tree search is used to discover diverse states. For every observed state s\ud835\udc60sitalic_s encountered during the search, successor states are expanded using base policy \u03c0\u03b8subscript\ud835\udf0b\ud835\udf03\\pi_{\\theta}italic_\u03c0 start_POSTSUBSCRIPT italic_\u03b8 end_POSTSUBSCRIPT and the current value model V\u03d5ksubscript\ud835\udc49subscriptitalic-\u03d5\ud835\udc58V_{\\phi_{k}}italic_V start_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT, and a formatted textual training example is formed using verbal representations of the next best action and successor state, as well as V\u03d5ksubscript\ud835\udc49subscriptitalic-\u03d5\ud835\udc58V_{\\phi_{k}}italic_V start_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT\u2019s outputted value reasoning (r\ud835\udc5fritalic_r) and numerical value (v\ud835\udc63vitalic_v) (top middle). These examples are used to fine-tune V\u03d5k+1subscript\ud835\udc49subscriptitalic-\u03d5\ud835\udc581V_{\\phi_{k+1}}italic_V start_POSTSUBSCRIPT italic_\u03d5 start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT, which will be used in the next iteration of the algorithm (top right). Value models learned during self-taught lookahead can be used to evaluate unseen states encountered during search on unseen tasks by simulating a step of lookahead including the next best action and the best successor state s~\u2032superscript~\ud835\udc60\u2032\\tilde{s}^{\\prime}over~ start_ARG italic_s end_ARG start_POSTSUPERSCRIPT \u2032 end_POSTSUPERSCRIPT (bottom).", "description": "Figure 1 illustrates the self-taught lookahead (STL) process.  The top-left panel shows the data generation phase, where a tree search explores various states.  For each state encountered, possible successor states are generated using a base policy and the current value model.  The top-middle panel shows how verbal representations of the best action and successor state, along with the value model's reasoning and numerical value, are used to create formatted training examples.  These examples are used to fine-tune an improved value model (top-right panel), which is then used in the next iteration of the algorithm. Finally, the bottom panel demonstrates how the learned value model can evaluate unseen states during search by simulating a single lookahead step, considering the best next action and resulting state.", "section": "3. Better State-Value Estimation with Self-Taught Lookahead"}, {"figure_path": "https://arxiv.org/html/2503.02878/x2.png", "caption": "Figure 2: Breadth-first search performance on Game-of-24 task on sets of tasks both seen and unseen during the self-improvement.", "description": "Figure 2 presents the results of applying self-taught lookahead to the Game-of-24 math reasoning task.  The figure displays the performance of breadth-first search using different value models. One uses a GPT-40 LLM directly as a value model (baseline), the other employs a llama-3.1-8b-instruct model fine-tuned using the self-taught lookahead method. The performance is assessed on two sets of tasks: one set consists of tasks encountered during the self-improvement process, and the other comprises unseen tasks. The x-axis indicates the number of iterations of self-taught lookahead, and the y-axis represents the accuracy achieved by the search algorithm.", "section": "4.2. Math Reasoning"}, {"figure_path": "https://arxiv.org/html/2503.02878/x3.png", "caption": "Figure 3: Compute and environmental efficiency during evaluation on WebShop with a gpt-3.5-turbo policy. Compute efficiency is measured in total (prompt and completion) tokens broken down by model type (closed and open source). Environmental efficiency is measured by the number of states expanded (webpages visited).", "description": "Figure 3 compares the computational and environmental efficiency of different methods for the WebShop task, using a gpt-3.5-turbo policy.  Computational efficiency is presented as the total number of tokens used (prompt and completion tokens combined), categorized by whether the model was open or closed source.  This highlights the cost difference between using open-source LLMs and large, closed-source models. Environmental efficiency is depicted as the number of states or 'webpages visited' during the search process.  This illustrates the impact of different search methods on resource consumption and the number of interactions required with the website.", "section": "5. Efficiency Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02878/x4.png", "caption": "Figure 4: Tradeoff between performance and efficiency on WebShop with a gpt-3.5-turbo policy. Pareto frontiers of existing methods and baselines are shown, illustrating the optimality of STL when considering the tradeoff between cost and average reward (top), environmental usage and average reward (middle), and cost and success rate (bottom).", "description": "Figure 4 illustrates the tradeoffs between performance and efficiency for different search methods on the WebShop task, using a gpt-3.5-turbo policy. The Pareto frontiers in the three subplots show the optimal balance between cost and average reward (top), environmental impact (measured by expanded states) and average reward (middle), and cost and success rate (bottom). Self-taught lookahead (STL) consistently demonstrates Pareto optimality, outperforming other methods across all three tradeoff scenarios.", "section": "5. Efficiency Analysis"}, {"figure_path": "https://arxiv.org/html/2503.02878/x5.png", "caption": "Figure 5: STL scaling trends on WebShop for llama-3 and qwen-2.5 model families when using a gpt-3.5-turbo policy with performance measured by average reward (top) and success rate (bottom).", "description": "This figure displays the scaling trends of Self-Taught Lookahead (STL) on the WebShop dataset using various llama-3 and qwen-2.5 model sizes.  The experiments were conducted with a gpt-3.5-turbo policy model. The top panel shows the average reward achieved by each model size, while the bottom panel shows the success rate.  The results illustrate how STL performs with different-sized models, indicating the potential for using smaller, more computationally efficient LLMs while maintaining reasonable performance.", "section": "5. Efficiency Analysis"}]