[{"figure_path": "https://arxiv.org/html/2501.09775/extracted/6135800/Figure/Accuracy_entireMMLU.png", "caption": "Figure 1: Accuracy Comparison Across Models on MMLU Categories with Direct and CoT Prompts", "description": "This figure presents a comparison of the accuracy achieved by different large language models (LLMs) when answering questions from the Massive Multitask Language Understanding (MMLU) benchmark.  Two prompting methods are compared: a 'direct' prompt, where the model is asked to answer directly, and a 'Chain of Thought' (CoT) prompt, where the model is first asked to provide reasoning before selecting an answer. The chart displays the accuracy rates for each LLM under both prompting methods, allowing for a direct visual comparison of the effect of CoT prompting on model performance across different LLMs.", "section": "Evaluation results"}, {"figure_path": "https://arxiv.org/html/2501.09775/extracted/6135800/Figure/Average_Prob_All.png", "caption": "Figure 2: Average Probabilities of Selected Option Across Models on MMLU with Direct and CoT Prompts", "description": "This figure displays the average probability assigned by seven different Large Language Models (LLMs) to their selected answer option across the Massive Multitask Language Understanding (MMLU) benchmark.  It compares the confidence levels when the models answer directly versus when they utilize chain-of-thought (CoT) prompting, which involves generating reasoning before selecting an answer.  The graph visually represents the average confidence across all questions within the MMLU dataset for each model, providing a comparison of model certainty with and without the explicit reasoning step.", "section": "3 Evaluation results"}, {"figure_path": "https://arxiv.org/html/2501.09775/extracted/6135800/Figure/Average_Prob_Correct.png", "caption": "Figure 3: Average Probabilities of Correctly Selected Option Across Models on MMLU with Direct and CoT Prompts", "description": "This figure displays a bar chart comparing the average probabilities of correctly selected options across seven different Large Language Models (LLMs) when using two distinct prompting methods: 'Direct' and 'Chain of Thought' (CoT).  The x-axis represents the different LLMs tested, and the y-axis shows the average probability of the model selecting the correct answer for the MMLU (Massive Multitask Language Understanding) benchmark.  Each LLM has two bars, one for the Direct prompt, where the model answers directly, and one for the CoT prompt, where the model first provides reasoning before selecting an answer.  The chart visually represents the difference in confidence levels (reflected in the probabilities) between these two prompting techniques when the models give correct answers.  This allows for comparison of how the confidence of each model is affected by reasoning before answering.", "section": "Evaluation results"}, {"figure_path": "https://arxiv.org/html/2501.09775/extracted/6135800/Figure/Average_Prob_Wrong.png", "caption": "Figure 4: Average Probabilities of Incorrectly Selected Option Across Models on MMLU with Direct and CoT Prompts", "description": "This figure displays a comparison of the average probabilities assigned to incorrectly chosen options across multiple Large Language Models (LLMs).  The comparison is made between two prompting methods: 'Direct,' where the model directly selects an answer, and 'CoT' (Chain of Thought), where the model provides reasoning before selecting an answer.  The x-axis represents the different LLMs tested, and the y-axis represents the average probability of selecting an incorrect option.  The figure visually demonstrates whether the LLMs exhibit higher confidence (higher probability) in their incorrect answers when using the CoT prompting method compared to the Direct method.", "section": "Evaluation results"}, {"figure_path": "https://arxiv.org/html/2501.09775/extracted/6135800/Figure/All_CorrectAnswer.png", "caption": "Figure 5: Probability Distribution of Correctly Selected Option Across Models in MMLU", "description": "This figure presents a detailed comparison of the probability distributions for correctly selected options across different Large Language Models (LLMs) evaluated on the Massive Multitask Language Understanding (MMLU) benchmark.  Each LLM's distribution is shown separately for both 'direct' and 'chain-of-thought' (CoT) prompting methods.  The x-axis represents the probability range (0 to 1), and the y-axis represents the frequency or density of answers falling within each probability bin. This allows for a visual analysis of the confidence levels exhibited by each model under different prompting conditions.  The goal is to show how the distribution of probabilities changes with CoT prompting, indicating higher confidence (probabilities closer to 1) in the correctly selected answers.", "section": "Evaluation results"}, {"figure_path": "https://arxiv.org/html/2501.09775/extracted/6135800/Figure/All_WrongAnswer.png", "caption": "Figure 6: Probability Distribution of Incorrectly Selected Option Across Models in MMLU", "description": "This figure presents the distribution of probabilities assigned to incorrectly selected options across various Large Language Models (LLMs) evaluated on the Massive Multitask Language Understanding (MMLU) benchmark.  Each bar represents an LLM, and different colors represent the probability range of the incorrectly chosen answer. The distribution of probabilities demonstrates that, for incorrectly answered questions, many LLMs exhibit high confidence scores in their wrong answers, particularly when chain of thought (CoT) prompting was utilized. This visualization helps to illustrate the relationship between the confidence estimates of the LLMs and the accuracy of their responses, particularly highlighting the cases where models are both incorrect and highly confident.", "section": "Evaluation results"}, {"figure_path": "https://arxiv.org/html/2501.09775/extracted/6135800/Figure/BothWrong_vs_Wrong2Correct.png", "caption": "Figure 7: Increments in accuracy, in the probability of the selected option, in the probability of the selected option for correct answers and in the probability of the selected option for incorrect answers for the different subjects in MMLU across models.", "description": "Figure 7 is a heatmap visualizing the changes in model confidence and accuracy across different subjects within the Massive Multitask Language Understanding (MMLU) benchmark.  It compares the performance of seven different large language models (LLMs) using two prompting techniques: direct answering and chain-of-thought (CoT) prompting.  The heatmap shows the increase (or decrease) in accuracy and the change in the probability assigned to the selected option (representing confidence).  This breakdown is presented separately for correctly and incorrectly answered questions, providing a nuanced view of how reasoning affects both accuracy and confidence levels across various tasks within the MMLU.", "section": "Evaluation results"}]