[{"figure_path": "https://arxiv.org/html/2501.10020/x1.png", "caption": "Figure 1: Examples of animatable 2D cartoon characters generated by Textoon.", "description": "This figure displays a variety of 2D cartoon characters created using the Textoon system.  The characters showcase the diverse styles and animation capabilities achievable with the model, demonstrating variations in hair, clothing, expressions and overall appearance.  Each character is an example of the output generated by the Textoon method, which is capable of producing a wide range of customizable 2D cartoon characters from text descriptions.", "section": "Abstract"}, {"figure_path": "https://arxiv.org/html/2501.10020/x2.png", "caption": "Figure 2: Pipeline of the Textoon. The framework includes text parsing, controllable appearance generation, re-editing, and component completion and repair modules.", "description": "This figure illustrates the Textoon framework's pipeline, which consists of four key modules: text parsing, controllable appearance generation, re-editing, and component completion and repair.  The text parsing module processes user text descriptions to extract relevant features. The controllable appearance generation module utilizes these features to generate the initial character appearance. The re-editing module allows users to make adjustments to the generated character. Finally, the component completion and repair module ensures that all components of the character are properly generated and connected.", "section": "3. Live2D Generation"}, {"figure_path": "https://arxiv.org/html/2501.10020/x3.png", "caption": "Figure 3: Meshes of different layers.", "description": "This figure shows the layered structure of a Live2D model, specifically illustrating the control meshes for the back hair and the upper garment layer.  Each layer in Live2D characters is segmented into a polygon mesh; manipulating the mesh points allows for smooth deformations and animation.  This layered structure facilitates smooth deformations by manipulating the positions of the mesh points. The image helps to explain the concept of component splitting within the Live2D generation process, where smaller layers are merged to simplify generation while potentially slightly impacting the expressiveness of detailed movements.", "section": "3. Live2D Generation"}, {"figure_path": "https://arxiv.org/html/2501.10020/x4.png", "caption": "Figure 4: Splitting model components, larger elements can be utilized to create short variations.", "description": "This figure illustrates the concept of component splitting in the Live2D model generation process.  By combining larger, simpler components, the system can efficiently create variations in hair length or other details.  The image shows how breaking down complex elements into fewer, larger ones can simplify the creation of diverse character designs while maintaining a reasonable level of detail. This method streamlines the generation process and improves efficiency.", "section": "3. Live2D Generation"}, {"figure_path": "https://arxiv.org/html/2501.10020/x5.png", "caption": "Figure 5: Using the fine-tuned LLM to parse component categories from complex input text.", "description": "This figure demonstrates the effectiveness of a fine-tuned large language model (LLM) in parsing complex text descriptions to extract relevant component categories for generating 2D cartoon characters.  The input is a detailed text description of a character, and the output shows how the LLM successfully identifies and extracts key features such as hair style, eye color, top type, sleeve type, skirt type, and boot type. This accurate parsing is crucial for the subsequent steps of controllable appearance generation in the Textoon framework.", "section": "3.3 Text Parsing"}, {"figure_path": "https://arxiv.org/html/2501.10020/x6.png", "caption": "Figure 6: The divisions of each component within our template model.", "description": "This figure shows a breakdown of the component parts used in the Textoon model for generating Live2D characters.  It visually depicts how the model separates a character's visual elements into distinct layers or components. These include the back hair, mid hair, front hair, top, sleeves, pants/skirt, and shoes.  This modular design is crucial for the system's ability to generate diverse and controllable character appearances. Each component is shown separately as a distinct part, highlighting how the model treats each element independently before assembling them into the final character image.", "section": "3. Live2D Generation"}, {"figure_path": "https://arxiv.org/html/2501.10020/x7.png", "caption": "Figure 7: Restoring the back hair: First, extract the pixels (b) from the generated image using the model pattern (a). Then, fill the area occluded by the head with pixels from the region connected to the front hair (c). Finally, perform image-to-image generation (d).", "description": "This figure illustrates the process of restoring the back hair in a generated Live2D character image.  The process involves three steps. First, pixels corresponding to the back hair are extracted from the generated image using a pre-defined model pattern (a). Second, the area of the back hair occluded by the head is filled using pixels from the adjacent front hair region (c). Finally, an image-to-image generation technique is employed to refine the back hair area (d), ensuring color consistency with the front hair and a natural appearance.", "section": "3.6. Component Completion"}, {"figure_path": "https://arxiv.org/html/2501.10020/x8.png", "caption": "Figure 8: Live2D model supporting ARKit lip-sync driving.", "description": "This figure demonstrates the integration of ARKit's facial tracking capabilities into a Live2D model for enhanced lip-sync animation.  It shows several examples of mouth movements generated using the combined system, highlighting the increased realism and precision achieved by incorporating the more detailed facial expression data from ARKit's 52 parameters, compared to the usual two parameters in most Live2D models.  The improved accuracy in syncing mouth movements to speech results in more natural and expressive animations.", "section": "3.7. Animation"}, {"figure_path": "https://arxiv.org/html/2501.10020/x9.png", "caption": "Figure 9: The overall animation effects of the generated Live2D model.", "description": "This figure showcases the animation capabilities of a Live2D model generated using the Textoon framework.  It demonstrates the smooth and realistic movement achieved through the integration of ARKit's face blendshapes, which significantly enhances the liveliness and expressiveness of the model's animations, specifically focusing on lip-sync accuracy and facial expressions.", "section": "3.7. Animation"}, {"figure_path": "https://arxiv.org/html/2501.10020/x10.png", "caption": "Figure 10: Examples of Live2D cartoon characters created along with their corresponding text prompts.", "description": "Figure 10 showcases several examples of Live2D cartoon characters automatically generated by the Textoon system.  Each character is accompanied by the text prompt used to create it, demonstrating the system's ability to translate textual descriptions into diverse and visually appealing character designs. The figure highlights the variety of hairstyles, clothing styles, facial features, and overall aesthetics achievable using the model.", "section": "Results"}]