[{"figure_path": "https://arxiv.org/html/2501.17433/extracted/6163167/pic/virus.png", "caption": "Figure 1: A three stage pipeline for harmful fine-tuning attack under guardrail moderation. i) At the first stage, the model is safety aligned with alignment data. ii) At the second stage, the service provider applies guardrail moderation to filter out the harmful samples over the uploaded fine-tuning data. iii) At the third stage, the filtered data is used for fine-tuning the aligned LLM. Our attack Virus is concerning how to construct the user dataset that can bypass the guardrail and break the victim LLM\u2019s safety alignment.", "description": "The figure illustrates a three-stage pipeline for a harmful fine-tuning attack against a large language model (LLM).  Stage 1 shows the initial safety alignment of the LLM using alignment data. Stage 2 depicts the guardrail moderation process, where the service provider filters out harmful samples from the uploaded fine-tuning data.  Stage 3 shows the fine-tuning of the aligned LLM with the filtered data. The 'Virus' attack focuses on creating a user dataset that bypasses the guardrail and compromises the LLM's safety alignment.", "section": "1. Introduction"}, {"figure_path": "https://arxiv.org/html/2501.17433/extracted/6163167/pic/HS_FS_hr.png", "caption": "Figure 2: Harmful score and Fine-tune accuracy under different harmful ratio. HFA refers to harmful fine-tuning attack with a harmful ratio of harmful data. BFA refers to benign fine-tuning attack with pure GSM8K data. BF is a special case when harmful ratio=0 for HF. The average leakage ratio (ratio of leak-through harmful data) of HF w/ moderation is 0.348. All the data in BFA an leak through the moderation.", "description": "This figure shows the impact of different harmful data ratios on the harmful score and fine-tuning accuracy of a language model.  A harmful ratio of 0% (BFA, Benign Fine-tuning Attack) represents a baseline where only benign data is used for fine-tuning.  Increasing the harmful ratio (HFA, Harmful Fine-tuning Attack) introduces more harmful data, leading to a higher harmful score (indicating the model's tendency to produce harmful outputs) and potentially lower fine-tuning accuracy (although in this instance, accuracy remains largely unaffected). The figure also demonstrates the effectiveness of the guardrail moderation system by showing that a significant portion of harmful data is filtered out (average leakage ratio of 0.348), mitigating the negative impact on the model's safety alignment. However, the key takeaway is that even with the guardrail,  a non-negligible fraction of harmful data still leaks through and compromises the model's safety.", "section": "3.2 Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.17433/extracted/6163167/pic/example_figure.png", "caption": "Figure 3: Example illustration of different fine-tuning attack techniques. a) For benign fine-tuning attack, benign QA pair is uploaded for fine-tuning. b) For harmful fine-tuning attack, only harmful samples are uploaded. c) For Mixing attack, a benign QA is concatenated with a harmful QA in order to circumvent guardrail, which unfortunately does not succeed. d) For Virus, the benign QA is concated with a harmful QA and the harmful QA is optimized with the dual goals: i) To bypass moderation. ii) To guarantee attack performance.", "description": "Figure 3 illustrates four different fine-tuning attack methods. (a) shows a benign fine-tuning attack where only benign question-answer pairs are used. This serves as a baseline for comparison. (b) shows a harmful fine-tuning attack where only harmful question-answer pairs are used, demonstrating the vulnerability of LLMs to such attacks.  (c) demonstrates the \"Mixing Attack\", which attempts to bypass guardrail moderation by concatenating benign and harmful question-answer pairs.  However, this method proves ineffective. (d) Finally, the figure illustrates the \"Virus\" attack, which successfully bypasses the guardrail. This is achieved by concatenating a benign and a harmful question-answer pair, but this time, the harmful part is optimized using a novel method that considers two objectives: 1) bypassing the guardrail's moderation and 2) maintaining effective attack performance. This dual optimization ensures that the malicious data evades detection while retaining its capability to degrade the safety alignment of the victim LLM.  The figure visually represents the differences in data construction and moderation outcomes for each of the four approaches.", "section": "4. Methodology"}, {"figure_path": "https://arxiv.org/html/2501.17433/extracted/6163167/pic/statistic.png", "caption": "Figure 4: Stepping over the data optimized by Virus with different \u03bb\ud835\udf06\\lambdaitalic_\u03bb, harmful loss and gradient similarity across fine-tuning rounds are displayed. When \u03bb=1\ud835\udf061\\lambda=1italic_\u03bb = 1, the method reduces to one of our failure attempt named guardrail jailbreak.", "description": "This figure visualizes the impact of the hyperparameter \u03bb (lambda) in the Virus algorithm on two key metrics: harmful loss and gradient similarity.  The x-axis represents the number of fine-tuning steps, while the y-axis shows the harmful loss and gradient similarity. Multiple lines are plotted, each corresponding to a different value of \u03bb.  The lines demonstrate how the harmful loss and the gradient similarity change as the optimization process proceeds with different values of \u03bb. When \u03bb is 0, Virus prioritizes gradient similarity, resulting in a decrease in harmful loss. As \u03bb increases, the focus shifts toward guardrail jailbreak, leading to higher harmful loss.  The case where \u03bb = 1 represents a failed attempt (guardrail jailbreak) where only the jailbreak goal was pursued.", "section": "5.3 Statistical Evaluation"}, {"figure_path": "https://arxiv.org/html/2501.17433/extracted/6163167/pic/onehot.png", "caption": "Figure 5: Illustration of flattened one-hot vector.", "description": "The figure illustrates how a sequence of tokens is represented as a flattened one-hot vector.  The vocabulary size is 6, meaning there are 6 unique tokens.  The number of optimizable tokens (n) is 3. Each token's position in the sequence is represented by a segment of the flattened vector.  Each segment is a one-hot encoding where only one bit is set to 1 (representing the selected token) and the rest are 0s. This representation is used because it allows easy manipulation and optimization of the tokens during the data optimization process of the proposed attack.", "section": "A.1 Formal Formulation"}]